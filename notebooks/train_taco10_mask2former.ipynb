{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jHOC2HC0sCNE"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUmK767kWQlq",
        "outputId": "3c8123f0-97c5-4a3b-eecf-ae12a88f1d54"
      },
      "outputs": [],
      "source": [
        "# clone and install Mask2Former\n",
        "%git clone https://github.com/facebookresearch/Mask2Former.git\n",
        "%cd Mask2Former\n",
        "%pip install -U opencv-python\n",
        "%pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "%pip install -r requirements.txt\n",
        "%cd mask2former/modeling/pixel_decoder/ops\n",
        "%python setup.py build install\n",
        "%cd ../../../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGxctvB_Pdkn",
        "outputId": "d5902280-aa5a-4479-9d48-d7f56b08da63"
      },
      "outputs": [],
      "source": [
        "%python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccGvME_lQkbN",
        "outputId": "6e924c7a-824e-4d4d-b9c1-bca31a1e151d"
      },
      "outputs": [],
      "source": [
        "%cd mask2former/modeling/pixel_decoder/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP44DZ_zQooA",
        "outputId": "8572d2f9-5040-4d8b-f286-45ac6827af18"
      },
      "outputs": [],
      "source": [
        "!sh make.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy0WqfqBQsqm",
        "outputId": "f96502dc-dddd-46d9-9d49-368fbc51767d"
      },
      "outputs": [],
      "source": [
        "%cd ../../../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Re-run the code cells from start when the following cell crashes!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# import Mask2Former project\n",
        "from mask2former import add_maskformer2_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJwaL06RNCY",
        "outputId": "3cbf4af7-a446-44b4-930c-5cf172550b9e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aQ-zjQCtRU1F"
      },
      "outputs": [],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "setup_logger(name=\"mask2former\")\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.projects.deeplab import add_deeplab_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wg5JCtyWRZOo"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7EO4nMI9Rb5P"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco10_train\", {}, data_dir_path + \"mapped_annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_val\", {}, data_dir_path + \"mapped_annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_test\", {}, data_dir_path + \"mapped_annotations_0_test.json\", data_dir_path + \"images/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y44xCY4Gu_ou"
      },
      "source": [
        "# Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IZL4O9aw3XXp"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.structures import PolygonMasks\n",
        "import copy\n",
        "import torch  # Import torch to convert images to tensors\n",
        "from argparse import ArgumentParser\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from mask2former import (\n",
        "    MaskFormerInstanceDatasetMapper,\n",
        "    InstanceSegEvaluator,\n",
        "    add_maskformer2_config,\n",
        ")\n",
        "from detectron2.evaluation import COCOEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6kaK-F3shWqR"
      },
      "outputs": [],
      "source": [
        "class Trainer(DefaultTrainer):\n",
        "    \"\"\"\n",
        "    Extension of the Trainer class adapted to MaskFormer.\n",
        "    \"\"\"\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
        "        return build_detection_train_loader(cfg, mapper=mapper)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "      if output_folder is None:\n",
        "              output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "      print(dataset_name)\n",
        "      return COCOEvaluator(dataset_name, output_dir=output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6FlsDz1hq_54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "add_deeplab_config(cfg)\n",
        "add_maskformer2_config(cfg)\n",
        "cfg.merge_from_file(\"configs/coco/instance-segmentation/maskformer2_R50_bs16_50ep.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4FdGn2TvPAn",
        "outputId": "e82fce7a-2c34-4680-f6e1-03c3e168c98d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[09/08 15:14:00 d2.engine.defaults]: Model:\n",
            "MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "[09/08 15:14:00 mask2former.data.dataset_mappers.mask_former_instance_dataset_mapper]: [MaskFormerInstanceDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/08 15:14:02 d2.data.datasets.coco]: Loading /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json takes 1.95 seconds.\n",
            "[09/08 15:14:02 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json\n",
            "[09/08 15:14:02 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[09/08 15:14:02 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 194          |   Other    | 1397         |   Bottle   | 344          |\n",
            "|  Bottle cap   | 226          |    Cup     | 150          |    Lid     | 63           |\n",
            "| Plastic bag.. | 697          |  Pop tab   | 75           |   Straw    | 108          |\n",
            "|   Cigarette   | 457          |            |              |            |              |\n",
            "|     total     | 3711         |            |              |            |              |\n",
            "[09/08 15:14:02 d2.data.build]: Using training sampler TrainingSampler\n",
            "[09/08 15:14:02 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/08 15:14:02 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[09/08 15:14:02 d2.data.common]: Serialized dataset takes 1.77 MiB\n",
            "[09/08 15:14:02 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[09/08 15:14:02 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model_final_3c8ec9.pkl: 176MB [00:09, 19.6MB/s]                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING [09/08 15:14:11 mask2former.modeling.transformer_decoder.mask2former_transformer_decoder]: Weight format of MultiScaleMaskedTransformerDecoder have changed! Please upgrade your models. Applying automatic conversion now ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (81, 256) in the checkpoint but (11, 256) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "criterion.empty_weight\n",
            "sem_seg_head.predictor.class_embed.{bias, weight}\n"
          ]
        }
      ],
      "source": [
        "cfg.DATASETS.TRAIN = (\"taco10_train\",)\n",
        "cfg.DATASETS.TEST = (\"taco10_val\",)\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "cfg.INPUT.DATASET_MAPPER_NAME = \"mask_former_instance\"\n",
        "\n",
        "cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 10\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"value\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0001\n",
        "cfg.SOLVER.MAX_ITER = 9600\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "cfg.TEST.EVAL_PERIOD = 2400\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuliSkJQer3",
        "outputId": "268d5276-d826-4155-b0a6-422244a096cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:30:06 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0021 s/iter. Inference: 2.3645 s/iter. Eval: 4.2799 s/iter. Total: 6.6473 s/iter. ETA=0:13:17\n",
            "[09/08 16:30:13 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0021 s/iter. Inference: 2.2912 s/iter. Eval: 4.3530 s/iter. Total: 6.6470 s/iter. ETA=0:13:10\n",
            "[09/08 16:30:20 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0021 s/iter. Inference: 2.2315 s/iter. Eval: 4.4244 s/iter. Total: 6.6588 s/iter. ETA=0:13:05\n",
            "[09/08 16:30:26 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0021 s/iter. Inference: 2.1563 s/iter. Eval: 4.4881 s/iter. Total: 6.6473 s/iter. ETA=0:12:57\n",
            "[09/08 16:30:32 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0021 s/iter. Inference: 2.0863 s/iter. Eval: 4.5467 s/iter. Total: 6.6360 s/iter. ETA=0:12:49\n",
            "[09/08 16:30:40 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0021 s/iter. Inference: 2.0346 s/iter. Eval: 4.6181 s/iter. Total: 6.6557 s/iter. ETA=0:12:45\n",
            "[09/08 16:30:48 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0021 s/iter. Inference: 1.9142 s/iter. Eval: 4.5803 s/iter. Total: 6.4975 s/iter. ETA=0:12:14\n",
            "[09/08 16:30:49 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:31:03 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0021 s/iter. Inference: 2.2027 s/iter. Eval: 4.5446 s/iter. Total: 6.7503 s/iter. ETA=0:12:36\n",
            "[09/08 16:31:12 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0022 s/iter. Inference: 2.1578 s/iter. Eval: 4.6532 s/iter. Total: 6.8139 s/iter. ETA=0:12:36\n",
            "[09/08 16:31:17 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0022 s/iter. Inference: 2.1157 s/iter. Eval: 4.6601 s/iter. Total: 6.7788 s/iter. ETA=0:12:25\n",
            "[09/08 16:31:18 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:31:31 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0022 s/iter. Inference: 2.3605 s/iter. Eval: 4.6254 s/iter. Total: 6.9890 s/iter. ETA=0:12:41\n",
            "[09/08 16:31:37 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0022 s/iter. Inference: 2.3096 s/iter. Eval: 4.6362 s/iter. Total: 6.9489 s/iter. ETA=0:12:30\n",
            "[09/08 16:31:45 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0022 s/iter. Inference: 2.2721 s/iter. Eval: 4.6903 s/iter. Total: 6.9655 s/iter. ETA=0:12:25\n",
            "[09/08 16:31:51 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0022 s/iter. Inference: 2.2173 s/iter. Eval: 4.7386 s/iter. Total: 6.9590 s/iter. ETA=0:12:17\n",
            "[09/08 16:31:58 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0022 s/iter. Inference: 2.1652 s/iter. Eval: 4.7861 s/iter. Total: 6.9544 s/iter. ETA=0:12:10\n",
            "[09/08 16:32:05 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0022 s/iter. Inference: 2.1155 s/iter. Eval: 4.8307 s/iter. Total: 6.9494 s/iter. ETA=0:12:02\n",
            "[09/08 16:32:12 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0022 s/iter. Inference: 2.0683 s/iter. Eval: 4.8724 s/iter. Total: 6.9439 s/iter. ETA=0:11:55\n",
            "[09/08 16:32:18 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0022 s/iter. Inference: 2.0232 s/iter. Eval: 4.9121 s/iter. Total: 6.9384 s/iter. ETA=0:11:47\n",
            "[09/08 16:32:25 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0022 s/iter. Inference: 1.9802 s/iter. Eval: 4.9512 s/iter. Total: 6.9346 s/iter. ETA=0:11:40\n",
            "[09/08 16:32:32 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0022 s/iter. Inference: 1.9391 s/iter. Eval: 4.9868 s/iter. Total: 6.9292 s/iter. ETA=0:11:32\n",
            "[09/08 16:32:38 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0022 s/iter. Inference: 1.8998 s/iter. Eval: 5.0216 s/iter. Total: 6.9247 s/iter. ETA=0:11:25\n",
            "[09/08 16:32:45 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0022 s/iter. Inference: 1.8622 s/iter. Eval: 5.0568 s/iter. Total: 6.9222 s/iter. ETA=0:11:18\n",
            "[09/08 16:32:52 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0023 s/iter. Inference: 1.8262 s/iter. Eval: 5.0882 s/iter. Total: 6.9176 s/iter. ETA=0:11:11\n",
            "[09/08 16:32:58 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0022 s/iter. Inference: 1.7915 s/iter. Eval: 5.1109 s/iter. Total: 6.9056 s/iter. ETA=0:11:02\n",
            "[09/08 16:33:05 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0023 s/iter. Inference: 1.7584 s/iter. Eval: 5.1333 s/iter. Total: 6.8949 s/iter. ETA=0:10:55\n",
            "[09/08 16:33:11 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0023 s/iter. Inference: 1.7264 s/iter. Eval: 5.1539 s/iter. Total: 6.8835 s/iter. ETA=0:10:47\n",
            "[09/08 16:33:17 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0023 s/iter. Inference: 1.6956 s/iter. Eval: 5.1738 s/iter. Total: 6.8727 s/iter. ETA=0:10:39\n",
            "[09/08 16:33:24 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0023 s/iter. Inference: 1.6660 s/iter. Eval: 5.1942 s/iter. Total: 6.8635 s/iter. ETA=0:10:31\n",
            "[09/08 16:33:30 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0023 s/iter. Inference: 1.6375 s/iter. Eval: 5.2134 s/iter. Total: 6.8541 s/iter. ETA=0:10:23\n",
            "[09/08 16:33:36 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0023 s/iter. Inference: 1.6100 s/iter. Eval: 5.2315 s/iter. Total: 6.8448 s/iter. ETA=0:10:16\n",
            "[09/08 16:33:43 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0023 s/iter. Inference: 1.5836 s/iter. Eval: 5.2494 s/iter. Total: 6.8363 s/iter. ETA=0:10:08\n",
            "[09/08 16:33:49 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0023 s/iter. Inference: 1.5580 s/iter. Eval: 5.2675 s/iter. Total: 6.8288 s/iter. ETA=0:10:00\n",
            "[09/08 16:33:55 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0023 s/iter. Inference: 1.5334 s/iter. Eval: 5.2842 s/iter. Total: 6.8209 s/iter. ETA=0:09:53\n",
            "[09/08 16:34:02 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0023 s/iter. Inference: 1.5096 s/iter. Eval: 5.2999 s/iter. Total: 6.8128 s/iter. ETA=0:09:45\n",
            "[09/08 16:34:12 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0022 s/iter. Inference: 1.4414 s/iter. Eval: 5.2097 s/iter. Total: 6.6544 s/iter. ETA=0:09:12\n",
            "[09/08 16:34:19 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0022 s/iter. Inference: 1.4205 s/iter. Eval: 5.2260 s/iter. Total: 6.6498 s/iter. ETA=0:09:05\n",
            "[09/08 16:34:25 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0022 s/iter. Inference: 1.3798 s/iter. Eval: 5.1533 s/iter. Total: 6.5364 s/iter. ETA=0:08:42\n",
            "[09/08 16:34:33 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0022 s/iter. Inference: 1.3419 s/iter. Eval: 5.1199 s/iter. Total: 6.4650 s/iter. ETA=0:08:24\n",
            "[09/08 16:34:41 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0022 s/iter. Inference: 1.3061 s/iter. Eval: 5.0883 s/iter. Total: 6.3976 s/iter. ETA=0:08:06\n",
            "[09/08 16:34:50 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0022 s/iter. Inference: 1.2724 s/iter. Eval: 5.0595 s/iter. Total: 6.3351 s/iter. ETA=0:07:48\n",
            "[09/08 16:34:58 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0022 s/iter. Inference: 1.2405 s/iter. Eval: 5.0316 s/iter. Total: 6.2754 s/iter. ETA=0:07:31\n",
            "[09/08 16:35:06 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0022 s/iter. Inference: 1.2103 s/iter. Eval: 5.0049 s/iter. Total: 6.2184 s/iter. ETA=0:07:15\n",
            "[09/08 16:35:15 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0022 s/iter. Inference: 1.1818 s/iter. Eval: 4.9798 s/iter. Total: 6.1648 s/iter. ETA=0:06:59\n",
            "[09/08 16:35:23 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0022 s/iter. Inference: 1.1546 s/iter. Eval: 4.9563 s/iter. Total: 6.1141 s/iter. ETA=0:06:43\n",
            "[09/08 16:35:31 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0022 s/iter. Inference: 1.1288 s/iter. Eval: 4.9335 s/iter. Total: 6.0654 s/iter. ETA=0:06:28\n",
            "[09/08 16:35:39 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0022 s/iter. Inference: 1.1042 s/iter. Eval: 4.9117 s/iter. Total: 6.0191 s/iter. ETA=0:06:13\n",
            "[09/08 16:35:48 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0022 s/iter. Inference: 1.0808 s/iter. Eval: 4.8917 s/iter. Total: 5.9756 s/iter. ETA=0:05:58\n",
            "[09/08 16:35:56 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0022 s/iter. Inference: 1.0584 s/iter. Eval: 4.8719 s/iter. Total: 5.9335 s/iter. ETA=0:05:44\n",
            "[09/08 16:36:04 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0022 s/iter. Inference: 1.0370 s/iter. Eval: 4.8530 s/iter. Total: 5.8932 s/iter. ETA=0:05:30\n",
            "[09/08 16:36:13 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0022 s/iter. Inference: 1.0166 s/iter. Eval: 4.8350 s/iter. Total: 5.8547 s/iter. ETA=0:05:16\n",
            "[09/08 16:36:21 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0022 s/iter. Inference: 0.9971 s/iter. Eval: 4.8175 s/iter. Total: 5.8177 s/iter. ETA=0:05:02\n",
            "[09/08 16:36:29 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 0.9783 s/iter. Eval: 4.8009 s/iter. Total: 5.7824 s/iter. ETA=0:04:49\n",
            "[09/08 16:36:37 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0022 s/iter. Inference: 0.9604 s/iter. Eval: 4.7849 s/iter. Total: 5.7485 s/iter. ETA=0:04:35\n",
            "[09/08 16:36:46 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0022 s/iter. Inference: 0.9432 s/iter. Eval: 4.7697 s/iter. Total: 5.7160 s/iter. ETA=0:04:22\n",
            "[09/08 16:36:54 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0022 s/iter. Inference: 0.9268 s/iter. Eval: 4.7556 s/iter. Total: 5.6854 s/iter. ETA=0:04:10\n",
            "[09/08 16:37:02 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0022 s/iter. Inference: 0.9109 s/iter. Eval: 4.7416 s/iter. Total: 5.6555 s/iter. ETA=0:03:57\n",
            "[09/08 16:37:11 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0022 s/iter. Inference: 0.8956 s/iter. Eval: 4.7283 s/iter. Total: 5.6269 s/iter. ETA=0:03:45\n",
            "[09/08 16:37:19 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0022 s/iter. Inference: 0.8808 s/iter. Eval: 4.7151 s/iter. Total: 5.5990 s/iter. ETA=0:03:32\n",
            "[09/08 16:37:26 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0022 s/iter. Inference: 0.8665 s/iter. Eval: 4.6897 s/iter. Total: 5.5593 s/iter. ETA=0:03:20\n",
            "[09/08 16:37:31 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:37:52 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 1.0028 s/iter. Eval: 4.6869 s/iter. Total: 5.6928 s/iter. ETA=0:03:13\n",
            "[09/08 16:37:53 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:38:13 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 1.1407 s/iter. Eval: 4.6898 s/iter. Total: 5.8336 s/iter. ETA=0:03:12\n",
            "[09/08 16:38:22 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 1.1274 s/iter. Eval: 4.6785 s/iter. Total: 5.8089 s/iter. ETA=0:03:00\n",
            "[09/08 16:38:30 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.1098 s/iter. Eval: 4.6674 s/iter. Total: 5.7803 s/iter. ETA=0:02:47\n",
            "[09/08 16:38:39 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.0928 s/iter. Eval: 4.6567 s/iter. Total: 5.7525 s/iter. ETA=0:02:35\n",
            "[09/08 16:38:47 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.0764 s/iter. Eval: 4.6461 s/iter. Total: 5.7255 s/iter. ETA=0:02:23\n",
            "[09/08 16:38:55 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.0606 s/iter. Eval: 4.6364 s/iter. Total: 5.7000 s/iter. ETA=0:02:11\n",
            "[09/08 16:39:01 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.0451 s/iter. Eval: 4.6070 s/iter. Total: 5.6551 s/iter. ETA=0:01:58\n",
            "[09/08 16:39:11 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.0304 s/iter. Eval: 4.6074 s/iter. Total: 5.6409 s/iter. ETA=0:01:47\n",
            "[09/08 16:39:20 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 1.0161 s/iter. Eval: 4.6032 s/iter. Total: 5.6224 s/iter. ETA=0:01:35\n",
            "[09/08 16:39:28 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 1.0021 s/iter. Eval: 4.5946 s/iter. Total: 5.5997 s/iter. ETA=0:01:23\n",
            "[09/08 16:39:36 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 0.9885 s/iter. Eval: 4.5861 s/iter. Total: 5.5777 s/iter. ETA=0:01:12\n",
            "[09/08 16:39:44 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 0.9755 s/iter. Eval: 4.5783 s/iter. Total: 5.5568 s/iter. ETA=0:01:01\n",
            "[09/08 16:39:53 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 0.9627 s/iter. Eval: 4.5709 s/iter. Total: 5.5366 s/iter. ETA=0:00:49\n",
            "[09/08 16:40:01 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 0.9504 s/iter. Eval: 4.5630 s/iter. Total: 5.5164 s/iter. ETA=0:00:38\n",
            "[09/08 16:40:09 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0022 s/iter. Inference: 0.9382 s/iter. Eval: 4.5503 s/iter. Total: 5.4916 s/iter. ETA=0:00:27\n",
            "[09/08 16:40:16 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0022 s/iter. Inference: 0.9265 s/iter. Eval: 4.5378 s/iter. Total: 5.4673 s/iter. ETA=0:00:16\n",
            "[09/08 16:40:24 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0022 s/iter. Inference: 0.9150 s/iter. Eval: 4.5261 s/iter. Total: 5.4441 s/iter. ETA=0:00:05\n",
            "[09/08 16:40:28 d2.evaluation.evaluator]: Total inference time: 0:13:07.769632 (5.432894 s / iter per device, on 1 devices)\n",
            "[09/08 16:40:28 d2.evaluation.evaluator]: Total inference pure compute time: 0:02:11 (0.909384 s / iter per device, on 1 devices)\n",
            "[09/08 16:40:28 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/08 16:40:28 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[09/08 16:40:28 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/08 16:40:28 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/08 16:40:28 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/08 16:40:28 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/08 16:40:28 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/08 16:40:28 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/08 16:40:28 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.24s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/08 16:40:29 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/08 16:40:29 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.26 seconds.\n",
            "[09/08 16:40:29 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/08 16:40:29 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.177\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.261\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.368\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.296\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.473\n",
            "[09/08 16:40:29 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.686 | 17.705 | 13.337 | 0.114 | 5.827 | 17.072 |\n",
            "[09/08 16:40:29 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 19.032 | Other      | 12.232 | Bottle     | 22.398 |\n",
            "| Bottle cap            | 13.081 | Cup        | 16.101 | Lid        | 11.376 |\n",
            "| Plastic bag + wrapper | 17.888 | Pop tab    | 1.329  | Straw      | 11.947 |\n",
            "| Cigarette             | 1.473  |            |        |            |        |\n",
            "[09/08 16:40:29 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[09/08 16:40:29 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/08 16:40:29 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/08 16:40:29 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/08 16:40:29 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/08 16:40:29 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/08 16:40:29 d2.evaluation.testing]: copypaste: 12.6857,17.7050,13.3369,0.1137,5.8266,17.0720\n",
            "[09/08 16:40:29 d2.utils.events]:  eta: 0:12:17  iter: 7199  total_loss: 25.44  loss_ce: 0.8967  loss_mask: 0.07143  loss_dice: 1.328  loss_ce_0: 1.782  loss_mask_0: 0.07533  loss_dice_0: 1.383  loss_ce_1: 1.292  loss_mask_1: 0.09269  loss_dice_1: 1.382  loss_ce_2: 1.102  loss_mask_2: 0.07675  loss_dice_2: 1.194  loss_ce_3: 0.9308  loss_mask_3: 0.06044  loss_dice_3: 1.153  loss_ce_4: 0.9602  loss_mask_4: 0.05872  loss_dice_4: 1.332  loss_ce_5: 0.9424  loss_mask_5: 0.06448  loss_dice_5: 1.203  loss_ce_6: 0.8763  loss_mask_6: 0.06518  loss_dice_6: 1.237  loss_ce_7: 0.9056  loss_mask_7: 0.06653  loss_dice_7: 1.214  loss_ce_8: 0.892  loss_mask_8: 0.07147  loss_dice_8: 1.078    time: 0.3812  last_time: 0.2984  data_time: 0.0133  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:40:35 d2.utils.events]:  eta: 0:12:11  iter: 7219  total_loss: 23.07  loss_ce: 0.6733  loss_mask: 0.07146  loss_dice: 1.212  loss_ce_0: 1.274  loss_mask_0: 0.1153  loss_dice_0: 1.241  loss_ce_1: 0.7596  loss_mask_1: 0.07071  loss_dice_1: 1.13  loss_ce_2: 0.8311  loss_mask_2: 0.06802  loss_dice_2: 1.137  loss_ce_3: 0.6772  loss_mask_3: 0.07499  loss_dice_3: 1.165  loss_ce_4: 0.6758  loss_mask_4: 0.06967  loss_dice_4: 1.091  loss_ce_5: 0.7031  loss_mask_5: 0.07158  loss_dice_5: 0.9968  loss_ce_6: 0.6751  loss_mask_6: 0.06542  loss_dice_6: 1.139  loss_ce_7: 0.6054  loss_mask_7: 0.0742  loss_dice_7: 1.186  loss_ce_8: 0.6582  loss_mask_8: 0.06584  loss_dice_8: 1.113    time: 0.3811  last_time: 0.3015  data_time: 0.0067  last_data_time: 0.0056   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:40:42 d2.utils.events]:  eta: 0:12:05  iter: 7239  total_loss: 11.95  loss_ce: 0.4717  loss_mask: 0.07937  loss_dice: 0.4037  loss_ce_0: 1.192  loss_mask_0: 0.08267  loss_dice_0: 0.6737  loss_ce_1: 0.7811  loss_mask_1: 0.1019  loss_dice_1: 0.5433  loss_ce_2: 0.6513  loss_mask_2: 0.09093  loss_dice_2: 0.4131  loss_ce_3: 0.5565  loss_mask_3: 0.08499  loss_dice_3: 0.4032  loss_ce_4: 0.5148  loss_mask_4: 0.08579  loss_dice_4: 0.5117  loss_ce_5: 0.4872  loss_mask_5: 0.09318  loss_dice_5: 0.5763  loss_ce_6: 0.5054  loss_mask_6: 0.0767  loss_dice_6: 0.3868  loss_ce_7: 0.5421  loss_mask_7: 0.08027  loss_dice_7: 0.3869  loss_ce_8: 0.5478  loss_mask_8: 0.08252  loss_dice_8: 0.3844    time: 0.3809  last_time: 0.3093  data_time: 0.0194  last_data_time: 0.0098   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:40:48 d2.utils.events]:  eta: 0:11:59  iter: 7259  total_loss: 14.44  loss_ce: 0.5939  loss_mask: 0.0401  loss_dice: 0.3602  loss_ce_0: 1.479  loss_mask_0: 0.04497  loss_dice_0: 0.6598  loss_ce_1: 0.933  loss_mask_1: 0.04096  loss_dice_1: 0.8005  loss_ce_2: 0.7158  loss_mask_2: 0.04464  loss_dice_2: 0.6735  loss_ce_3: 0.6705  loss_mask_3: 0.04273  loss_dice_3: 0.4327  loss_ce_4: 0.5666  loss_mask_4: 0.04213  loss_dice_4: 0.5077  loss_ce_5: 0.62  loss_mask_5: 0.03848  loss_dice_5: 0.4616  loss_ce_6: 0.6423  loss_mask_6: 0.04104  loss_dice_6: 0.4473  loss_ce_7: 0.5809  loss_mask_7: 0.03426  loss_dice_7: 0.41  loss_ce_8: 0.5524  loss_mask_8: 0.04081  loss_dice_8: 0.5266    time: 0.3807  last_time: 0.3028  data_time: 0.0079  last_data_time: 0.0079   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:40:54 d2.utils.events]:  eta: 0:11:52  iter: 7279  total_loss: 17.21  loss_ce: 0.642  loss_mask: 0.09302  loss_dice: 0.6932  loss_ce_0: 1.299  loss_mask_0: 0.07737  loss_dice_0: 1.29  loss_ce_1: 0.9329  loss_mask_1: 0.1089  loss_dice_1: 0.9083  loss_ce_2: 0.798  loss_mask_2: 0.1303  loss_dice_2: 0.723  loss_ce_3: 0.7135  loss_mask_3: 0.09892  loss_dice_3: 0.7501  loss_ce_4: 0.725  loss_mask_4: 0.1141  loss_dice_4: 0.6567  loss_ce_5: 0.7125  loss_mask_5: 0.1043  loss_dice_5: 0.7708  loss_ce_6: 0.681  loss_mask_6: 0.09384  loss_dice_6: 0.6284  loss_ce_7: 0.6897  loss_mask_7: 0.09667  loss_dice_7: 0.6081  loss_ce_8: 0.6202  loss_mask_8: 0.09095  loss_dice_8: 0.5989    time: 0.3805  last_time: 0.3242  data_time: 0.0067  last_data_time: 0.0070   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:01 d2.utils.events]:  eta: 0:11:46  iter: 7299  total_loss: 19.44  loss_ce: 0.5251  loss_mask: 0.1089  loss_dice: 0.8217  loss_ce_0: 1.44  loss_mask_0: 0.1126  loss_dice_0: 0.7143  loss_ce_1: 1.018  loss_mask_1: 0.08922  loss_dice_1: 1.089  loss_ce_2: 0.9025  loss_mask_2: 0.09467  loss_dice_2: 0.9214  loss_ce_3: 0.7488  loss_mask_3: 0.0971  loss_dice_3: 0.8918  loss_ce_4: 0.5848  loss_mask_4: 0.0888  loss_dice_4: 0.8523  loss_ce_5: 0.6539  loss_mask_5: 0.09993  loss_dice_5: 0.6668  loss_ce_6: 0.6517  loss_mask_6: 0.1062  loss_dice_6: 0.7979  loss_ce_7: 0.5686  loss_mask_7: 0.1157  loss_dice_7: 0.9285  loss_ce_8: 0.5353  loss_mask_8: 0.1006  loss_dice_8: 0.7785    time: 0.3803  last_time: 0.3324  data_time: 0.0069  last_data_time: 0.0117   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:07 d2.utils.events]:  eta: 0:11:40  iter: 7319  total_loss: 15.93  loss_ce: 0.6174  loss_mask: 0.05308  loss_dice: 0.6159  loss_ce_0: 1.517  loss_mask_0: 0.08272  loss_dice_0: 0.8737  loss_ce_1: 0.9639  loss_mask_1: 0.05599  loss_dice_1: 0.7422  loss_ce_2: 0.8606  loss_mask_2: 0.06783  loss_dice_2: 0.7057  loss_ce_3: 0.7593  loss_mask_3: 0.05899  loss_dice_3: 0.7221  loss_ce_4: 0.6953  loss_mask_4: 0.05349  loss_dice_4: 0.6245  loss_ce_5: 0.7007  loss_mask_5: 0.05707  loss_dice_5: 0.7937  loss_ce_6: 0.6374  loss_mask_6: 0.05246  loss_dice_6: 0.6446  loss_ce_7: 0.6453  loss_mask_7: 0.05165  loss_dice_7: 0.6506  loss_ce_8: 0.6896  loss_mask_8: 0.05061  loss_dice_8: 0.6171    time: 0.3802  last_time: 0.3060  data_time: 0.0113  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:13 d2.utils.events]:  eta: 0:11:34  iter: 7339  total_loss: 13.65  loss_ce: 0.5353  loss_mask: 0.03988  loss_dice: 0.6954  loss_ce_0: 1.35  loss_mask_0: 0.04411  loss_dice_0: 0.8558  loss_ce_1: 0.9696  loss_mask_1: 0.04282  loss_dice_1: 0.8515  loss_ce_2: 0.7125  loss_mask_2: 0.0403  loss_dice_2: 0.7304  loss_ce_3: 0.601  loss_mask_3: 0.03962  loss_dice_3: 0.7004  loss_ce_4: 0.5486  loss_mask_4: 0.03764  loss_dice_4: 0.5175  loss_ce_5: 0.4942  loss_mask_5: 0.03922  loss_dice_5: 0.6253  loss_ce_6: 0.4904  loss_mask_6: 0.03966  loss_dice_6: 0.6173  loss_ce_7: 0.5633  loss_mask_7: 0.03804  loss_dice_7: 0.6591  loss_ce_8: 0.519  loss_mask_8: 0.03451  loss_dice_8: 0.6348    time: 0.3800  last_time: 0.3038  data_time: 0.0071  last_data_time: 0.0112   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:19 d2.utils.events]:  eta: 0:11:28  iter: 7359  total_loss: 13.58  loss_ce: 0.7479  loss_mask: 0.05596  loss_dice: 0.5104  loss_ce_0: 1.046  loss_mask_0: 0.07715  loss_dice_0: 0.7099  loss_ce_1: 0.9401  loss_mask_1: 0.05095  loss_dice_1: 0.6041  loss_ce_2: 0.7907  loss_mask_2: 0.0622  loss_dice_2: 0.5234  loss_ce_3: 0.6635  loss_mask_3: 0.06421  loss_dice_3: 0.6304  loss_ce_4: 0.7052  loss_mask_4: 0.05465  loss_dice_4: 0.5674  loss_ce_5: 0.6896  loss_mask_5: 0.05218  loss_dice_5: 0.658  loss_ce_6: 0.7167  loss_mask_6: 0.05248  loss_dice_6: 0.6302  loss_ce_7: 0.6762  loss_mask_7: 0.05669  loss_dice_7: 0.5148  loss_ce_8: 0.6864  loss_mask_8: 0.05321  loss_dice_8: 0.6285    time: 0.3798  last_time: 0.3043  data_time: 0.0076  last_data_time: 0.0111   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:26 d2.utils.events]:  eta: 0:11:22  iter: 7379  total_loss: 16.7  loss_ce: 0.8744  loss_mask: 0.05511  loss_dice: 0.6102  loss_ce_0: 1.524  loss_mask_0: 0.05545  loss_dice_0: 0.5921  loss_ce_1: 0.926  loss_mask_1: 0.06914  loss_dice_1: 0.7871  loss_ce_2: 0.8166  loss_mask_2: 0.0677  loss_dice_2: 0.5879  loss_ce_3: 0.8305  loss_mask_3: 0.06544  loss_dice_3: 0.6419  loss_ce_4: 0.9273  loss_mask_4: 0.06367  loss_dice_4: 0.6192  loss_ce_5: 0.8144  loss_mask_5: 0.05909  loss_dice_5: 0.6434  loss_ce_6: 0.643  loss_mask_6: 0.06298  loss_dice_6: 0.7557  loss_ce_7: 0.7363  loss_mask_7: 0.06528  loss_dice_7: 0.8508  loss_ce_8: 0.8359  loss_mask_8: 0.05379  loss_dice_8: 0.7289    time: 0.3796  last_time: 0.3474  data_time: 0.0062  last_data_time: 0.0069   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:32 d2.utils.events]:  eta: 0:11:15  iter: 7399  total_loss: 20.88  loss_ce: 0.6272  loss_mask: 0.05321  loss_dice: 0.9427  loss_ce_0: 1.348  loss_mask_0: 0.04572  loss_dice_0: 1.09  loss_ce_1: 0.961  loss_mask_1: 0.03732  loss_dice_1: 0.9258  loss_ce_2: 0.7691  loss_mask_2: 0.04746  loss_dice_2: 1.029  loss_ce_3: 0.7509  loss_mask_3: 0.04196  loss_dice_3: 0.8344  loss_ce_4: 0.5861  loss_mask_4: 0.05319  loss_dice_4: 0.9736  loss_ce_5: 0.6488  loss_mask_5: 0.04756  loss_dice_5: 1.106  loss_ce_6: 0.7042  loss_mask_6: 0.04912  loss_dice_6: 0.8329  loss_ce_7: 0.6693  loss_mask_7: 0.03624  loss_dice_7: 0.9353  loss_ce_8: 0.7938  loss_mask_8: 0.03428  loss_dice_8: 0.9859    time: 0.3794  last_time: 0.3034  data_time: 0.0124  last_data_time: 0.0053   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:38 d2.utils.events]:  eta: 0:11:09  iter: 7419  total_loss: 13.61  loss_ce: 0.6065  loss_mask: 0.09674  loss_dice: 0.4997  loss_ce_0: 1.034  loss_mask_0: 0.1077  loss_dice_0: 0.6892  loss_ce_1: 0.6714  loss_mask_1: 0.09462  loss_dice_1: 0.3924  loss_ce_2: 0.602  loss_mask_2: 0.09905  loss_dice_2: 0.4263  loss_ce_3: 0.6632  loss_mask_3: 0.09219  loss_dice_3: 0.4426  loss_ce_4: 0.674  loss_mask_4: 0.09271  loss_dice_4: 0.4784  loss_ce_5: 0.6341  loss_mask_5: 0.09759  loss_dice_5: 0.4557  loss_ce_6: 0.62  loss_mask_6: 0.09654  loss_dice_6: 0.3783  loss_ce_7: 0.6378  loss_mask_7: 0.1083  loss_dice_7: 0.3453  loss_ce_8: 0.6342  loss_mask_8: 0.09831  loss_dice_8: 0.3578    time: 0.3792  last_time: 0.3292  data_time: 0.0070  last_data_time: 0.0077   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:45 d2.utils.events]:  eta: 0:11:03  iter: 7439  total_loss: 15.58  loss_ce: 0.6038  loss_mask: 0.08036  loss_dice: 0.9904  loss_ce_0: 1.267  loss_mask_0: 0.08307  loss_dice_0: 1.247  loss_ce_1: 0.9979  loss_mask_1: 0.06337  loss_dice_1: 0.8046  loss_ce_2: 0.6655  loss_mask_2: 0.07061  loss_dice_2: 0.6097  loss_ce_3: 0.6534  loss_mask_3: 0.06477  loss_dice_3: 0.8828  loss_ce_4: 0.5661  loss_mask_4: 0.07961  loss_dice_4: 0.7145  loss_ce_5: 0.5841  loss_mask_5: 0.07574  loss_dice_5: 0.8888  loss_ce_6: 0.6005  loss_mask_6: 0.0781  loss_dice_6: 0.869  loss_ce_7: 0.563  loss_mask_7: 0.07755  loss_dice_7: 0.905  loss_ce_8: 0.5666  loss_mask_8: 0.07776  loss_dice_8: 0.8171    time: 0.3791  last_time: 0.3244  data_time: 0.0161  last_data_time: 0.0044   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:51 d2.utils.events]:  eta: 0:10:56  iter: 7459  total_loss: 12.58  loss_ce: 0.5482  loss_mask: 0.1045  loss_dice: 0.3877  loss_ce_0: 0.8231  loss_mask_0: 0.1213  loss_dice_0: 0.7616  loss_ce_1: 0.6108  loss_mask_1: 0.09178  loss_dice_1: 0.3873  loss_ce_2: 0.6046  loss_mask_2: 0.07948  loss_dice_2: 0.4066  loss_ce_3: 0.5556  loss_mask_3: 0.07921  loss_dice_3: 0.6483  loss_ce_4: 0.5177  loss_mask_4: 0.1072  loss_dice_4: 0.4772  loss_ce_5: 0.4768  loss_mask_5: 0.0921  loss_dice_5: 0.5296  loss_ce_6: 0.4962  loss_mask_6: 0.09757  loss_dice_6: 0.4173  loss_ce_7: 0.4877  loss_mask_7: 0.1038  loss_dice_7: 0.387  loss_ce_8: 0.4976  loss_mask_8: 0.101  loss_dice_8: 0.4004    time: 0.3789  last_time: 0.3143  data_time: 0.0069  last_data_time: 0.0066   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:41:57 d2.utils.events]:  eta: 0:10:50  iter: 7479  total_loss: 17.4  loss_ce: 0.8152  loss_mask: 0.07295  loss_dice: 0.6551  loss_ce_0: 1.331  loss_mask_0: 0.08486  loss_dice_0: 0.9979  loss_ce_1: 1.109  loss_mask_1: 0.07809  loss_dice_1: 0.6239  loss_ce_2: 0.8905  loss_mask_2: 0.06947  loss_dice_2: 0.7422  loss_ce_3: 0.8815  loss_mask_3: 0.06458  loss_dice_3: 0.7405  loss_ce_4: 0.8957  loss_mask_4: 0.07602  loss_dice_4: 0.7736  loss_ce_5: 0.8815  loss_mask_5: 0.06847  loss_dice_5: 0.6582  loss_ce_6: 0.8055  loss_mask_6: 0.06738  loss_dice_6: 0.7939  loss_ce_7: 0.7777  loss_mask_7: 0.06704  loss_dice_7: 0.851  loss_ce_8: 0.8064  loss_mask_8: 0.06926  loss_dice_8: 0.7181    time: 0.3787  last_time: 0.4231  data_time: 0.0122  last_data_time: 0.1249   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:03 d2.utils.events]:  eta: 0:10:43  iter: 7499  total_loss: 21.67  loss_ce: 0.5846  loss_mask: 0.05955  loss_dice: 0.9899  loss_ce_0: 1.35  loss_mask_0: 0.05339  loss_dice_0: 1.074  loss_ce_1: 1.028  loss_mask_1: 0.07446  loss_dice_1: 1.185  loss_ce_2: 0.888  loss_mask_2: 0.05436  loss_dice_2: 1.201  loss_ce_3: 0.6711  loss_mask_3: 0.06924  loss_dice_3: 1.089  loss_ce_4: 0.6911  loss_mask_4: 0.07261  loss_dice_4: 1.143  loss_ce_5: 0.8199  loss_mask_5: 0.06785  loss_dice_5: 0.9199  loss_ce_6: 0.7571  loss_mask_6: 0.06192  loss_dice_6: 0.9929  loss_ce_7: 0.6328  loss_mask_7: 0.07171  loss_dice_7: 0.848  loss_ce_8: 0.6711  loss_mask_8: 0.05436  loss_dice_8: 0.9918    time: 0.3785  last_time: 0.3289  data_time: 0.0064  last_data_time: 0.0061   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:10 d2.utils.events]:  eta: 0:10:37  iter: 7519  total_loss: 15.56  loss_ce: 0.712  loss_mask: 0.04059  loss_dice: 0.4501  loss_ce_0: 1.23  loss_mask_0: 0.07147  loss_dice_0: 0.7786  loss_ce_1: 0.8503  loss_mask_1: 0.0583  loss_dice_1: 0.6506  loss_ce_2: 0.7456  loss_mask_2: 0.05166  loss_dice_2: 0.5655  loss_ce_3: 0.7505  loss_mask_3: 0.03757  loss_dice_3: 0.5139  loss_ce_4: 0.7394  loss_mask_4: 0.04256  loss_dice_4: 0.5442  loss_ce_5: 0.7578  loss_mask_5: 0.0413  loss_dice_5: 0.4912  loss_ce_6: 0.7106  loss_mask_6: 0.04079  loss_dice_6: 0.5721  loss_ce_7: 0.6305  loss_mask_7: 0.03868  loss_dice_7: 0.4832  loss_ce_8: 0.7013  loss_mask_8: 0.03424  loss_dice_8: 0.5337    time: 0.3783  last_time: 0.3039  data_time: 0.0070  last_data_time: 0.0067   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:16 d2.utils.events]:  eta: 0:10:31  iter: 7539  total_loss: 13.9  loss_ce: 0.6394  loss_mask: 0.04533  loss_dice: 0.5244  loss_ce_0: 1.13  loss_mask_0: 0.06006  loss_dice_0: 0.943  loss_ce_1: 0.9964  loss_mask_1: 0.04165  loss_dice_1: 0.9744  loss_ce_2: 0.8122  loss_mask_2: 0.05248  loss_dice_2: 0.542  loss_ce_3: 0.6245  loss_mask_3: 0.04105  loss_dice_3: 0.7122  loss_ce_4: 0.6183  loss_mask_4: 0.04267  loss_dice_4: 0.6481  loss_ce_5: 0.655  loss_mask_5: 0.04185  loss_dice_5: 0.6655  loss_ce_6: 0.629  loss_mask_6: 0.04459  loss_dice_6: 0.5971  loss_ce_7: 0.5717  loss_mask_7: 0.04276  loss_dice_7: 0.724  loss_ce_8: 0.6041  loss_mask_8: 0.0412  loss_dice_8: 0.7385    time: 0.3782  last_time: 0.3007  data_time: 0.0131  last_data_time: 0.0072   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:22 d2.utils.events]:  eta: 0:10:24  iter: 7559  total_loss: 22.13  loss_ce: 0.7337  loss_mask: 0.05116  loss_dice: 1.227  loss_ce_0: 1.453  loss_mask_0: 0.05431  loss_dice_0: 1.107  loss_ce_1: 1.052  loss_mask_1: 0.05244  loss_dice_1: 1.309  loss_ce_2: 0.8925  loss_mask_2: 0.04444  loss_dice_2: 0.8394  loss_ce_3: 0.7456  loss_mask_3: 0.04388  loss_dice_3: 1.079  loss_ce_4: 0.8183  loss_mask_4: 0.04336  loss_dice_4: 0.9987  loss_ce_5: 0.8498  loss_mask_5: 0.04976  loss_dice_5: 0.951  loss_ce_6: 0.8343  loss_mask_6: 0.04167  loss_dice_6: 1.014  loss_ce_7: 0.5825  loss_mask_7: 0.04622  loss_dice_7: 1.315  loss_ce_8: 0.6132  loss_mask_8: 0.0459  loss_dice_8: 1.224    time: 0.3780  last_time: 0.3305  data_time: 0.0076  last_data_time: 0.0105   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:29 d2.utils.events]:  eta: 0:10:18  iter: 7579  total_loss: 13.47  loss_ce: 0.6161  loss_mask: 0.04114  loss_dice: 0.5297  loss_ce_0: 1.556  loss_mask_0: 0.04793  loss_dice_0: 0.7762  loss_ce_1: 1.014  loss_mask_1: 0.04488  loss_dice_1: 0.5032  loss_ce_2: 0.8973  loss_mask_2: 0.05644  loss_dice_2: 0.5179  loss_ce_3: 0.7456  loss_mask_3: 0.04657  loss_dice_3: 0.4829  loss_ce_4: 0.6892  loss_mask_4: 0.04293  loss_dice_4: 0.5359  loss_ce_5: 0.7037  loss_mask_5: 0.042  loss_dice_5: 0.4587  loss_ce_6: 0.5973  loss_mask_6: 0.03353  loss_dice_6: 0.4507  loss_ce_7: 0.5776  loss_mask_7: 0.0464  loss_dice_7: 0.532  loss_ce_8: 0.5668  loss_mask_8: 0.04109  loss_dice_8: 0.3711    time: 0.3779  last_time: 0.3088  data_time: 0.0127  last_data_time: 0.0097   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:35 d2.utils.events]:  eta: 0:10:12  iter: 7599  total_loss: 17.26  loss_ce: 0.5186  loss_mask: 0.07878  loss_dice: 0.929  loss_ce_0: 1.125  loss_mask_0: 0.1073  loss_dice_0: 0.9762  loss_ce_1: 0.7479  loss_mask_1: 0.109  loss_dice_1: 0.8036  loss_ce_2: 0.701  loss_mask_2: 0.09877  loss_dice_2: 0.7509  loss_ce_3: 0.593  loss_mask_3: 0.08737  loss_dice_3: 0.6202  loss_ce_4: 0.497  loss_mask_4: 0.08648  loss_dice_4: 0.6079  loss_ce_5: 0.4391  loss_mask_5: 0.09403  loss_dice_5: 0.6665  loss_ce_6: 0.5331  loss_mask_6: 0.09885  loss_dice_6: 0.5618  loss_ce_7: 0.4468  loss_mask_7: 0.08295  loss_dice_7: 0.5608  loss_ce_8: 0.4403  loss_mask_8: 0.1003  loss_dice_8: 0.5664    time: 0.3777  last_time: 0.3273  data_time: 0.0062  last_data_time: 0.0091   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:41 d2.utils.events]:  eta: 0:10:06  iter: 7619  total_loss: 15.83  loss_ce: 0.7391  loss_mask: 0.07541  loss_dice: 0.5666  loss_ce_0: 1.653  loss_mask_0: 0.125  loss_dice_0: 0.8014  loss_ce_1: 0.9998  loss_mask_1: 0.08039  loss_dice_1: 0.6049  loss_ce_2: 0.9464  loss_mask_2: 0.07645  loss_dice_2: 0.7326  loss_ce_3: 0.7431  loss_mask_3: 0.07541  loss_dice_3: 0.5277  loss_ce_4: 0.6648  loss_mask_4: 0.07345  loss_dice_4: 0.6148  loss_ce_5: 0.769  loss_mask_5: 0.07249  loss_dice_5: 0.5133  loss_ce_6: 0.7174  loss_mask_6: 0.06862  loss_dice_6: 0.5659  loss_ce_7: 0.782  loss_mask_7: 0.07004  loss_dice_7: 0.4638  loss_ce_8: 0.7905  loss_mask_8: 0.0747  loss_dice_8: 0.6105    time: 0.3775  last_time: 0.2973  data_time: 0.0066  last_data_time: 0.0042   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:47 d2.utils.events]:  eta: 0:10:00  iter: 7639  total_loss: 14.37  loss_ce: 0.638  loss_mask: 0.07788  loss_dice: 0.416  loss_ce_0: 1.462  loss_mask_0: 0.07947  loss_dice_0: 0.8227  loss_ce_1: 0.9189  loss_mask_1: 0.09811  loss_dice_1: 0.6463  loss_ce_2: 0.8594  loss_mask_2: 0.08754  loss_dice_2: 0.5161  loss_ce_3: 0.8244  loss_mask_3: 0.08323  loss_dice_3: 0.4789  loss_ce_4: 0.7377  loss_mask_4: 0.08324  loss_dice_4: 0.5171  loss_ce_5: 0.7766  loss_mask_5: 0.08851  loss_dice_5: 0.4882  loss_ce_6: 0.7699  loss_mask_6: 0.07698  loss_dice_6: 0.4246  loss_ce_7: 0.6316  loss_mask_7: 0.07897  loss_dice_7: 0.4778  loss_ce_8: 0.6197  loss_mask_8: 0.07494  loss_dice_8: 0.4413    time: 0.3774  last_time: 0.3341  data_time: 0.0072  last_data_time: 0.0093   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:42:54 d2.utils.events]:  eta: 0:09:53  iter: 7659  total_loss: 18.3  loss_ce: 0.7526  loss_mask: 0.04409  loss_dice: 0.895  loss_ce_0: 1.426  loss_mask_0: 0.0538  loss_dice_0: 1.254  loss_ce_1: 1.054  loss_mask_1: 0.0416  loss_dice_1: 0.5988  loss_ce_2: 0.9267  loss_mask_2: 0.04245  loss_dice_2: 1.046  loss_ce_3: 0.7689  loss_mask_3: 0.04552  loss_dice_3: 0.9892  loss_ce_4: 0.7253  loss_mask_4: 0.05059  loss_dice_4: 0.8015  loss_ce_5: 0.7782  loss_mask_5: 0.04448  loss_dice_5: 0.8842  loss_ce_6: 0.7348  loss_mask_6: 0.04158  loss_dice_6: 0.8576  loss_ce_7: 0.6677  loss_mask_7: 0.04438  loss_dice_7: 0.9762  loss_ce_8: 0.6609  loss_mask_8: 0.04263  loss_dice_8: 0.8006    time: 0.3772  last_time: 0.3249  data_time: 0.0153  last_data_time: 0.0052   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:00 d2.utils.events]:  eta: 0:09:47  iter: 7679  total_loss: 19.93  loss_ce: 0.8341  loss_mask: 0.07884  loss_dice: 1.065  loss_ce_0: 1.622  loss_mask_0: 0.09514  loss_dice_0: 1.123  loss_ce_1: 1.155  loss_mask_1: 0.07359  loss_dice_1: 0.8639  loss_ce_2: 1.131  loss_mask_2: 0.08907  loss_dice_2: 0.7021  loss_ce_3: 0.9697  loss_mask_3: 0.07966  loss_dice_3: 0.8111  loss_ce_4: 0.9623  loss_mask_4: 0.07918  loss_dice_4: 0.8289  loss_ce_5: 0.9654  loss_mask_5: 0.07929  loss_dice_5: 0.684  loss_ce_6: 0.8536  loss_mask_6: 0.08101  loss_dice_6: 0.7708  loss_ce_7: 0.817  loss_mask_7: 0.07527  loss_dice_7: 0.6767  loss_ce_8: 0.838  loss_mask_8: 0.08926  loss_dice_8: 0.7369    time: 0.3771  last_time: 0.3342  data_time: 0.0200  last_data_time: 0.0103   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:07 d2.utils.events]:  eta: 0:09:41  iter: 7699  total_loss: 16.37  loss_ce: 0.7356  loss_mask: 0.0524  loss_dice: 0.4969  loss_ce_0: 1.246  loss_mask_0: 0.04577  loss_dice_0: 0.6069  loss_ce_1: 0.8863  loss_mask_1: 0.07821  loss_dice_1: 0.5932  loss_ce_2: 0.7279  loss_mask_2: 0.06039  loss_dice_2: 0.6278  loss_ce_3: 0.726  loss_mask_3: 0.05733  loss_dice_3: 0.571  loss_ce_4: 0.6398  loss_mask_4: 0.06139  loss_dice_4: 0.5176  loss_ce_5: 0.6949  loss_mask_5: 0.0549  loss_dice_5: 0.5318  loss_ce_6: 0.6649  loss_mask_6: 0.05509  loss_dice_6: 0.4686  loss_ce_7: 0.6485  loss_mask_7: 0.05502  loss_dice_7: 0.5427  loss_ce_8: 0.7412  loss_mask_8: 0.05284  loss_dice_8: 0.5091    time: 0.3769  last_time: 0.3021  data_time: 0.0061  last_data_time: 0.0065   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:13 d2.utils.events]:  eta: 0:09:35  iter: 7719  total_loss: 21.52  loss_ce: 0.9495  loss_mask: 0.1195  loss_dice: 0.8591  loss_ce_0: 1.649  loss_mask_0: 0.08675  loss_dice_0: 1.072  loss_ce_1: 1.175  loss_mask_1: 0.1034  loss_dice_1: 0.912  loss_ce_2: 1.204  loss_mask_2: 0.1138  loss_dice_2: 0.8528  loss_ce_3: 1.083  loss_mask_3: 0.09389  loss_dice_3: 1.269  loss_ce_4: 0.9851  loss_mask_4: 0.09027  loss_dice_4: 0.9964  loss_ce_5: 0.9628  loss_mask_5: 0.1245  loss_dice_5: 1.021  loss_ce_6: 1.031  loss_mask_6: 0.1069  loss_dice_6: 1.019  loss_ce_7: 0.9553  loss_mask_7: 0.09838  loss_dice_7: 1.03  loss_ce_8: 0.9199  loss_mask_8: 0.1146  loss_dice_8: 1.045    time: 0.3768  last_time: 0.3310  data_time: 0.0087  last_data_time: 0.0088   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:19 d2.utils.events]:  eta: 0:09:29  iter: 7739  total_loss: 17.95  loss_ce: 0.8024  loss_mask: 0.09492  loss_dice: 0.6085  loss_ce_0: 1.651  loss_mask_0: 0.114  loss_dice_0: 0.8254  loss_ce_1: 1.162  loss_mask_1: 0.1021  loss_dice_1: 0.7992  loss_ce_2: 1.083  loss_mask_2: 0.1091  loss_dice_2: 0.6193  loss_ce_3: 0.9265  loss_mask_3: 0.09445  loss_dice_3: 0.4657  loss_ce_4: 0.7712  loss_mask_4: 0.08497  loss_dice_4: 0.5617  loss_ce_5: 0.8027  loss_mask_5: 0.09504  loss_dice_5: 0.6124  loss_ce_6: 0.7106  loss_mask_6: 0.08627  loss_dice_6: 0.5457  loss_ce_7: 0.7191  loss_mask_7: 0.09627  loss_dice_7: 0.6089  loss_ce_8: 0.7288  loss_mask_8: 0.1014  loss_dice_8: 0.4817    time: 0.3766  last_time: 0.2943  data_time: 0.0108  last_data_time: 0.0051   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:26 d2.utils.events]:  eta: 0:09:23  iter: 7759  total_loss: 19.31  loss_ce: 0.8492  loss_mask: 0.0673  loss_dice: 0.9428  loss_ce_0: 1.529  loss_mask_0: 0.0669  loss_dice_0: 1.225  loss_ce_1: 1.063  loss_mask_1: 0.08878  loss_dice_1: 0.9153  loss_ce_2: 0.8791  loss_mask_2: 0.07341  loss_dice_2: 0.961  loss_ce_3: 0.8392  loss_mask_3: 0.07105  loss_dice_3: 1.012  loss_ce_4: 0.8536  loss_mask_4: 0.06864  loss_dice_4: 0.9035  loss_ce_5: 0.9474  loss_mask_5: 0.0753  loss_dice_5: 0.9904  loss_ce_6: 0.7873  loss_mask_6: 0.06783  loss_dice_6: 0.9316  loss_ce_7: 0.842  loss_mask_7: 0.06516  loss_dice_7: 0.8578  loss_ce_8: 0.8287  loss_mask_8: 0.07142  loss_dice_8: 0.7945    time: 0.3764  last_time: 0.3021  data_time: 0.0065  last_data_time: 0.0053   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:32 d2.utils.events]:  eta: 0:09:16  iter: 7779  total_loss: 22.53  loss_ce: 0.7877  loss_mask: 0.06963  loss_dice: 1.069  loss_ce_0: 1.717  loss_mask_0: 0.1256  loss_dice_0: 1.002  loss_ce_1: 1.228  loss_mask_1: 0.1105  loss_dice_1: 1.096  loss_ce_2: 1.07  loss_mask_2: 0.1168  loss_dice_2: 1.159  loss_ce_3: 0.8759  loss_mask_3: 0.08605  loss_dice_3: 1.108  loss_ce_4: 0.7863  loss_mask_4: 0.08816  loss_dice_4: 1.332  loss_ce_5: 0.818  loss_mask_5: 0.08154  loss_dice_5: 1.078  loss_ce_6: 0.8601  loss_mask_6: 0.07533  loss_dice_6: 1.053  loss_ce_7: 0.7791  loss_mask_7: 0.07975  loss_dice_7: 0.9356  loss_ce_8: 0.8555  loss_mask_8: 0.07003  loss_dice_8: 1.1    time: 0.3763  last_time: 0.3006  data_time: 0.0170  last_data_time: 0.0063   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:39 d2.utils.events]:  eta: 0:09:11  iter: 7799  total_loss: 18.23  loss_ce: 0.8072  loss_mask: 0.05869  loss_dice: 0.7343  loss_ce_0: 1.575  loss_mask_0: 0.07052  loss_dice_0: 0.8069  loss_ce_1: 1.029  loss_mask_1: 0.07331  loss_dice_1: 1.04  loss_ce_2: 0.8913  loss_mask_2: 0.0507  loss_dice_2: 0.6118  loss_ce_3: 0.8371  loss_mask_3: 0.06002  loss_dice_3: 0.5227  loss_ce_4: 0.7544  loss_mask_4: 0.06368  loss_dice_4: 0.5443  loss_ce_5: 0.886  loss_mask_5: 0.0572  loss_dice_5: 0.543  loss_ce_6: 0.8174  loss_mask_6: 0.059  loss_dice_6: 0.7732  loss_ce_7: 0.7975  loss_mask_7: 0.05584  loss_dice_7: 0.7973  loss_ce_8: 0.8101  loss_mask_8: 0.06009  loss_dice_8: 0.6492    time: 0.3761  last_time: 0.3362  data_time: 0.0088  last_data_time: 0.0079   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:45 d2.utils.events]:  eta: 0:09:05  iter: 7819  total_loss: 16.08  loss_ce: 0.3876  loss_mask: 0.04787  loss_dice: 0.6874  loss_ce_0: 1.188  loss_mask_0: 0.06338  loss_dice_0: 0.7894  loss_ce_1: 0.7471  loss_mask_1: 0.06627  loss_dice_1: 0.9176  loss_ce_2: 0.6323  loss_mask_2: 0.05371  loss_dice_2: 0.8106  loss_ce_3: 0.6032  loss_mask_3: 0.05011  loss_dice_3: 0.9966  loss_ce_4: 0.538  loss_mask_4: 0.04984  loss_dice_4: 0.6156  loss_ce_5: 0.4627  loss_mask_5: 0.05157  loss_dice_5: 0.7272  loss_ce_6: 0.4407  loss_mask_6: 0.05432  loss_dice_6: 0.8079  loss_ce_7: 0.4805  loss_mask_7: 0.04985  loss_dice_7: 0.7686  loss_ce_8: 0.3929  loss_mask_8: 0.04767  loss_dice_8: 0.719    time: 0.3760  last_time: 0.2950  data_time: 0.0141  last_data_time: 0.0037   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:51 d2.utils.events]:  eta: 0:08:58  iter: 7839  total_loss: 17.22  loss_ce: 0.5964  loss_mask: 0.09598  loss_dice: 0.549  loss_ce_0: 1.3  loss_mask_0: 0.109  loss_dice_0: 1.015  loss_ce_1: 1.014  loss_mask_1: 0.1016  loss_dice_1: 1.015  loss_ce_2: 0.7527  loss_mask_2: 0.09843  loss_dice_2: 0.965  loss_ce_3: 0.65  loss_mask_3: 0.0916  loss_dice_3: 0.9332  loss_ce_4: 0.6503  loss_mask_4: 0.09438  loss_dice_4: 0.841  loss_ce_5: 0.6045  loss_mask_5: 0.09287  loss_dice_5: 0.9834  loss_ce_6: 0.6079  loss_mask_6: 0.09543  loss_dice_6: 0.5888  loss_ce_7: 0.5128  loss_mask_7: 0.09832  loss_dice_7: 0.8142  loss_ce_8: 0.5443  loss_mask_8: 0.0929  loss_dice_8: 0.5513    time: 0.3758  last_time: 0.3352  data_time: 0.0115  last_data_time: 0.0060   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:43:57 d2.utils.events]:  eta: 0:08:52  iter: 7859  total_loss: 15.16  loss_ce: 0.6841  loss_mask: 0.08756  loss_dice: 0.4134  loss_ce_0: 1.083  loss_mask_0: 0.105  loss_dice_0: 0.8198  loss_ce_1: 0.8968  loss_mask_1: 0.08573  loss_dice_1: 0.584  loss_ce_2: 0.7936  loss_mask_2: 0.1139  loss_dice_2: 0.5744  loss_ce_3: 0.6019  loss_mask_3: 0.1058  loss_dice_3: 0.6181  loss_ce_4: 0.6592  loss_mask_4: 0.09898  loss_dice_4: 0.6051  loss_ce_5: 0.7321  loss_mask_5: 0.07735  loss_dice_5: 0.4996  loss_ce_6: 0.6061  loss_mask_6: 0.08521  loss_dice_6: 0.4678  loss_ce_7: 0.5862  loss_mask_7: 0.08293  loss_dice_7: 0.5028  loss_ce_8: 0.7444  loss_mask_8: 0.07727  loss_dice_8: 0.5599    time: 0.3757  last_time: 0.2960  data_time: 0.0106  last_data_time: 0.0021   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:04 d2.utils.events]:  eta: 0:08:46  iter: 7879  total_loss: 15.95  loss_ce: 0.6516  loss_mask: 0.07464  loss_dice: 0.6702  loss_ce_0: 0.9819  loss_mask_0: 0.09351  loss_dice_0: 0.7926  loss_ce_1: 0.7831  loss_mask_1: 0.08416  loss_dice_1: 0.6027  loss_ce_2: 0.9283  loss_mask_2: 0.09676  loss_dice_2: 0.5638  loss_ce_3: 0.6924  loss_mask_3: 0.09198  loss_dice_3: 0.6079  loss_ce_4: 0.712  loss_mask_4: 0.06942  loss_dice_4: 0.5392  loss_ce_5: 0.906  loss_mask_5: 0.0799  loss_dice_5: 0.6348  loss_ce_6: 0.6254  loss_mask_6: 0.07514  loss_dice_6: 0.6223  loss_ce_7: 0.729  loss_mask_7: 0.06749  loss_dice_7: 0.6118  loss_ce_8: 0.6374  loss_mask_8: 0.08232  loss_dice_8: 0.6077    time: 0.3755  last_time: 0.3092  data_time: 0.0068  last_data_time: 0.0083   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:10 d2.utils.events]:  eta: 0:08:40  iter: 7899  total_loss: 17.76  loss_ce: 0.5381  loss_mask: 0.111  loss_dice: 0.5749  loss_ce_0: 1.041  loss_mask_0: 0.1174  loss_dice_0: 0.7679  loss_ce_1: 0.6859  loss_mask_1: 0.1059  loss_dice_1: 0.635  loss_ce_2: 0.7165  loss_mask_2: 0.08554  loss_dice_2: 0.4195  loss_ce_3: 0.5178  loss_mask_3: 0.09168  loss_dice_3: 0.6364  loss_ce_4: 0.6446  loss_mask_4: 0.0906  loss_dice_4: 0.5838  loss_ce_5: 0.6269  loss_mask_5: 0.08751  loss_dice_5: 0.5868  loss_ce_6: 0.5398  loss_mask_6: 0.09582  loss_dice_6: 0.57  loss_ce_7: 0.5348  loss_mask_7: 0.09683  loss_dice_7: 0.6081  loss_ce_8: 0.5142  loss_mask_8: 0.1282  loss_dice_8: 0.5549    time: 0.3754  last_time: 0.2988  data_time: 0.0078  last_data_time: 0.0068   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:16 d2.utils.events]:  eta: 0:08:34  iter: 7919  total_loss: 16.8  loss_ce: 0.6057  loss_mask: 0.06371  loss_dice: 0.7612  loss_ce_0: 1.375  loss_mask_0: 0.07999  loss_dice_0: 0.6936  loss_ce_1: 0.8825  loss_mask_1: 0.07932  loss_dice_1: 0.94  loss_ce_2: 0.8235  loss_mask_2: 0.05825  loss_dice_2: 0.8555  loss_ce_3: 0.7288  loss_mask_3: 0.06472  loss_dice_3: 0.9419  loss_ce_4: 0.6827  loss_mask_4: 0.06926  loss_dice_4: 1.024  loss_ce_5: 0.6472  loss_mask_5: 0.06418  loss_dice_5: 0.8108  loss_ce_6: 0.6502  loss_mask_6: 0.05653  loss_dice_6: 0.7517  loss_ce_7: 0.6233  loss_mask_7: 0.06578  loss_dice_7: 0.6508  loss_ce_8: 0.5874  loss_mask_8: 0.06524  loss_dice_8: 0.7306    time: 0.3752  last_time: 0.3412  data_time: 0.0084  last_data_time: 0.0104   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:22 d2.utils.events]:  eta: 0:08:27  iter: 7939  total_loss: 18.6  loss_ce: 0.6236  loss_mask: 0.06386  loss_dice: 0.9927  loss_ce_0: 1.295  loss_mask_0: 0.06595  loss_dice_0: 1.059  loss_ce_1: 0.8646  loss_mask_1: 0.05655  loss_dice_1: 0.8581  loss_ce_2: 0.7774  loss_mask_2: 0.06142  loss_dice_2: 0.9387  loss_ce_3: 0.6992  loss_mask_3: 0.06436  loss_dice_3: 0.9629  loss_ce_4: 0.6548  loss_mask_4: 0.06627  loss_dice_4: 1.142  loss_ce_5: 0.6957  loss_mask_5: 0.05629  loss_dice_5: 1.081  loss_ce_6: 0.6139  loss_mask_6: 0.06568  loss_dice_6: 0.9565  loss_ce_7: 0.5869  loss_mask_7: 0.06368  loss_dice_7: 1.137  loss_ce_8: 0.6602  loss_mask_8: 0.06729  loss_dice_8: 0.9639    time: 0.3750  last_time: 0.2974  data_time: 0.0078  last_data_time: 0.0060   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:29 d2.utils.events]:  eta: 0:08:21  iter: 7959  total_loss: 17.03  loss_ce: 0.7136  loss_mask: 0.05347  loss_dice: 0.5938  loss_ce_0: 1.729  loss_mask_0: 0.05918  loss_dice_0: 0.8027  loss_ce_1: 1.121  loss_mask_1: 0.0777  loss_dice_1: 0.6319  loss_ce_2: 1.055  loss_mask_2: 0.07073  loss_dice_2: 0.7004  loss_ce_3: 0.7386  loss_mask_3: 0.06367  loss_dice_3: 0.6523  loss_ce_4: 0.7634  loss_mask_4: 0.05985  loss_dice_4: 0.7529  loss_ce_5: 0.717  loss_mask_5: 0.05856  loss_dice_5: 0.509  loss_ce_6: 0.6717  loss_mask_6: 0.05391  loss_dice_6: 0.6338  loss_ce_7: 0.6585  loss_mask_7: 0.05474  loss_dice_7: 0.662  loss_ce_8: 0.7081  loss_mask_8: 0.06482  loss_dice_8: 0.6675    time: 0.3749  last_time: 0.3252  data_time: 0.0078  last_data_time: 0.0045   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:35 d2.utils.events]:  eta: 0:08:15  iter: 7979  total_loss: 19.54  loss_ce: 0.5482  loss_mask: 0.05787  loss_dice: 0.6639  loss_ce_0: 1.338  loss_mask_0: 0.05082  loss_dice_0: 1.159  loss_ce_1: 0.7999  loss_mask_1: 0.05713  loss_dice_1: 0.8591  loss_ce_2: 0.832  loss_mask_2: 0.06074  loss_dice_2: 0.7048  loss_ce_3: 0.6566  loss_mask_3: 0.06179  loss_dice_3: 0.7469  loss_ce_4: 0.7922  loss_mask_4: 0.05713  loss_dice_4: 0.7125  loss_ce_5: 0.6588  loss_mask_5: 0.05932  loss_dice_5: 0.7349  loss_ce_6: 0.6136  loss_mask_6: 0.05241  loss_dice_6: 0.83  loss_ce_7: 0.6594  loss_mask_7: 0.05454  loss_dice_7: 0.8094  loss_ce_8: 0.7377  loss_mask_8: 0.05255  loss_dice_8: 0.8567    time: 0.3747  last_time: 0.3053  data_time: 0.0070  last_data_time: 0.0053   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:41 d2.utils.events]:  eta: 0:08:09  iter: 7999  total_loss: 18.41  loss_ce: 0.8596  loss_mask: 0.04402  loss_dice: 0.6152  loss_ce_0: 1.811  loss_mask_0: 0.04619  loss_dice_0: 0.8405  loss_ce_1: 0.9839  loss_mask_1: 0.04788  loss_dice_1: 1.041  loss_ce_2: 0.8914  loss_mask_2: 0.04526  loss_dice_2: 0.853  loss_ce_3: 0.771  loss_mask_3: 0.04724  loss_dice_3: 0.825  loss_ce_4: 0.8207  loss_mask_4: 0.04691  loss_dice_4: 0.735  loss_ce_5: 0.7673  loss_mask_5: 0.04294  loss_dice_5: 0.7259  loss_ce_6: 0.751  loss_mask_6: 0.04331  loss_dice_6: 0.7812  loss_ce_7: 0.7376  loss_mask_7: 0.04357  loss_dice_7: 0.6067  loss_ce_8: 0.7335  loss_mask_8: 0.04377  loss_dice_8: 0.7969    time: 0.3746  last_time: 0.3042  data_time: 0.0110  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:48 d2.utils.events]:  eta: 0:08:03  iter: 8019  total_loss: 16.61  loss_ce: 0.5786  loss_mask: 0.05421  loss_dice: 0.568  loss_ce_0: 1.059  loss_mask_0: 0.06023  loss_dice_0: 0.85  loss_ce_1: 0.8231  loss_mask_1: 0.0786  loss_dice_1: 0.628  loss_ce_2: 0.8236  loss_mask_2: 0.04463  loss_dice_2: 0.5185  loss_ce_3: 0.6936  loss_mask_3: 0.05984  loss_dice_3: 0.501  loss_ce_4: 0.7014  loss_mask_4: 0.0489  loss_dice_4: 0.5097  loss_ce_5: 0.6179  loss_mask_5: 0.05552  loss_dice_5: 0.47  loss_ce_6: 0.6006  loss_mask_6: 0.05693  loss_dice_6: 0.5573  loss_ce_7: 0.6787  loss_mask_7: 0.05216  loss_dice_7: 0.4712  loss_ce_8: 0.5817  loss_mask_8: 0.05229  loss_dice_8: 0.5121    time: 0.3744  last_time: 0.3340  data_time: 0.0113  last_data_time: 0.0104   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:44:54 d2.utils.events]:  eta: 0:07:57  iter: 8039  total_loss: 19.08  loss_ce: 0.6937  loss_mask: 0.05233  loss_dice: 0.7051  loss_ce_0: 1.725  loss_mask_0: 0.06249  loss_dice_0: 0.7659  loss_ce_1: 1.214  loss_mask_1: 0.08532  loss_dice_1: 0.7216  loss_ce_2: 0.9483  loss_mask_2: 0.05025  loss_dice_2: 0.7147  loss_ce_3: 0.8794  loss_mask_3: 0.05154  loss_dice_3: 0.8125  loss_ce_4: 0.7405  loss_mask_4: 0.04901  loss_dice_4: 0.7356  loss_ce_5: 0.7072  loss_mask_5: 0.06042  loss_dice_5: 0.7388  loss_ce_6: 0.7311  loss_mask_6: 0.04762  loss_dice_6: 0.7329  loss_ce_7: 0.7307  loss_mask_7: 0.05343  loss_dice_7: 0.7585  loss_ce_8: 0.7135  loss_mask_8: 0.06112  loss_dice_8: 0.7197    time: 0.3743  last_time: 0.3394  data_time: 0.0119  last_data_time: 0.0104   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:00 d2.utils.events]:  eta: 0:07:51  iter: 8059  total_loss: 12.65  loss_ce: 0.4215  loss_mask: 0.06565  loss_dice: 0.4893  loss_ce_0: 1.006  loss_mask_0: 0.08572  loss_dice_0: 0.724  loss_ce_1: 0.5544  loss_mask_1: 0.0739  loss_dice_1: 0.4773  loss_ce_2: 0.5181  loss_mask_2: 0.06833  loss_dice_2: 0.5399  loss_ce_3: 0.494  loss_mask_3: 0.06845  loss_dice_3: 0.6326  loss_ce_4: 0.3968  loss_mask_4: 0.06609  loss_dice_4: 0.4274  loss_ce_5: 0.4029  loss_mask_5: 0.08367  loss_dice_5: 0.4793  loss_ce_6: 0.4007  loss_mask_6: 0.06651  loss_dice_6: 0.5126  loss_ce_7: 0.3809  loss_mask_7: 0.07427  loss_dice_7: 0.4091  loss_ce_8: 0.3893  loss_mask_8: 0.0602  loss_dice_8: 0.4744    time: 0.3741  last_time: 0.3114  data_time: 0.0081  last_data_time: 0.0131   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:07 d2.utils.events]:  eta: 0:07:44  iter: 8079  total_loss: 16.52  loss_ce: 0.6856  loss_mask: 0.09084  loss_dice: 0.7461  loss_ce_0: 1.328  loss_mask_0: 0.0733  loss_dice_0: 0.8262  loss_ce_1: 0.7831  loss_mask_1: 0.1076  loss_dice_1: 0.8607  loss_ce_2: 0.7334  loss_mask_2: 0.1092  loss_dice_2: 0.597  loss_ce_3: 0.7196  loss_mask_3: 0.1178  loss_dice_3: 0.6174  loss_ce_4: 0.7154  loss_mask_4: 0.09624  loss_dice_4: 0.5282  loss_ce_5: 0.7451  loss_mask_5: 0.1011  loss_dice_5: 0.5971  loss_ce_6: 0.6791  loss_mask_6: 0.07676  loss_dice_6: 0.6306  loss_ce_7: 0.6718  loss_mask_7: 0.07457  loss_dice_7: 0.5216  loss_ce_8: 0.6805  loss_mask_8: 0.08244  loss_dice_8: 0.5708    time: 0.3740  last_time: 0.3106  data_time: 0.0198  last_data_time: 0.0063   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:13 d2.utils.events]:  eta: 0:07:38  iter: 8099  total_loss: 20.12  loss_ce: 0.7605  loss_mask: 0.08525  loss_dice: 0.7501  loss_ce_0: 1.265  loss_mask_0: 0.0795  loss_dice_0: 0.8855  loss_ce_1: 1.033  loss_mask_1: 0.1009  loss_dice_1: 0.9191  loss_ce_2: 0.8731  loss_mask_2: 0.1045  loss_dice_2: 0.7812  loss_ce_3: 0.8033  loss_mask_3: 0.09667  loss_dice_3: 0.6516  loss_ce_4: 0.8023  loss_mask_4: 0.09875  loss_dice_4: 0.7048  loss_ce_5: 0.6889  loss_mask_5: 0.09559  loss_dice_5: 0.7113  loss_ce_6: 0.6882  loss_mask_6: 0.1046  loss_dice_6: 0.7549  loss_ce_7: 0.6541  loss_mask_7: 0.09002  loss_dice_7: 0.7656  loss_ce_8: 0.645  loss_mask_8: 0.08746  loss_dice_8: 0.7451    time: 0.3738  last_time: 0.3108  data_time: 0.0063  last_data_time: 0.0086   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:19 d2.utils.events]:  eta: 0:07:32  iter: 8119  total_loss: 18.15  loss_ce: 0.8503  loss_mask: 0.04681  loss_dice: 0.6638  loss_ce_0: 1.465  loss_mask_0: 0.04748  loss_dice_0: 0.8433  loss_ce_1: 1.008  loss_mask_1: 0.05377  loss_dice_1: 0.7103  loss_ce_2: 0.9486  loss_mask_2: 0.04483  loss_dice_2: 0.7546  loss_ce_3: 0.854  loss_mask_3: 0.04193  loss_dice_3: 0.622  loss_ce_4: 0.8804  loss_mask_4: 0.04509  loss_dice_4: 0.5642  loss_ce_5: 0.765  loss_mask_5: 0.03799  loss_dice_5: 0.5955  loss_ce_6: 0.8761  loss_mask_6: 0.04396  loss_dice_6: 0.7969  loss_ce_7: 0.8566  loss_mask_7: 0.04659  loss_dice_7: 0.6815  loss_ce_8: 0.7959  loss_mask_8: 0.04603  loss_dice_8: 0.6037    time: 0.3737  last_time: 0.3118  data_time: 0.0071  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:25 d2.utils.events]:  eta: 0:07:26  iter: 8139  total_loss: 16.3  loss_ce: 0.5661  loss_mask: 0.04422  loss_dice: 0.4809  loss_ce_0: 1.432  loss_mask_0: 0.05919  loss_dice_0: 0.6257  loss_ce_1: 0.7843  loss_mask_1: 0.05876  loss_dice_1: 0.7246  loss_ce_2: 0.6187  loss_mask_2: 0.0419  loss_dice_2: 0.7139  loss_ce_3: 0.589  loss_mask_3: 0.04909  loss_dice_3: 0.5505  loss_ce_4: 0.6872  loss_mask_4: 0.05189  loss_dice_4: 0.713  loss_ce_5: 0.4989  loss_mask_5: 0.05107  loss_dice_5: 0.5967  loss_ce_6: 0.5683  loss_mask_6: 0.04695  loss_dice_6: 0.5685  loss_ce_7: 0.5538  loss_mask_7: 0.03871  loss_dice_7: 0.6776  loss_ce_8: 0.6242  loss_mask_8: 0.04589  loss_dice_8: 0.5242    time: 0.3735  last_time: 0.3235  data_time: 0.0063  last_data_time: 0.0026   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:32 d2.utils.events]:  eta: 0:07:20  iter: 8159  total_loss: 18.97  loss_ce: 0.9475  loss_mask: 0.07268  loss_dice: 0.8459  loss_ce_0: 1.526  loss_mask_0: 0.1047  loss_dice_0: 1.109  loss_ce_1: 1.115  loss_mask_1: 0.07317  loss_dice_1: 1.073  loss_ce_2: 1.076  loss_mask_2: 0.0681  loss_dice_2: 0.9996  loss_ce_3: 1.036  loss_mask_3: 0.07341  loss_dice_3: 0.9246  loss_ce_4: 1.025  loss_mask_4: 0.07222  loss_dice_4: 0.9641  loss_ce_5: 0.9682  loss_mask_5: 0.06552  loss_dice_5: 0.8718  loss_ce_6: 0.9464  loss_mask_6: 0.07006  loss_dice_6: 0.9368  loss_ce_7: 0.9625  loss_mask_7: 0.06907  loss_dice_7: 0.8515  loss_ce_8: 1.01  loss_mask_8: 0.06831  loss_dice_8: 0.8248    time: 0.3734  last_time: 0.3047  data_time: 0.0066  last_data_time: 0.0062   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:38 d2.utils.events]:  eta: 0:07:14  iter: 8179  total_loss: 13.47  loss_ce: 0.5604  loss_mask: 0.04116  loss_dice: 0.4247  loss_ce_0: 1.291  loss_mask_0: 0.04647  loss_dice_0: 0.67  loss_ce_1: 0.7829  loss_mask_1: 0.0379  loss_dice_1: 0.4807  loss_ce_2: 0.7728  loss_mask_2: 0.04365  loss_dice_2: 0.4581  loss_ce_3: 0.6753  loss_mask_3: 0.03348  loss_dice_3: 0.3954  loss_ce_4: 0.57  loss_mask_4: 0.03742  loss_dice_4: 0.7754  loss_ce_5: 0.5881  loss_mask_5: 0.04739  loss_dice_5: 0.3628  loss_ce_6: 0.6137  loss_mask_6: 0.03659  loss_dice_6: 0.632  loss_ce_7: 0.5877  loss_mask_7: 0.04306  loss_dice_7: 0.5658  loss_ce_8: 0.5545  loss_mask_8: 0.04598  loss_dice_8: 0.5125    time: 0.3732  last_time: 0.3079  data_time: 0.0061  last_data_time: 0.0056   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:44 d2.utils.events]:  eta: 0:07:08  iter: 8199  total_loss: 19.39  loss_ce: 0.8451  loss_mask: 0.06023  loss_dice: 0.7371  loss_ce_0: 1.551  loss_mask_0: 0.07642  loss_dice_0: 1.239  loss_ce_1: 1.187  loss_mask_1: 0.05406  loss_dice_1: 1.058  loss_ce_2: 1.093  loss_mask_2: 0.06202  loss_dice_2: 1.088  loss_ce_3: 0.9349  loss_mask_3: 0.06287  loss_dice_3: 0.8842  loss_ce_4: 0.8585  loss_mask_4: 0.06053  loss_dice_4: 1.178  loss_ce_5: 0.8943  loss_mask_5: 0.05553  loss_dice_5: 0.9913  loss_ce_6: 0.8334  loss_mask_6: 0.05444  loss_dice_6: 0.9954  loss_ce_7: 0.86  loss_mask_7: 0.05518  loss_dice_7: 0.7428  loss_ce_8: 0.8869  loss_mask_8: 0.04775  loss_dice_8: 0.9857    time: 0.3731  last_time: 0.3132  data_time: 0.0260  last_data_time: 0.0095   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:51 d2.utils.events]:  eta: 0:07:01  iter: 8219  total_loss: 15.47  loss_ce: 0.6759  loss_mask: 0.105  loss_dice: 0.5457  loss_ce_0: 1.262  loss_mask_0: 0.1168  loss_dice_0: 0.6226  loss_ce_1: 0.9084  loss_mask_1: 0.1202  loss_dice_1: 0.791  loss_ce_2: 0.8238  loss_mask_2: 0.1296  loss_dice_2: 0.6323  loss_ce_3: 0.7921  loss_mask_3: 0.1024  loss_dice_3: 0.5209  loss_ce_4: 0.7428  loss_mask_4: 0.1072  loss_dice_4: 0.6451  loss_ce_5: 0.7572  loss_mask_5: 0.1162  loss_dice_5: 0.5866  loss_ce_6: 0.6509  loss_mask_6: 0.1048  loss_dice_6: 0.5414  loss_ce_7: 0.7094  loss_mask_7: 0.09733  loss_dice_7: 0.6282  loss_ce_8: 0.6834  loss_mask_8: 0.1032  loss_dice_8: 0.549    time: 0.3730  last_time: 0.3067  data_time: 0.0067  last_data_time: 0.0074   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:45:57 d2.utils.events]:  eta: 0:06:55  iter: 8239  total_loss: 21.99  loss_ce: 0.9482  loss_mask: 0.1156  loss_dice: 0.9895  loss_ce_0: 1.897  loss_mask_0: 0.1169  loss_dice_0: 1.154  loss_ce_1: 1.223  loss_mask_1: 0.1197  loss_dice_1: 1.026  loss_ce_2: 1.004  loss_mask_2: 0.1404  loss_dice_2: 1.023  loss_ce_3: 1.084  loss_mask_3: 0.1154  loss_dice_3: 1.113  loss_ce_4: 1.132  loss_mask_4: 0.1177  loss_dice_4: 1.075  loss_ce_5: 0.9141  loss_mask_5: 0.0989  loss_dice_5: 0.9882  loss_ce_6: 0.9005  loss_mask_6: 0.1213  loss_dice_6: 0.9455  loss_ce_7: 0.8826  loss_mask_7: 0.09245  loss_dice_7: 1.024  loss_ce_8: 0.8992  loss_mask_8: 0.0856  loss_dice_8: 0.9152    time: 0.3728  last_time: 0.3138  data_time: 0.0069  last_data_time: 0.0062   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:03 d2.utils.events]:  eta: 0:06:49  iter: 8259  total_loss: 15.47  loss_ce: 0.54  loss_mask: 0.05471  loss_dice: 0.5337  loss_ce_0: 1.509  loss_mask_0: 0.06736  loss_dice_0: 0.9322  loss_ce_1: 0.8703  loss_mask_1: 0.06534  loss_dice_1: 0.5964  loss_ce_2: 0.7462  loss_mask_2: 0.05819  loss_dice_2: 0.5216  loss_ce_3: 0.633  loss_mask_3: 0.0609  loss_dice_3: 0.5816  loss_ce_4: 0.6505  loss_mask_4: 0.05626  loss_dice_4: 0.5843  loss_ce_5: 0.5511  loss_mask_5: 0.05334  loss_dice_5: 0.4681  loss_ce_6: 0.5844  loss_mask_6: 0.05295  loss_dice_6: 0.5411  loss_ce_7: 0.5534  loss_mask_7: 0.05362  loss_dice_7: 0.5231  loss_ce_8: 0.5433  loss_mask_8: 0.05185  loss_dice_8: 0.5567    time: 0.3727  last_time: 0.3015  data_time: 0.0075  last_data_time: 0.0044   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:10 d2.utils.events]:  eta: 0:06:43  iter: 8279  total_loss: 17.49  loss_ce: 0.5899  loss_mask: 0.05846  loss_dice: 0.4712  loss_ce_0: 1.469  loss_mask_0: 0.08314  loss_dice_0: 0.7664  loss_ce_1: 1.115  loss_mask_1: 0.06319  loss_dice_1: 0.6557  loss_ce_2: 0.7491  loss_mask_2: 0.06572  loss_dice_2: 0.7281  loss_ce_3: 0.6914  loss_mask_3: 0.06164  loss_dice_3: 0.705  loss_ce_4: 0.5872  loss_mask_4: 0.06027  loss_dice_4: 0.6037  loss_ce_5: 0.6458  loss_mask_5: 0.06925  loss_dice_5: 0.691  loss_ce_6: 0.5865  loss_mask_6: 0.06139  loss_dice_6: 0.7847  loss_ce_7: 0.5816  loss_mask_7: 0.0553  loss_dice_7: 0.49  loss_ce_8: 0.5711  loss_mask_8: 0.05638  loss_dice_8: 0.5442    time: 0.3725  last_time: 0.3139  data_time: 0.0204  last_data_time: 0.0021   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:16 d2.utils.events]:  eta: 0:06:37  iter: 8299  total_loss: 18.61  loss_ce: 0.7278  loss_mask: 0.07126  loss_dice: 0.7051  loss_ce_0: 1.604  loss_mask_0: 0.0856  loss_dice_0: 0.9918  loss_ce_1: 1.14  loss_mask_1: 0.06562  loss_dice_1: 0.7643  loss_ce_2: 1.141  loss_mask_2: 0.08273  loss_dice_2: 0.84  loss_ce_3: 0.964  loss_mask_3: 0.07933  loss_dice_3: 0.6442  loss_ce_4: 1.001  loss_mask_4: 0.08155  loss_dice_4: 0.7945  loss_ce_5: 0.8931  loss_mask_5: 0.08849  loss_dice_5: 0.5937  loss_ce_6: 0.7813  loss_mask_6: 0.07242  loss_dice_6: 0.6025  loss_ce_7: 0.7352  loss_mask_7: 0.07125  loss_dice_7: 0.834  loss_ce_8: 0.845  loss_mask_8: 0.07435  loss_dice_8: 0.871    time: 0.3724  last_time: 0.3247  data_time: 0.0077  last_data_time: 0.0065   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:22 d2.utils.events]:  eta: 0:06:31  iter: 8319  total_loss: 14.94  loss_ce: 0.6973  loss_mask: 0.05004  loss_dice: 0.4588  loss_ce_0: 1.605  loss_mask_0: 0.06205  loss_dice_0: 0.9418  loss_ce_1: 1.015  loss_mask_1: 0.06029  loss_dice_1: 0.7525  loss_ce_2: 0.8382  loss_mask_2: 0.0659  loss_dice_2: 0.7616  loss_ce_3: 0.7465  loss_mask_3: 0.06025  loss_dice_3: 0.5662  loss_ce_4: 0.8135  loss_mask_4: 0.05681  loss_dice_4: 0.5036  loss_ce_5: 0.7928  loss_mask_5: 0.05764  loss_dice_5: 0.5582  loss_ce_6: 0.685  loss_mask_6: 0.05531  loss_dice_6: 0.5216  loss_ce_7: 0.7597  loss_mask_7: 0.05696  loss_dice_7: 0.4629  loss_ce_8: 0.6971  loss_mask_8: 0.05839  loss_dice_8: 0.4664    time: 0.3722  last_time: 0.3276  data_time: 0.0058  last_data_time: 0.0046   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:28 d2.utils.events]:  eta: 0:06:25  iter: 8339  total_loss: 17.18  loss_ce: 0.5994  loss_mask: 0.06284  loss_dice: 0.6659  loss_ce_0: 1.058  loss_mask_0: 0.06398  loss_dice_0: 0.9881  loss_ce_1: 0.8107  loss_mask_1: 0.06249  loss_dice_1: 0.8867  loss_ce_2: 0.7897  loss_mask_2: 0.07302  loss_dice_2: 0.8046  loss_ce_3: 0.6695  loss_mask_3: 0.05252  loss_dice_3: 0.7917  loss_ce_4: 0.6728  loss_mask_4: 0.05491  loss_dice_4: 0.6474  loss_ce_5: 0.6442  loss_mask_5: 0.06198  loss_dice_5: 0.7433  loss_ce_6: 0.6674  loss_mask_6: 0.06905  loss_dice_6: 0.6151  loss_ce_7: 0.6189  loss_mask_7: 0.06604  loss_dice_7: 0.5655  loss_ce_8: 0.6855  loss_mask_8: 0.07226  loss_dice_8: 0.7061    time: 0.3721  last_time: 0.3268  data_time: 0.0075  last_data_time: 0.0111   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:35 d2.utils.events]:  eta: 0:06:19  iter: 8359  total_loss: 18.92  loss_ce: 0.5944  loss_mask: 0.06454  loss_dice: 0.7168  loss_ce_0: 1.21  loss_mask_0: 0.0719  loss_dice_0: 1.052  loss_ce_1: 1.072  loss_mask_1: 0.07012  loss_dice_1: 0.8164  loss_ce_2: 0.972  loss_mask_2: 0.06782  loss_dice_2: 0.8392  loss_ce_3: 0.6832  loss_mask_3: 0.0599  loss_dice_3: 0.943  loss_ce_4: 0.7717  loss_mask_4: 0.0563  loss_dice_4: 0.7568  loss_ce_5: 0.673  loss_mask_5: 0.05642  loss_dice_5: 0.72  loss_ce_6: 0.6451  loss_mask_6: 0.05909  loss_dice_6: 0.8562  loss_ce_7: 0.6393  loss_mask_7: 0.05703  loss_dice_7: 0.7611  loss_ce_8: 0.589  loss_mask_8: 0.05525  loss_dice_8: 0.7961    time: 0.3720  last_time: 0.3286  data_time: 0.0075  last_data_time: 0.0062   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:41 d2.utils.events]:  eta: 0:06:12  iter: 8379  total_loss: 22.13  loss_ce: 0.8627  loss_mask: 0.07967  loss_dice: 0.7461  loss_ce_0: 1.42  loss_mask_0: 0.07204  loss_dice_0: 1.004  loss_ce_1: 1.264  loss_mask_1: 0.1111  loss_dice_1: 0.8402  loss_ce_2: 1.223  loss_mask_2: 0.09345  loss_dice_2: 0.8068  loss_ce_3: 0.9011  loss_mask_3: 0.09542  loss_dice_3: 0.8668  loss_ce_4: 0.9566  loss_mask_4: 0.09934  loss_dice_4: 0.8473  loss_ce_5: 0.7798  loss_mask_5: 0.08742  loss_dice_5: 0.7652  loss_ce_6: 0.7219  loss_mask_6: 0.07965  loss_dice_6: 0.7767  loss_ce_7: 0.7715  loss_mask_7: 0.08366  loss_dice_7: 0.8862  loss_ce_8: 0.7813  loss_mask_8: 0.08344  loss_dice_8: 0.7202    time: 0.3718  last_time: 0.4175  data_time: 0.0200  last_data_time: 0.0982   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:47 d2.utils.events]:  eta: 0:06:06  iter: 8399  total_loss: 15.84  loss_ce: 0.8412  loss_mask: 0.05505  loss_dice: 0.4857  loss_ce_0: 1.461  loss_mask_0: 0.06935  loss_dice_0: 0.5547  loss_ce_1: 1.108  loss_mask_1: 0.05037  loss_dice_1: 0.5615  loss_ce_2: 0.9808  loss_mask_2: 0.05355  loss_dice_2: 0.4233  loss_ce_3: 0.847  loss_mask_3: 0.05588  loss_dice_3: 0.4418  loss_ce_4: 0.8088  loss_mask_4: 0.0528  loss_dice_4: 0.5686  loss_ce_5: 0.8128  loss_mask_5: 0.04825  loss_dice_5: 0.4859  loss_ce_6: 0.8479  loss_mask_6: 0.04724  loss_dice_6: 0.6151  loss_ce_7: 0.8268  loss_mask_7: 0.05231  loss_dice_7: 0.4452  loss_ce_8: 0.795  loss_mask_8: 0.05063  loss_dice_8: 0.5453    time: 0.3717  last_time: 0.3280  data_time: 0.0064  last_data_time: 0.0051   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:46:53 d2.utils.events]:  eta: 0:06:00  iter: 8419  total_loss: 15.78  loss_ce: 0.6938  loss_mask: 0.04887  loss_dice: 0.6443  loss_ce_0: 1.47  loss_mask_0: 0.05267  loss_dice_0: 0.7656  loss_ce_1: 0.976  loss_mask_1: 0.06011  loss_dice_1: 0.6776  loss_ce_2: 0.8294  loss_mask_2: 0.05678  loss_dice_2: 0.6282  loss_ce_3: 0.6598  loss_mask_3: 0.04828  loss_dice_3: 0.5254  loss_ce_4: 0.6481  loss_mask_4: 0.0509  loss_dice_4: 0.726  loss_ce_5: 0.6327  loss_mask_5: 0.04802  loss_dice_5: 0.7284  loss_ce_6: 0.6867  loss_mask_6: 0.04502  loss_dice_6: 0.7698  loss_ce_7: 0.6713  loss_mask_7: 0.04808  loss_dice_7: 0.6771  loss_ce_8: 0.6016  loss_mask_8: 0.04754  loss_dice_8: 0.5692    time: 0.3715  last_time: 0.2961  data_time: 0.0063  last_data_time: 0.0063   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:00 d2.utils.events]:  eta: 0:05:54  iter: 8439  total_loss: 10.72  loss_ce: 0.3532  loss_mask: 0.03908  loss_dice: 0.3587  loss_ce_0: 1.024  loss_mask_0: 0.03749  loss_dice_0: 0.5504  loss_ce_1: 0.7094  loss_mask_1: 0.03593  loss_dice_1: 0.4745  loss_ce_2: 0.5756  loss_mask_2: 0.03487  loss_dice_2: 0.4559  loss_ce_3: 0.5273  loss_mask_3: 0.03792  loss_dice_3: 0.6808  loss_ce_4: 0.434  loss_mask_4: 0.03581  loss_dice_4: 0.5094  loss_ce_5: 0.5248  loss_mask_5: 0.04008  loss_dice_5: 0.3878  loss_ce_6: 0.4482  loss_mask_6: 0.03824  loss_dice_6: 0.4129  loss_ce_7: 0.4266  loss_mask_7: 0.0365  loss_dice_7: 0.4871  loss_ce_8: 0.4431  loss_mask_8: 0.0365  loss_dice_8: 0.3533    time: 0.3714  last_time: 0.3255  data_time: 0.0121  last_data_time: 0.0063   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:06 d2.utils.events]:  eta: 0:05:48  iter: 8459  total_loss: 13.99  loss_ce: 0.5799  loss_mask: 0.04177  loss_dice: 0.3865  loss_ce_0: 1.253  loss_mask_0: 0.05493  loss_dice_0: 0.6111  loss_ce_1: 0.9074  loss_mask_1: 0.04685  loss_dice_1: 0.6494  loss_ce_2: 0.8336  loss_mask_2: 0.0542  loss_dice_2: 0.6336  loss_ce_3: 0.6913  loss_mask_3: 0.04797  loss_dice_3: 0.4792  loss_ce_4: 0.7165  loss_mask_4: 0.04754  loss_dice_4: 0.532  loss_ce_5: 0.7305  loss_mask_5: 0.04575  loss_dice_5: 0.3825  loss_ce_6: 0.5853  loss_mask_6: 0.04442  loss_dice_6: 0.3799  loss_ce_7: 0.6988  loss_mask_7: 0.04167  loss_dice_7: 0.4072  loss_ce_8: 0.6892  loss_mask_8: 0.04172  loss_dice_8: 0.5339    time: 0.3713  last_time: 0.2965  data_time: 0.0072  last_data_time: 0.0018   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:12 d2.utils.events]:  eta: 0:05:42  iter: 8479  total_loss: 18.34  loss_ce: 0.9271  loss_mask: 0.1002  loss_dice: 0.9082  loss_ce_0: 1.847  loss_mask_0: 0.1416  loss_dice_0: 0.8967  loss_ce_1: 1.245  loss_mask_1: 0.164  loss_dice_1: 0.7942  loss_ce_2: 1.039  loss_mask_2: 0.1532  loss_dice_2: 0.6717  loss_ce_3: 0.9955  loss_mask_3: 0.1329  loss_dice_3: 0.6708  loss_ce_4: 0.9183  loss_mask_4: 0.1359  loss_dice_4: 0.6085  loss_ce_5: 0.8484  loss_mask_5: 0.1133  loss_dice_5: 0.6272  loss_ce_6: 0.7879  loss_mask_6: 0.1077  loss_dice_6: 0.6078  loss_ce_7: 0.7679  loss_mask_7: 0.09795  loss_dice_7: 0.6725  loss_ce_8: 0.7584  loss_mask_8: 0.1101  loss_dice_8: 0.6405    time: 0.3711  last_time: 0.3161  data_time: 0.0068  last_data_time: 0.0083   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:18 d2.utils.events]:  eta: 0:05:36  iter: 8499  total_loss: 12.51  loss_ce: 0.3882  loss_mask: 0.03727  loss_dice: 0.3748  loss_ce_0: 1.234  loss_mask_0: 0.07353  loss_dice_0: 0.5898  loss_ce_1: 0.7575  loss_mask_1: 0.0602  loss_dice_1: 0.5  loss_ce_2: 0.5974  loss_mask_2: 0.05574  loss_dice_2: 0.5402  loss_ce_3: 0.5215  loss_mask_3: 0.051  loss_dice_3: 0.4863  loss_ce_4: 0.4888  loss_mask_4: 0.05048  loss_dice_4: 0.5173  loss_ce_5: 0.4778  loss_mask_5: 0.05479  loss_dice_5: 0.4571  loss_ce_6: 0.4234  loss_mask_6: 0.03836  loss_dice_6: 0.5582  loss_ce_7: 0.4055  loss_mask_7: 0.04302  loss_dice_7: 0.5441  loss_ce_8: 0.401  loss_mask_8: 0.04135  loss_dice_8: 0.4424    time: 0.3710  last_time: 0.3059  data_time: 0.0065  last_data_time: 0.0079   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:25 d2.utils.events]:  eta: 0:05:30  iter: 8519  total_loss: 11.99  loss_ce: 0.3993  loss_mask: 0.0783  loss_dice: 0.5671  loss_ce_0: 1.153  loss_mask_0: 0.09238  loss_dice_0: 0.8001  loss_ce_1: 0.6981  loss_mask_1: 0.09342  loss_dice_1: 0.524  loss_ce_2: 0.5596  loss_mask_2: 0.08648  loss_dice_2: 0.6066  loss_ce_3: 0.4738  loss_mask_3: 0.07524  loss_dice_3: 0.5148  loss_ce_4: 0.3949  loss_mask_4: 0.08756  loss_dice_4: 0.5276  loss_ce_5: 0.4067  loss_mask_5: 0.07529  loss_dice_5: 0.5416  loss_ce_6: 0.3813  loss_mask_6: 0.08783  loss_dice_6: 0.4105  loss_ce_7: 0.3806  loss_mask_7: 0.07863  loss_dice_7: 0.5792  loss_ce_8: 0.407  loss_mask_8: 0.07579  loss_dice_8: 0.5388    time: 0.3708  last_time: 0.3220  data_time: 0.0070  last_data_time: 0.0086   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:31 d2.utils.events]:  eta: 0:05:24  iter: 8539  total_loss: 12.81  loss_ce: 0.3891  loss_mask: 0.05863  loss_dice: 0.6133  loss_ce_0: 1.211  loss_mask_0: 0.06677  loss_dice_0: 0.5938  loss_ce_1: 0.6458  loss_mask_1: 0.05349  loss_dice_1: 0.5386  loss_ce_2: 0.6957  loss_mask_2: 0.05941  loss_dice_2: 0.4853  loss_ce_3: 0.6344  loss_mask_3: 0.05588  loss_dice_3: 0.4356  loss_ce_4: 0.5104  loss_mask_4: 0.05213  loss_dice_4: 0.5607  loss_ce_5: 0.496  loss_mask_5: 0.06335  loss_dice_5: 0.6159  loss_ce_6: 0.4542  loss_mask_6: 0.05764  loss_dice_6: 0.5462  loss_ce_7: 0.4598  loss_mask_7: 0.07488  loss_dice_7: 0.519  loss_ce_8: 0.4865  loss_mask_8: 0.05984  loss_dice_8: 0.5658    time: 0.3707  last_time: 0.3324  data_time: 0.0072  last_data_time: 0.0093   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:37 d2.utils.events]:  eta: 0:05:18  iter: 8559  total_loss: 17.99  loss_ce: 0.6615  loss_mask: 0.03329  loss_dice: 0.806  loss_ce_0: 1.745  loss_mask_0: 0.06706  loss_dice_0: 0.9328  loss_ce_1: 1.041  loss_mask_1: 0.04287  loss_dice_1: 0.9927  loss_ce_2: 0.8748  loss_mask_2: 0.0324  loss_dice_2: 1.033  loss_ce_3: 0.6726  loss_mask_3: 0.03287  loss_dice_3: 0.8284  loss_ce_4: 0.7941  loss_mask_4: 0.03206  loss_dice_4: 0.786  loss_ce_5: 0.7105  loss_mask_5: 0.0335  loss_dice_5: 0.9627  loss_ce_6: 0.6612  loss_mask_6: 0.03264  loss_dice_6: 0.7781  loss_ce_7: 0.6666  loss_mask_7: 0.03477  loss_dice_7: 0.8102  loss_ce_8: 0.5823  loss_mask_8: 0.03215  loss_dice_8: 0.6469    time: 0.3706  last_time: 0.3070  data_time: 0.0189  last_data_time: 0.0085   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:44 d2.utils.events]:  eta: 0:05:11  iter: 8579  total_loss: 15.18  loss_ce: 0.6279  loss_mask: 0.06145  loss_dice: 0.6898  loss_ce_0: 1.095  loss_mask_0: 0.0625  loss_dice_0: 0.7388  loss_ce_1: 0.7874  loss_mask_1: 0.06564  loss_dice_1: 0.856  loss_ce_2: 0.6315  loss_mask_2: 0.06053  loss_dice_2: 0.773  loss_ce_3: 0.579  loss_mask_3: 0.06005  loss_dice_3: 0.6815  loss_ce_4: 0.5724  loss_mask_4: 0.06208  loss_dice_4: 0.7279  loss_ce_5: 0.6258  loss_mask_5: 0.05905  loss_dice_5: 0.674  loss_ce_6: 0.6196  loss_mask_6: 0.05797  loss_dice_6: 0.5542  loss_ce_7: 0.7058  loss_mask_7: 0.06527  loss_dice_7: 0.6115  loss_ce_8: 0.6508  loss_mask_8: 0.06219  loss_dice_8: 0.6477    time: 0.3704  last_time: 0.3026  data_time: 0.0066  last_data_time: 0.0102   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:50 d2.utils.events]:  eta: 0:05:05  iter: 8599  total_loss: 14.54  loss_ce: 0.6119  loss_mask: 0.05059  loss_dice: 0.5862  loss_ce_0: 1.304  loss_mask_0: 0.06328  loss_dice_0: 0.958  loss_ce_1: 0.8441  loss_mask_1: 0.069  loss_dice_1: 0.669  loss_ce_2: 0.7622  loss_mask_2: 0.06416  loss_dice_2: 0.6973  loss_ce_3: 0.6637  loss_mask_3: 0.07798  loss_dice_3: 0.5646  loss_ce_4: 0.6737  loss_mask_4: 0.04934  loss_dice_4: 0.6399  loss_ce_5: 0.6078  loss_mask_5: 0.05205  loss_dice_5: 0.7732  loss_ce_6: 0.597  loss_mask_6: 0.05586  loss_dice_6: 0.6347  loss_ce_7: 0.5457  loss_mask_7: 0.05208  loss_dice_7: 0.6717  loss_ce_8: 0.5903  loss_mask_8: 0.05549  loss_dice_8: 0.716    time: 0.3703  last_time: 0.3352  data_time: 0.0078  last_data_time: 0.0061   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:47:56 d2.utils.events]:  eta: 0:04:59  iter: 8619  total_loss: 16.09  loss_ce: 0.6769  loss_mask: 0.06025  loss_dice: 0.5433  loss_ce_0: 1.292  loss_mask_0: 0.0725  loss_dice_0: 0.7409  loss_ce_1: 1.032  loss_mask_1: 0.05889  loss_dice_1: 0.7367  loss_ce_2: 0.9244  loss_mask_2: 0.06493  loss_dice_2: 0.8958  loss_ce_3: 0.8027  loss_mask_3: 0.06132  loss_dice_3: 0.65  loss_ce_4: 0.7499  loss_mask_4: 0.06627  loss_dice_4: 0.6009  loss_ce_5: 0.7439  loss_mask_5: 0.05981  loss_dice_5: 0.5412  loss_ce_6: 0.7173  loss_mask_6: 0.06437  loss_dice_6: 0.8057  loss_ce_7: 0.7263  loss_mask_7: 0.0696  loss_dice_7: 0.5938  loss_ce_8: 0.705  loss_mask_8: 0.0602  loss_dice_8: 0.6605    time: 0.3702  last_time: 0.3028  data_time: 0.0085  last_data_time: 0.0066   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:02 d2.utils.events]:  eta: 0:04:53  iter: 8639  total_loss: 24.61  loss_ce: 0.809  loss_mask: 0.05043  loss_dice: 1.177  loss_ce_0: 1.511  loss_mask_0: 0.04084  loss_dice_0: 1.307  loss_ce_1: 1.315  loss_mask_1: 0.05335  loss_dice_1: 1.244  loss_ce_2: 1.089  loss_mask_2: 0.07025  loss_dice_2: 1.053  loss_ce_3: 0.9164  loss_mask_3: 0.05398  loss_dice_3: 1.089  loss_ce_4: 0.8561  loss_mask_4: 0.04823  loss_dice_4: 1.053  loss_ce_5: 0.9101  loss_mask_5: 0.0485  loss_dice_5: 1.269  loss_ce_6: 0.8803  loss_mask_6: 0.05178  loss_dice_6: 0.9992  loss_ce_7: 0.8373  loss_mask_7: 0.04593  loss_dice_7: 1.141  loss_ce_8: 0.9427  loss_mask_8: 0.04759  loss_dice_8: 1.091    time: 0.3700  last_time: 0.3017  data_time: 0.0109  last_data_time: 0.0061   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:09 d2.utils.events]:  eta: 0:04:47  iter: 8659  total_loss: 25.04  loss_ce: 0.8805  loss_mask: 0.1546  loss_dice: 1.23  loss_ce_0: 1.58  loss_mask_0: 0.1693  loss_dice_0: 1.413  loss_ce_1: 1.163  loss_mask_1: 0.1795  loss_dice_1: 1.303  loss_ce_2: 1.143  loss_mask_2: 0.1887  loss_dice_2: 1.234  loss_ce_3: 1.057  loss_mask_3: 0.1879  loss_dice_3: 1.235  loss_ce_4: 0.974  loss_mask_4: 0.1928  loss_dice_4: 1.12  loss_ce_5: 0.8714  loss_mask_5: 0.1879  loss_dice_5: 1.169  loss_ce_6: 0.8973  loss_mask_6: 0.1866  loss_dice_6: 1.162  loss_ce_7: 0.894  loss_mask_7: 0.1759  loss_dice_7: 1.095  loss_ce_8: 0.8994  loss_mask_8: 0.1693  loss_dice_8: 1.094    time: 0.3699  last_time: 0.2960  data_time: 0.0090  last_data_time: 0.0067   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:15 d2.utils.events]:  eta: 0:04:41  iter: 8679  total_loss: 12.08  loss_ce: 0.6256  loss_mask: 0.08917  loss_dice: 0.5579  loss_ce_0: 1.042  loss_mask_0: 0.09531  loss_dice_0: 0.9057  loss_ce_1: 0.7966  loss_mask_1: 0.09598  loss_dice_1: 0.7453  loss_ce_2: 0.6454  loss_mask_2: 0.1048  loss_dice_2: 0.7244  loss_ce_3: 0.5554  loss_mask_3: 0.09634  loss_dice_3: 0.6071  loss_ce_4: 0.56  loss_mask_4: 0.089  loss_dice_4: 0.7875  loss_ce_5: 0.5851  loss_mask_5: 0.0999  loss_dice_5: 0.6516  loss_ce_6: 0.647  loss_mask_6: 0.1014  loss_dice_6: 0.6293  loss_ce_7: 0.6314  loss_mask_7: 0.09066  loss_dice_7: 0.5929  loss_ce_8: 0.6309  loss_mask_8: 0.08675  loss_dice_8: 0.6199    time: 0.3698  last_time: 0.3067  data_time: 0.0132  last_data_time: 0.0099   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:21 d2.utils.events]:  eta: 0:04:35  iter: 8699  total_loss: 23.59  loss_ce: 0.938  loss_mask: 0.09609  loss_dice: 1.01  loss_ce_0: 1.282  loss_mask_0: 0.142  loss_dice_0: 1.123  loss_ce_1: 1.056  loss_mask_1: 0.1206  loss_dice_1: 1.056  loss_ce_2: 0.9522  loss_mask_2: 0.1082  loss_dice_2: 0.8977  loss_ce_3: 0.9155  loss_mask_3: 0.09793  loss_dice_3: 0.8957  loss_ce_4: 0.8975  loss_mask_4: 0.1031  loss_dice_4: 0.9816  loss_ce_5: 0.8952  loss_mask_5: 0.0926  loss_dice_5: 0.9755  loss_ce_6: 0.8873  loss_mask_6: 0.08863  loss_dice_6: 1.061  loss_ce_7: 0.9615  loss_mask_7: 0.08964  loss_dice_7: 1.039  loss_ce_8: 0.9294  loss_mask_8: 0.09477  loss_dice_8: 1.049    time: 0.3696  last_time: 0.3233  data_time: 0.0070  last_data_time: 0.0112   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:27 d2.utils.events]:  eta: 0:04:29  iter: 8719  total_loss: 18.2  loss_ce: 0.8484  loss_mask: 0.05803  loss_dice: 0.6455  loss_ce_0: 1.352  loss_mask_0: 0.09989  loss_dice_0: 0.8996  loss_ce_1: 1.023  loss_mask_1: 0.08191  loss_dice_1: 0.6842  loss_ce_2: 0.9329  loss_mask_2: 0.08985  loss_dice_2: 0.7566  loss_ce_3: 0.8755  loss_mask_3: 0.06228  loss_dice_3: 0.6081  loss_ce_4: 0.783  loss_mask_4: 0.05861  loss_dice_4: 0.768  loss_ce_5: 0.8204  loss_mask_5: 0.06933  loss_dice_5: 0.6851  loss_ce_6: 0.8418  loss_mask_6: 0.06109  loss_dice_6: 0.5713  loss_ce_7: 0.8553  loss_mask_7: 0.06374  loss_dice_7: 0.6609  loss_ce_8: 0.8402  loss_mask_8: 0.05206  loss_dice_8: 0.6765    time: 0.3695  last_time: 0.3119  data_time: 0.0088  last_data_time: 0.0102   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:34 d2.utils.events]:  eta: 0:04:23  iter: 8739  total_loss: 17.95  loss_ce: 0.6402  loss_mask: 0.04608  loss_dice: 0.7131  loss_ce_0: 1.669  loss_mask_0: 0.05931  loss_dice_0: 0.9738  loss_ce_1: 0.8809  loss_mask_1: 0.0518  loss_dice_1: 0.7863  loss_ce_2: 0.8472  loss_mask_2: 0.0417  loss_dice_2: 0.6384  loss_ce_3: 0.7808  loss_mask_3: 0.04459  loss_dice_3: 0.8352  loss_ce_4: 0.732  loss_mask_4: 0.04327  loss_dice_4: 0.8042  loss_ce_5: 0.6853  loss_mask_5: 0.05237  loss_dice_5: 0.7676  loss_ce_6: 0.5665  loss_mask_6: 0.04803  loss_dice_6: 0.6098  loss_ce_7: 0.5672  loss_mask_7: 0.04777  loss_dice_7: 0.6154  loss_ce_8: 0.5623  loss_mask_8: 0.04993  loss_dice_8: 0.5519    time: 0.3694  last_time: 0.3090  data_time: 0.0171  last_data_time: 0.0060   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:40 d2.utils.events]:  eta: 0:04:16  iter: 8759  total_loss: 17.16  loss_ce: 0.518  loss_mask: 0.0513  loss_dice: 0.83  loss_ce_0: 1.081  loss_mask_0: 0.06485  loss_dice_0: 0.9351  loss_ce_1: 0.7802  loss_mask_1: 0.05491  loss_dice_1: 0.8216  loss_ce_2: 0.6665  loss_mask_2: 0.05536  loss_dice_2: 0.7459  loss_ce_3: 0.5368  loss_mask_3: 0.04783  loss_dice_3: 0.7242  loss_ce_4: 0.553  loss_mask_4: 0.06146  loss_dice_4: 0.8317  loss_ce_5: 0.5498  loss_mask_5: 0.05422  loss_dice_5: 0.7196  loss_ce_6: 0.5306  loss_mask_6: 0.05195  loss_dice_6: 0.87  loss_ce_7: 0.5226  loss_mask_7: 0.05098  loss_dice_7: 0.8552  loss_ce_8: 0.5618  loss_mask_8: 0.0519  loss_dice_8: 0.6396    time: 0.3693  last_time: 0.3248  data_time: 0.0065  last_data_time: 0.0072   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:46 d2.utils.events]:  eta: 0:04:10  iter: 8779  total_loss: 16.09  loss_ce: 0.5616  loss_mask: 0.0757  loss_dice: 0.7253  loss_ce_0: 1.239  loss_mask_0: 0.05864  loss_dice_0: 0.9164  loss_ce_1: 0.7249  loss_mask_1: 0.06391  loss_dice_1: 0.7669  loss_ce_2: 0.6279  loss_mask_2: 0.08921  loss_dice_2: 0.7934  loss_ce_3: 0.5953  loss_mask_3: 0.05922  loss_dice_3: 0.8619  loss_ce_4: 0.6118  loss_mask_4: 0.06895  loss_dice_4: 0.972  loss_ce_5: 0.5707  loss_mask_5: 0.05198  loss_dice_5: 0.8384  loss_ce_6: 0.6187  loss_mask_6: 0.06446  loss_dice_6: 0.907  loss_ce_7: 0.5577  loss_mask_7: 0.04917  loss_dice_7: 0.9782  loss_ce_8: 0.5956  loss_mask_8: 0.04846  loss_dice_8: 0.7684    time: 0.3691  last_time: 0.3330  data_time: 0.0107  last_data_time: 0.0092   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:53 d2.utils.events]:  eta: 0:04:04  iter: 8799  total_loss: 18.99  loss_ce: 0.7403  loss_mask: 0.07532  loss_dice: 0.5695  loss_ce_0: 1.294  loss_mask_0: 0.09352  loss_dice_0: 0.82  loss_ce_1: 0.9408  loss_mask_1: 0.09395  loss_dice_1: 0.6573  loss_ce_2: 0.805  loss_mask_2: 0.08775  loss_dice_2: 0.6617  loss_ce_3: 0.747  loss_mask_3: 0.08155  loss_dice_3: 0.561  loss_ce_4: 0.6921  loss_mask_4: 0.09081  loss_dice_4: 0.6379  loss_ce_5: 0.6934  loss_mask_5: 0.08415  loss_dice_5: 0.6576  loss_ce_6: 0.7017  loss_mask_6: 0.07678  loss_dice_6: 0.5539  loss_ce_7: 0.6953  loss_mask_7: 0.07691  loss_dice_7: 0.5977  loss_ce_8: 0.7159  loss_mask_8: 0.08368  loss_dice_8: 0.5631    time: 0.3690  last_time: 0.2990  data_time: 0.0073  last_data_time: 0.0068   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:48:59 d2.utils.events]:  eta: 0:03:58  iter: 8819  total_loss: 16  loss_ce: 0.7525  loss_mask: 0.06192  loss_dice: 0.5125  loss_ce_0: 1.319  loss_mask_0: 0.04998  loss_dice_0: 0.6525  loss_ce_1: 0.9439  loss_mask_1: 0.05896  loss_dice_1: 0.6363  loss_ce_2: 0.9895  loss_mask_2: 0.06703  loss_dice_2: 0.5538  loss_ce_3: 0.8698  loss_mask_3: 0.06375  loss_dice_3: 0.585  loss_ce_4: 0.8346  loss_mask_4: 0.06168  loss_dice_4: 0.4589  loss_ce_5: 0.8625  loss_mask_5: 0.06666  loss_dice_5: 0.4977  loss_ce_6: 0.7824  loss_mask_6: 0.06326  loss_dice_6: 0.6165  loss_ce_7: 0.8022  loss_mask_7: 0.05387  loss_dice_7: 0.5713  loss_ce_8: 0.7717  loss_mask_8: 0.06496  loss_dice_8: 0.5607    time: 0.3689  last_time: 0.3221  data_time: 0.0066  last_data_time: 0.0046   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:06 d2.utils.events]:  eta: 0:03:52  iter: 8839  total_loss: 15.23  loss_ce: 0.7129  loss_mask: 0.06094  loss_dice: 0.5794  loss_ce_0: 1.225  loss_mask_0: 0.08562  loss_dice_0: 0.7467  loss_ce_1: 0.7487  loss_mask_1: 0.08002  loss_dice_1: 0.5502  loss_ce_2: 0.708  loss_mask_2: 0.07115  loss_dice_2: 0.7076  loss_ce_3: 0.6862  loss_mask_3: 0.06856  loss_dice_3: 0.6803  loss_ce_4: 0.7597  loss_mask_4: 0.07577  loss_dice_4: 0.5103  loss_ce_5: 0.7429  loss_mask_5: 0.07025  loss_dice_5: 0.5481  loss_ce_6: 0.7194  loss_mask_6: 0.07327  loss_dice_6: 0.5424  loss_ce_7: 0.7199  loss_mask_7: 0.06724  loss_dice_7: 0.6151  loss_ce_8: 0.7091  loss_mask_8: 0.0614  loss_dice_8: 0.5044    time: 0.3688  last_time: 0.4190  data_time: 0.0405  last_data_time: 0.1007   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:12 d2.utils.events]:  eta: 0:03:46  iter: 8859  total_loss: 15.52  loss_ce: 0.839  loss_mask: 0.05838  loss_dice: 0.6569  loss_ce_0: 1.375  loss_mask_0: 0.07609  loss_dice_0: 0.7993  loss_ce_1: 0.881  loss_mask_1: 0.07403  loss_dice_1: 0.6377  loss_ce_2: 0.9451  loss_mask_2: 0.06623  loss_dice_2: 0.7453  loss_ce_3: 0.7346  loss_mask_3: 0.08859  loss_dice_3: 0.7608  loss_ce_4: 0.802  loss_mask_4: 0.06549  loss_dice_4: 0.6899  loss_ce_5: 0.7836  loss_mask_5: 0.05523  loss_dice_5: 0.5321  loss_ce_6: 0.8498  loss_mask_6: 0.05892  loss_dice_6: 0.7907  loss_ce_7: 0.834  loss_mask_7: 0.06387  loss_dice_7: 0.625  loss_ce_8: 0.835  loss_mask_8: 0.06109  loss_dice_8: 0.4807    time: 0.3687  last_time: 0.3223  data_time: 0.0066  last_data_time: 0.0042   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:18 d2.utils.events]:  eta: 0:03:40  iter: 8879  total_loss: 15.57  loss_ce: 0.6891  loss_mask: 0.08778  loss_dice: 0.5558  loss_ce_0: 1.719  loss_mask_0: 0.1035  loss_dice_0: 0.7649  loss_ce_1: 1.023  loss_mask_1: 0.09442  loss_dice_1: 0.6176  loss_ce_2: 0.8307  loss_mask_2: 0.1044  loss_dice_2: 0.4934  loss_ce_3: 0.797  loss_mask_3: 0.1006  loss_dice_3: 0.5392  loss_ce_4: 0.7213  loss_mask_4: 0.08805  loss_dice_4: 0.5032  loss_ce_5: 0.7236  loss_mask_5: 0.0916  loss_dice_5: 0.5698  loss_ce_6: 0.7247  loss_mask_6: 0.09026  loss_dice_6: 0.4935  loss_ce_7: 0.6805  loss_mask_7: 0.08919  loss_dice_7: 0.5311  loss_ce_8: 0.6801  loss_mask_8: 0.08745  loss_dice_8: 0.4829    time: 0.3686  last_time: 0.3289  data_time: 0.0116  last_data_time: 0.0074   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:25 d2.utils.events]:  eta: 0:03:34  iter: 8899  total_loss: 20.51  loss_ce: 0.694  loss_mask: 0.08598  loss_dice: 0.72  loss_ce_0: 1.276  loss_mask_0: 0.1064  loss_dice_0: 1.097  loss_ce_1: 0.9579  loss_mask_1: 0.1042  loss_dice_1: 1.122  loss_ce_2: 0.8592  loss_mask_2: 0.08793  loss_dice_2: 1.099  loss_ce_3: 0.7271  loss_mask_3: 0.08059  loss_dice_3: 0.9369  loss_ce_4: 0.7298  loss_mask_4: 0.08162  loss_dice_4: 0.8301  loss_ce_5: 0.6141  loss_mask_5: 0.08709  loss_dice_5: 0.6928  loss_ce_6: 0.7974  loss_mask_6: 0.08363  loss_dice_6: 0.8064  loss_ce_7: 0.6888  loss_mask_7: 0.08667  loss_dice_7: 0.7468  loss_ce_8: 0.6916  loss_mask_8: 0.08733  loss_dice_8: 0.6418    time: 0.3685  last_time: 0.3044  data_time: 0.0066  last_data_time: 0.0062   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:31 d2.utils.events]:  eta: 0:03:28  iter: 8919  total_loss: 29.42  loss_ce: 0.9972  loss_mask: 0.04746  loss_dice: 1.509  loss_ce_0: 1.615  loss_mask_0: 0.0639  loss_dice_0: 1.553  loss_ce_1: 1.431  loss_mask_1: 0.04787  loss_dice_1: 1.32  loss_ce_2: 1.381  loss_mask_2: 0.05637  loss_dice_2: 1.454  loss_ce_3: 1.299  loss_mask_3: 0.05988  loss_dice_3: 1.434  loss_ce_4: 1.195  loss_mask_4: 0.05941  loss_dice_4: 1.325  loss_ce_5: 1.067  loss_mask_5: 0.05779  loss_dice_5: 1.412  loss_ce_6: 1.128  loss_mask_6: 0.04868  loss_dice_6: 1.305  loss_ce_7: 1.083  loss_mask_7: 0.05338  loss_dice_7: 1.56  loss_ce_8: 1.113  loss_mask_8: 0.05228  loss_dice_8: 1.26    time: 0.3683  last_time: 0.3326  data_time: 0.0089  last_data_time: 0.0114   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:37 d2.utils.events]:  eta: 0:03:21  iter: 8939  total_loss: 23.14  loss_ce: 0.8963  loss_mask: 0.05208  loss_dice: 1.094  loss_ce_0: 1.989  loss_mask_0: 0.07169  loss_dice_0: 1.214  loss_ce_1: 1.263  loss_mask_1: 0.06555  loss_dice_1: 1.199  loss_ce_2: 1.044  loss_mask_2: 0.06741  loss_dice_2: 1.166  loss_ce_3: 0.8925  loss_mask_3: 0.06543  loss_dice_3: 0.9891  loss_ce_4: 0.8283  loss_mask_4: 0.0538  loss_dice_4: 1.042  loss_ce_5: 0.9503  loss_mask_5: 0.07625  loss_dice_5: 0.9924  loss_ce_6: 0.8335  loss_mask_6: 0.05052  loss_dice_6: 1.317  loss_ce_7: 0.7165  loss_mask_7: 0.05251  loss_dice_7: 1.271  loss_ce_8: 0.7701  loss_mask_8: 0.05772  loss_dice_8: 1.174    time: 0.3682  last_time: 0.3174  data_time: 0.0074  last_data_time: 0.0053   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:43 d2.utils.events]:  eta: 0:03:15  iter: 8959  total_loss: 15.25  loss_ce: 0.6289  loss_mask: 0.05902  loss_dice: 0.6852  loss_ce_0: 1.363  loss_mask_0: 0.06041  loss_dice_0: 0.751  loss_ce_1: 0.957  loss_mask_1: 0.07374  loss_dice_1: 0.9367  loss_ce_2: 0.8845  loss_mask_2: 0.06044  loss_dice_2: 0.7548  loss_ce_3: 0.8484  loss_mask_3: 0.0518  loss_dice_3: 0.7257  loss_ce_4: 0.7887  loss_mask_4: 0.06805  loss_dice_4: 0.6229  loss_ce_5: 0.7455  loss_mask_5: 0.06109  loss_dice_5: 0.7029  loss_ce_6: 0.7168  loss_mask_6: 0.05775  loss_dice_6: 0.5572  loss_ce_7: 0.7226  loss_mask_7: 0.06163  loss_dice_7: 0.6007  loss_ce_8: 0.7128  loss_mask_8: 0.06102  loss_dice_8: 0.6395    time: 0.3681  last_time: 0.3296  data_time: 0.0067  last_data_time: 0.0042   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:50 d2.utils.events]:  eta: 0:03:09  iter: 8979  total_loss: 18.15  loss_ce: 0.7001  loss_mask: 0.08273  loss_dice: 0.8116  loss_ce_0: 1.413  loss_mask_0: 0.09766  loss_dice_0: 0.9931  loss_ce_1: 0.9618  loss_mask_1: 0.06021  loss_dice_1: 0.774  loss_ce_2: 0.784  loss_mask_2: 0.08441  loss_dice_2: 0.6649  loss_ce_3: 0.7968  loss_mask_3: 0.09517  loss_dice_3: 0.7887  loss_ce_4: 0.7336  loss_mask_4: 0.0981  loss_dice_4: 0.7119  loss_ce_5: 0.8247  loss_mask_5: 0.07042  loss_dice_5: 0.8494  loss_ce_6: 0.7058  loss_mask_6: 0.09201  loss_dice_6: 1.02  loss_ce_7: 0.7705  loss_mask_7: 0.09345  loss_dice_7: 0.7736  loss_ce_8: 0.7754  loss_mask_8: 0.08638  loss_dice_8: 0.8678    time: 0.3679  last_time: 0.3014  data_time: 0.0064  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:49:56 d2.utils.events]:  eta: 0:03:03  iter: 8999  total_loss: 26.67  loss_ce: 1.012  loss_mask: 0.1968  loss_dice: 0.9153  loss_ce_0: 2.055  loss_mask_0: 0.1861  loss_dice_0: 1.274  loss_ce_1: 1.488  loss_mask_1: 0.1727  loss_dice_1: 1.185  loss_ce_2: 1.151  loss_mask_2: 0.1524  loss_dice_2: 1.075  loss_ce_3: 1.158  loss_mask_3: 0.1719  loss_dice_3: 1.106  loss_ce_4: 1.099  loss_mask_4: 0.1786  loss_dice_4: 1.014  loss_ce_5: 1.074  loss_mask_5: 0.1915  loss_dice_5: 1.07  loss_ce_6: 1.058  loss_mask_6: 0.1735  loss_dice_6: 0.9223  loss_ce_7: 0.9107  loss_mask_7: 0.1768  loss_dice_7: 1.164  loss_ce_8: 1.032  loss_mask_8: 0.1886  loss_dice_8: 1.102    time: 0.3678  last_time: 0.3031  data_time: 0.0115  last_data_time: 0.0050   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:02 d2.utils.events]:  eta: 0:02:57  iter: 9019  total_loss: 20.3  loss_ce: 0.8332  loss_mask: 0.04839  loss_dice: 0.8418  loss_ce_0: 1.534  loss_mask_0: 0.06723  loss_dice_0: 1.073  loss_ce_1: 1.001  loss_mask_1: 0.0557  loss_dice_1: 0.9535  loss_ce_2: 1.068  loss_mask_2: 0.05638  loss_dice_2: 0.8692  loss_ce_3: 0.9008  loss_mask_3: 0.06055  loss_dice_3: 0.7378  loss_ce_4: 0.8152  loss_mask_4: 0.06329  loss_dice_4: 0.9888  loss_ce_5: 0.8465  loss_mask_5: 0.04711  loss_dice_5: 0.8587  loss_ce_6: 0.8548  loss_mask_6: 0.04407  loss_dice_6: 0.8588  loss_ce_7: 0.8455  loss_mask_7: 0.04198  loss_dice_7: 0.7779  loss_ce_8: 0.8041  loss_mask_8: 0.04624  loss_dice_8: 0.7095    time: 0.3677  last_time: 0.3080  data_time: 0.0121  last_data_time: 0.0086   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:09 d2.utils.events]:  eta: 0:02:51  iter: 9039  total_loss: 22.79  loss_ce: 0.8083  loss_mask: 0.154  loss_dice: 0.9059  loss_ce_0: 1.461  loss_mask_0: 0.1718  loss_dice_0: 0.9555  loss_ce_1: 1.17  loss_mask_1: 0.1721  loss_dice_1: 1.005  loss_ce_2: 1.014  loss_mask_2: 0.1747  loss_dice_2: 1.137  loss_ce_3: 0.8721  loss_mask_3: 0.1535  loss_dice_3: 1.038  loss_ce_4: 0.8163  loss_mask_4: 0.1626  loss_dice_4: 0.9361  loss_ce_5: 0.8032  loss_mask_5: 0.1512  loss_dice_5: 0.8302  loss_ce_6: 0.8494  loss_mask_6: 0.1339  loss_dice_6: 0.6665  loss_ce_7: 0.8329  loss_mask_7: 0.1438  loss_dice_7: 0.872  loss_ce_8: 0.798  loss_mask_8: 0.1552  loss_dice_8: 0.9868    time: 0.3676  last_time: 0.3335  data_time: 0.0129  last_data_time: 0.0144   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:15 d2.utils.events]:  eta: 0:02:45  iter: 9059  total_loss: 16.92  loss_ce: 0.6119  loss_mask: 0.06369  loss_dice: 0.8418  loss_ce_0: 1.085  loss_mask_0: 0.06258  loss_dice_0: 1.128  loss_ce_1: 1.016  loss_mask_1: 0.05916  loss_dice_1: 0.8774  loss_ce_2: 0.8268  loss_mask_2: 0.04432  loss_dice_2: 0.9024  loss_ce_3: 0.6813  loss_mask_3: 0.05533  loss_dice_3: 0.9038  loss_ce_4: 0.7241  loss_mask_4: 0.06255  loss_dice_4: 1.015  loss_ce_5: 0.6014  loss_mask_5: 0.06356  loss_dice_5: 0.8061  loss_ce_6: 0.6138  loss_mask_6: 0.05927  loss_dice_6: 0.797  loss_ce_7: 0.6206  loss_mask_7: 0.05924  loss_dice_7: 0.8005  loss_ce_8: 0.6149  loss_mask_8: 0.06045  loss_dice_8: 0.7523    time: 0.3675  last_time: 0.3382  data_time: 0.0070  last_data_time: 0.0086   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:21 d2.utils.events]:  eta: 0:02:39  iter: 9079  total_loss: 18.16  loss_ce: 0.6959  loss_mask: 0.05888  loss_dice: 0.6462  loss_ce_0: 1.283  loss_mask_0: 0.07915  loss_dice_0: 0.8049  loss_ce_1: 0.9535  loss_mask_1: 0.06253  loss_dice_1: 0.8206  loss_ce_2: 0.7934  loss_mask_2: 0.05737  loss_dice_2: 0.9337  loss_ce_3: 0.7008  loss_mask_3: 0.04983  loss_dice_3: 0.6482  loss_ce_4: 0.6168  loss_mask_4: 0.05411  loss_dice_4: 0.6953  loss_ce_5: 0.6082  loss_mask_5: 0.06268  loss_dice_5: 0.8206  loss_ce_6: 0.7617  loss_mask_6: 0.05874  loss_dice_6: 0.8157  loss_ce_7: 0.6422  loss_mask_7: 0.05489  loss_dice_7: 0.69  loss_ce_8: 0.6419  loss_mask_8: 0.05966  loss_dice_8: 0.5945    time: 0.3674  last_time: 0.3067  data_time: 0.0107  last_data_time: 0.0062   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:28 d2.utils.events]:  eta: 0:02:33  iter: 9099  total_loss: 12.7  loss_ce: 0.4651  loss_mask: 0.05277  loss_dice: 0.372  loss_ce_0: 1.227  loss_mask_0: 0.07179  loss_dice_0: 0.8595  loss_ce_1: 0.7999  loss_mask_1: 0.0768  loss_dice_1: 0.4667  loss_ce_2: 0.6298  loss_mask_2: 0.07036  loss_dice_2: 0.5167  loss_ce_3: 0.475  loss_mask_3: 0.06165  loss_dice_3: 0.4239  loss_ce_4: 0.4261  loss_mask_4: 0.06353  loss_dice_4: 0.4661  loss_ce_5: 0.3854  loss_mask_5: 0.05519  loss_dice_5: 0.4242  loss_ce_6: 0.3806  loss_mask_6: 0.05785  loss_dice_6: 0.4422  loss_ce_7: 0.4291  loss_mask_7: 0.06035  loss_dice_7: 0.3934  loss_ce_8: 0.4656  loss_mask_8: 0.0511  loss_dice_8: 0.42    time: 0.3673  last_time: 0.3021  data_time: 0.0066  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:34 d2.utils.events]:  eta: 0:02:26  iter: 9119  total_loss: 19.45  loss_ce: 0.7682  loss_mask: 0.0903  loss_dice: 0.7597  loss_ce_0: 1.624  loss_mask_0: 0.1133  loss_dice_0: 1.064  loss_ce_1: 1.23  loss_mask_1: 0.08801  loss_dice_1: 0.7851  loss_ce_2: 1.025  loss_mask_2: 0.09318  loss_dice_2: 0.8797  loss_ce_3: 0.9232  loss_mask_3: 0.08968  loss_dice_3: 0.7976  loss_ce_4: 0.928  loss_mask_4: 0.08357  loss_dice_4: 0.7605  loss_ce_5: 0.8513  loss_mask_5: 0.09139  loss_dice_5: 0.756  loss_ce_6: 0.8176  loss_mask_6: 0.0982  loss_dice_6: 0.7275  loss_ce_7: 0.7751  loss_mask_7: 0.09024  loss_dice_7: 0.6801  loss_ce_8: 0.7327  loss_mask_8: 0.09464  loss_dice_8: 0.7615    time: 0.3672  last_time: 0.2994  data_time: 0.0100  last_data_time: 0.0059   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:41 d2.utils.events]:  eta: 0:02:20  iter: 9139  total_loss: 13.32  loss_ce: 0.5172  loss_mask: 0.06237  loss_dice: 0.5903  loss_ce_0: 1.002  loss_mask_0: 0.08121  loss_dice_0: 0.7119  loss_ce_1: 0.8286  loss_mask_1: 0.072  loss_dice_1: 0.5746  loss_ce_2: 0.664  loss_mask_2: 0.07023  loss_dice_2: 0.6545  loss_ce_3: 0.5801  loss_mask_3: 0.06804  loss_dice_3: 0.6108  loss_ce_4: 0.553  loss_mask_4: 0.06145  loss_dice_4: 0.5828  loss_ce_5: 0.5698  loss_mask_5: 0.06866  loss_dice_5: 0.6888  loss_ce_6: 0.5051  loss_mask_6: 0.06676  loss_dice_6: 0.5747  loss_ce_7: 0.4961  loss_mask_7: 0.0615  loss_dice_7: 0.4866  loss_ce_8: 0.5167  loss_mask_8: 0.06067  loss_dice_8: 0.6155    time: 0.3671  last_time: 0.3011  data_time: 0.0279  last_data_time: 0.0050   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:47 d2.utils.events]:  eta: 0:02:14  iter: 9159  total_loss: 20.97  loss_ce: 0.8472  loss_mask: 0.06115  loss_dice: 0.8476  loss_ce_0: 1.849  loss_mask_0: 0.07133  loss_dice_0: 1.02  loss_ce_1: 1.358  loss_mask_1: 0.06545  loss_dice_1: 0.8261  loss_ce_2: 1.125  loss_mask_2: 0.07234  loss_dice_2: 0.8293  loss_ce_3: 1.011  loss_mask_3: 0.07661  loss_dice_3: 0.9735  loss_ce_4: 1.06  loss_mask_4: 0.06553  loss_dice_4: 0.7828  loss_ce_5: 1.053  loss_mask_5: 0.05909  loss_dice_5: 0.7852  loss_ce_6: 0.976  loss_mask_6: 0.06199  loss_dice_6: 0.8985  loss_ce_7: 0.96  loss_mask_7: 0.06875  loss_dice_7: 0.9262  loss_ce_8: 0.8134  loss_mask_8: 0.05879  loss_dice_8: 0.6981    time: 0.3670  last_time: 0.3002  data_time: 0.0130  last_data_time: 0.0063   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:50:53 d2.utils.events]:  eta: 0:02:08  iter: 9179  total_loss: 18.58  loss_ce: 0.6823  loss_mask: 0.06662  loss_dice: 0.7924  loss_ce_0: 1.468  loss_mask_0: 0.08097  loss_dice_0: 0.8064  loss_ce_1: 1.081  loss_mask_1: 0.07609  loss_dice_1: 0.9519  loss_ce_2: 1.007  loss_mask_2: 0.071  loss_dice_2: 0.8113  loss_ce_3: 0.7731  loss_mask_3: 0.07052  loss_dice_3: 0.7162  loss_ce_4: 0.6746  loss_mask_4: 0.06588  loss_dice_4: 0.702  loss_ce_5: 0.7618  loss_mask_5: 0.06974  loss_dice_5: 1.004  loss_ce_6: 0.7008  loss_mask_6: 0.06372  loss_dice_6: 0.5184  loss_ce_7: 0.6741  loss_mask_7: 0.0677  loss_dice_7: 0.7635  loss_ce_8: 0.6786  loss_mask_8: 0.0672  loss_dice_8: 0.7386    time: 0.3669  last_time: 0.2991  data_time: 0.0066  last_data_time: 0.0063   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:00 d2.utils.events]:  eta: 0:02:02  iter: 9199  total_loss: 20.29  loss_ce: 0.9639  loss_mask: 0.1043  loss_dice: 0.8164  loss_ce_0: 1.504  loss_mask_0: 0.09728  loss_dice_0: 1.176  loss_ce_1: 1.167  loss_mask_1: 0.133  loss_dice_1: 1.101  loss_ce_2: 1.179  loss_mask_2: 0.1137  loss_dice_2: 0.9195  loss_ce_3: 1.129  loss_mask_3: 0.09479  loss_dice_3: 0.7739  loss_ce_4: 0.9133  loss_mask_4: 0.108  loss_dice_4: 0.6994  loss_ce_5: 0.9779  loss_mask_5: 0.09556  loss_dice_5: 0.8031  loss_ce_6: 0.9012  loss_mask_6: 0.09525  loss_dice_6: 0.9848  loss_ce_7: 0.8125  loss_mask_7: 0.09939  loss_dice_7: 0.7444  loss_ce_8: 0.8424  loss_mask_8: 0.09902  loss_dice_8: 0.6819    time: 0.3667  last_time: 0.3027  data_time: 0.0100  last_data_time: 0.0079   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:06 d2.utils.events]:  eta: 0:01:56  iter: 9219  total_loss: 15.79  loss_ce: 0.5047  loss_mask: 0.08125  loss_dice: 0.6509  loss_ce_0: 1.252  loss_mask_0: 0.06867  loss_dice_0: 0.5963  loss_ce_1: 0.7967  loss_mask_1: 0.09989  loss_dice_1: 0.602  loss_ce_2: 0.7461  loss_mask_2: 0.09893  loss_dice_2: 0.5355  loss_ce_3: 0.6583  loss_mask_3: 0.06901  loss_dice_3: 0.7461  loss_ce_4: 0.6401  loss_mask_4: 0.0729  loss_dice_4: 0.6605  loss_ce_5: 0.5376  loss_mask_5: 0.07903  loss_dice_5: 0.6548  loss_ce_6: 0.6806  loss_mask_6: 0.07293  loss_dice_6: 0.6238  loss_ce_7: 0.5153  loss_mask_7: 0.07634  loss_dice_7: 0.6129  loss_ce_8: 0.4913  loss_mask_8: 0.0782  loss_dice_8: 0.6432    time: 0.3666  last_time: 0.3005  data_time: 0.0071  last_data_time: 0.0076   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:13 d2.utils.events]:  eta: 0:01:50  iter: 9239  total_loss: 20  loss_ce: 0.8427  loss_mask: 0.05247  loss_dice: 0.7121  loss_ce_0: 1.589  loss_mask_0: 0.05896  loss_dice_0: 0.9547  loss_ce_1: 1.165  loss_mask_1: 0.06731  loss_dice_1: 0.7126  loss_ce_2: 1.016  loss_mask_2: 0.05275  loss_dice_2: 0.7975  loss_ce_3: 0.8419  loss_mask_3: 0.0514  loss_dice_3: 0.8646  loss_ce_4: 0.9099  loss_mask_4: 0.05473  loss_dice_4: 0.7957  loss_ce_5: 0.8703  loss_mask_5: 0.05471  loss_dice_5: 0.6861  loss_ce_6: 0.822  loss_mask_6: 0.05764  loss_dice_6: 0.7637  loss_ce_7: 0.9436  loss_mask_7: 0.05437  loss_dice_7: 0.7308  loss_ce_8: 0.8241  loss_mask_8: 0.05024  loss_dice_8: 0.638    time: 0.3665  last_time: 0.3243  data_time: 0.0140  last_data_time: 0.0053   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:19 d2.utils.events]:  eta: 0:01:44  iter: 9259  total_loss: 19.7  loss_ce: 0.775  loss_mask: 0.1302  loss_dice: 0.9101  loss_ce_0: 1.697  loss_mask_0: 0.0772  loss_dice_0: 0.9808  loss_ce_1: 0.9453  loss_mask_1: 0.0637  loss_dice_1: 0.9439  loss_ce_2: 0.8712  loss_mask_2: 0.0654  loss_dice_2: 0.8606  loss_ce_3: 0.7754  loss_mask_3: 0.09248  loss_dice_3: 0.8753  loss_ce_4: 0.7787  loss_mask_4: 0.1007  loss_dice_4: 0.7551  loss_ce_5: 0.7738  loss_mask_5: 0.09628  loss_dice_5: 0.8234  loss_ce_6: 0.7693  loss_mask_6: 0.1108  loss_dice_6: 0.9114  loss_ce_7: 0.6864  loss_mask_7: 0.1059  loss_dice_7: 0.9645  loss_ce_8: 0.7358  loss_mask_8: 0.09223  loss_dice_8: 0.8641    time: 0.3664  last_time: 0.3236  data_time: 0.0135  last_data_time: 0.0073   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:25 d2.utils.events]:  eta: 0:01:38  iter: 9279  total_loss: 15.26  loss_ce: 0.5696  loss_mask: 0.04813  loss_dice: 0.4738  loss_ce_0: 1.265  loss_mask_0: 0.07561  loss_dice_0: 0.8166  loss_ce_1: 1.012  loss_mask_1: 0.06249  loss_dice_1: 0.8415  loss_ce_2: 0.7872  loss_mask_2: 0.05523  loss_dice_2: 0.6412  loss_ce_3: 0.7289  loss_mask_3: 0.04262  loss_dice_3: 0.623  loss_ce_4: 0.6174  loss_mask_4: 0.04793  loss_dice_4: 0.5737  loss_ce_5: 0.6264  loss_mask_5: 0.05009  loss_dice_5: 0.4503  loss_ce_6: 0.545  loss_mask_6: 0.04643  loss_dice_6: 0.5509  loss_ce_7: 0.6278  loss_mask_7: 0.04554  loss_dice_7: 0.5768  loss_ce_8: 0.5845  loss_mask_8: 0.04915  loss_dice_8: 0.5561    time: 0.3663  last_time: 0.3031  data_time: 0.0103  last_data_time: 0.0070   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:31 d2.utils.events]:  eta: 0:01:31  iter: 9299  total_loss: 13.14  loss_ce: 0.599  loss_mask: 0.04955  loss_dice: 0.4884  loss_ce_0: 0.888  loss_mask_0: 0.06762  loss_dice_0: 0.6584  loss_ce_1: 0.8099  loss_mask_1: 0.05685  loss_dice_1: 0.4918  loss_ce_2: 0.7482  loss_mask_2: 0.04732  loss_dice_2: 0.393  loss_ce_3: 0.6396  loss_mask_3: 0.05453  loss_dice_3: 0.4934  loss_ce_4: 0.7167  loss_mask_4: 0.05194  loss_dice_4: 0.4898  loss_ce_5: 0.677  loss_mask_5: 0.05198  loss_dice_5: 0.4599  loss_ce_6: 0.598  loss_mask_6: 0.05113  loss_dice_6: 0.5432  loss_ce_7: 0.5867  loss_mask_7: 0.04689  loss_dice_7: 0.5262  loss_ce_8: 0.5817  loss_mask_8: 0.05602  loss_dice_8: 0.5754    time: 0.3662  last_time: 0.3030  data_time: 0.0062  last_data_time: 0.0097   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:38 d2.utils.events]:  eta: 0:01:25  iter: 9319  total_loss: 18.92  loss_ce: 0.5429  loss_mask: 0.1062  loss_dice: 0.6559  loss_ce_0: 1.286  loss_mask_0: 0.121  loss_dice_0: 0.9075  loss_ce_1: 0.9497  loss_mask_1: 0.1258  loss_dice_1: 0.5638  loss_ce_2: 0.8939  loss_mask_2: 0.1219  loss_dice_2: 0.6474  loss_ce_3: 0.6373  loss_mask_3: 0.1005  loss_dice_3: 0.7198  loss_ce_4: 0.6196  loss_mask_4: 0.1051  loss_dice_4: 0.7569  loss_ce_5: 0.6453  loss_mask_5: 0.1076  loss_dice_5: 0.7926  loss_ce_6: 0.5452  loss_mask_6: 0.09721  loss_dice_6: 0.5814  loss_ce_7: 0.5208  loss_mask_7: 0.105  loss_dice_7: 0.6168  loss_ce_8: 0.5808  loss_mask_8: 0.1076  loss_dice_8: 0.7495    time: 0.3661  last_time: 0.3321  data_time: 0.0080  last_data_time: 0.0376   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:44 d2.utils.events]:  eta: 0:01:19  iter: 9339  total_loss: 13.71  loss_ce: 0.4759  loss_mask: 0.04394  loss_dice: 0.5356  loss_ce_0: 1.157  loss_mask_0: 0.06005  loss_dice_0: 0.7006  loss_ce_1: 0.7091  loss_mask_1: 0.05761  loss_dice_1: 0.5609  loss_ce_2: 0.781  loss_mask_2: 0.04903  loss_dice_2: 0.6049  loss_ce_3: 0.5656  loss_mask_3: 0.04786  loss_dice_3: 0.6031  loss_ce_4: 0.5597  loss_mask_4: 0.05207  loss_dice_4: 0.5477  loss_ce_5: 0.5018  loss_mask_5: 0.04458  loss_dice_5: 0.5122  loss_ce_6: 0.4656  loss_mask_6: 0.0451  loss_dice_6: 0.5119  loss_ce_7: 0.5385  loss_mask_7: 0.04597  loss_dice_7: 0.5457  loss_ce_8: 0.4614  loss_mask_8: 0.04685  loss_dice_8: 0.3955    time: 0.3660  last_time: 0.3213  data_time: 0.0063  last_data_time: 0.0038   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:50 d2.utils.events]:  eta: 0:01:13  iter: 9359  total_loss: 13.94  loss_ce: 0.5356  loss_mask: 0.03191  loss_dice: 0.473  loss_ce_0: 1.094  loss_mask_0: 0.03634  loss_dice_0: 0.6303  loss_ce_1: 0.7722  loss_mask_1: 0.04085  loss_dice_1: 0.4787  loss_ce_2: 0.5675  loss_mask_2: 0.03383  loss_dice_2: 0.5268  loss_ce_3: 0.5447  loss_mask_3: 0.03168  loss_dice_3: 0.4569  loss_ce_4: 0.5426  loss_mask_4: 0.02627  loss_dice_4: 0.4677  loss_ce_5: 0.5681  loss_mask_5: 0.03699  loss_dice_5: 0.5305  loss_ce_6: 0.5249  loss_mask_6: 0.03108  loss_dice_6: 0.5397  loss_ce_7: 0.5495  loss_mask_7: 0.03726  loss_dice_7: 0.5196  loss_ce_8: 0.5373  loss_mask_8: 0.03412  loss_dice_8: 0.4815    time: 0.3658  last_time: 0.2991  data_time: 0.0063  last_data_time: 0.0062   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:51:56 d2.utils.events]:  eta: 0:01:07  iter: 9379  total_loss: 15.06  loss_ce: 0.6447  loss_mask: 0.0536  loss_dice: 0.8973  loss_ce_0: 1.211  loss_mask_0: 0.0751  loss_dice_0: 0.92  loss_ce_1: 0.8004  loss_mask_1: 0.06166  loss_dice_1: 0.8133  loss_ce_2: 0.8791  loss_mask_2: 0.06083  loss_dice_2: 0.6198  loss_ce_3: 0.6238  loss_mask_3: 0.05589  loss_dice_3: 0.5653  loss_ce_4: 0.6374  loss_mask_4: 0.06171  loss_dice_4: 0.8287  loss_ce_5: 0.6708  loss_mask_5: 0.05626  loss_dice_5: 0.8519  loss_ce_6: 0.5212  loss_mask_6: 0.05863  loss_dice_6: 0.6656  loss_ce_7: 0.712  loss_mask_7: 0.05468  loss_dice_7: 0.706  loss_ce_8: 0.6858  loss_mask_8: 0.06469  loss_dice_8: 0.7867    time: 0.3657  last_time: 0.3306  data_time: 0.0061  last_data_time: 0.0045   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:03 d2.utils.events]:  eta: 0:01:01  iter: 9399  total_loss: 17.85  loss_ce: 0.6251  loss_mask: 0.1235  loss_dice: 0.7084  loss_ce_0: 1.166  loss_mask_0: 0.1507  loss_dice_0: 0.9564  loss_ce_1: 0.8839  loss_mask_1: 0.1256  loss_dice_1: 0.9574  loss_ce_2: 0.7934  loss_mask_2: 0.1313  loss_dice_2: 0.8325  loss_ce_3: 0.8242  loss_mask_3: 0.1413  loss_dice_3: 0.7398  loss_ce_4: 0.6737  loss_mask_4: 0.1553  loss_dice_4: 0.8869  loss_ce_5: 0.6344  loss_mask_5: 0.1293  loss_dice_5: 0.8199  loss_ce_6: 0.7678  loss_mask_6: 0.1352  loss_dice_6: 1.019  loss_ce_7: 0.555  loss_mask_7: 0.1345  loss_dice_7: 0.8687  loss_ce_8: 0.6528  loss_mask_8: 0.1277  loss_dice_8: 0.7346    time: 0.3656  last_time: 0.3149  data_time: 0.0071  last_data_time: 0.0060   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:09 d2.utils.events]:  eta: 0:00:55  iter: 9419  total_loss: 22.78  loss_ce: 0.9435  loss_mask: 0.05624  loss_dice: 0.7292  loss_ce_0: 1.837  loss_mask_0: 0.08379  loss_dice_0: 1.07  loss_ce_1: 1.067  loss_mask_1: 0.06319  loss_dice_1: 0.9039  loss_ce_2: 0.9829  loss_mask_2: 0.06166  loss_dice_2: 0.8189  loss_ce_3: 0.9736  loss_mask_3: 0.06524  loss_dice_3: 0.8825  loss_ce_4: 0.9579  loss_mask_4: 0.06442  loss_dice_4: 0.7994  loss_ce_5: 0.931  loss_mask_5: 0.06356  loss_dice_5: 0.7225  loss_ce_6: 0.869  loss_mask_6: 0.06054  loss_dice_6: 0.7474  loss_ce_7: 0.9603  loss_mask_7: 0.05327  loss_dice_7: 0.7773  loss_ce_8: 1.008  loss_mask_8: 0.04625  loss_dice_8: 0.7374    time: 0.3655  last_time: 0.3285  data_time: 0.0068  last_data_time: 0.0082   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:15 d2.utils.events]:  eta: 0:00:49  iter: 9439  total_loss: 17.32  loss_ce: 0.6274  loss_mask: 0.05941  loss_dice: 0.7759  loss_ce_0: 1.075  loss_mask_0: 0.07415  loss_dice_0: 0.828  loss_ce_1: 0.903  loss_mask_1: 0.08476  loss_dice_1: 0.7801  loss_ce_2: 0.7814  loss_mask_2: 0.06428  loss_dice_2: 0.8316  loss_ce_3: 0.7695  loss_mask_3: 0.06398  loss_dice_3: 0.7324  loss_ce_4: 0.6875  loss_mask_4: 0.05586  loss_dice_4: 0.8251  loss_ce_5: 0.6605  loss_mask_5: 0.0557  loss_dice_5: 0.7123  loss_ce_6: 0.6144  loss_mask_6: 0.05489  loss_dice_6: 0.8495  loss_ce_7: 0.6134  loss_mask_7: 0.05319  loss_dice_7: 0.6717  loss_ce_8: 0.6126  loss_mask_8: 0.05793  loss_dice_8: 0.8057    time: 0.3654  last_time: 0.3304  data_time: 0.0071  last_data_time: 0.0065   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:22 d2.utils.events]:  eta: 0:00:42  iter: 9459  total_loss: 18.59  loss_ce: 0.5822  loss_mask: 0.07003  loss_dice: 0.8613  loss_ce_0: 1.223  loss_mask_0: 0.07896  loss_dice_0: 1.068  loss_ce_1: 0.8673  loss_mask_1: 0.08139  loss_dice_1: 1.143  loss_ce_2: 0.7556  loss_mask_2: 0.08129  loss_dice_2: 1.063  loss_ce_3: 0.7214  loss_mask_3: 0.08167  loss_dice_3: 0.8036  loss_ce_4: 0.7172  loss_mask_4: 0.08047  loss_dice_4: 0.8608  loss_ce_5: 0.6335  loss_mask_5: 0.08275  loss_dice_5: 0.8035  loss_ce_6: 0.6494  loss_mask_6: 0.06982  loss_dice_6: 0.8742  loss_ce_7: 0.6966  loss_mask_7: 0.06897  loss_dice_7: 0.8225  loss_ce_8: 0.7757  loss_mask_8: 0.07158  loss_dice_8: 0.9095    time: 0.3653  last_time: 0.3020  data_time: 0.0242  last_data_time: 0.0071   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:28 d2.utils.events]:  eta: 0:00:36  iter: 9479  total_loss: 20.05  loss_ce: 0.9932  loss_mask: 0.1171  loss_dice: 1.028  loss_ce_0: 1.556  loss_mask_0: 0.1229  loss_dice_0: 1.247  loss_ce_1: 1.097  loss_mask_1: 0.1093  loss_dice_1: 1.003  loss_ce_2: 1.212  loss_mask_2: 0.1408  loss_dice_2: 0.8264  loss_ce_3: 0.88  loss_mask_3: 0.1215  loss_dice_3: 0.8715  loss_ce_4: 0.9574  loss_mask_4: 0.1213  loss_dice_4: 0.7789  loss_ce_5: 0.8859  loss_mask_5: 0.1313  loss_dice_5: 0.6591  loss_ce_6: 0.8738  loss_mask_6: 0.1227  loss_dice_6: 1.004  loss_ce_7: 0.8782  loss_mask_7: 0.1136  loss_dice_7: 0.7644  loss_ce_8: 0.9372  loss_mask_8: 0.1331  loss_dice_8: 0.9395    time: 0.3652  last_time: 0.3139  data_time: 0.0077  last_data_time: 0.0168   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:34 d2.utils.events]:  eta: 0:00:30  iter: 9499  total_loss: 17.53  loss_ce: 0.7342  loss_mask: 0.07923  loss_dice: 0.5928  loss_ce_0: 1.374  loss_mask_0: 0.0752  loss_dice_0: 0.8713  loss_ce_1: 0.8874  loss_mask_1: 0.08833  loss_dice_1: 0.692  loss_ce_2: 0.7903  loss_mask_2: 0.07866  loss_dice_2: 0.6231  loss_ce_3: 0.6814  loss_mask_3: 0.09161  loss_dice_3: 0.7559  loss_ce_4: 0.7089  loss_mask_4: 0.07837  loss_dice_4: 0.7438  loss_ce_5: 0.686  loss_mask_5: 0.0821  loss_dice_5: 0.6958  loss_ce_6: 0.7111  loss_mask_6: 0.07791  loss_dice_6: 0.7702  loss_ce_7: 0.7126  loss_mask_7: 0.06791  loss_dice_7: 0.653  loss_ce_8: 0.7041  loss_mask_8: 0.07548  loss_dice_8: 0.7941    time: 0.3651  last_time: 0.3004  data_time: 0.0064  last_data_time: 0.0069   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:40 d2.utils.events]:  eta: 0:00:24  iter: 9519  total_loss: 23.88  loss_ce: 0.7634  loss_mask: 0.1073  loss_dice: 0.8057  loss_ce_0: 1.728  loss_mask_0: 0.09382  loss_dice_0: 1.135  loss_ce_1: 1.082  loss_mask_1: 0.09762  loss_dice_1: 1.203  loss_ce_2: 1.025  loss_mask_2: 0.09822  loss_dice_2: 0.9894  loss_ce_3: 1.003  loss_mask_3: 0.08865  loss_dice_3: 0.8232  loss_ce_4: 0.7519  loss_mask_4: 0.0946  loss_dice_4: 0.974  loss_ce_5: 0.809  loss_mask_5: 0.1031  loss_dice_5: 0.9444  loss_ce_6: 0.8044  loss_mask_6: 0.1119  loss_dice_6: 0.8177  loss_ce_7: 0.7279  loss_mask_7: 0.1108  loss_dice_7: 0.8012  loss_ce_8: 0.7329  loss_mask_8: 0.1057  loss_dice_8: 0.8137    time: 0.3650  last_time: 0.3022  data_time: 0.0089  last_data_time: 0.0048   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:47 d2.utils.events]:  eta: 0:00:18  iter: 9539  total_loss: 17.46  loss_ce: 0.7047  loss_mask: 0.06473  loss_dice: 0.5463  loss_ce_0: 1.39  loss_mask_0: 0.08962  loss_dice_0: 0.9507  loss_ce_1: 0.9266  loss_mask_1: 0.09055  loss_dice_1: 0.5692  loss_ce_2: 0.912  loss_mask_2: 0.0589  loss_dice_2: 0.5458  loss_ce_3: 0.8959  loss_mask_3: 0.05544  loss_dice_3: 0.6856  loss_ce_4: 0.8545  loss_mask_4: 0.05881  loss_dice_4: 0.7355  loss_ce_5: 0.8426  loss_mask_5: 0.06884  loss_dice_5: 0.6594  loss_ce_6: 0.8414  loss_mask_6: 0.05701  loss_dice_6: 0.5292  loss_ce_7: 0.7284  loss_mask_7: 0.06265  loss_dice_7: 0.633  loss_ce_8: 0.7275  loss_mask_8: 0.05575  loss_dice_8: 0.5216    time: 0.3648  last_time: 0.3136  data_time: 0.0075  last_data_time: 0.0127   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:53 d2.utils.events]:  eta: 0:00:12  iter: 9559  total_loss: 16.02  loss_ce: 0.634  loss_mask: 0.05817  loss_dice: 0.6851  loss_ce_0: 1.512  loss_mask_0: 0.06241  loss_dice_0: 0.6882  loss_ce_1: 0.8626  loss_mask_1: 0.06401  loss_dice_1: 0.5863  loss_ce_2: 0.7086  loss_mask_2: 0.05873  loss_dice_2: 0.603  loss_ce_3: 0.6633  loss_mask_3: 0.05696  loss_dice_3: 0.5524  loss_ce_4: 0.6457  loss_mask_4: 0.0619  loss_dice_4: 0.6425  loss_ce_5: 0.6759  loss_mask_5: 0.05432  loss_dice_5: 0.6677  loss_ce_6: 0.6418  loss_mask_6: 0.06537  loss_dice_6: 0.5883  loss_ce_7: 0.6287  loss_mask_7: 0.05766  loss_dice_7: 0.5481  loss_ce_8: 0.6079  loss_mask_8: 0.05721  loss_dice_8: 0.752    time: 0.3647  last_time: 0.2982  data_time: 0.0065  last_data_time: 0.0037   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:52:59 d2.utils.events]:  eta: 0:00:06  iter: 9579  total_loss: 18.71  loss_ce: 0.958  loss_mask: 0.09345  loss_dice: 0.5983  loss_ce_0: 1.438  loss_mask_0: 0.104  loss_dice_0: 0.9417  loss_ce_1: 1.168  loss_mask_1: 0.1091  loss_dice_1: 0.7481  loss_ce_2: 0.9697  loss_mask_2: 0.08766  loss_dice_2: 0.8363  loss_ce_3: 0.8983  loss_mask_3: 0.09514  loss_dice_3: 0.7472  loss_ce_4: 0.9003  loss_mask_4: 0.09129  loss_dice_4: 0.5908  loss_ce_5: 0.8666  loss_mask_5: 0.07932  loss_dice_5: 0.5933  loss_ce_6: 0.8254  loss_mask_6: 0.09262  loss_dice_6: 0.7909  loss_ce_7: 0.7396  loss_mask_7: 0.09051  loss_dice_7: 0.6267  loss_ce_8: 0.9888  loss_mask_8: 0.07923  loss_dice_8: 0.6529    time: 0.3647  last_time: 0.3003  data_time: 0.0158  last_data_time: 0.0036   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:53:06 d2.utils.events]:  eta: 0:00:00  iter: 9599  total_loss: 19.41  loss_ce: 0.9338  loss_mask: 0.09941  loss_dice: 0.8611  loss_ce_0: 1.487  loss_mask_0: 0.08615  loss_dice_0: 0.9398  loss_ce_1: 1.19  loss_mask_1: 0.09375  loss_dice_1: 0.9687  loss_ce_2: 1.071  loss_mask_2: 0.1016  loss_dice_2: 0.7325  loss_ce_3: 0.9003  loss_mask_3: 0.09446  loss_dice_3: 0.9065  loss_ce_4: 0.8381  loss_mask_4: 0.1036  loss_dice_4: 0.8534  loss_ce_5: 0.9221  loss_mask_5: 0.1008  loss_dice_5: 0.888  loss_ce_6: 0.8267  loss_mask_6: 0.1085  loss_dice_6: 0.9175  loss_ce_7: 0.916  loss_mask_7: 0.1023  loss_dice_7: 0.7721  loss_ce_8: 1.031  loss_mask_8: 0.1159  loss_dice_8: 0.8104    time: 0.3646  last_time: 0.3339  data_time: 0.0070  last_data_time: 0.0075   lr: 0.0001  max_mem: 37659M\n",
            "[09/08 16:53:06 d2.engine.hooks]: Overall training speed: 9598 iterations in 0:58:19 (0.3646 s / it)\n",
            "[09/08 16:53:06 d2.engine.hooks]: Total training time: 1:38:45 (0:40:26 on hooks)\n",
            "[09/08 16:53:06 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[09/08 16:53:06 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[09/08 16:53:06 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/08 16:53:06 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[09/08 16:53:06 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "taco10_val\n",
            "[09/08 16:53:06 d2.evaluation.evaluator]: Start inference on 150 batches\n",
            "[09/08 16:53:46 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0014 s/iter. Inference: 0.1055 s/iter. Eval: 3.8651 s/iter. Total: 3.9721 s/iter. ETA=0:09:12\n",
            "[09/08 16:53:54 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0016 s/iter. Inference: 0.1049 s/iter. Eval: 3.8199 s/iter. Total: 3.9267 s/iter. ETA=0:08:57\n",
            "[09/08 16:54:01 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 3.7906 s/iter. Total: 3.8973 s/iter. ETA=0:08:46\n",
            "[09/08 16:54:09 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0017 s/iter. Inference: 0.1044 s/iter. Eval: 3.7749 s/iter. Total: 3.8815 s/iter. ETA=0:08:36\n",
            "[09/08 16:54:17 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0018 s/iter. Inference: 0.1044 s/iter. Eval: 3.7676 s/iter. Total: 3.8742 s/iter. ETA=0:08:27\n",
            "[09/08 16:54:24 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0018 s/iter. Inference: 0.1044 s/iter. Eval: 3.7575 s/iter. Total: 3.8642 s/iter. ETA=0:08:18\n",
            "[09/08 16:54:29 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:54:49 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0018 s/iter. Inference: 0.9576 s/iter. Eval: 3.8232 s/iter. Total: 4.7832 s/iter. ETA=0:10:07\n",
            "[09/08 16:54:49 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:55:10 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0019 s/iter. Inference: 1.7880 s/iter. Eval: 3.8855 s/iter. Total: 5.6760 s/iter. ETA=0:11:55\n",
            "[09/08 16:55:16 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0019 s/iter. Inference: 1.7317 s/iter. Eval: 3.9401 s/iter. Total: 5.6743 s/iter. ETA=0:11:49\n",
            "[09/08 16:55:16 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:55:31 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0019 s/iter. Inference: 2.1834 s/iter. Eval: 3.9241 s/iter. Total: 6.1101 s/iter. ETA=0:12:37\n",
            "[09/08 16:55:41 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0019 s/iter. Inference: 2.1393 s/iter. Eval: 4.1334 s/iter. Total: 6.2753 s/iter. ETA=0:12:51\n",
            "[09/08 16:55:48 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0020 s/iter. Inference: 2.0769 s/iter. Eval: 4.2412 s/iter. Total: 6.3209 s/iter. ETA=0:12:51\n",
            "[09/08 16:55:55 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0020 s/iter. Inference: 1.9959 s/iter. Eval: 4.3404 s/iter. Total: 6.3390 s/iter. ETA=0:12:47\n",
            "[09/08 16:55:56 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:56:10 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0020 s/iter. Inference: 2.3865 s/iter. Eval: 4.2945 s/iter. Total: 6.6838 s/iter. ETA=0:13:22\n",
            "[09/08 16:56:16 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0020 s/iter. Inference: 2.3123 s/iter. Eval: 4.3701 s/iter. Total: 6.6852 s/iter. ETA=0:13:15\n",
            "[09/08 16:56:23 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0021 s/iter. Inference: 2.2520 s/iter. Eval: 4.4391 s/iter. Total: 6.6939 s/iter. ETA=0:13:09\n",
            "[09/08 16:56:30 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0020 s/iter. Inference: 2.1761 s/iter. Eval: 4.5035 s/iter. Total: 6.6825 s/iter. ETA=0:13:01\n",
            "[09/08 16:56:36 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0021 s/iter. Inference: 2.1054 s/iter. Eval: 4.5614 s/iter. Total: 6.6697 s/iter. ETA=0:12:53\n",
            "[09/08 16:56:43 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0021 s/iter. Inference: 2.0531 s/iter. Eval: 4.6331 s/iter. Total: 6.6891 s/iter. ETA=0:12:49\n",
            "[09/08 16:56:51 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0021 s/iter. Inference: 1.9315 s/iter. Eval: 4.5947 s/iter. Total: 6.5291 s/iter. ETA=0:12:17\n",
            "[09/08 16:56:53 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:57:06 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0021 s/iter. Inference: 2.2136 s/iter. Eval: 4.5589 s/iter. Total: 6.7755 s/iter. ETA=0:12:38\n",
            "[09/08 16:57:15 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0021 s/iter. Inference: 2.1690 s/iter. Eval: 4.6679 s/iter. Total: 6.8398 s/iter. ETA=0:12:39\n",
            "[09/08 16:57:21 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0021 s/iter. Inference: 2.1260 s/iter. Eval: 4.6774 s/iter. Total: 6.8065 s/iter. ETA=0:12:28\n",
            "[09/08 16:57:22 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 16:57:35 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0022 s/iter. Inference: 2.3723 s/iter. Eval: 4.6427 s/iter. Total: 7.0181 s/iter. ETA=0:12:44\n",
            "[09/08 16:57:41 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0022 s/iter. Inference: 2.3208 s/iter. Eval: 4.6526 s/iter. Total: 6.9765 s/iter. ETA=0:12:33\n",
            "[09/08 16:57:48 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0022 s/iter. Inference: 2.2828 s/iter. Eval: 4.7057 s/iter. Total: 6.9916 s/iter. ETA=0:12:28\n",
            "[09/08 16:57:55 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0022 s/iter. Inference: 2.2276 s/iter. Eval: 4.7537 s/iter. Total: 6.9844 s/iter. ETA=0:12:20\n",
            "[09/08 16:58:02 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0022 s/iter. Inference: 2.1751 s/iter. Eval: 4.8004 s/iter. Total: 6.9787 s/iter. ETA=0:12:12\n",
            "[09/08 16:58:08 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0022 s/iter. Inference: 2.1253 s/iter. Eval: 4.8456 s/iter. Total: 6.9741 s/iter. ETA=0:12:05\n",
            "[09/08 16:58:15 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0022 s/iter. Inference: 2.0778 s/iter. Eval: 4.8875 s/iter. Total: 6.9685 s/iter. ETA=0:11:57\n",
            "[09/08 16:58:22 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0022 s/iter. Inference: 2.0325 s/iter. Eval: 4.9282 s/iter. Total: 6.9639 s/iter. ETA=0:11:50\n",
            "[09/08 16:58:29 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0022 s/iter. Inference: 1.9894 s/iter. Eval: 4.9667 s/iter. Total: 6.9593 s/iter. ETA=0:11:42\n",
            "[09/08 16:58:35 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0022 s/iter. Inference: 1.9480 s/iter. Eval: 5.0026 s/iter. Total: 6.9539 s/iter. ETA=0:11:35\n",
            "[09/08 16:58:42 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0022 s/iter. Inference: 1.9085 s/iter. Eval: 5.0374 s/iter. Total: 6.9491 s/iter. ETA=0:11:27\n",
            "[09/08 16:58:49 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0022 s/iter. Inference: 1.8708 s/iter. Eval: 5.0715 s/iter. Total: 6.9456 s/iter. ETA=0:11:20\n",
            "[09/08 16:58:56 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0022 s/iter. Inference: 1.8346 s/iter. Eval: 5.1031 s/iter. Total: 6.9410 s/iter. ETA=0:11:13\n",
            "[09/08 16:59:02 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0022 s/iter. Inference: 1.7999 s/iter. Eval: 5.1340 s/iter. Total: 6.9371 s/iter. ETA=0:11:05\n",
            "[09/08 16:59:09 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0022 s/iter. Inference: 1.7664 s/iter. Eval: 5.1559 s/iter. Total: 6.9255 s/iter. ETA=0:10:57\n",
            "[09/08 16:59:15 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0022 s/iter. Inference: 1.7342 s/iter. Eval: 5.1765 s/iter. Total: 6.9140 s/iter. ETA=0:10:49\n",
            "[09/08 16:59:21 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0022 s/iter. Inference: 1.7034 s/iter. Eval: 5.1970 s/iter. Total: 6.9037 s/iter. ETA=0:10:42\n",
            "[09/08 16:59:28 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0022 s/iter. Inference: 1.6738 s/iter. Eval: 5.2168 s/iter. Total: 6.8938 s/iter. ETA=0:10:34\n",
            "[09/08 16:59:34 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0023 s/iter. Inference: 1.6451 s/iter. Eval: 5.2353 s/iter. Total: 6.8837 s/iter. ETA=0:10:26\n",
            "[09/08 16:59:41 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0023 s/iter. Inference: 1.6175 s/iter. Eval: 5.2534 s/iter. Total: 6.8742 s/iter. ETA=0:10:18\n",
            "[09/08 16:59:47 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0023 s/iter. Inference: 1.5908 s/iter. Eval: 5.2707 s/iter. Total: 6.8648 s/iter. ETA=0:10:10\n",
            "[09/08 16:59:53 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0023 s/iter. Inference: 1.5652 s/iter. Eval: 5.2878 s/iter. Total: 6.8563 s/iter. ETA=0:10:03\n",
            "[09/08 17:00:00 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0023 s/iter. Inference: 1.5403 s/iter. Eval: 5.3040 s/iter. Total: 6.8477 s/iter. ETA=0:09:55\n",
            "[09/08 17:00:06 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0023 s/iter. Inference: 1.5164 s/iter. Eval: 5.3200 s/iter. Total: 6.8398 s/iter. ETA=0:09:48\n",
            "[09/08 17:00:17 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0023 s/iter. Inference: 1.4478 s/iter. Eval: 5.2289 s/iter. Total: 6.6801 s/iter. ETA=0:09:14\n",
            "[09/08 17:00:23 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0023 s/iter. Inference: 1.4269 s/iter. Eval: 5.2462 s/iter. Total: 6.6765 s/iter. ETA=0:09:07\n",
            "[09/08 17:00:29 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0023 s/iter. Inference: 1.3860 s/iter. Eval: 5.1728 s/iter. Total: 6.5622 s/iter. ETA=0:08:44\n",
            "[09/08 17:00:37 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0023 s/iter. Inference: 1.3479 s/iter. Eval: 5.1391 s/iter. Total: 6.4902 s/iter. ETA=0:08:26\n",
            "[09/08 17:00:46 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0023 s/iter. Inference: 1.3120 s/iter. Eval: 5.1082 s/iter. Total: 6.4235 s/iter. ETA=0:08:08\n",
            "[09/08 17:00:54 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0022 s/iter. Inference: 1.2780 s/iter. Eval: 5.0780 s/iter. Total: 6.3593 s/iter. ETA=0:07:50\n",
            "[09/08 17:01:02 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0022 s/iter. Inference: 1.2459 s/iter. Eval: 5.0498 s/iter. Total: 6.2990 s/iter. ETA=0:07:33\n",
            "[09/08 17:01:11 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0022 s/iter. Inference: 1.2156 s/iter. Eval: 5.0233 s/iter. Total: 6.2421 s/iter. ETA=0:07:16\n",
            "[09/08 17:01:19 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0022 s/iter. Inference: 1.1868 s/iter. Eval: 4.9978 s/iter. Total: 6.1878 s/iter. ETA=0:07:00\n",
            "[09/08 17:01:27 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0022 s/iter. Inference: 1.1595 s/iter. Eval: 4.9741 s/iter. Total: 6.1368 s/iter. ETA=0:06:45\n",
            "[09/08 17:01:36 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0022 s/iter. Inference: 1.1335 s/iter. Eval: 4.9508 s/iter. Total: 6.0876 s/iter. ETA=0:06:29\n",
            "[09/08 17:01:44 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0022 s/iter. Inference: 1.1088 s/iter. Eval: 4.9285 s/iter. Total: 6.0405 s/iter. ETA=0:06:14\n",
            "[09/08 17:01:52 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0022 s/iter. Inference: 1.0853 s/iter. Eval: 4.9074 s/iter. Total: 5.9959 s/iter. ETA=0:05:59\n",
            "[09/08 17:02:00 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0022 s/iter. Inference: 1.0629 s/iter. Eval: 4.8872 s/iter. Total: 5.9532 s/iter. ETA=0:05:45\n",
            "[09/08 17:02:09 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0022 s/iter. Inference: 1.0414 s/iter. Eval: 4.8679 s/iter. Total: 5.9125 s/iter. ETA=0:05:31\n",
            "[09/08 17:02:17 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0022 s/iter. Inference: 1.0209 s/iter. Eval: 4.8501 s/iter. Total: 5.8741 s/iter. ETA=0:05:17\n",
            "[09/08 17:02:25 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0022 s/iter. Inference: 1.0012 s/iter. Eval: 4.8326 s/iter. Total: 5.8370 s/iter. ETA=0:05:03\n",
            "[09/08 17:02:34 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 0.9825 s/iter. Eval: 4.8155 s/iter. Total: 5.8011 s/iter. ETA=0:04:50\n",
            "[09/08 17:02:42 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0022 s/iter. Inference: 0.9644 s/iter. Eval: 4.7994 s/iter. Total: 5.7670 s/iter. ETA=0:04:36\n",
            "[09/08 17:02:50 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0022 s/iter. Inference: 0.9471 s/iter. Eval: 4.7837 s/iter. Total: 5.7340 s/iter. ETA=0:04:23\n",
            "[09/08 17:02:58 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0022 s/iter. Inference: 0.9305 s/iter. Eval: 4.7688 s/iter. Total: 5.7024 s/iter. ETA=0:04:10\n",
            "[09/08 17:03:07 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0022 s/iter. Inference: 0.9145 s/iter. Eval: 4.7547 s/iter. Total: 5.6723 s/iter. ETA=0:03:58\n",
            "[09/08 17:03:15 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0022 s/iter. Inference: 0.8991 s/iter. Eval: 4.7408 s/iter. Total: 5.6431 s/iter. ETA=0:03:45\n",
            "[09/08 17:03:23 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0022 s/iter. Inference: 0.8843 s/iter. Eval: 4.7278 s/iter. Total: 5.6152 s/iter. ETA=0:03:33\n",
            "[09/08 17:03:30 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0022 s/iter. Inference: 0.8700 s/iter. Eval: 4.7026 s/iter. Total: 5.5757 s/iter. ETA=0:03:20\n",
            "[09/08 17:03:36 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:03:56 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 1.0088 s/iter. Eval: 4.6993 s/iter. Total: 5.7112 s/iter. ETA=0:03:14\n",
            "[09/08 17:03:57 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:04:18 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 1.1499 s/iter. Eval: 4.7019 s/iter. Total: 5.8549 s/iter. ETA=0:03:13\n",
            "[09/08 17:04:27 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 1.1364 s/iter. Eval: 4.6915 s/iter. Total: 5.8310 s/iter. ETA=0:03:00\n",
            "[09/08 17:04:36 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.1187 s/iter. Eval: 4.6801 s/iter. Total: 5.8019 s/iter. ETA=0:02:48\n",
            "[09/08 17:04:44 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.1015 s/iter. Eval: 4.6692 s/iter. Total: 5.7739 s/iter. ETA=0:02:35\n",
            "[09/08 17:04:52 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.0850 s/iter. Eval: 4.6586 s/iter. Total: 5.7466 s/iter. ETA=0:02:23\n",
            "[09/08 17:05:00 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.0689 s/iter. Eval: 4.6484 s/iter. Total: 5.7204 s/iter. ETA=0:02:11\n",
            "[09/08 17:05:06 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.0533 s/iter. Eval: 4.6191 s/iter. Total: 5.6755 s/iter. ETA=0:01:59\n",
            "[09/08 17:05:16 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.0385 s/iter. Eval: 4.6200 s/iter. Total: 5.6616 s/iter. ETA=0:01:47\n",
            "[09/08 17:05:25 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 1.0241 s/iter. Eval: 4.6155 s/iter. Total: 5.6426 s/iter. ETA=0:01:35\n",
            "[09/08 17:05:33 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 1.0100 s/iter. Eval: 4.6070 s/iter. Total: 5.6200 s/iter. ETA=0:01:24\n",
            "[09/08 17:05:41 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 0.9963 s/iter. Eval: 4.5985 s/iter. Total: 5.5979 s/iter. ETA=0:01:12\n",
            "[09/08 17:05:50 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 0.9831 s/iter. Eval: 4.5904 s/iter. Total: 5.5765 s/iter. ETA=0:01:01\n",
            "[09/08 17:05:58 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 0.9702 s/iter. Eval: 4.5821 s/iter. Total: 5.5554 s/iter. ETA=0:00:49\n",
            "[09/08 17:06:06 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 0.9577 s/iter. Eval: 4.5742 s/iter. Total: 5.5350 s/iter. ETA=0:00:38\n",
            "[09/08 17:06:14 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0021 s/iter. Inference: 0.9455 s/iter. Eval: 4.5615 s/iter. Total: 5.5100 s/iter. ETA=0:00:27\n",
            "[09/08 17:06:21 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0021 s/iter. Inference: 0.9336 s/iter. Eval: 4.5489 s/iter. Total: 5.4856 s/iter. ETA=0:00:16\n",
            "[09/08 17:06:29 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0021 s/iter. Inference: 0.9221 s/iter. Eval: 4.5370 s/iter. Total: 5.4621 s/iter. ETA=0:00:05\n",
            "[09/08 17:06:33 d2.evaluation.evaluator]: Total inference time: 0:13:10.414562 (5.451135 s / iter per device, on 1 devices)\n",
            "[09/08 17:06:33 d2.evaluation.evaluator]: Total inference pure compute time: 0:02:12 (0.916397 s / iter per device, on 1 devices)\n",
            "[09/08 17:06:33 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/08 17:06:33 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[09/08 17:06:33 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/08 17:06:33 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/08 17:06:34 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/08 17:06:34 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/08 17:06:34 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/08 17:06:34 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/08 17:06:34 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.39s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/08 17:06:34 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/08 17:06:35 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.36 seconds.\n",
            "[09/08 17:06:35 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/08 17:06:35 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.06 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.111\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.169\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.110\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.093\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.136\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.262\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.409\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.269\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
            "[09/08 17:06:35 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 11.149 | 16.870 | 11.007 | 0.029 | 9.306 | 13.596 |\n",
            "[09/08 17:06:35 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 17.415 | Other      | 14.520 | Bottle     | 22.932 |\n",
            "| Bottle cap            | 7.041  | Cup        | 14.696 | Lid        | 4.569  |\n",
            "| Plastic bag + wrapper | 20.364 | Pop tab    | 1.662  | Straw      | 6.790  |\n",
            "| Cigarette             | 1.502  |            |        |            |        |\n",
            "[09/08 17:06:35 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[09/08 17:06:35 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/08 17:06:35 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/08 17:06:35 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/08 17:06:35 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/08 17:06:35 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/08 17:06:35 d2.evaluation.testing]: copypaste: 11.1490,16.8699,11.0066,0.0288,9.3059,13.5957\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6nDuTA80_jl",
        "outputId": "a2396ba7-8b96-45fe-9c7b-0bb7c68c5e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[09/08 17:06:36 mask2former.data.dataset_mappers.mask_former_instance_dataset_mapper]: [MaskFormerInstanceDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/08 17:06:36 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_test.json\n",
            "[09/08 17:06:36 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 37           |   Other    | 151          |   Bottle   | 54           |\n",
            "|  Bottle cap   | 41           |    Cup     | 17           |    Lid     | 15           |\n",
            "| Plastic bag.. | 84           |  Pop tab   | 11           |   Straw    | 38           |\n",
            "|   Cigarette   | 121          |            |              |            |              |\n",
            "|     total     | 569          |            |              |            |              |\n",
            "[09/08 17:06:36 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/08 17:06:36 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[09/08 17:06:36 d2.data.common]: Serialized dataset takes 0.25 MiB\n",
            "[09/08 17:06:36 d2.evaluation.evaluator]: Start inference on 150 batches\n",
            "[09/08 17:07:13 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0027 s/iter. Inference: 0.1029 s/iter. Eval: 3.7568 s/iter. Total: 3.8624 s/iter. ETA=0:08:56\n",
            "[09/08 17:07:21 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0028 s/iter. Inference: 0.1024 s/iter. Eval: 3.7381 s/iter. Total: 3.8436 s/iter. ETA=0:08:46\n",
            "[09/08 17:07:28 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0029 s/iter. Inference: 0.1021 s/iter. Eval: 3.7287 s/iter. Total: 3.8341 s/iter. ETA=0:08:37\n",
            "[09/08 17:07:36 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0030 s/iter. Inference: 0.1022 s/iter. Eval: 3.7338 s/iter. Total: 3.8394 s/iter. ETA=0:08:30\n",
            "[09/08 17:07:44 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0030 s/iter. Inference: 0.1020 s/iter. Eval: 3.7286 s/iter. Total: 3.8341 s/iter. ETA=0:08:22\n",
            "[09/08 17:07:49 d2.evaluation.evaluator]: Inference done 20/150. Dataloading: 0.0031 s/iter. Inference: 0.1034 s/iter. Eval: 3.8163 s/iter. Total: 3.9233 s/iter. ETA=0:08:30\n",
            "[09/08 17:07:49 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:08:00 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0031 s/iter. Inference: 0.6083 s/iter. Eval: 3.7339 s/iter. Total: 4.3460 s/iter. ETA=0:09:20\n",
            "[09/08 17:08:06 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0032 s/iter. Inference: 0.5940 s/iter. Eval: 3.8834 s/iter. Total: 4.4814 s/iter. ETA=0:09:33\n",
            "[09/08 17:08:13 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0032 s/iter. Inference: 0.6062 s/iter. Eval: 4.0155 s/iter. Total: 4.6257 s/iter. ETA=0:09:47\n",
            "[09/08 17:08:20 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0033 s/iter. Inference: 0.5810 s/iter. Eval: 4.1324 s/iter. Total: 4.7173 s/iter. ETA=0:09:54\n",
            "[09/08 17:08:26 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0032 s/iter. Inference: 0.5582 s/iter. Eval: 4.2383 s/iter. Total: 4.8006 s/iter. ETA=0:10:00\n",
            "[09/08 17:08:32 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0033 s/iter. Inference: 0.5377 s/iter. Eval: 4.3333 s/iter. Total: 4.8752 s/iter. ETA=0:10:04\n",
            "[09/08 17:08:39 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0033 s/iter. Inference: 0.5193 s/iter. Eval: 4.4185 s/iter. Total: 4.9419 s/iter. ETA=0:10:07\n",
            "[09/08 17:08:44 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:08:58 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0033 s/iter. Inference: 0.9672 s/iter. Eval: 4.3612 s/iter. Total: 5.3326 s/iter. ETA=0:10:45\n",
            "[09/08 17:09:07 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0033 s/iter. Inference: 0.9550 s/iter. Eval: 4.5197 s/iter. Total: 5.4789 s/iter. ETA=0:10:57\n",
            "[09/08 17:09:08 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:09:21 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0033 s/iter. Inference: 1.3276 s/iter. Eval: 4.4776 s/iter. Total: 5.8093 s/iter. ETA=0:11:31\n",
            "[09/08 17:09:28 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0033 s/iter. Inference: 1.2951 s/iter. Eval: 4.5555 s/iter. Total: 5.8548 s/iter. ETA=0:11:30\n",
            "[09/08 17:09:35 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0033 s/iter. Inference: 1.2726 s/iter. Eval: 4.6100 s/iter. Total: 5.8868 s/iter. ETA=0:11:28\n",
            "[09/08 17:09:40 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0034 s/iter. Inference: 1.2330 s/iter. Eval: 4.6256 s/iter. Total: 5.8629 s/iter. ETA=0:11:20\n",
            "[09/08 17:09:47 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0033 s/iter. Inference: 1.2094 s/iter. Eval: 4.6915 s/iter. Total: 5.9051 s/iter. ETA=0:11:19\n",
            "[09/08 17:09:54 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0034 s/iter. Inference: 1.1798 s/iter. Eval: 4.7516 s/iter. Total: 5.9358 s/iter. ETA=0:11:16\n",
            "[09/08 17:10:01 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0034 s/iter. Inference: 1.1470 s/iter. Eval: 4.8103 s/iter. Total: 5.9616 s/iter. ETA=0:11:13\n",
            "[09/08 17:10:08 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0035 s/iter. Inference: 1.1161 s/iter. Eval: 4.8637 s/iter. Total: 5.9843 s/iter. ETA=0:11:10\n",
            "[09/08 17:10:14 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0035 s/iter. Inference: 1.0871 s/iter. Eval: 4.9133 s/iter. Total: 6.0048 s/iter. ETA=0:11:06\n",
            "[09/08 17:10:21 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0035 s/iter. Inference: 1.0597 s/iter. Eval: 4.9607 s/iter. Total: 6.0250 s/iter. ETA=0:11:02\n",
            "[09/08 17:10:28 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0035 s/iter. Inference: 1.0339 s/iter. Eval: 5.0060 s/iter. Total: 6.0445 s/iter. ETA=0:10:58\n",
            "[09/08 17:10:34 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0035 s/iter. Inference: 1.0094 s/iter. Eval: 5.0500 s/iter. Total: 6.0640 s/iter. ETA=0:10:54\n",
            "[09/08 17:10:41 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0035 s/iter. Inference: 0.9863 s/iter. Eval: 5.0910 s/iter. Total: 6.0818 s/iter. ETA=0:10:50\n",
            "[09/08 17:10:48 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0035 s/iter. Inference: 0.9642 s/iter. Eval: 5.1200 s/iter. Total: 6.0888 s/iter. ETA=0:10:45\n",
            "[09/08 17:10:54 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0035 s/iter. Inference: 0.9432 s/iter. Eval: 5.1476 s/iter. Total: 6.0954 s/iter. ETA=0:10:40\n",
            "[09/08 17:11:00 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0035 s/iter. Inference: 0.9233 s/iter. Eval: 5.1739 s/iter. Total: 6.1018 s/iter. ETA=0:10:34\n",
            "[09/08 17:11:07 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0035 s/iter. Inference: 0.9043 s/iter. Eval: 5.1990 s/iter. Total: 6.1080 s/iter. ETA=0:10:29\n",
            "[09/08 17:11:13 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0035 s/iter. Inference: 0.8862 s/iter. Eval: 5.2229 s/iter. Total: 6.1137 s/iter. ETA=0:10:23\n",
            "[09/08 17:11:19 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0035 s/iter. Inference: 0.8689 s/iter. Eval: 5.2451 s/iter. Total: 6.1186 s/iter. ETA=0:10:17\n",
            "[09/08 17:11:26 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0035 s/iter. Inference: 0.8524 s/iter. Eval: 5.2665 s/iter. Total: 6.1235 s/iter. ETA=0:10:12\n",
            "[09/08 17:11:32 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0035 s/iter. Inference: 0.8367 s/iter. Eval: 5.2870 s/iter. Total: 6.1283 s/iter. ETA=0:10:06\n",
            "[09/08 17:11:38 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0035 s/iter. Inference: 0.8216 s/iter. Eval: 5.3079 s/iter. Total: 6.1341 s/iter. ETA=0:10:01\n",
            "[09/08 17:11:44 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0035 s/iter. Inference: 0.7920 s/iter. Eval: 5.2107 s/iter. Total: 6.0074 s/iter. ETA=0:09:36\n",
            "[09/08 17:11:53 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0035 s/iter. Inference: 0.7650 s/iter. Eval: 5.1648 s/iter. Total: 5.9344 s/iter. ETA=0:09:17\n",
            "[09/08 17:11:59 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0035 s/iter. Inference: 0.7527 s/iter. Eval: 5.1850 s/iter. Total: 5.9424 s/iter. ETA=0:09:12\n",
            "[09/08 17:12:04 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0035 s/iter. Inference: 0.7280 s/iter. Eval: 5.0836 s/iter. Total: 5.8161 s/iter. ETA=0:08:49\n",
            "[09/08 17:12:12 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0035 s/iter. Inference: 0.7058 s/iter. Eval: 5.0459 s/iter. Total: 5.7562 s/iter. ETA=0:08:32\n",
            "[09/08 17:12:21 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0035 s/iter. Inference: 0.6851 s/iter. Eval: 5.0113 s/iter. Total: 5.7010 s/iter. ETA=0:08:15\n",
            "[09/08 17:12:26 d2.evaluation.evaluator]: Inference done 65/150. Dataloading: 0.0035 s/iter. Inference: 0.6659 s/iter. Eval: 4.9347 s/iter. Total: 5.6052 s/iter. ETA=0:07:56\n",
            "[09/08 17:12:35 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0035 s/iter. Inference: 0.6479 s/iter. Eval: 4.9059 s/iter. Total: 5.5584 s/iter. ETA=0:07:41\n",
            "[09/08 17:12:43 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0035 s/iter. Inference: 0.6310 s/iter. Eval: 4.8801 s/iter. Total: 5.5156 s/iter. ETA=0:07:26\n",
            "[09/08 17:12:51 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0035 s/iter. Inference: 0.6151 s/iter. Eval: 4.8545 s/iter. Total: 5.4742 s/iter. ETA=0:07:12\n",
            "[09/08 17:13:00 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0035 s/iter. Inference: 0.6002 s/iter. Eval: 4.8308 s/iter. Total: 5.4355 s/iter. ETA=0:06:58\n",
            "[09/08 17:13:08 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0035 s/iter. Inference: 0.5861 s/iter. Eval: 4.8085 s/iter. Total: 5.3991 s/iter. ETA=0:06:44\n",
            "[09/08 17:13:16 d2.evaluation.evaluator]: Inference done 77/150. Dataloading: 0.0035 s/iter. Inference: 0.5728 s/iter. Eval: 4.7875 s/iter. Total: 5.3647 s/iter. ETA=0:06:31\n",
            "[09/08 17:13:25 d2.evaluation.evaluator]: Inference done 79/150. Dataloading: 0.0035 s/iter. Inference: 0.5602 s/iter. Eval: 4.7675 s/iter. Total: 5.3322 s/iter. ETA=0:06:18\n",
            "[09/08 17:13:33 d2.evaluation.evaluator]: Inference done 81/150. Dataloading: 0.0035 s/iter. Inference: 0.5483 s/iter. Eval: 4.7485 s/iter. Total: 5.3012 s/iter. ETA=0:06:05\n",
            "[09/08 17:13:41 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0035 s/iter. Inference: 0.5370 s/iter. Eval: 4.7316 s/iter. Total: 5.2730 s/iter. ETA=0:05:53\n",
            "[09/08 17:13:50 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0035 s/iter. Inference: 0.5262 s/iter. Eval: 4.7143 s/iter. Total: 5.2450 s/iter. ETA=0:05:40\n",
            "[09/08 17:13:58 d2.evaluation.evaluator]: Inference done 87/150. Dataloading: 0.0034 s/iter. Inference: 0.5160 s/iter. Eval: 4.6977 s/iter. Total: 5.2182 s/iter. ETA=0:05:28\n",
            "[09/08 17:14:06 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0034 s/iter. Inference: 0.5063 s/iter. Eval: 4.6822 s/iter. Total: 5.1929 s/iter. ETA=0:05:16\n",
            "[09/08 17:14:15 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0034 s/iter. Inference: 0.4970 s/iter. Eval: 4.6674 s/iter. Total: 5.1687 s/iter. ETA=0:05:04\n",
            "[09/08 17:14:23 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0034 s/iter. Inference: 0.4881 s/iter. Eval: 4.6530 s/iter. Total: 5.1455 s/iter. ETA=0:04:53\n",
            "[09/08 17:14:31 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0034 s/iter. Inference: 0.4796 s/iter. Eval: 4.6397 s/iter. Total: 5.1237 s/iter. ETA=0:04:41\n",
            "[09/08 17:14:40 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0034 s/iter. Inference: 0.4715 s/iter. Eval: 4.6270 s/iter. Total: 5.1029 s/iter. ETA=0:04:30\n",
            "[09/08 17:14:48 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0034 s/iter. Inference: 0.4638 s/iter. Eval: 4.6155 s/iter. Total: 5.0836 s/iter. ETA=0:04:19\n",
            "[09/08 17:14:56 d2.evaluation.evaluator]: Inference done 101/150. Dataloading: 0.0034 s/iter. Inference: 0.4563 s/iter. Eval: 4.6037 s/iter. Total: 5.0643 s/iter. ETA=0:04:08\n",
            "[09/08 17:15:05 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0034 s/iter. Inference: 0.4492 s/iter. Eval: 4.5925 s/iter. Total: 5.0460 s/iter. ETA=0:03:57\n",
            "[09/08 17:15:13 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0034 s/iter. Inference: 0.4423 s/iter. Eval: 4.5814 s/iter. Total: 5.0281 s/iter. ETA=0:03:46\n",
            "[09/08 17:15:21 d2.evaluation.evaluator]: Inference done 107/150. Dataloading: 0.0034 s/iter. Inference: 0.4358 s/iter. Eval: 4.5708 s/iter. Total: 5.0108 s/iter. ETA=0:03:35\n",
            "[09/08 17:15:28 d2.evaluation.evaluator]: Inference done 109/150. Dataloading: 0.0034 s/iter. Inference: 0.4293 s/iter. Eval: 4.5472 s/iter. Total: 4.9808 s/iter. ETA=0:03:24\n",
            "[09/08 17:15:36 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0034 s/iter. Inference: 0.4232 s/iter. Eval: 4.5380 s/iter. Total: 4.9655 s/iter. ETA=0:03:13\n",
            "[09/08 17:15:45 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0034 s/iter. Inference: 0.4174 s/iter. Eval: 4.5295 s/iter. Total: 4.9512 s/iter. ETA=0:03:03\n",
            "[09/08 17:15:54 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0034 s/iter. Inference: 0.4119 s/iter. Eval: 4.5311 s/iter. Total: 4.9473 s/iter. ETA=0:02:53\n",
            "[09/08 17:16:03 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0034 s/iter. Inference: 0.4064 s/iter. Eval: 4.5225 s/iter. Total: 4.9333 s/iter. ETA=0:02:42\n",
            "[09/08 17:16:08 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:16:29 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0034 s/iter. Inference: 0.5486 s/iter. Eval: 4.5227 s/iter. Total: 5.0757 s/iter. ETA=0:02:37\n",
            "[09/08 17:16:30 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/08 17:16:50 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0034 s/iter. Inference: 0.6880 s/iter. Eval: 4.5272 s/iter. Total: 5.2195 s/iter. ETA=0:02:36\n",
            "[09/08 17:16:59 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0034 s/iter. Inference: 0.6827 s/iter. Eval: 4.5190 s/iter. Total: 5.2060 s/iter. ETA=0:02:25\n",
            "[09/08 17:17:08 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0034 s/iter. Inference: 0.6730 s/iter. Eval: 4.5108 s/iter. Total: 5.1882 s/iter. ETA=0:02:14\n",
            "[09/08 17:17:16 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0034 s/iter. Inference: 0.6636 s/iter. Eval: 4.5032 s/iter. Total: 5.1712 s/iter. ETA=0:02:04\n",
            "[09/08 17:17:24 d2.evaluation.evaluator]: Inference done 128/150. Dataloading: 0.0034 s/iter. Inference: 0.6546 s/iter. Eval: 4.4956 s/iter. Total: 5.1545 s/iter. ETA=0:01:53\n",
            "[09/08 17:17:32 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0034 s/iter. Inference: 0.6458 s/iter. Eval: 4.4884 s/iter. Total: 5.1385 s/iter. ETA=0:01:42\n",
            "[09/08 17:17:41 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0034 s/iter. Inference: 0.6373 s/iter. Eval: 4.4814 s/iter. Total: 5.1230 s/iter. ETA=0:01:32\n",
            "[09/08 17:17:49 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0034 s/iter. Inference: 0.6291 s/iter. Eval: 4.4751 s/iter. Total: 5.1085 s/iter. ETA=0:01:21\n",
            "[09/08 17:17:57 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0034 s/iter. Inference: 0.6211 s/iter. Eval: 4.4687 s/iter. Total: 5.0942 s/iter. ETA=0:01:11\n",
            "[09/08 17:18:06 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0034 s/iter. Inference: 0.6134 s/iter. Eval: 4.4626 s/iter. Total: 5.0803 s/iter. ETA=0:01:00\n",
            "[09/08 17:18:15 d2.evaluation.evaluator]: Inference done 140/150. Dataloading: 0.0034 s/iter. Inference: 0.6061 s/iter. Eval: 4.4654 s/iter. Total: 5.0758 s/iter. ETA=0:00:50\n",
            "[09/08 17:18:24 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0034 s/iter. Inference: 0.5988 s/iter. Eval: 4.4594 s/iter. Total: 5.0625 s/iter. ETA=0:00:40\n",
            "[09/08 17:18:32 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0034 s/iter. Inference: 0.5917 s/iter. Eval: 4.4535 s/iter. Total: 5.0495 s/iter. ETA=0:00:30\n",
            "[09/08 17:18:40 d2.evaluation.evaluator]: Inference done 146/150. Dataloading: 0.0034 s/iter. Inference: 0.5848 s/iter. Eval: 4.4475 s/iter. Total: 5.0367 s/iter. ETA=0:00:20\n",
            "[09/08 17:18:48 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0034 s/iter. Inference: 0.5781 s/iter. Eval: 4.4399 s/iter. Total: 5.0223 s/iter. ETA=0:00:10\n",
            "[09/08 17:18:56 d2.evaluation.evaluator]: Inference done 150/150. Dataloading: 0.0034 s/iter. Inference: 0.5715 s/iter. Eval: 4.4297 s/iter. Total: 5.0056 s/iter. ETA=0:00:00\n",
            "[09/08 17:18:56 d2.evaluation.evaluator]: Total inference time: 0:12:05.854926 (5.005896 s / iter per device, on 1 devices)\n",
            "[09/08 17:18:56 d2.evaluation.evaluator]: Total inference pure compute time: 0:01:22 (0.571533 s / iter per device, on 1 devices)\n",
            "[09/08 17:18:56 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/08 17:18:56 d2.evaluation.coco_evaluation]: Saving results to output/coco_instances_results.json\n",
            "[09/08 17:18:56 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/08 17:18:56 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/08 17:18:57 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/08 17:18:57 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/08 17:18:57 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/08 17:18:57 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/08 17:18:57 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.37s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/08 17:18:58 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/08 17:18:58 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.37 seconds.\n",
            "[09/08 17:18:58 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/08 17:18:58 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.032\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.029\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.039\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.076\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.142\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.147\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.080\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.170\n",
            "[09/08 17:18:58 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 3.193 | 5.703  | 2.939  | 0.779 | 1.585 | 3.943 |\n",
            "[09/08 17:18:58 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 6.081 | Other      | 2.678 | Bottle     | 6.434 |\n",
            "| Bottle cap            | 4.466 | Cup        | 0.483 | Lid        | 6.960 |\n",
            "| Plastic bag + wrapper | 3.927 | Pop tab    | 0.000 | Straw      | 0.047 |\n",
            "| Cigarette             | 0.854 |            |       |            |       |\n",
            "OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0, 'AP-Can': 0.0, 'AP-Other': 0.0, 'AP-Bottle': 0.0, 'AP-Bottle cap': 0.0, 'AP-Cup': 0.0, 'AP-Lid': 0.0, 'AP-Plastic bag + wrapper': 0.0, 'AP-Pop tab': 0.0, 'AP-Straw': 0.0, 'AP-Cigarette': 0.0}), ('segm', {'AP': 3.193065691667557, 'AP50': 5.703386765399098, 'AP75': 2.938582806066522, 'APs': 0.778566636226451, 'APm': 1.5847383821228838, 'APl': 3.9431678117503046, 'AP-Can': 6.080923098419165, 'AP-Other': 2.678336797706245, 'AP-Bottle': 6.434421827587629, 'AP-Bottle cap': 4.466271195968005, 'AP-Cup': 0.4831948081901288, 'AP-Lid': 6.960185380240151, 'AP-Plastic bag + wrapper': 3.9270299063861582, 'AP-Pop tab': 0.0, 'AP-Straw': 0.046592894583575996, 'AP-Cigarette': 0.8537010075945125})])\n"
          ]
        }
      ],
      "source": [
        "from detectron2.evaluation import inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "evaluator = COCOEvaluator(\"taco10_test\", output_dir=\"output\")\n",
        "mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
        "test_loader = build_detection_test_loader(cfg, \"taco10_test\", mapper=mapper)\n",
        "metrics = inference_on_dataset(trainer.model, test_loader, evaluator)\n",
        "print(metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
