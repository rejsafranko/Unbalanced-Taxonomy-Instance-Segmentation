{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jHOC2HC0sCNE"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iUmK767kWQlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad249fe-51b9-438a-a489-9d5ff9913e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mask2Former' already exists and is not an empty directory.\n",
            "/content/Mask2Former\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-w7_xj3ws\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-w7_xj3ws\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (9.4.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.11.0)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.23.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2024.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (4.66.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm->-r requirements.txt (line 4)) (1.3.0)\n",
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n",
            "running build\n",
            "running build_py\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "/content/Mask2Former\n"
          ]
        }
      ],
      "source": [
        "# clone and install Mask2Former\n",
        "!git clone https://github.com/facebookresearch/Mask2Former.git\n",
        "%cd Mask2Former\n",
        "!pip install -U opencv-python\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!pip install -r requirements.txt\n",
        "%cd mask2former/modeling/pixel_decoder/ops\n",
        "!python setup.py build install\n",
        "%cd ../../../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xGxctvB_Pdkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9297aa24-6d03-44dd-87e2-5219af0ffb25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-4d6fvaa_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-4d6fvaa_\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 5b72c27ae39f99db75d43f18fd1312e1ea934e60\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ccGvME_lQkbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3392658b-605a-47d6-906c-5edf011d0ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n"
          ]
        }
      ],
      "source": [
        "%cd mask2former/modeling/pixel_decoder/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xP44DZ_zQooA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8a38fa-a65d-414b-8ec8-fd07a37163db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build\n",
            "running build_py\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ],
      "source": [
        "!sh make.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fy0WqfqBQsqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2da420-aa48-4c11-ac8e-10cfbb8769c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former\n"
          ]
        }
      ],
      "source": [
        "%cd ../../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# import Mask2Former project\n",
        "from mask2former import add_maskformer2_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJwaL06RNCY",
        "outputId": "beb2ee37-3275-402d-c28f-e44d302a2a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aQ-zjQCtRU1F"
      },
      "outputs": [],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "setup_logger(name=\"mask2former\")\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.projects.deeplab import add_deeplab_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wg5JCtyWRZOo"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7EO4nMI9Rb5P"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco10_train\", {}, data_dir_path + \"mapped_annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_val\", {}, data_dir_path + \"mapped_annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_test\", {}, data_dir_path + \"mapped_annotations_0_test.json\", data_dir_path + \"images/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y44xCY4Gu_ou"
      },
      "source": [
        "# Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IZL4O9aw3XXp"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.structures import PolygonMasks\n",
        "import copy\n",
        "import torch  # Import torch to convert images to tensors\n",
        "from argparse import ArgumentParser\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from mask2former import (\n",
        "    MaskFormerInstanceDatasetMapper,\n",
        "    InstanceSegEvaluator,\n",
        "    add_maskformer2_config,\n",
        ")\n",
        "from detectron2.evaluation import COCOEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6kaK-F3shWqR"
      },
      "outputs": [],
      "source": [
        "class Trainer(DefaultTrainer):\n",
        "    \"\"\"\n",
        "    Extension of the Trainer class adapted to MaskFormer.\n",
        "    \"\"\"\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
        "        return build_detection_train_loader(cfg, mapper=mapper)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "      if output_folder is None:\n",
        "              output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "      print(dataset_name)\n",
        "      return COCOEvaluator(dataset_name, output_dir=output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6FlsDz1hq_54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "add_deeplab_config(cfg)\n",
        "add_maskformer2_config(cfg)\n",
        "cfg.merge_from_file(\"configs/coco/instance-segmentation/maskformer2_R50_bs16_50ep.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4FdGn2TvPAn",
        "outputId": "c91a8dce-6dec-4f90-8d79-9a7981089296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/04 10:50:22 d2.engine.defaults]: Model:\n",
            "MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "[09/04 10:50:22 mask2former.data.dataset_mappers.mask_former_instance_dataset_mapper]: [MaskFormerInstanceDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/04 10:50:24 d2.data.datasets.coco]: Loading /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json takes 1.96 seconds.\n",
            "[09/04 10:50:24 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json\n",
            "[09/04 10:50:24 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[09/04 10:50:24 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 194          |   Other    | 1397         |   Bottle   | 344          |\n",
            "|  Bottle cap   | 226          |    Cup     | 150          |    Lid     | 63           |\n",
            "| Plastic bag.. | 697          |  Pop tab   | 75           |   Straw    | 108          |\n",
            "|   Cigarette   | 457          |            |              |            |              |\n",
            "|     total     | 3711         |            |              |            |              |\n",
            "[09/04 10:50:24 d2.data.build]: Using training sampler TrainingSampler\n",
            "[09/04 10:50:24 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/04 10:50:24 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[09/04 10:50:24 d2.data.common]: Serialized dataset takes 1.77 MiB\n",
            "[09/04 10:50:24 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[09/04 10:50:24 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_3c8ec9.pkl: 176MB [00:15, 11.2MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING [09/04 10:50:40 mask2former.modeling.transformer_decoder.mask2former_transformer_decoder]: Weight format of MultiScaleMaskedTransformerDecoder have changed! Please upgrade your models. Applying automatic conversion now ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (81, 256) in the checkpoint but (11, 256) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "criterion.empty_weight\n",
            "sem_seg_head.predictor.class_embed.{bias, weight}\n"
          ]
        }
      ],
      "source": [
        "cfg.DATASETS.TRAIN = (\"taco10_train\",)\n",
        "cfg.DATASETS.TEST = (\"taco10_val\",)\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "cfg.INPUT.DATASET_MAPPER_NAME = \"mask_former_instance\"\n",
        "\n",
        "cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 10\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"value\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0001\n",
        "cfg.SOLVER.MAX_ITER = 10800\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "cfg.TEST.EVAL_PERIOD = 1200\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuliSkJQer3",
        "outputId": "60082196-0f11-4c9f-9ffc-2dd2d68f9ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:28:40 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0022 s/iter. Inference: 2.3018 s/iter. Eval: 4.6547 s/iter. Total: 6.9596 s/iter. ETA=0:12:38\n",
            "[09/04 13:28:46 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0022 s/iter. Inference: 2.2521 s/iter. Eval: 4.6653 s/iter. Total: 6.9206 s/iter. ETA=0:12:27\n",
            "[09/04 13:28:53 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0022 s/iter. Inference: 2.2066 s/iter. Eval: 4.7166 s/iter. Total: 6.9263 s/iter. ETA=0:12:21\n",
            "[09/04 13:29:00 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0022 s/iter. Inference: 2.1533 s/iter. Eval: 4.7669 s/iter. Total: 6.9235 s/iter. ETA=0:12:13\n",
            "[09/04 13:29:06 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0022 s/iter. Inference: 2.1028 s/iter. Eval: 4.8133 s/iter. Total: 6.9193 s/iter. ETA=0:12:06\n",
            "[09/04 13:29:13 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0022 s/iter. Inference: 2.0547 s/iter. Eval: 4.8576 s/iter. Total: 6.9156 s/iter. ETA=0:11:59\n",
            "[09/04 13:29:20 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0022 s/iter. Inference: 2.0089 s/iter. Eval: 4.8989 s/iter. Total: 6.9110 s/iter. ETA=0:11:51\n",
            "[09/04 13:29:27 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0022 s/iter. Inference: 1.9654 s/iter. Eval: 4.9377 s/iter. Total: 6.9064 s/iter. ETA=0:11:44\n",
            "[09/04 13:29:33 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0022 s/iter. Inference: 1.9237 s/iter. Eval: 4.9765 s/iter. Total: 6.9034 s/iter. ETA=0:11:37\n",
            "[09/04 13:29:40 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0022 s/iter. Inference: 1.8838 s/iter. Eval: 5.0121 s/iter. Total: 6.8992 s/iter. ETA=0:11:29\n",
            "[09/04 13:29:47 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0022 s/iter. Inference: 1.8457 s/iter. Eval: 5.0458 s/iter. Total: 6.8948 s/iter. ETA=0:11:22\n",
            "[09/04 13:29:53 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0022 s/iter. Inference: 1.8092 s/iter. Eval: 5.0797 s/iter. Total: 6.8921 s/iter. ETA=0:11:15\n",
            "[09/04 13:30:00 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0022 s/iter. Inference: 1.7742 s/iter. Eval: 5.1121 s/iter. Total: 6.8896 s/iter. ETA=0:11:08\n",
            "[09/04 13:30:07 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0023 s/iter. Inference: 1.7406 s/iter. Eval: 5.1342 s/iter. Total: 6.8781 s/iter. ETA=0:11:00\n",
            "[09/04 13:30:13 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0023 s/iter. Inference: 1.7083 s/iter. Eval: 5.1565 s/iter. Total: 6.8681 s/iter. ETA=0:10:52\n",
            "[09/04 13:30:19 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0023 s/iter. Inference: 1.6774 s/iter. Eval: 5.1766 s/iter. Total: 6.8573 s/iter. ETA=0:10:44\n",
            "[09/04 13:30:26 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0023 s/iter. Inference: 1.6476 s/iter. Eval: 5.1964 s/iter. Total: 6.8473 s/iter. ETA=0:10:36\n",
            "[09/04 13:30:32 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0023 s/iter. Inference: 1.6189 s/iter. Eval: 5.2157 s/iter. Total: 6.8379 s/iter. ETA=0:10:29\n",
            "[09/04 13:30:38 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0023 s/iter. Inference: 1.5913 s/iter. Eval: 5.2353 s/iter. Total: 6.8299 s/iter. ETA=0:10:21\n",
            "[09/04 13:30:45 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0023 s/iter. Inference: 1.5647 s/iter. Eval: 5.2532 s/iter. Total: 6.8212 s/iter. ETA=0:10:13\n",
            "[09/04 13:30:51 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0023 s/iter. Inference: 1.5390 s/iter. Eval: 5.2712 s/iter. Total: 6.8135 s/iter. ETA=0:10:06\n",
            "[09/04 13:30:57 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0023 s/iter. Inference: 1.5142 s/iter. Eval: 5.2878 s/iter. Total: 6.8053 s/iter. ETA=0:09:58\n",
            "[09/04 13:31:04 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0023 s/iter. Inference: 1.4903 s/iter. Eval: 5.3056 s/iter. Total: 6.7992 s/iter. ETA=0:09:51\n",
            "[09/04 13:31:10 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0023 s/iter. Inference: 1.4672 s/iter. Eval: 5.3212 s/iter. Total: 6.7918 s/iter. ETA=0:09:44\n",
            "[09/04 13:31:21 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0023 s/iter. Inference: 1.4009 s/iter. Eval: 5.2332 s/iter. Total: 6.6374 s/iter. ETA=0:09:10\n",
            "[09/04 13:31:27 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0023 s/iter. Inference: 1.3807 s/iter. Eval: 5.2491 s/iter. Total: 6.6331 s/iter. ETA=0:09:03\n",
            "[09/04 13:31:34 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0023 s/iter. Inference: 1.3412 s/iter. Eval: 5.1784 s/iter. Total: 6.5228 s/iter. ETA=0:08:41\n",
            "[09/04 13:31:42 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0023 s/iter. Inference: 1.3043 s/iter. Eval: 5.1448 s/iter. Total: 6.4524 s/iter. ETA=0:08:23\n",
            "[09/04 13:31:50 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0022 s/iter. Inference: 1.2697 s/iter. Eval: 5.1131 s/iter. Total: 6.3860 s/iter. ETA=0:08:05\n",
            "[09/04 13:31:59 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0023 s/iter. Inference: 1.2369 s/iter. Eval: 5.0843 s/iter. Total: 6.3244 s/iter. ETA=0:07:48\n",
            "[09/04 13:32:07 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0023 s/iter. Inference: 1.2060 s/iter. Eval: 5.0571 s/iter. Total: 6.2664 s/iter. ETA=0:07:31\n",
            "[09/04 13:32:15 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0022 s/iter. Inference: 1.1768 s/iter. Eval: 5.0311 s/iter. Total: 6.2111 s/iter. ETA=0:07:14\n",
            "[09/04 13:32:24 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0022 s/iter. Inference: 1.1490 s/iter. Eval: 5.0061 s/iter. Total: 6.1583 s/iter. ETA=0:06:58\n",
            "[09/04 13:32:32 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0022 s/iter. Inference: 1.1226 s/iter. Eval: 4.9817 s/iter. Total: 6.1075 s/iter. ETA=0:06:43\n",
            "[09/04 13:32:40 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0022 s/iter. Inference: 1.0975 s/iter. Eval: 4.9590 s/iter. Total: 6.0597 s/iter. ETA=0:06:27\n",
            "[09/04 13:32:49 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0022 s/iter. Inference: 1.0736 s/iter. Eval: 4.9370 s/iter. Total: 6.0139 s/iter. ETA=0:06:12\n",
            "[09/04 13:32:57 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0022 s/iter. Inference: 1.0509 s/iter. Eval: 4.9159 s/iter. Total: 5.9700 s/iter. ETA=0:05:58\n",
            "[09/04 13:33:05 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0022 s/iter. Inference: 1.0292 s/iter. Eval: 4.8968 s/iter. Total: 5.9292 s/iter. ETA=0:05:43\n",
            "[09/04 13:33:14 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0022 s/iter. Inference: 1.0086 s/iter. Eval: 4.8775 s/iter. Total: 5.8893 s/iter. ETA=0:05:29\n",
            "[09/04 13:33:22 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0022 s/iter. Inference: 0.9888 s/iter. Eval: 4.8592 s/iter. Total: 5.8512 s/iter. ETA=0:05:15\n",
            "[09/04 13:33:30 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0022 s/iter. Inference: 0.9699 s/iter. Eval: 4.8417 s/iter. Total: 5.8147 s/iter. ETA=0:05:02\n",
            "[09/04 13:33:39 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 0.9517 s/iter. Eval: 4.8251 s/iter. Total: 5.7799 s/iter. ETA=0:04:48\n",
            "[09/04 13:33:47 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0022 s/iter. Inference: 0.9344 s/iter. Eval: 4.8091 s/iter. Total: 5.7466 s/iter. ETA=0:04:35\n",
            "[09/04 13:33:55 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0022 s/iter. Inference: 0.9177 s/iter. Eval: 4.7947 s/iter. Total: 5.7155 s/iter. ETA=0:04:22\n",
            "[09/04 13:34:04 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0022 s/iter. Inference: 0.9017 s/iter. Eval: 4.7799 s/iter. Total: 5.6847 s/iter. ETA=0:04:10\n",
            "[09/04 13:34:12 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0022 s/iter. Inference: 0.8862 s/iter. Eval: 4.7661 s/iter. Total: 5.6554 s/iter. ETA=0:03:57\n",
            "[09/04 13:34:20 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0022 s/iter. Inference: 0.8714 s/iter. Eval: 4.7523 s/iter. Total: 5.6268 s/iter. ETA=0:03:45\n",
            "[09/04 13:34:29 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0022 s/iter. Inference: 0.8572 s/iter. Eval: 4.7389 s/iter. Total: 5.5992 s/iter. ETA=0:03:32\n",
            "[09/04 13:34:36 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0022 s/iter. Inference: 0.8433 s/iter. Eval: 4.7131 s/iter. Total: 5.5595 s/iter. ETA=0:03:20\n",
            "[09/04 13:34:41 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:35:01 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 0.9760 s/iter. Eval: 4.7096 s/iter. Total: 5.6887 s/iter. ETA=0:03:13\n",
            "[09/04 13:35:02 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:35:22 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 1.1127 s/iter. Eval: 4.7119 s/iter. Total: 5.8277 s/iter. ETA=0:03:12\n",
            "[09/04 13:35:31 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 1.0998 s/iter. Eval: 4.7002 s/iter. Total: 5.8031 s/iter. ETA=0:02:59\n",
            "[09/04 13:35:39 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.0826 s/iter. Eval: 4.6888 s/iter. Total: 5.7746 s/iter. ETA=0:02:47\n",
            "[09/04 13:35:48 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.0661 s/iter. Eval: 4.6777 s/iter. Total: 5.7470 s/iter. ETA=0:02:35\n",
            "[09/04 13:35:56 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.0501 s/iter. Eval: 4.6671 s/iter. Total: 5.7204 s/iter. ETA=0:02:23\n",
            "[09/04 13:36:04 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.0347 s/iter. Eval: 4.6571 s/iter. Total: 5.6949 s/iter. ETA=0:02:10\n",
            "[09/04 13:36:10 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.0196 s/iter. Eval: 4.6277 s/iter. Total: 5.6504 s/iter. ETA=0:01:58\n",
            "[09/04 13:36:20 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.0053 s/iter. Eval: 4.6283 s/iter. Total: 5.6367 s/iter. ETA=0:01:47\n",
            "[09/04 13:36:29 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 0.9914 s/iter. Eval: 4.6241 s/iter. Total: 5.6187 s/iter. ETA=0:01:35\n",
            "[09/04 13:36:37 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 0.9779 s/iter. Eval: 4.6152 s/iter. Total: 5.5962 s/iter. ETA=0:01:23\n",
            "[09/04 13:36:45 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 0.9647 s/iter. Eval: 4.6068 s/iter. Total: 5.5746 s/iter. ETA=0:01:12\n",
            "[09/04 13:36:54 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 0.9520 s/iter. Eval: 4.5986 s/iter. Total: 5.5537 s/iter. ETA=0:01:01\n",
            "[09/04 13:37:02 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 0.9396 s/iter. Eval: 4.5903 s/iter. Total: 5.5330 s/iter. ETA=0:00:49\n",
            "[09/04 13:37:10 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 0.9275 s/iter. Eval: 4.5826 s/iter. Total: 5.5132 s/iter. ETA=0:00:38\n",
            "[09/04 13:37:18 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0022 s/iter. Inference: 0.9157 s/iter. Eval: 4.5697 s/iter. Total: 5.4885 s/iter. ETA=0:00:27\n",
            "[09/04 13:37:26 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0022 s/iter. Inference: 0.9043 s/iter. Eval: 4.5572 s/iter. Total: 5.4645 s/iter. ETA=0:00:16\n",
            "[09/04 13:37:33 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0022 s/iter. Inference: 0.8931 s/iter. Eval: 4.5456 s/iter. Total: 5.4418 s/iter. ETA=0:00:05\n",
            "[09/04 13:37:37 d2.evaluation.evaluator]: Total inference time: 0:13:07.451679 (5.430701 s / iter per device, on 1 devices)\n",
            "[09/04 13:37:37 d2.evaluation.evaluator]: Total inference pure compute time: 0:02:08 (0.887674 s / iter per device, on 1 devices)\n",
            "[09/04 13:37:37 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/04 13:37:37 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[09/04 13:37:37 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/04 13:37:37 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/04 13:37:38 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/04 13:37:38 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/04 13:37:38 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/04 13:37:38 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/04 13:37:38 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.30s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/04 13:37:38 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/04 13:37:38 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.30 seconds.\n",
            "[09/04 13:37:38 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/04 13:37:39 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.120\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.172\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.126\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.052\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.262\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.393\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.401\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.234\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.475\n",
            "[09/04 13:37:39 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 11.956 | 17.220 | 12.554 | 0.048 | 5.230 | 16.882 |\n",
            "[09/04 13:37:39 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 22.478 | Other      | 12.836 | Bottle     | 24.217 |\n",
            "| Bottle cap            | 12.325 | Cup        | 13.666 | Lid        | 5.379  |\n",
            "| Plastic bag + wrapper | 16.534 | Pop tab    | 0.348  | Straw      | 9.255  |\n",
            "| Cigarette             | 2.521  |            |        |            |        |\n",
            "[09/04 13:37:39 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[09/04 13:37:39 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/04 13:37:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/04 13:37:39 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/04 13:37:39 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/04 13:37:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/04 13:37:39 d2.evaluation.testing]: copypaste: 11.9558,17.2201,12.5535,0.0476,5.2295,16.8822\n",
            "[09/04 13:37:39 d2.utils.events]:  eta: 0:06:08  iter: 9599  total_loss: 14.61  loss_ce: 0.6079  loss_mask: 0.07071  loss_dice: 0.6285  loss_ce_0: 1.217  loss_mask_0: 0.09019  loss_dice_0: 0.8892  loss_ce_1: 0.8541  loss_mask_1: 0.09818  loss_dice_1: 0.6656  loss_ce_2: 0.7434  loss_mask_2: 0.102  loss_dice_2: 0.6765  loss_ce_3: 0.6428  loss_mask_3: 0.07239  loss_dice_3: 0.6594  loss_ce_4: 0.6098  loss_mask_4: 0.0631  loss_dice_4: 0.5566  loss_ce_5: 0.5612  loss_mask_5: 0.09133  loss_dice_5: 0.5732  loss_ce_6: 0.5667  loss_mask_6: 0.06536  loss_dice_6: 0.631  loss_ce_7: 0.578  loss_mask_7: 0.07361  loss_dice_7: 0.648  loss_ce_8: 0.5265  loss_mask_8: 0.06335  loss_dice_8: 0.5817    time: 0.3651  last_time: 0.3336  data_time: 0.0073  last_data_time: 0.0078   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:37:45 d2.utils.events]:  eta: 0:06:03  iter: 9619  total_loss: 15.12  loss_ce: 0.6768  loss_mask: 0.1033  loss_dice: 0.8118  loss_ce_0: 1.237  loss_mask_0: 0.1027  loss_dice_0: 0.7386  loss_ce_1: 0.8646  loss_mask_1: 0.07569  loss_dice_1: 0.7987  loss_ce_2: 0.7243  loss_mask_2: 0.07506  loss_dice_2: 0.7652  loss_ce_3: 0.7733  loss_mask_3: 0.08773  loss_dice_3: 0.7261  loss_ce_4: 0.6886  loss_mask_4: 0.09276  loss_dice_4: 0.6429  loss_ce_5: 0.6793  loss_mask_5: 0.08384  loss_dice_5: 0.6775  loss_ce_6: 0.6748  loss_mask_6: 0.09197  loss_dice_6: 0.7838  loss_ce_7: 0.5899  loss_mask_7: 0.08755  loss_dice_7: 0.8216  loss_ce_8: 0.6843  loss_mask_8: 0.07898  loss_dice_8: 0.6209    time: 0.3651  last_time: 0.3109  data_time: 0.0083  last_data_time: 0.0089   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:37:51 d2.utils.events]:  eta: 0:05:56  iter: 9639  total_loss: 22.32  loss_ce: 0.8711  loss_mask: 0.04647  loss_dice: 0.73  loss_ce_0: 1.435  loss_mask_0: 0.05878  loss_dice_0: 1.019  loss_ce_1: 0.9642  loss_mask_1: 0.06543  loss_dice_1: 1.087  loss_ce_2: 0.9885  loss_mask_2: 0.05471  loss_dice_2: 0.878  loss_ce_3: 0.8732  loss_mask_3: 0.04957  loss_dice_3: 0.8212  loss_ce_4: 0.775  loss_mask_4: 0.05922  loss_dice_4: 0.7921  loss_ce_5: 0.8024  loss_mask_5: 0.05163  loss_dice_5: 0.8088  loss_ce_6: 0.8039  loss_mask_6: 0.05018  loss_dice_6: 0.7259  loss_ce_7: 0.8077  loss_mask_7: 0.04638  loss_dice_7: 0.8584  loss_ce_8: 0.7822  loss_mask_8: 0.05782  loss_dice_8: 0.8424    time: 0.3649  last_time: 0.3158  data_time: 0.0072  last_data_time: 0.0157   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:37:58 d2.utils.events]:  eta: 0:05:50  iter: 9659  total_loss: 16.14  loss_ce: 0.8446  loss_mask: 0.03549  loss_dice: 0.6578  loss_ce_0: 1.787  loss_mask_0: 0.04029  loss_dice_0: 0.9493  loss_ce_1: 1.093  loss_mask_1: 0.04313  loss_dice_1: 0.7307  loss_ce_2: 0.9716  loss_mask_2: 0.04075  loss_dice_2: 0.7804  loss_ce_3: 0.8253  loss_mask_3: 0.03956  loss_dice_3: 0.7783  loss_ce_4: 0.8806  loss_mask_4: 0.03783  loss_dice_4: 0.5726  loss_ce_5: 0.876  loss_mask_5: 0.04072  loss_dice_5: 0.6521  loss_ce_6: 0.7867  loss_mask_6: 0.03696  loss_dice_6: 0.7201  loss_ce_7: 0.7649  loss_mask_7: 0.0464  loss_dice_7: 0.6126  loss_ce_8: 0.8153  loss_mask_8: 0.03834  loss_dice_8: 0.5667    time: 0.3648  last_time: 0.3055  data_time: 0.0069  last_data_time: 0.0073   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:04 d2.utils.events]:  eta: 0:05:44  iter: 9679  total_loss: 16.67  loss_ce: 0.6442  loss_mask: 0.05933  loss_dice: 0.6412  loss_ce_0: 1.388  loss_mask_0: 0.101  loss_dice_0: 0.9076  loss_ce_1: 0.8047  loss_mask_1: 0.1012  loss_dice_1: 0.8213  loss_ce_2: 0.7154  loss_mask_2: 0.07099  loss_dice_2: 0.6277  loss_ce_3: 0.6168  loss_mask_3: 0.08602  loss_dice_3: 0.7085  loss_ce_4: 0.6031  loss_mask_4: 0.1044  loss_dice_4: 0.7496  loss_ce_5: 0.6372  loss_mask_5: 0.1133  loss_dice_5: 0.8414  loss_ce_6: 0.6514  loss_mask_6: 0.07606  loss_dice_6: 0.7682  loss_ce_7: 0.6186  loss_mask_7: 0.06633  loss_dice_7: 0.586  loss_ce_8: 0.6331  loss_mask_8: 0.06583  loss_dice_8: 0.6475    time: 0.3648  last_time: 0.3254  data_time: 0.0070  last_data_time: 0.0061   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:11 d2.utils.events]:  eta: 0:05:38  iter: 9699  total_loss: 19  loss_ce: 0.5906  loss_mask: 0.06534  loss_dice: 0.7696  loss_ce_0: 1.002  loss_mask_0: 0.09619  loss_dice_0: 0.9182  loss_ce_1: 0.9897  loss_mask_1: 0.07291  loss_dice_1: 1.114  loss_ce_2: 0.8798  loss_mask_2: 0.06655  loss_dice_2: 0.8212  loss_ce_3: 0.73  loss_mask_3: 0.06333  loss_dice_3: 0.8559  loss_ce_4: 0.6744  loss_mask_4: 0.06166  loss_dice_4: 0.7689  loss_ce_5: 0.7517  loss_mask_5: 0.06132  loss_dice_5: 0.8092  loss_ce_6: 0.6551  loss_mask_6: 0.06558  loss_dice_6: 0.6955  loss_ce_7: 0.5954  loss_mask_7: 0.07445  loss_dice_7: 0.7806  loss_ce_8: 0.602  loss_mask_8: 0.0694  loss_dice_8: 0.7426    time: 0.3647  last_time: 0.3079  data_time: 0.0114  last_data_time: 0.0111   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:17 d2.utils.events]:  eta: 0:05:32  iter: 9719  total_loss: 18.24  loss_ce: 0.7252  loss_mask: 0.1224  loss_dice: 0.8113  loss_ce_0: 1.009  loss_mask_0: 0.162  loss_dice_0: 1.052  loss_ce_1: 0.6983  loss_mask_1: 0.1306  loss_dice_1: 0.7926  loss_ce_2: 0.5993  loss_mask_2: 0.1381  loss_dice_2: 0.6345  loss_ce_3: 0.59  loss_mask_3: 0.1488  loss_dice_3: 0.9121  loss_ce_4: 0.8058  loss_mask_4: 0.1509  loss_dice_4: 0.8111  loss_ce_5: 0.5998  loss_mask_5: 0.1682  loss_dice_5: 1.055  loss_ce_6: 0.6617  loss_mask_6: 0.1391  loss_dice_6: 0.8872  loss_ce_7: 0.7075  loss_mask_7: 0.1255  loss_dice_7: 0.6453  loss_ce_8: 0.6618  loss_mask_8: 0.1204  loss_dice_8: 0.7323    time: 0.3646  last_time: 0.3031  data_time: 0.0063  last_data_time: 0.0054   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:23 d2.utils.events]:  eta: 0:05:26  iter: 9739  total_loss: 17.33  loss_ce: 0.7257  loss_mask: 0.05512  loss_dice: 0.6039  loss_ce_0: 1.209  loss_mask_0: 0.06797  loss_dice_0: 0.8728  loss_ce_1: 0.7499  loss_mask_1: 0.0623  loss_dice_1: 0.9089  loss_ce_2: 0.7445  loss_mask_2: 0.06733  loss_dice_2: 1.055  loss_ce_3: 0.7168  loss_mask_3: 0.0601  loss_dice_3: 0.5952  loss_ce_4: 0.5771  loss_mask_4: 0.05887  loss_dice_4: 0.6716  loss_ce_5: 0.7211  loss_mask_5: 0.06472  loss_dice_5: 0.6565  loss_ce_6: 0.6882  loss_mask_6: 0.06305  loss_dice_6: 0.6959  loss_ce_7: 0.7672  loss_mask_7: 0.0594  loss_dice_7: 0.6935  loss_ce_8: 0.6587  loss_mask_8: 0.05648  loss_dice_8: 0.667    time: 0.3645  last_time: 0.2943  data_time: 0.0078  last_data_time: 0.0054   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:29 d2.utils.events]:  eta: 0:05:19  iter: 9759  total_loss: 18.34  loss_ce: 0.7723  loss_mask: 0.099  loss_dice: 0.6419  loss_ce_0: 1.503  loss_mask_0: 0.1235  loss_dice_0: 0.9178  loss_ce_1: 0.9223  loss_mask_1: 0.1492  loss_dice_1: 0.6225  loss_ce_2: 0.8611  loss_mask_2: 0.1356  loss_dice_2: 0.8489  loss_ce_3: 0.8194  loss_mask_3: 0.1104  loss_dice_3: 0.782  loss_ce_4: 0.8196  loss_mask_4: 0.1044  loss_dice_4: 0.8796  loss_ce_5: 0.7705  loss_mask_5: 0.1103  loss_dice_5: 0.8128  loss_ce_6: 0.7475  loss_mask_6: 0.1062  loss_dice_6: 0.776  loss_ce_7: 0.7829  loss_mask_7: 0.1051  loss_dice_7: 0.8855  loss_ce_8: 0.7466  loss_mask_8: 0.1274  loss_dice_8: 0.6723    time: 0.3643  last_time: 0.3228  data_time: 0.0070  last_data_time: 0.0023   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:36 d2.utils.events]:  eta: 0:05:13  iter: 9779  total_loss: 23.33  loss_ce: 0.9387  loss_mask: 0.05092  loss_dice: 1.178  loss_ce_0: 1.653  loss_mask_0: 0.07186  loss_dice_0: 1.241  loss_ce_1: 1.445  loss_mask_1: 0.05395  loss_dice_1: 1.141  loss_ce_2: 1.173  loss_mask_2: 0.05055  loss_dice_2: 1.14  loss_ce_3: 1.097  loss_mask_3: 0.0584  loss_dice_3: 1.201  loss_ce_4: 1.058  loss_mask_4: 0.05775  loss_dice_4: 1.067  loss_ce_5: 0.9568  loss_mask_5: 0.05673  loss_dice_5: 1.19  loss_ce_6: 0.9042  loss_mask_6: 0.04872  loss_dice_6: 1.007  loss_ce_7: 0.9096  loss_mask_7: 0.06318  loss_dice_7: 1.087  loss_ce_8: 0.9405  loss_mask_8: 0.0576  loss_dice_8: 1.04    time: 0.3642  last_time: 0.3101  data_time: 0.0074  last_data_time: 0.0063   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:42 d2.utils.events]:  eta: 0:05:07  iter: 9799  total_loss: 19.84  loss_ce: 0.7546  loss_mask: 0.07192  loss_dice: 0.8273  loss_ce_0: 1.555  loss_mask_0: 0.06766  loss_dice_0: 0.8433  loss_ce_1: 1.138  loss_mask_1: 0.07501  loss_dice_1: 0.874  loss_ce_2: 0.99  loss_mask_2: 0.08898  loss_dice_2: 0.8986  loss_ce_3: 0.8263  loss_mask_3: 0.07277  loss_dice_3: 0.7054  loss_ce_4: 0.8123  loss_mask_4: 0.0706  loss_dice_4: 0.7657  loss_ce_5: 0.8054  loss_mask_5: 0.06988  loss_dice_5: 0.697  loss_ce_6: 0.8162  loss_mask_6: 0.06719  loss_dice_6: 0.6336  loss_ce_7: 0.8035  loss_mask_7: 0.07597  loss_dice_7: 0.673  loss_ce_8: 0.7867  loss_mask_8: 0.06343  loss_dice_8: 0.6995    time: 0.3642  last_time: 0.3295  data_time: 0.0145  last_data_time: 0.0065   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:48 d2.utils.events]:  eta: 0:05:01  iter: 9819  total_loss: 23.87  loss_ce: 0.8048  loss_mask: 0.07669  loss_dice: 1.028  loss_ce_0: 1.251  loss_mask_0: 0.07738  loss_dice_0: 1.447  loss_ce_1: 1.04  loss_mask_1: 0.09422  loss_dice_1: 1.063  loss_ce_2: 0.9748  loss_mask_2: 0.08559  loss_dice_2: 0.9761  loss_ce_3: 0.8561  loss_mask_3: 0.06688  loss_dice_3: 1.095  loss_ce_4: 0.7832  loss_mask_4: 0.07574  loss_dice_4: 1.228  loss_ce_5: 0.7971  loss_mask_5: 0.07287  loss_dice_5: 1.146  loss_ce_6: 0.8244  loss_mask_6: 0.07099  loss_dice_6: 0.9114  loss_ce_7: 0.772  loss_mask_7: 0.07497  loss_dice_7: 1.023  loss_ce_8: 0.8441  loss_mask_8: 0.08033  loss_dice_8: 1.106    time: 0.3641  last_time: 0.3322  data_time: 0.0097  last_data_time: 0.0092   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:38:55 d2.utils.events]:  eta: 0:04:55  iter: 9839  total_loss: 17.45  loss_ce: 0.8314  loss_mask: 0.066  loss_dice: 0.7267  loss_ce_0: 1.183  loss_mask_0: 0.08357  loss_dice_0: 1.027  loss_ce_1: 0.9665  loss_mask_1: 0.07453  loss_dice_1: 0.6885  loss_ce_2: 0.9547  loss_mask_2: 0.06985  loss_dice_2: 0.9325  loss_ce_3: 0.8828  loss_mask_3: 0.0621  loss_dice_3: 0.9426  loss_ce_4: 0.9284  loss_mask_4: 0.06542  loss_dice_4: 0.9727  loss_ce_5: 0.9193  loss_mask_5: 0.07305  loss_dice_5: 1.012  loss_ce_6: 0.9003  loss_mask_6: 0.07346  loss_dice_6: 0.9142  loss_ce_7: 0.8043  loss_mask_7: 0.07599  loss_dice_7: 0.9738  loss_ce_8: 0.8045  loss_mask_8: 0.06974  loss_dice_8: 0.8093    time: 0.3640  last_time: 0.2990  data_time: 0.0080  last_data_time: 0.0056   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:01 d2.utils.events]:  eta: 0:04:49  iter: 9859  total_loss: 19.69  loss_ce: 0.8271  loss_mask: 0.1065  loss_dice: 0.6799  loss_ce_0: 1.3  loss_mask_0: 0.1212  loss_dice_0: 0.9017  loss_ce_1: 0.9197  loss_mask_1: 0.0965  loss_dice_1: 0.9098  loss_ce_2: 1.028  loss_mask_2: 0.1171  loss_dice_2: 0.8373  loss_ce_3: 0.8673  loss_mask_3: 0.1135  loss_dice_3: 0.7637  loss_ce_4: 0.7965  loss_mask_4: 0.09763  loss_dice_4: 0.8141  loss_ce_5: 0.8323  loss_mask_5: 0.09554  loss_dice_5: 0.8293  loss_ce_6: 0.8179  loss_mask_6: 0.1112  loss_dice_6: 0.7222  loss_ce_7: 0.8051  loss_mask_7: 0.1015  loss_dice_7: 0.773  loss_ce_8: 0.8124  loss_mask_8: 0.1061  loss_dice_8: 0.7049    time: 0.3639  last_time: 0.2968  data_time: 0.0065  last_data_time: 0.0054   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:07 d2.utils.events]:  eta: 0:04:43  iter: 9879  total_loss: 18.05  loss_ce: 0.8226  loss_mask: 0.08031  loss_dice: 0.5771  loss_ce_0: 1.58  loss_mask_0: 0.07754  loss_dice_0: 0.7817  loss_ce_1: 1.199  loss_mask_1: 0.09169  loss_dice_1: 0.7496  loss_ce_2: 1.096  loss_mask_2: 0.08069  loss_dice_2: 0.6577  loss_ce_3: 0.8714  loss_mask_3: 0.08096  loss_dice_3: 0.76  loss_ce_4: 0.9071  loss_mask_4: 0.08868  loss_dice_4: 0.5387  loss_ce_5: 0.8849  loss_mask_5: 0.07977  loss_dice_5: 0.5617  loss_ce_6: 0.8769  loss_mask_6: 0.07791  loss_dice_6: 0.6992  loss_ce_7: 0.8763  loss_mask_7: 0.07592  loss_dice_7: 0.6261  loss_ce_8: 0.8234  loss_mask_8: 0.07968  loss_dice_8: 0.733    time: 0.3637  last_time: 0.3009  data_time: 0.0069  last_data_time: 0.0069   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:13 d2.utils.events]:  eta: 0:04:37  iter: 9899  total_loss: 20.07  loss_ce: 0.8485  loss_mask: 0.1139  loss_dice: 0.5921  loss_ce_0: 1.214  loss_mask_0: 0.1359  loss_dice_0: 1.077  loss_ce_1: 1.024  loss_mask_1: 0.1133  loss_dice_1: 0.739  loss_ce_2: 1.133  loss_mask_2: 0.1584  loss_dice_2: 0.6993  loss_ce_3: 1.032  loss_mask_3: 0.1333  loss_dice_3: 0.7018  loss_ce_4: 1.033  loss_mask_4: 0.1413  loss_dice_4: 0.6962  loss_ce_5: 0.9508  loss_mask_5: 0.1376  loss_dice_5: 0.8332  loss_ce_6: 0.9065  loss_mask_6: 0.1213  loss_dice_6: 0.7331  loss_ce_7: 0.9155  loss_mask_7: 0.1124  loss_dice_7: 0.6722  loss_ce_8: 0.7938  loss_mask_8: 0.1089  loss_dice_8: 0.8441    time: 0.3636  last_time: 0.2987  data_time: 0.0068  last_data_time: 0.0047   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:20 d2.utils.events]:  eta: 0:04:30  iter: 9919  total_loss: 29.62  loss_ce: 1.085  loss_mask: 0.1048  loss_dice: 1.471  loss_ce_0: 1.602  loss_mask_0: 0.1176  loss_dice_0: 1.632  loss_ce_1: 1.236  loss_mask_1: 0.106  loss_dice_1: 1.373  loss_ce_2: 1.059  loss_mask_2: 0.1002  loss_dice_2: 1.345  loss_ce_3: 0.9964  loss_mask_3: 0.1196  loss_dice_3: 1.405  loss_ce_4: 1.143  loss_mask_4: 0.1125  loss_dice_4: 1.341  loss_ce_5: 0.9138  loss_mask_5: 0.1186  loss_dice_5: 1.384  loss_ce_6: 1.084  loss_mask_6: 0.1017  loss_dice_6: 1.377  loss_ce_7: 0.9424  loss_mask_7: 0.1075  loss_dice_7: 1.381  loss_ce_8: 0.9637  loss_mask_8: 0.1114  loss_dice_8: 1.456    time: 0.3635  last_time: 0.3228  data_time: 0.0072  last_data_time: 0.0022   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:26 d2.utils.events]:  eta: 0:04:24  iter: 9939  total_loss: 23.17  loss_ce: 0.7594  loss_mask: 0.08873  loss_dice: 0.7877  loss_ce_0: 1.243  loss_mask_0: 0.126  loss_dice_0: 1.175  loss_ce_1: 1.032  loss_mask_1: 0.09414  loss_dice_1: 0.9284  loss_ce_2: 0.8533  loss_mask_2: 0.09904  loss_dice_2: 0.8026  loss_ce_3: 0.8418  loss_mask_3: 0.09303  loss_dice_3: 0.9619  loss_ce_4: 0.8547  loss_mask_4: 0.09886  loss_dice_4: 0.7624  loss_ce_5: 0.8841  loss_mask_5: 0.09457  loss_dice_5: 1.072  loss_ce_6: 0.763  loss_mask_6: 0.07966  loss_dice_6: 0.9963  loss_ce_7: 0.8034  loss_mask_7: 0.09186  loss_dice_7: 0.7696  loss_ce_8: 0.9221  loss_mask_8: 0.08563  loss_dice_8: 0.8635    time: 0.3634  last_time: 0.3068  data_time: 0.0078  last_data_time: 0.0076   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:32 d2.utils.events]:  eta: 0:04:18  iter: 9959  total_loss: 23.76  loss_ce: 1.083  loss_mask: 0.1131  loss_dice: 1.008  loss_ce_0: 1.818  loss_mask_0: 0.126  loss_dice_0: 0.9233  loss_ce_1: 1.122  loss_mask_1: 0.1281  loss_dice_1: 1.061  loss_ce_2: 1.137  loss_mask_2: 0.1392  loss_dice_2: 0.9011  loss_ce_3: 1.102  loss_mask_3: 0.122  loss_dice_3: 1.041  loss_ce_4: 0.966  loss_mask_4: 0.112  loss_dice_4: 0.9013  loss_ce_5: 1.031  loss_mask_5: 0.1212  loss_dice_5: 0.9324  loss_ce_6: 1.079  loss_mask_6: 0.1219  loss_dice_6: 0.924  loss_ce_7: 1.028  loss_mask_7: 0.1323  loss_dice_7: 1.033  loss_ce_8: 1.062  loss_mask_8: 0.1216  loss_dice_8: 0.9239    time: 0.3633  last_time: 0.3126  data_time: 0.0115  last_data_time: 0.0160   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:38 d2.utils.events]:  eta: 0:04:12  iter: 9979  total_loss: 20.55  loss_ce: 0.672  loss_mask: 0.08433  loss_dice: 0.914  loss_ce_0: 1.489  loss_mask_0: 0.1207  loss_dice_0: 1.018  loss_ce_1: 1.132  loss_mask_1: 0.07857  loss_dice_1: 1.093  loss_ce_2: 0.9539  loss_mask_2: 0.09136  loss_dice_2: 0.8849  loss_ce_3: 0.7553  loss_mask_3: 0.07978  loss_dice_3: 0.9855  loss_ce_4: 0.8055  loss_mask_4: 0.08417  loss_dice_4: 1.053  loss_ce_5: 0.7113  loss_mask_5: 0.0863  loss_dice_5: 0.7463  loss_ce_6: 0.897  loss_mask_6: 0.0796  loss_dice_6: 0.9023  loss_ce_7: 0.876  loss_mask_7: 0.07878  loss_dice_7: 0.9765  loss_ce_8: 0.8029  loss_mask_8: 0.08124  loss_dice_8: 1.007    time: 0.3632  last_time: 0.3000  data_time: 0.0105  last_data_time: 0.0056   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:45 d2.utils.events]:  eta: 0:04:06  iter: 9999  total_loss: 19.96  loss_ce: 0.7131  loss_mask: 0.1136  loss_dice: 0.9004  loss_ce_0: 1.254  loss_mask_0: 0.1077  loss_dice_0: 0.9213  loss_ce_1: 0.9573  loss_mask_1: 0.151  loss_dice_1: 0.816  loss_ce_2: 0.9148  loss_mask_2: 0.1256  loss_dice_2: 0.8969  loss_ce_3: 0.7717  loss_mask_3: 0.1319  loss_dice_3: 0.8365  loss_ce_4: 0.7267  loss_mask_4: 0.1367  loss_dice_4: 0.869  loss_ce_5: 0.7361  loss_mask_5: 0.1007  loss_dice_5: 1.005  loss_ce_6: 0.7142  loss_mask_6: 0.1076  loss_dice_6: 0.9448  loss_ce_7: 0.696  loss_mask_7: 0.119  loss_dice_7: 0.8859  loss_ce_8: 0.7576  loss_mask_8: 0.1207  loss_dice_8: 0.8806    time: 0.3631  last_time: 0.2979  data_time: 0.0082  last_data_time: 0.0045   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:52 d2.utils.events]:  eta: 0:04:00  iter: 10019  total_loss: 19.94  loss_ce: 0.7275  loss_mask: 0.1252  loss_dice: 0.9445  loss_ce_0: 1.157  loss_mask_0: 0.1022  loss_dice_0: 1.002  loss_ce_1: 0.9787  loss_mask_1: 0.106  loss_dice_1: 1.057  loss_ce_2: 0.8687  loss_mask_2: 0.1177  loss_dice_2: 1.037  loss_ce_3: 0.8291  loss_mask_3: 0.1284  loss_dice_3: 0.8948  loss_ce_4: 0.7492  loss_mask_4: 0.1188  loss_dice_4: 0.965  loss_ce_5: 0.7424  loss_mask_5: 0.1246  loss_dice_5: 1.033  loss_ce_6: 0.7104  loss_mask_6: 0.1148  loss_dice_6: 0.9376  loss_ce_7: 0.7153  loss_mask_7: 0.1207  loss_dice_7: 0.9093  loss_ce_8: 0.7261  loss_mask_8: 0.1175  loss_dice_8: 1.096    time: 0.3630  last_time: 0.3110  data_time: 0.0066  last_data_time: 0.0059   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:39:58 d2.utils.events]:  eta: 0:03:53  iter: 10039  total_loss: 19.73  loss_ce: 0.7264  loss_mask: 0.05692  loss_dice: 0.6353  loss_ce_0: 1.661  loss_mask_0: 0.08113  loss_dice_0: 1.052  loss_ce_1: 1.02  loss_mask_1: 0.06525  loss_dice_1: 0.8895  loss_ce_2: 1.033  loss_mask_2: 0.06501  loss_dice_2: 0.79  loss_ce_3: 0.9545  loss_mask_3: 0.05378  loss_dice_3: 0.8196  loss_ce_4: 0.836  loss_mask_4: 0.06332  loss_dice_4: 0.7421  loss_ce_5: 0.744  loss_mask_5: 0.05049  loss_dice_5: 0.7388  loss_ce_6: 0.7518  loss_mask_6: 0.05015  loss_dice_6: 0.6354  loss_ce_7: 0.7732  loss_mask_7: 0.04689  loss_dice_7: 0.701  loss_ce_8: 0.7735  loss_mask_8: 0.05363  loss_dice_8: 0.7086    time: 0.3629  last_time: 0.3018  data_time: 0.0070  last_data_time: 0.0020   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:04 d2.utils.events]:  eta: 0:03:47  iter: 10059  total_loss: 18.71  loss_ce: 0.6984  loss_mask: 0.06981  loss_dice: 0.7719  loss_ce_0: 1.308  loss_mask_0: 0.04171  loss_dice_0: 1.093  loss_ce_1: 0.9858  loss_mask_1: 0.05656  loss_dice_1: 0.9427  loss_ce_2: 0.8777  loss_mask_2: 0.07178  loss_dice_2: 0.8839  loss_ce_3: 0.817  loss_mask_3: 0.04952  loss_dice_3: 0.8404  loss_ce_4: 0.8163  loss_mask_4: 0.06626  loss_dice_4: 0.9748  loss_ce_5: 0.821  loss_mask_5: 0.05154  loss_dice_5: 0.8771  loss_ce_6: 0.7141  loss_mask_6: 0.0599  loss_dice_6: 0.9245  loss_ce_7: 0.6682  loss_mask_7: 0.07796  loss_dice_7: 0.8953  loss_ce_8: 0.704  loss_mask_8: 0.0746  loss_dice_8: 0.8924    time: 0.3628  last_time: 0.3014  data_time: 0.0063  last_data_time: 0.0076   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:10 d2.utils.events]:  eta: 0:03:41  iter: 10079  total_loss: 12.9  loss_ce: 0.4811  loss_mask: 0.04843  loss_dice: 0.5243  loss_ce_0: 1.372  loss_mask_0: 0.06669  loss_dice_0: 0.7288  loss_ce_1: 0.7887  loss_mask_1: 0.06027  loss_dice_1: 0.5581  loss_ce_2: 0.731  loss_mask_2: 0.05937  loss_dice_2: 0.5368  loss_ce_3: 0.562  loss_mask_3: 0.0506  loss_dice_3: 0.5904  loss_ce_4: 0.5112  loss_mask_4: 0.05772  loss_dice_4: 0.4886  loss_ce_5: 0.4899  loss_mask_5: 0.06553  loss_dice_5: 0.6051  loss_ce_6: 0.4854  loss_mask_6: 0.054  loss_dice_6: 0.4387  loss_ce_7: 0.4446  loss_mask_7: 0.05638  loss_dice_7: 0.5127  loss_ce_8: 0.4501  loss_mask_8: 0.05316  loss_dice_8: 0.4352    time: 0.3627  last_time: 0.3186  data_time: 0.0071  last_data_time: 0.0164   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:17 d2.utils.events]:  eta: 0:03:35  iter: 10099  total_loss: 15.46  loss_ce: 0.6715  loss_mask: 0.08086  loss_dice: 0.5756  loss_ce_0: 1.209  loss_mask_0: 0.07953  loss_dice_0: 0.9844  loss_ce_1: 0.8683  loss_mask_1: 0.07561  loss_dice_1: 0.6195  loss_ce_2: 0.7905  loss_mask_2: 0.07974  loss_dice_2: 0.499  loss_ce_3: 0.6731  loss_mask_3: 0.07456  loss_dice_3: 0.7578  loss_ce_4: 0.6756  loss_mask_4: 0.0737  loss_dice_4: 0.55  loss_ce_5: 0.6644  loss_mask_5: 0.07789  loss_dice_5: 0.6864  loss_ce_6: 0.6634  loss_mask_6: 0.06787  loss_dice_6: 0.6392  loss_ce_7: 0.7179  loss_mask_7: 0.07379  loss_dice_7: 0.5236  loss_ce_8: 0.7291  loss_mask_8: 0.07643  loss_dice_8: 0.6732    time: 0.3626  last_time: 0.3253  data_time: 0.0074  last_data_time: 0.0036   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:23 d2.utils.events]:  eta: 0:03:29  iter: 10119  total_loss: 20.77  loss_ce: 0.9959  loss_mask: 0.05986  loss_dice: 0.6243  loss_ce_0: 1.564  loss_mask_0: 0.06666  loss_dice_0: 0.8277  loss_ce_1: 1.071  loss_mask_1: 0.0939  loss_dice_1: 0.7778  loss_ce_2: 0.9785  loss_mask_2: 0.08392  loss_dice_2: 0.7375  loss_ce_3: 0.9821  loss_mask_3: 0.07407  loss_dice_3: 0.8757  loss_ce_4: 1.152  loss_mask_4: 0.0751  loss_dice_4: 0.7537  loss_ce_5: 1.153  loss_mask_5: 0.07346  loss_dice_5: 0.6162  loss_ce_6: 1.085  loss_mask_6: 0.08072  loss_dice_6: 0.6454  loss_ce_7: 1.131  loss_mask_7: 0.07363  loss_dice_7: 0.6651  loss_ce_8: 0.9326  loss_mask_8: 0.07118  loss_dice_8: 0.6481    time: 0.3626  last_time: 0.3160  data_time: 0.0121  last_data_time: 0.0066   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:29 d2.utils.events]:  eta: 0:03:23  iter: 10139  total_loss: 19.6  loss_ce: 0.5197  loss_mask: 0.06499  loss_dice: 0.6493  loss_ce_0: 1.674  loss_mask_0: 0.09903  loss_dice_0: 0.8537  loss_ce_1: 1.001  loss_mask_1: 0.07878  loss_dice_1: 0.7435  loss_ce_2: 0.7333  loss_mask_2: 0.08323  loss_dice_2: 0.7944  loss_ce_3: 0.603  loss_mask_3: 0.06116  loss_dice_3: 0.5752  loss_ce_4: 0.7546  loss_mask_4: 0.0759  loss_dice_4: 0.7043  loss_ce_5: 0.5144  loss_mask_5: 0.08251  loss_dice_5: 0.6896  loss_ce_6: 0.7239  loss_mask_6: 0.07623  loss_dice_6: 0.667  loss_ce_7: 0.5009  loss_mask_7: 0.06364  loss_dice_7: 0.6217  loss_ce_8: 0.4973  loss_mask_8: 0.07565  loss_dice_8: 0.6069    time: 0.3625  last_time: 0.3281  data_time: 0.0067  last_data_time: 0.0061   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:36 d2.utils.events]:  eta: 0:03:16  iter: 10159  total_loss: 19.72  loss_ce: 0.6545  loss_mask: 0.07597  loss_dice: 0.831  loss_ce_0: 1.185  loss_mask_0: 0.06036  loss_dice_0: 1.001  loss_ce_1: 1.02  loss_mask_1: 0.07669  loss_dice_1: 0.7871  loss_ce_2: 0.8677  loss_mask_2: 0.0746  loss_dice_2: 0.8698  loss_ce_3: 0.7499  loss_mask_3: 0.06881  loss_dice_3: 0.8356  loss_ce_4: 0.7434  loss_mask_4: 0.06907  loss_dice_4: 0.8187  loss_ce_5: 0.7058  loss_mask_5: 0.08754  loss_dice_5: 0.8892  loss_ce_6: 0.6608  loss_mask_6: 0.06854  loss_dice_6: 0.8126  loss_ce_7: 0.65  loss_mask_7: 0.0762  loss_dice_7: 0.883  loss_ce_8: 0.638  loss_mask_8: 0.06623  loss_dice_8: 0.8298    time: 0.3624  last_time: 0.3135  data_time: 0.0064  last_data_time: 0.0024   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:42 d2.utils.events]:  eta: 0:03:10  iter: 10179  total_loss: 20.15  loss_ce: 0.9356  loss_mask: 0.06825  loss_dice: 1.004  loss_ce_0: 1.617  loss_mask_0: 0.08495  loss_dice_0: 1.109  loss_ce_1: 1.039  loss_mask_1: 0.09079  loss_dice_1: 1.222  loss_ce_2: 0.9854  loss_mask_2: 0.08677  loss_dice_2: 1.082  loss_ce_3: 0.9812  loss_mask_3: 0.0795  loss_dice_3: 1.126  loss_ce_4: 0.9189  loss_mask_4: 0.0718  loss_dice_4: 0.704  loss_ce_5: 0.9532  loss_mask_5: 0.07302  loss_dice_5: 0.7598  loss_ce_6: 0.946  loss_mask_6: 0.07324  loss_dice_6: 1.023  loss_ce_7: 0.9643  loss_mask_7: 0.07789  loss_dice_7: 0.9549  loss_ce_8: 0.9327  loss_mask_8: 0.06954  loss_dice_8: 0.7726    time: 0.3623  last_time: 0.2985  data_time: 0.0093  last_data_time: 0.0065   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:48 d2.utils.events]:  eta: 0:03:04  iter: 10199  total_loss: 23.41  loss_ce: 1.031  loss_mask: 0.09935  loss_dice: 1.011  loss_ce_0: 1.574  loss_mask_0: 0.08555  loss_dice_0: 1.496  loss_ce_1: 1.27  loss_mask_1: 0.09856  loss_dice_1: 0.95  loss_ce_2: 1.197  loss_mask_2: 0.08321  loss_dice_2: 0.9887  loss_ce_3: 1.199  loss_mask_3: 0.1054  loss_dice_3: 1.061  loss_ce_4: 0.9898  loss_mask_4: 0.095  loss_dice_4: 1.057  loss_ce_5: 1.095  loss_mask_5: 0.09248  loss_dice_5: 1.114  loss_ce_6: 0.9389  loss_mask_6: 0.09562  loss_dice_6: 0.8988  loss_ce_7: 1.065  loss_mask_7: 0.09795  loss_dice_7: 1.161  loss_ce_8: 1.033  loss_mask_8: 0.09147  loss_dice_8: 1.03    time: 0.3622  last_time: 0.3012  data_time: 0.0067  last_data_time: 0.0062   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:40:54 d2.utils.events]:  eta: 0:02:58  iter: 10219  total_loss: 22.2  loss_ce: 0.7833  loss_mask: 0.1718  loss_dice: 0.6791  loss_ce_0: 1.356  loss_mask_0: 0.1694  loss_dice_0: 1.064  loss_ce_1: 1.134  loss_mask_1: 0.1454  loss_dice_1: 0.902  loss_ce_2: 0.9825  loss_mask_2: 0.165  loss_dice_2: 0.8618  loss_ce_3: 0.9362  loss_mask_3: 0.1658  loss_dice_3: 0.7182  loss_ce_4: 0.9386  loss_mask_4: 0.1743  loss_dice_4: 0.8161  loss_ce_5: 0.8473  loss_mask_5: 0.1785  loss_dice_5: 0.6417  loss_ce_6: 0.829  loss_mask_6: 0.15  loss_dice_6: 0.759  loss_ce_7: 0.7922  loss_mask_7: 0.1578  loss_dice_7: 0.6897  loss_ce_8: 0.7848  loss_mask_8: 0.1687  loss_dice_8: 0.8191    time: 0.3621  last_time: 0.3293  data_time: 0.0068  last_data_time: 0.0106   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:01 d2.utils.events]:  eta: 0:02:52  iter: 10239  total_loss: 22.42  loss_ce: 0.9384  loss_mask: 0.04195  loss_dice: 0.9105  loss_ce_0: 1.76  loss_mask_0: 0.05862  loss_dice_0: 1.356  loss_ce_1: 1.336  loss_mask_1: 0.05293  loss_dice_1: 1.102  loss_ce_2: 1.316  loss_mask_2: 0.05782  loss_dice_2: 1.1  loss_ce_3: 1.05  loss_mask_3: 0.04245  loss_dice_3: 0.9795  loss_ce_4: 1.111  loss_mask_4: 0.04102  loss_dice_4: 1.007  loss_ce_5: 1.145  loss_mask_5: 0.04243  loss_dice_5: 0.9179  loss_ce_6: 1.019  loss_mask_6: 0.03934  loss_dice_6: 0.9644  loss_ce_7: 0.946  loss_mask_7: 0.0446  loss_dice_7: 1.046  loss_ce_8: 0.9887  loss_mask_8: 0.0375  loss_dice_8: 0.9372    time: 0.3620  last_time: 0.3024  data_time: 0.0072  last_data_time: 0.0049   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:07 d2.utils.events]:  eta: 0:02:46  iter: 10259  total_loss: 17.35  loss_ce: 0.7317  loss_mask: 0.0777  loss_dice: 0.8075  loss_ce_0: 1.401  loss_mask_0: 0.1261  loss_dice_0: 0.7179  loss_ce_1: 0.702  loss_mask_1: 0.08074  loss_dice_1: 0.7513  loss_ce_2: 0.8429  loss_mask_2: 0.08211  loss_dice_2: 0.711  loss_ce_3: 0.7397  loss_mask_3: 0.07882  loss_dice_3: 0.725  loss_ce_4: 0.7228  loss_mask_4: 0.07973  loss_dice_4: 0.5108  loss_ce_5: 0.7529  loss_mask_5: 0.07718  loss_dice_5: 0.6857  loss_ce_6: 0.6332  loss_mask_6: 0.07905  loss_dice_6: 0.6057  loss_ce_7: 0.6527  loss_mask_7: 0.08563  loss_dice_7: 0.7775  loss_ce_8: 0.739  loss_mask_8: 0.07545  loss_dice_8: 0.6451    time: 0.3619  last_time: 0.3012  data_time: 0.0113  last_data_time: 0.0042   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:13 d2.utils.events]:  eta: 0:02:40  iter: 10279  total_loss: 11.69  loss_ce: 0.4434  loss_mask: 0.09558  loss_dice: 0.3989  loss_ce_0: 0.9714  loss_mask_0: 0.09704  loss_dice_0: 0.7888  loss_ce_1: 0.6335  loss_mask_1: 0.1122  loss_dice_1: 0.4377  loss_ce_2: 0.5492  loss_mask_2: 0.1297  loss_dice_2: 0.4232  loss_ce_3: 0.5645  loss_mask_3: 0.1055  loss_dice_3: 0.3664  loss_ce_4: 0.5239  loss_mask_4: 0.1153  loss_dice_4: 0.4786  loss_ce_5: 0.5257  loss_mask_5: 0.1145  loss_dice_5: 0.4103  loss_ce_6: 0.4868  loss_mask_6: 0.09548  loss_dice_6: 0.3837  loss_ce_7: 0.4919  loss_mask_7: 0.1071  loss_dice_7: 0.3707  loss_ce_8: 0.4894  loss_mask_8: 0.1049  loss_dice_8: 0.4469    time: 0.3618  last_time: 0.3023  data_time: 0.0070  last_data_time: 0.0054   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:20 d2.utils.events]:  eta: 0:02:33  iter: 10299  total_loss: 19.12  loss_ce: 0.6616  loss_mask: 0.05633  loss_dice: 0.5874  loss_ce_0: 1.258  loss_mask_0: 0.06526  loss_dice_0: 1.029  loss_ce_1: 0.9295  loss_mask_1: 0.06578  loss_dice_1: 0.5838  loss_ce_2: 0.8586  loss_mask_2: 0.07302  loss_dice_2: 0.9799  loss_ce_3: 0.7496  loss_mask_3: 0.05383  loss_dice_3: 0.7554  loss_ce_4: 0.6445  loss_mask_4: 0.07013  loss_dice_4: 0.7551  loss_ce_5: 0.683  loss_mask_5: 0.06319  loss_dice_5: 0.7541  loss_ce_6: 0.6397  loss_mask_6: 0.05805  loss_dice_6: 0.8705  loss_ce_7: 0.7077  loss_mask_7: 0.05419  loss_dice_7: 0.7778  loss_ce_8: 0.7418  loss_mask_8: 0.06654  loss_dice_8: 0.7777    time: 0.3617  last_time: 0.2975  data_time: 0.0104  last_data_time: 0.0039   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:26 d2.utils.events]:  eta: 0:02:27  iter: 10319  total_loss: 17.43  loss_ce: 0.5765  loss_mask: 0.1567  loss_dice: 0.6641  loss_ce_0: 1.202  loss_mask_0: 0.1766  loss_dice_0: 0.9591  loss_ce_1: 0.954  loss_mask_1: 0.1818  loss_dice_1: 0.7832  loss_ce_2: 0.802  loss_mask_2: 0.2125  loss_dice_2: 0.7557  loss_ce_3: 0.7045  loss_mask_3: 0.1729  loss_dice_3: 0.7246  loss_ce_4: 0.7099  loss_mask_4: 0.1604  loss_dice_4: 0.7505  loss_ce_5: 0.5851  loss_mask_5: 0.1595  loss_dice_5: 0.7514  loss_ce_6: 0.5504  loss_mask_6: 0.165  loss_dice_6: 0.744  loss_ce_7: 0.5796  loss_mask_7: 0.1714  loss_dice_7: 0.7627  loss_ce_8: 0.6035  loss_mask_8: 0.1659  loss_dice_8: 0.7218    time: 0.3616  last_time: 0.3302  data_time: 0.0108  last_data_time: 0.0062   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:32 d2.utils.events]:  eta: 0:02:21  iter: 10339  total_loss: 14.56  loss_ce: 0.5233  loss_mask: 0.06107  loss_dice: 0.7005  loss_ce_0: 1.255  loss_mask_0: 0.07192  loss_dice_0: 0.9267  loss_ce_1: 0.7927  loss_mask_1: 0.06854  loss_dice_1: 0.8264  loss_ce_2: 0.7304  loss_mask_2: 0.07851  loss_dice_2: 0.7004  loss_ce_3: 0.6751  loss_mask_3: 0.06121  loss_dice_3: 0.5996  loss_ce_4: 0.5057  loss_mask_4: 0.06242  loss_dice_4: 0.6205  loss_ce_5: 0.6066  loss_mask_5: 0.06317  loss_dice_5: 0.6273  loss_ce_6: 0.477  loss_mask_6: 0.06059  loss_dice_6: 0.6014  loss_ce_7: 0.5384  loss_mask_7: 0.07051  loss_dice_7: 0.6207  loss_ce_8: 0.4847  loss_mask_8: 0.05831  loss_dice_8: 0.6866    time: 0.3615  last_time: 0.3037  data_time: 0.0068  last_data_time: 0.0065   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:39 d2.utils.events]:  eta: 0:02:15  iter: 10359  total_loss: 21.11  loss_ce: 0.8533  loss_mask: 0.06276  loss_dice: 0.7209  loss_ce_0: 1.377  loss_mask_0: 0.1221  loss_dice_0: 1.057  loss_ce_1: 1.213  loss_mask_1: 0.08395  loss_dice_1: 0.9301  loss_ce_2: 0.9676  loss_mask_2: 0.07256  loss_dice_2: 1.144  loss_ce_3: 0.8336  loss_mask_3: 0.06437  loss_dice_3: 0.6758  loss_ce_4: 0.7706  loss_mask_4: 0.06363  loss_dice_4: 0.7308  loss_ce_5: 0.848  loss_mask_5: 0.06941  loss_dice_5: 0.6674  loss_ce_6: 0.8774  loss_mask_6: 0.06614  loss_dice_6: 0.7479  loss_ce_7: 0.8236  loss_mask_7: 0.06807  loss_dice_7: 0.7171  loss_ce_8: 0.8502  loss_mask_8: 0.06634  loss_dice_8: 0.8153    time: 0.3614  last_time: 0.3075  data_time: 0.0065  last_data_time: 0.0068   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:45 d2.utils.events]:  eta: 0:02:09  iter: 10379  total_loss: 19.48  loss_ce: 0.7925  loss_mask: 0.09386  loss_dice: 0.6162  loss_ce_0: 1.309  loss_mask_0: 0.08687  loss_dice_0: 0.6906  loss_ce_1: 1.017  loss_mask_1: 0.07265  loss_dice_1: 0.6248  loss_ce_2: 1.055  loss_mask_2: 0.09295  loss_dice_2: 0.6349  loss_ce_3: 1.066  loss_mask_3: 0.08138  loss_dice_3: 0.4912  loss_ce_4: 0.894  loss_mask_4: 0.07643  loss_dice_4: 0.6091  loss_ce_5: 0.8751  loss_mask_5: 0.1006  loss_dice_5: 0.5491  loss_ce_6: 0.7606  loss_mask_6: 0.1124  loss_dice_6: 0.6603  loss_ce_7: 0.7554  loss_mask_7: 0.09809  loss_dice_7: 0.6377  loss_ce_8: 0.77  loss_mask_8: 0.1003  loss_dice_8: 0.6505    time: 0.3613  last_time: 0.3269  data_time: 0.0077  last_data_time: 0.0080   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:51 d2.utils.events]:  eta: 0:02:03  iter: 10399  total_loss: 19.58  loss_ce: 0.6866  loss_mask: 0.0748  loss_dice: 0.7843  loss_ce_0: 1.113  loss_mask_0: 0.1243  loss_dice_0: 1.076  loss_ce_1: 0.7545  loss_mask_1: 0.1368  loss_dice_1: 0.9743  loss_ce_2: 0.7337  loss_mask_2: 0.0871  loss_dice_2: 0.8045  loss_ce_3: 0.7932  loss_mask_3: 0.07997  loss_dice_3: 0.9558  loss_ce_4: 0.7579  loss_mask_4: 0.08709  loss_dice_4: 0.7217  loss_ce_5: 0.7081  loss_mask_5: 0.08227  loss_dice_5: 0.7891  loss_ce_6: 0.7346  loss_mask_6: 0.07711  loss_dice_6: 0.7705  loss_ce_7: 0.6514  loss_mask_7: 0.07486  loss_dice_7: 0.9266  loss_ce_8: 0.7717  loss_mask_8: 0.0937  loss_dice_8: 0.8676    time: 0.3612  last_time: 0.3081  data_time: 0.0065  last_data_time: 0.0056   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:41:57 d2.utils.events]:  eta: 0:01:56  iter: 10419  total_loss: 18.13  loss_ce: 0.6871  loss_mask: 0.08503  loss_dice: 0.7868  loss_ce_0: 1.324  loss_mask_0: 0.07151  loss_dice_0: 0.8554  loss_ce_1: 0.7797  loss_mask_1: 0.0917  loss_dice_1: 0.647  loss_ce_2: 0.9866  loss_mask_2: 0.08578  loss_dice_2: 0.6715  loss_ce_3: 0.7109  loss_mask_3: 0.09045  loss_dice_3: 0.7762  loss_ce_4: 0.6464  loss_mask_4: 0.08225  loss_dice_4: 0.7226  loss_ce_5: 0.6424  loss_mask_5: 0.102  loss_dice_5: 0.7497  loss_ce_6: 0.6457  loss_mask_6: 0.09906  loss_dice_6: 0.6921  loss_ce_7: 0.7185  loss_mask_7: 0.1048  loss_dice_7: 0.7337  loss_ce_8: 0.7329  loss_mask_8: 0.103  loss_dice_8: 0.691    time: 0.3611  last_time: 0.3237  data_time: 0.0149  last_data_time: 0.0057   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:04 d2.utils.events]:  eta: 0:01:50  iter: 10439  total_loss: 20.12  loss_ce: 0.5632  loss_mask: 0.07276  loss_dice: 1.113  loss_ce_0: 1.245  loss_mask_0: 0.09198  loss_dice_0: 1.494  loss_ce_1: 0.9437  loss_mask_1: 0.07283  loss_dice_1: 1.121  loss_ce_2: 0.7589  loss_mask_2: 0.08358  loss_dice_2: 1.311  loss_ce_3: 0.7084  loss_mask_3: 0.07072  loss_dice_3: 1.251  loss_ce_4: 0.5902  loss_mask_4: 0.07685  loss_dice_4: 1.142  loss_ce_5: 0.6204  loss_mask_5: 0.07093  loss_dice_5: 1.254  loss_ce_6: 0.5025  loss_mask_6: 0.07003  loss_dice_6: 1.082  loss_ce_7: 0.4163  loss_mask_7: 0.06477  loss_dice_7: 1.052  loss_ce_8: 0.3902  loss_mask_8: 0.06487  loss_dice_8: 0.9182    time: 0.3610  last_time: 0.3301  data_time: 0.0070  last_data_time: 0.0044   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:10 d2.utils.events]:  eta: 0:01:44  iter: 10459  total_loss: 17.74  loss_ce: 0.6264  loss_mask: 0.09771  loss_dice: 0.7083  loss_ce_0: 1.347  loss_mask_0: 0.07274  loss_dice_0: 1.154  loss_ce_1: 1.082  loss_mask_1: 0.1142  loss_dice_1: 1.11  loss_ce_2: 0.9497  loss_mask_2: 0.1081  loss_dice_2: 0.8399  loss_ce_3: 0.8719  loss_mask_3: 0.1026  loss_dice_3: 0.7481  loss_ce_4: 0.8038  loss_mask_4: 0.1118  loss_dice_4: 0.9029  loss_ce_5: 0.7723  loss_mask_5: 0.1105  loss_dice_5: 0.787  loss_ce_6: 0.6597  loss_mask_6: 0.1134  loss_dice_6: 0.735  loss_ce_7: 0.6858  loss_mask_7: 0.0922  loss_dice_7: 0.8765  loss_ce_8: 0.6812  loss_mask_8: 0.1009  loss_dice_8: 0.7753    time: 0.3609  last_time: 0.2985  data_time: 0.0079  last_data_time: 0.0059   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:16 d2.utils.events]:  eta: 0:01:38  iter: 10479  total_loss: 18.75  loss_ce: 0.803  loss_mask: 0.03516  loss_dice: 0.7997  loss_ce_0: 1.469  loss_mask_0: 0.03804  loss_dice_0: 0.8194  loss_ce_1: 0.9798  loss_mask_1: 0.04827  loss_dice_1: 1.127  loss_ce_2: 0.9845  loss_mask_2: 0.03651  loss_dice_2: 0.8449  loss_ce_3: 0.876  loss_mask_3: 0.03798  loss_dice_3: 0.9489  loss_ce_4: 0.8767  loss_mask_4: 0.0324  loss_dice_4: 0.6769  loss_ce_5: 0.8242  loss_mask_5: 0.03357  loss_dice_5: 0.7292  loss_ce_6: 0.799  loss_mask_6: 0.03419  loss_dice_6: 0.731  loss_ce_7: 0.7542  loss_mask_7: 0.03563  loss_dice_7: 0.7386  loss_ce_8: 0.8147  loss_mask_8: 0.03575  loss_dice_8: 0.7859    time: 0.3609  last_time: 0.3075  data_time: 0.0069  last_data_time: 0.0047   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:22 d2.utils.events]:  eta: 0:01:32  iter: 10499  total_loss: 16.02  loss_ce: 0.6591  loss_mask: 0.08449  loss_dice: 0.4926  loss_ce_0: 1.115  loss_mask_0: 0.07186  loss_dice_0: 0.7499  loss_ce_1: 0.9221  loss_mask_1: 0.08264  loss_dice_1: 0.6029  loss_ce_2: 0.7152  loss_mask_2: 0.08492  loss_dice_2: 0.6029  loss_ce_3: 0.8315  loss_mask_3: 0.07752  loss_dice_3: 0.4671  loss_ce_4: 0.8275  loss_mask_4: 0.0741  loss_dice_4: 0.4867  loss_ce_5: 0.5886  loss_mask_5: 0.0859  loss_dice_5: 0.5422  loss_ce_6: 0.6754  loss_mask_6: 0.0758  loss_dice_6: 0.5918  loss_ce_7: 0.7078  loss_mask_7: 0.08686  loss_dice_7: 0.529  loss_ce_8: 0.6542  loss_mask_8: 0.08533  loss_dice_8: 0.4494    time: 0.3608  last_time: 0.3272  data_time: 0.0073  last_data_time: 0.0073   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:29 d2.utils.events]:  eta: 0:01:26  iter: 10519  total_loss: 23.69  loss_ce: 0.6868  loss_mask: 0.0872  loss_dice: 0.9159  loss_ce_0: 1.49  loss_mask_0: 0.1144  loss_dice_0: 1.161  loss_ce_1: 1.009  loss_mask_1: 0.1146  loss_dice_1: 1.101  loss_ce_2: 0.8326  loss_mask_2: 0.08544  loss_dice_2: 1.052  loss_ce_3: 0.8687  loss_mask_3: 0.08131  loss_dice_3: 0.9716  loss_ce_4: 0.8635  loss_mask_4: 0.07437  loss_dice_4: 0.9482  loss_ce_5: 0.7554  loss_mask_5: 0.07982  loss_dice_5: 1.002  loss_ce_6: 0.675  loss_mask_6: 0.08344  loss_dice_6: 0.929  loss_ce_7: 0.7042  loss_mask_7: 0.08291  loss_dice_7: 1.055  loss_ce_8: 0.6983  loss_mask_8: 0.08584  loss_dice_8: 0.9536    time: 0.3607  last_time: 0.3298  data_time: 0.0076  last_data_time: 0.0203   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:35 d2.utils.events]:  eta: 0:01:19  iter: 10539  total_loss: 20.23  loss_ce: 0.7989  loss_mask: 0.07422  loss_dice: 0.8171  loss_ce_0: 1.679  loss_mask_0: 0.084  loss_dice_0: 0.7947  loss_ce_1: 1.096  loss_mask_1: 0.08183  loss_dice_1: 0.9349  loss_ce_2: 0.9699  loss_mask_2: 0.07841  loss_dice_2: 0.8206  loss_ce_3: 0.9441  loss_mask_3: 0.0824  loss_dice_3: 0.8725  loss_ce_4: 0.9268  loss_mask_4: 0.09895  loss_dice_4: 0.955  loss_ce_5: 0.826  loss_mask_5: 0.09139  loss_dice_5: 0.8427  loss_ce_6: 0.8247  loss_mask_6: 0.08318  loss_dice_6: 0.9143  loss_ce_7: 0.8324  loss_mask_7: 0.07651  loss_dice_7: 0.9071  loss_ce_8: 0.7686  loss_mask_8: 0.07737  loss_dice_8: 0.731    time: 0.3606  last_time: 0.3522  data_time: 0.0128  last_data_time: 0.0134   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:41 d2.utils.events]:  eta: 0:01:13  iter: 10559  total_loss: 13.66  loss_ce: 0.6644  loss_mask: 0.06079  loss_dice: 0.5229  loss_ce_0: 1.061  loss_mask_0: 0.09234  loss_dice_0: 0.666  loss_ce_1: 0.7706  loss_mask_1: 0.06925  loss_dice_1: 0.6026  loss_ce_2: 0.6959  loss_mask_2: 0.07354  loss_dice_2: 0.4932  loss_ce_3: 0.6353  loss_mask_3: 0.07126  loss_dice_3: 0.654  loss_ce_4: 0.6416  loss_mask_4: 0.07573  loss_dice_4: 0.6692  loss_ce_5: 0.6221  loss_mask_5: 0.07061  loss_dice_5: 0.5987  loss_ce_6: 0.5993  loss_mask_6: 0.06707  loss_dice_6: 0.6904  loss_ce_7: 0.6149  loss_mask_7: 0.06666  loss_dice_7: 0.705  loss_ce_8: 0.619  loss_mask_8: 0.06501  loss_dice_8: 0.556    time: 0.3605  last_time: 0.3235  data_time: 0.0071  last_data_time: 0.0143   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:48 d2.utils.events]:  eta: 0:01:07  iter: 10579  total_loss: 15.38  loss_ce: 0.4691  loss_mask: 0.05182  loss_dice: 0.504  loss_ce_0: 1.316  loss_mask_0: 0.07306  loss_dice_0: 0.7463  loss_ce_1: 0.875  loss_mask_1: 0.05936  loss_dice_1: 0.7202  loss_ce_2: 0.7875  loss_mask_2: 0.06217  loss_dice_2: 0.5855  loss_ce_3: 0.6051  loss_mask_3: 0.06588  loss_dice_3: 0.5058  loss_ce_4: 0.5731  loss_mask_4: 0.06136  loss_dice_4: 0.5364  loss_ce_5: 0.557  loss_mask_5: 0.05963  loss_dice_5: 0.4797  loss_ce_6: 0.5371  loss_mask_6: 0.06125  loss_dice_6: 0.4937  loss_ce_7: 0.478  loss_mask_7: 0.05836  loss_dice_7: 0.564  loss_ce_8: 0.4695  loss_mask_8: 0.0619  loss_dice_8: 0.5339    time: 0.3604  last_time: 0.3114  data_time: 0.0083  last_data_time: 0.0113   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:42:54 d2.utils.events]:  eta: 0:01:01  iter: 10599  total_loss: 15.91  loss_ce: 0.601  loss_mask: 0.05848  loss_dice: 0.7787  loss_ce_0: 1.393  loss_mask_0: 0.07844  loss_dice_0: 0.7616  loss_ce_1: 0.9257  loss_mask_1: 0.07567  loss_dice_1: 0.7385  loss_ce_2: 0.7857  loss_mask_2: 0.07031  loss_dice_2: 0.6266  loss_ce_3: 0.6946  loss_mask_3: 0.06402  loss_dice_3: 0.5377  loss_ce_4: 0.6125  loss_mask_4: 0.06035  loss_dice_4: 0.4767  loss_ce_5: 0.6831  loss_mask_5: 0.05439  loss_dice_5: 0.5608  loss_ce_6: 0.6235  loss_mask_6: 0.05724  loss_dice_6: 0.6853  loss_ce_7: 0.6272  loss_mask_7: 0.05511  loss_dice_7: 0.5058  loss_ce_8: 0.6123  loss_mask_8: 0.05644  loss_dice_8: 0.6874    time: 0.3604  last_time: 0.3179  data_time: 0.0205  last_data_time: 0.0063   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:00 d2.utils.events]:  eta: 0:00:55  iter: 10619  total_loss: 17.27  loss_ce: 0.6303  loss_mask: 0.04828  loss_dice: 0.7572  loss_ce_0: 1.326  loss_mask_0: 0.07029  loss_dice_0: 1.048  loss_ce_1: 0.9072  loss_mask_1: 0.0563  loss_dice_1: 0.9004  loss_ce_2: 0.7253  loss_mask_2: 0.05189  loss_dice_2: 0.7668  loss_ce_3: 0.6507  loss_mask_3: 0.04457  loss_dice_3: 0.7851  loss_ce_4: 0.7281  loss_mask_4: 0.04054  loss_dice_4: 0.6282  loss_ce_5: 0.7528  loss_mask_5: 0.04915  loss_dice_5: 0.8826  loss_ce_6: 0.6233  loss_mask_6: 0.04509  loss_dice_6: 0.6506  loss_ce_7: 0.6197  loss_mask_7: 0.04119  loss_dice_7: 0.6417  loss_ce_8: 0.5717  loss_mask_8: 0.04071  loss_dice_8: 0.7136    time: 0.3603  last_time: 0.3333  data_time: 0.0070  last_data_time: 0.0028   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:07 d2.utils.events]:  eta: 0:00:49  iter: 10639  total_loss: 19.59  loss_ce: 0.6499  loss_mask: 0.06472  loss_dice: 0.731  loss_ce_0: 1.603  loss_mask_0: 0.09106  loss_dice_0: 1.03  loss_ce_1: 1.213  loss_mask_1: 0.07074  loss_dice_1: 0.6921  loss_ce_2: 0.9801  loss_mask_2: 0.05829  loss_dice_2: 0.804  loss_ce_3: 0.8958  loss_mask_3: 0.06078  loss_dice_3: 0.6512  loss_ce_4: 0.686  loss_mask_4: 0.0665  loss_dice_4: 0.6736  loss_ce_5: 0.7098  loss_mask_5: 0.05297  loss_dice_5: 0.712  loss_ce_6: 0.6619  loss_mask_6: 0.06289  loss_dice_6: 0.678  loss_ce_7: 0.6297  loss_mask_7: 0.0545  loss_dice_7: 0.6759  loss_ce_8: 0.6308  loss_mask_8: 0.05742  loss_dice_8: 0.6552    time: 0.3602  last_time: 0.3452  data_time: 0.0068  last_data_time: 0.0065   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:13 d2.utils.events]:  eta: 0:00:43  iter: 10659  total_loss: 15.46  loss_ce: 0.6121  loss_mask: 0.0641  loss_dice: 0.6187  loss_ce_0: 1.099  loss_mask_0: 0.08798  loss_dice_0: 0.7742  loss_ce_1: 0.813  loss_mask_1: 0.07571  loss_dice_1: 0.696  loss_ce_2: 0.7688  loss_mask_2: 0.07991  loss_dice_2: 0.7634  loss_ce_3: 0.6239  loss_mask_3: 0.06901  loss_dice_3: 0.7407  loss_ce_4: 0.753  loss_mask_4: 0.07074  loss_dice_4: 0.8547  loss_ce_5: 0.6225  loss_mask_5: 0.07809  loss_dice_5: 0.5952  loss_ce_6: 0.5454  loss_mask_6: 0.07078  loss_dice_6: 0.6487  loss_ce_7: 0.5467  loss_mask_7: 0.07223  loss_dice_7: 0.6416  loss_ce_8: 0.6204  loss_mask_8: 0.0707  loss_dice_8: 0.7375    time: 0.3601  last_time: 0.3227  data_time: 0.0069  last_data_time: 0.0061   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:19 d2.utils.events]:  eta: 0:00:36  iter: 10679  total_loss: 23.77  loss_ce: 0.8808  loss_mask: 0.05555  loss_dice: 0.9943  loss_ce_0: 1.887  loss_mask_0: 0.05196  loss_dice_0: 1.192  loss_ce_1: 1.334  loss_mask_1: 0.07185  loss_dice_1: 1.014  loss_ce_2: 1.1  loss_mask_2: 0.07831  loss_dice_2: 0.9583  loss_ce_3: 0.9172  loss_mask_3: 0.06733  loss_dice_3: 0.8032  loss_ce_4: 0.9475  loss_mask_4: 0.05699  loss_dice_4: 1.068  loss_ce_5: 0.9084  loss_mask_5: 0.05283  loss_dice_5: 0.9631  loss_ce_6: 0.8599  loss_mask_6: 0.05939  loss_dice_6: 0.8776  loss_ce_7: 0.854  loss_mask_7: 0.06455  loss_dice_7: 0.9156  loss_ce_8: 0.8733  loss_mask_8: 0.04912  loss_dice_8: 0.799    time: 0.3600  last_time: 0.3024  data_time: 0.0072  last_data_time: 0.0046   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:26 d2.utils.events]:  eta: 0:00:30  iter: 10699  total_loss: 20.11  loss_ce: 0.8213  loss_mask: 0.05907  loss_dice: 0.9528  loss_ce_0: 1.339  loss_mask_0: 0.07509  loss_dice_0: 1.202  loss_ce_1: 0.9093  loss_mask_1: 0.08259  loss_dice_1: 1.36  loss_ce_2: 0.9332  loss_mask_2: 0.09256  loss_dice_2: 0.8986  loss_ce_3: 0.8418  loss_mask_3: 0.07581  loss_dice_3: 0.9099  loss_ce_4: 0.8349  loss_mask_4: 0.08441  loss_dice_4: 1.14  loss_ce_5: 0.7816  loss_mask_5: 0.07838  loss_dice_5: 1.066  loss_ce_6: 0.7662  loss_mask_6: 0.06803  loss_dice_6: 1.283  loss_ce_7: 0.8173  loss_mask_7: 0.0572  loss_dice_7: 1.033  loss_ce_8: 0.7352  loss_mask_8: 0.0626  loss_dice_8: 1.07    time: 0.3599  last_time: 0.2990  data_time: 0.0135  last_data_time: 0.0050   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:32 d2.utils.events]:  eta: 0:00:24  iter: 10719  total_loss: 16.03  loss_ce: 0.659  loss_mask: 0.08973  loss_dice: 0.6142  loss_ce_0: 1.336  loss_mask_0: 0.1285  loss_dice_0: 0.9803  loss_ce_1: 0.9293  loss_mask_1: 0.1043  loss_dice_1: 0.6942  loss_ce_2: 0.8507  loss_mask_2: 0.1072  loss_dice_2: 0.8251  loss_ce_3: 0.751  loss_mask_3: 0.1048  loss_dice_3: 0.6231  loss_ce_4: 0.6835  loss_mask_4: 0.1057  loss_dice_4: 0.6966  loss_ce_5: 0.6641  loss_mask_5: 0.1011  loss_dice_5: 0.7098  loss_ce_6: 0.6675  loss_mask_6: 0.09789  loss_dice_6: 0.6357  loss_ce_7: 0.6661  loss_mask_7: 0.09156  loss_dice_7: 0.6153  loss_ce_8: 0.6319  loss_mask_8: 0.08934  loss_dice_8: 0.6792    time: 0.3599  last_time: 0.3311  data_time: 0.0076  last_data_time: 0.0061   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:39 d2.utils.events]:  eta: 0:00:18  iter: 10739  total_loss: 23.25  loss_ce: 0.7872  loss_mask: 0.0878  loss_dice: 1.054  loss_ce_0: 1.493  loss_mask_0: 0.08382  loss_dice_0: 1.205  loss_ce_1: 1.017  loss_mask_1: 0.09031  loss_dice_1: 1.313  loss_ce_2: 0.9093  loss_mask_2: 0.1041  loss_dice_2: 1.049  loss_ce_3: 0.8148  loss_mask_3: 0.08495  loss_dice_3: 1.105  loss_ce_4: 0.8217  loss_mask_4: 0.08518  loss_dice_4: 1.09  loss_ce_5: 0.7959  loss_mask_5: 0.09129  loss_dice_5: 1.245  loss_ce_6: 0.7982  loss_mask_6: 0.07485  loss_dice_6: 1.032  loss_ce_7: 0.7851  loss_mask_7: 0.09697  loss_dice_7: 1.037  loss_ce_8: 0.6857  loss_mask_8: 0.09092  loss_dice_8: 1.109    time: 0.3598  last_time: 0.3045  data_time: 0.0074  last_data_time: 0.0073   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:45 d2.utils.events]:  eta: 0:00:12  iter: 10759  total_loss: 21.85  loss_ce: 0.9473  loss_mask: 0.09197  loss_dice: 0.9984  loss_ce_0: 1.734  loss_mask_0: 0.0949  loss_dice_0: 1.182  loss_ce_1: 1.125  loss_mask_1: 0.09299  loss_dice_1: 1.193  loss_ce_2: 0.9866  loss_mask_2: 0.1158  loss_dice_2: 1.177  loss_ce_3: 1.037  loss_mask_3: 0.09524  loss_dice_3: 1.079  loss_ce_4: 1.115  loss_mask_4: 0.09592  loss_dice_4: 1.04  loss_ce_5: 1.022  loss_mask_5: 0.09562  loss_dice_5: 0.8955  loss_ce_6: 0.8771  loss_mask_6: 0.07241  loss_dice_6: 0.8989  loss_ce_7: 0.9573  loss_mask_7: 0.08952  loss_dice_7: 1.015  loss_ce_8: 0.8658  loss_mask_8: 0.06937  loss_dice_8: 1.054    time: 0.3597  last_time: 0.3231  data_time: 0.0076  last_data_time: 0.0177   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:51 d2.utils.events]:  eta: 0:00:06  iter: 10779  total_loss: 17.25  loss_ce: 0.6162  loss_mask: 0.06642  loss_dice: 0.6016  loss_ce_0: 1.074  loss_mask_0: 0.0931  loss_dice_0: 0.659  loss_ce_1: 0.8587  loss_mask_1: 0.07392  loss_dice_1: 0.4472  loss_ce_2: 0.7939  loss_mask_2: 0.07605  loss_dice_2: 0.7071  loss_ce_3: 0.7555  loss_mask_3: 0.05681  loss_dice_3: 0.5335  loss_ce_4: 0.6666  loss_mask_4: 0.06569  loss_dice_4: 0.6959  loss_ce_5: 0.5975  loss_mask_5: 0.07237  loss_dice_5: 0.5881  loss_ce_6: 0.6954  loss_mask_6: 0.06123  loss_dice_6: 0.4564  loss_ce_7: 0.6509  loss_mask_7: 0.06919  loss_dice_7: 0.4974  loss_ce_8: 0.6337  loss_mask_8: 0.07133  loss_dice_8: 0.6167    time: 0.3596  last_time: 0.2948  data_time: 0.0071  last_data_time: 0.0056   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:58 d2.utils.events]:  eta: 0:00:00  iter: 10799  total_loss: 30.76  loss_ce: 0.986  loss_mask: 0.07599  loss_dice: 1.47  loss_ce_0: 1.644  loss_mask_0: 0.09699  loss_dice_0: 1.734  loss_ce_1: 1.403  loss_mask_1: 0.07899  loss_dice_1: 1.52  loss_ce_2: 1.312  loss_mask_2: 0.07965  loss_dice_2: 1.459  loss_ce_3: 1.297  loss_mask_3: 0.07823  loss_dice_3: 1.374  loss_ce_4: 1.101  loss_mask_4: 0.08729  loss_dice_4: 1.284  loss_ce_5: 1.113  loss_mask_5: 0.07218  loss_dice_5: 1.49  loss_ce_6: 1.108  loss_mask_6: 0.06876  loss_dice_6: 1.246  loss_ce_7: 1.109  loss_mask_7: 0.07256  loss_dice_7: 1.232  loss_ce_8: 1.133  loss_mask_8: 0.07273  loss_dice_8: 1.261    time: 0.3595  last_time: 0.3099  data_time: 0.0073  last_data_time: 0.0104   lr: 0.0001  max_mem: 37648M\n",
            "[09/04 13:43:58 d2.engine.hooks]: Overall training speed: 10798 iterations in 1:04:42 (0.3595 s / it)\n",
            "[09/04 13:43:58 d2.engine.hooks]: Total training time: 2:53:07 (1:48:25 on hooks)\n",
            "[09/04 13:43:58 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[09/04 13:43:58 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[09/04 13:43:58 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/04 13:43:58 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[09/04 13:43:58 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "taco10_val\n",
            "[09/04 13:43:58 d2.evaluation.evaluator]: Start inference on 150 batches\n",
            "[09/04 13:44:38 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0015 s/iter. Inference: 0.1050 s/iter. Eval: 3.8919 s/iter. Total: 3.9984 s/iter. ETA=0:09:15\n",
            "[09/04 13:44:46 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0016 s/iter. Inference: 0.1043 s/iter. Eval: 3.8373 s/iter. Total: 3.9436 s/iter. ETA=0:09:00\n",
            "[09/04 13:44:53 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0017 s/iter. Inference: 0.1037 s/iter. Eval: 3.8062 s/iter. Total: 3.9122 s/iter. ETA=0:08:48\n",
            "[09/04 13:45:01 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0018 s/iter. Inference: 0.1040 s/iter. Eval: 3.7860 s/iter. Total: 3.8923 s/iter. ETA=0:08:37\n",
            "[09/04 13:45:08 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0018 s/iter. Inference: 0.1043 s/iter. Eval: 3.7706 s/iter. Total: 3.8772 s/iter. ETA=0:08:27\n",
            "[09/04 13:45:16 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0019 s/iter. Inference: 0.1040 s/iter. Eval: 3.7613 s/iter. Total: 3.8677 s/iter. ETA=0:08:18\n",
            "[09/04 13:45:20 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:45:40 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0019 s/iter. Inference: 0.9708 s/iter. Eval: 3.8258 s/iter. Total: 4.7992 s/iter. ETA=0:10:09\n",
            "[09/04 13:45:41 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:46:02 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0020 s/iter. Inference: 1.7938 s/iter. Eval: 3.8882 s/iter. Total: 5.6846 s/iter. ETA=0:11:56\n",
            "[09/04 13:46:08 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0020 s/iter. Inference: 1.7378 s/iter. Eval: 3.9426 s/iter. Total: 5.6831 s/iter. ETA=0:11:50\n",
            "[09/04 13:46:08 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:46:22 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0021 s/iter. Inference: 2.1561 s/iter. Eval: 3.9408 s/iter. Total: 6.0997 s/iter. ETA=0:12:36\n",
            "[09/04 13:46:23 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:46:37 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0021 s/iter. Inference: 2.5709 s/iter. Eval: 3.9379 s/iter. Total: 6.5116 s/iter. ETA=0:13:20\n",
            "[09/04 13:46:44 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0021 s/iter. Inference: 2.4793 s/iter. Eval: 4.0560 s/iter. Total: 6.5381 s/iter. ETA=0:13:17\n",
            "[09/04 13:46:52 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0021 s/iter. Inference: 2.4052 s/iter. Eval: 4.1637 s/iter. Total: 6.5718 s/iter. ETA=0:13:15\n",
            "[09/04 13:46:53 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:47:06 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0021 s/iter. Inference: 2.7672 s/iter. Eval: 4.1248 s/iter. Total: 6.8950 s/iter. ETA=0:13:47\n",
            "[09/04 13:47:13 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0021 s/iter. Inference: 2.6785 s/iter. Eval: 4.2054 s/iter. Total: 6.8869 s/iter. ETA=0:13:39\n",
            "[09/04 13:47:19 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0022 s/iter. Inference: 2.5840 s/iter. Eval: 4.2795 s/iter. Total: 6.8664 s/iter. ETA=0:13:30\n",
            "[09/04 13:47:26 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0021 s/iter. Inference: 2.4962 s/iter. Eval: 4.3512 s/iter. Total: 6.8504 s/iter. ETA=0:13:21\n",
            "[09/04 13:47:32 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0021 s/iter. Inference: 2.4146 s/iter. Eval: 4.4146 s/iter. Total: 6.8322 s/iter. ETA=0:13:12\n",
            "[09/04 13:47:39 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0021 s/iter. Inference: 2.3435 s/iter. Eval: 4.4895 s/iter. Total: 6.8361 s/iter. ETA=0:13:06\n",
            "[09/04 13:47:47 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0021 s/iter. Inference: 2.2039 s/iter. Eval: 4.4620 s/iter. Total: 6.6689 s/iter. ETA=0:12:33\n",
            "[09/04 13:47:48 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:48:02 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0021 s/iter. Inference: 2.4640 s/iter. Eval: 4.4337 s/iter. Total: 6.9007 s/iter. ETA=0:12:52\n",
            "[09/04 13:48:02 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:48:16 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0022 s/iter. Inference: 2.7174 s/iter. Eval: 4.4060 s/iter. Total: 7.1265 s/iter. ETA=0:13:11\n",
            "[09/04 13:48:22 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0022 s/iter. Inference: 2.6531 s/iter. Eval: 4.4205 s/iter. Total: 7.0767 s/iter. ETA=0:12:58\n",
            "[09/04 13:48:22 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:48:36 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0022 s/iter. Inference: 2.8883 s/iter. Eval: 4.3943 s/iter. Total: 7.2857 s/iter. ETA=0:13:14\n",
            "[09/04 13:48:42 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0022 s/iter. Inference: 2.8229 s/iter. Eval: 4.4120 s/iter. Total: 7.2380 s/iter. ETA=0:13:01\n",
            "[09/04 13:48:49 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0022 s/iter. Inference: 2.7662 s/iter. Eval: 4.4711 s/iter. Total: 7.2404 s/iter. ETA=0:12:54\n",
            "[09/04 13:48:56 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0022 s/iter. Inference: 2.6986 s/iter. Eval: 4.5258 s/iter. Total: 7.2276 s/iter. ETA=0:12:46\n",
            "[09/04 13:49:03 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0022 s/iter. Inference: 2.6344 s/iter. Eval: 4.5791 s/iter. Total: 7.2167 s/iter. ETA=0:12:37\n",
            "[09/04 13:49:09 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0022 s/iter. Inference: 2.5733 s/iter. Eval: 4.6304 s/iter. Total: 7.2069 s/iter. ETA=0:12:29\n",
            "[09/04 13:49:16 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0022 s/iter. Inference: 2.5153 s/iter. Eval: 4.6775 s/iter. Total: 7.1960 s/iter. ETA=0:12:21\n",
            "[09/04 13:49:23 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0022 s/iter. Inference: 2.4598 s/iter. Eval: 4.7230 s/iter. Total: 7.1860 s/iter. ETA=0:12:12\n",
            "[09/04 13:49:30 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0022 s/iter. Inference: 2.4068 s/iter. Eval: 4.7666 s/iter. Total: 7.1767 s/iter. ETA=0:12:04\n",
            "[09/04 13:49:37 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0022 s/iter. Inference: 2.3563 s/iter. Eval: 4.8071 s/iter. Total: 7.1667 s/iter. ETA=0:11:56\n",
            "[09/04 13:49:43 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0022 s/iter. Inference: 2.3079 s/iter. Eval: 4.8456 s/iter. Total: 7.1567 s/iter. ETA=0:11:48\n",
            "[09/04 13:49:50 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0022 s/iter. Inference: 2.2616 s/iter. Eval: 4.8843 s/iter. Total: 7.1492 s/iter. ETA=0:11:40\n",
            "[09/04 13:49:57 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0022 s/iter. Inference: 2.2172 s/iter. Eval: 4.9196 s/iter. Total: 7.1401 s/iter. ETA=0:11:32\n",
            "[09/04 13:50:03 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0022 s/iter. Inference: 2.1747 s/iter. Eval: 4.9466 s/iter. Total: 7.1246 s/iter. ETA=0:11:23\n",
            "[09/04 13:50:09 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0022 s/iter. Inference: 2.1337 s/iter. Eval: 4.9724 s/iter. Total: 7.1093 s/iter. ETA=0:11:15\n",
            "[09/04 13:50:16 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0022 s/iter. Inference: 2.0944 s/iter. Eval: 4.9968 s/iter. Total: 7.0944 s/iter. ETA=0:11:06\n",
            "[09/04 13:50:22 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0022 s/iter. Inference: 2.0565 s/iter. Eval: 5.0199 s/iter. Total: 7.0797 s/iter. ETA=0:10:58\n",
            "[09/04 13:50:29 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0022 s/iter. Inference: 2.0201 s/iter. Eval: 5.0440 s/iter. Total: 7.0674 s/iter. ETA=0:10:50\n",
            "[09/04 13:50:35 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0022 s/iter. Inference: 1.9850 s/iter. Eval: 5.0658 s/iter. Total: 7.0542 s/iter. ETA=0:10:41\n",
            "[09/04 13:50:41 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0022 s/iter. Inference: 1.9513 s/iter. Eval: 5.0876 s/iter. Total: 7.0422 s/iter. ETA=0:10:33\n",
            "[09/04 13:50:48 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0023 s/iter. Inference: 1.9187 s/iter. Eval: 5.1084 s/iter. Total: 7.0304 s/iter. ETA=0:10:25\n",
            "[09/04 13:50:54 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0023 s/iter. Inference: 1.8872 s/iter. Eval: 5.1287 s/iter. Total: 7.0192 s/iter. ETA=0:10:17\n",
            "[09/04 13:51:00 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0023 s/iter. Inference: 1.8568 s/iter. Eval: 5.1477 s/iter. Total: 7.0079 s/iter. ETA=0:10:09\n",
            "[09/04 13:51:07 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0023 s/iter. Inference: 1.8275 s/iter. Eval: 5.1660 s/iter. Total: 6.9969 s/iter. ETA=0:10:01\n",
            "[09/04 13:51:18 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0022 s/iter. Inference: 1.7438 s/iter. Eval: 5.0842 s/iter. Total: 6.8313 s/iter. ETA=0:09:26\n",
            "[09/04 13:51:24 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0022 s/iter. Inference: 1.7181 s/iter. Eval: 5.1024 s/iter. Total: 6.8239 s/iter. ETA=0:09:19\n",
            "[09/04 13:51:30 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0022 s/iter. Inference: 1.6682 s/iter. Eval: 5.0358 s/iter. Total: 6.7073 s/iter. ETA=0:08:56\n",
            "[09/04 13:51:38 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0022 s/iter. Inference: 1.6217 s/iter. Eval: 5.0062 s/iter. Total: 6.6311 s/iter. ETA=0:08:37\n",
            "[09/04 13:51:47 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0022 s/iter. Inference: 1.5778 s/iter. Eval: 4.9783 s/iter. Total: 6.5594 s/iter. ETA=0:08:18\n",
            "[09/04 13:51:55 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0022 s/iter. Inference: 1.5364 s/iter. Eval: 4.9523 s/iter. Total: 6.4919 s/iter. ETA=0:08:00\n",
            "[09/04 13:52:03 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0022 s/iter. Inference: 1.4973 s/iter. Eval: 4.9275 s/iter. Total: 6.4281 s/iter. ETA=0:07:42\n",
            "[09/04 13:52:12 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0022 s/iter. Inference: 1.4603 s/iter. Eval: 4.9040 s/iter. Total: 6.3676 s/iter. ETA=0:07:25\n",
            "[09/04 13:52:20 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0022 s/iter. Inference: 1.4252 s/iter. Eval: 4.8823 s/iter. Total: 6.3107 s/iter. ETA=0:07:09\n",
            "[09/04 13:52:28 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0022 s/iter. Inference: 1.3918 s/iter. Eval: 4.8614 s/iter. Total: 6.2564 s/iter. ETA=0:06:52\n",
            "[09/04 13:52:37 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0022 s/iter. Inference: 1.3602 s/iter. Eval: 4.8424 s/iter. Total: 6.2058 s/iter. ETA=0:06:37\n",
            "[09/04 13:52:45 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0022 s/iter. Inference: 1.3300 s/iter. Eval: 4.8233 s/iter. Total: 6.1565 s/iter. ETA=0:06:21\n",
            "[09/04 13:52:53 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0022 s/iter. Inference: 1.3012 s/iter. Eval: 4.8045 s/iter. Total: 6.1090 s/iter. ETA=0:06:06\n",
            "[09/04 13:53:02 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0022 s/iter. Inference: 1.2738 s/iter. Eval: 4.7875 s/iter. Total: 6.0645 s/iter. ETA=0:05:51\n",
            "[09/04 13:53:10 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0022 s/iter. Inference: 1.2476 s/iter. Eval: 4.7710 s/iter. Total: 6.0218 s/iter. ETA=0:05:37\n",
            "[09/04 13:53:18 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0022 s/iter. Inference: 1.2226 s/iter. Eval: 4.7555 s/iter. Total: 5.9812 s/iter. ETA=0:05:22\n",
            "[09/04 13:53:27 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0022 s/iter. Inference: 1.1987 s/iter. Eval: 4.7402 s/iter. Total: 5.9420 s/iter. ETA=0:05:08\n",
            "[09/04 13:53:35 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 1.1757 s/iter. Eval: 4.7257 s/iter. Total: 5.9046 s/iter. ETA=0:04:55\n",
            "[09/04 13:53:43 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0022 s/iter. Inference: 1.1537 s/iter. Eval: 4.7120 s/iter. Total: 5.8689 s/iter. ETA=0:04:41\n",
            "[09/04 13:53:52 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0022 s/iter. Inference: 1.1327 s/iter. Eval: 4.6991 s/iter. Total: 5.8349 s/iter. ETA=0:04:28\n",
            "[09/04 13:54:00 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0022 s/iter. Inference: 1.1124 s/iter. Eval: 4.6859 s/iter. Total: 5.8014 s/iter. ETA=0:04:15\n",
            "[09/04 13:54:08 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0022 s/iter. Inference: 1.0928 s/iter. Eval: 4.6741 s/iter. Total: 5.7701 s/iter. ETA=0:04:02\n",
            "[09/04 13:54:17 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0022 s/iter. Inference: 1.0741 s/iter. Eval: 4.6625 s/iter. Total: 5.7396 s/iter. ETA=0:03:49\n",
            "[09/04 13:54:25 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0022 s/iter. Inference: 1.0560 s/iter. Eval: 4.6508 s/iter. Total: 5.7099 s/iter. ETA=0:03:36\n",
            "[09/04 13:54:32 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0022 s/iter. Inference: 1.0385 s/iter. Eval: 4.6271 s/iter. Total: 5.6687 s/iter. ETA=0:03:24\n",
            "[09/04 13:54:37 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:54:57 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 1.1661 s/iter. Eval: 4.6259 s/iter. Total: 5.7951 s/iter. ETA=0:03:17\n",
            "[09/04 13:54:58 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:55:19 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 1.3066 s/iter. Eval: 4.6292 s/iter. Total: 5.9390 s/iter. ETA=0:03:15\n",
            "[09/04 13:55:28 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 1.2904 s/iter. Eval: 4.6192 s/iter. Total: 5.9128 s/iter. ETA=0:03:03\n",
            "[09/04 13:55:36 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.2700 s/iter. Eval: 4.6097 s/iter. Total: 5.8828 s/iter. ETA=0:02:50\n",
            "[09/04 13:55:45 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.2503 s/iter. Eval: 4.5998 s/iter. Total: 5.8533 s/iter. ETA=0:02:38\n",
            "[09/04 13:55:53 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.2313 s/iter. Eval: 4.5904 s/iter. Total: 5.8248 s/iter. ETA=0:02:25\n",
            "[09/04 13:56:01 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.2129 s/iter. Eval: 4.5815 s/iter. Total: 5.7974 s/iter. ETA=0:02:13\n",
            "[09/04 13:56:07 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.1949 s/iter. Eval: 4.5532 s/iter. Total: 5.7512 s/iter. ETA=0:02:00\n",
            "[09/04 13:56:17 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.1779 s/iter. Eval: 4.5554 s/iter. Total: 5.7364 s/iter. ETA=0:01:48\n",
            "[09/04 13:56:26 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 1.1613 s/iter. Eval: 4.5523 s/iter. Total: 5.7166 s/iter. ETA=0:01:37\n",
            "[09/04 13:56:34 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 1.1450 s/iter. Eval: 4.5447 s/iter. Total: 5.6928 s/iter. ETA=0:01:25\n",
            "[09/04 13:56:42 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 1.1293 s/iter. Eval: 4.5377 s/iter. Total: 5.6700 s/iter. ETA=0:01:13\n",
            "[09/04 13:56:51 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 1.1141 s/iter. Eval: 4.5303 s/iter. Total: 5.6475 s/iter. ETA=0:01:02\n",
            "[09/04 13:56:59 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 1.0993 s/iter. Eval: 4.5230 s/iter. Total: 5.6253 s/iter. ETA=0:00:50\n",
            "[09/04 13:57:07 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 1.0849 s/iter. Eval: 4.5159 s/iter. Total: 5.6038 s/iter. ETA=0:00:39\n",
            "[09/04 13:57:15 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0022 s/iter. Inference: 1.0709 s/iter. Eval: 4.5039 s/iter. Total: 5.5779 s/iter. ETA=0:00:27\n",
            "[09/04 13:57:22 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0022 s/iter. Inference: 1.0572 s/iter. Eval: 4.4922 s/iter. Total: 5.5525 s/iter. ETA=0:00:16\n",
            "[09/04 13:57:30 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0022 s/iter. Inference: 1.0439 s/iter. Eval: 4.4811 s/iter. Total: 5.5281 s/iter. ETA=0:00:05\n",
            "[09/04 13:57:34 d2.evaluation.evaluator]: Total inference time: 0:13:19.858615 (5.516266 s / iter per device, on 1 devices)\n",
            "[09/04 13:57:34 d2.evaluation.evaluator]: Total inference pure compute time: 0:02:30 (1.037430 s / iter per device, on 1 devices)\n",
            "[09/04 13:57:34 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/04 13:57:34 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[09/04 13:57:34 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/04 13:57:34 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/04 13:57:34 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.12 seconds.\n",
            "[09/04 13:57:34 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/04 13:57:34 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/04 13:57:34 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/04 13:57:34 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.33s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/04 13:57:35 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/04 13:57:36 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.32 seconds.\n",
            "[09/04 13:57:36 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/04 13:57:36 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.138\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.100\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.134\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.235\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.329\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.346\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.194\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.451\n",
            "[09/04 13:57:36 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 9.801 | 13.824 | 10.039 | 0.137 | 3.828 | 13.352 |\n",
            "[09/04 13:57:36 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 20.707 | Other      | 13.313 | Bottle     | 15.571 |\n",
            "| Bottle cap            | 13.181 | Cup        | 5.442  | Lid        | 1.096  |\n",
            "| Plastic bag + wrapper | 21.612 | Pop tab    | 0.088  | Straw      | 5.642  |\n",
            "| Cigarette             | 1.359  |            |        |            |        |\n",
            "[09/04 13:57:36 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[09/04 13:57:36 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/04 13:57:36 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/04 13:57:36 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/04 13:57:36 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/04 13:57:36 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/04 13:57:36 d2.evaluation.testing]: copypaste: 9.8011,13.8237,10.0387,0.1368,3.8281,13.3519\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.evaluation import inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "evaluator = COCOEvaluator(\"taco10_test\", output_dir=\"output\")\n",
        "mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
        "test_loader = build_detection_test_loader(cfg, \"taco10_test\", mapper=mapper)\n",
        "metrics = inference_on_dataset(trainer.model, test_loader, evaluator)\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6nDuTA80_jl",
        "outputId": "62126719-d877-4094-8072-725e9bd4388a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/04 13:57:37 mask2former.data.dataset_mappers.mask_former_instance_dataset_mapper]: [MaskFormerInstanceDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/04 13:57:37 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_test.json\n",
            "[09/04 13:57:37 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 37           |   Other    | 151          |   Bottle   | 54           |\n",
            "|  Bottle cap   | 41           |    Cup     | 17           |    Lid     | 15           |\n",
            "| Plastic bag.. | 84           |  Pop tab   | 11           |   Straw    | 38           |\n",
            "|   Cigarette   | 121          |            |              |            |              |\n",
            "|     total     | 569          |            |              |            |              |\n",
            "[09/04 13:57:37 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/04 13:57:37 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[09/04 13:57:37 d2.data.common]: Serialized dataset takes 0.25 MiB\n",
            "[09/04 13:57:37 d2.evaluation.evaluator]: Start inference on 150 batches\n",
            "[09/04 13:58:15 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0027 s/iter. Inference: 0.1020 s/iter. Eval: 3.7386 s/iter. Total: 3.8433 s/iter. ETA=0:08:54\n",
            "[09/04 13:58:22 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0028 s/iter. Inference: 0.1017 s/iter. Eval: 3.7275 s/iter. Total: 3.8323 s/iter. ETA=0:08:45\n",
            "[09/04 13:58:30 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0029 s/iter. Inference: 0.1016 s/iter. Eval: 3.7170 s/iter. Total: 3.8219 s/iter. ETA=0:08:35\n",
            "[09/04 13:58:37 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0029 s/iter. Inference: 0.1015 s/iter. Eval: 3.7120 s/iter. Total: 3.8169 s/iter. ETA=0:08:27\n",
            "[09/04 13:58:45 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0029 s/iter. Inference: 0.1015 s/iter. Eval: 3.7135 s/iter. Total: 3.8185 s/iter. ETA=0:08:20\n",
            "[09/04 13:58:50 d2.evaluation.evaluator]: Inference done 20/150. Dataloading: 0.0029 s/iter. Inference: 0.1029 s/iter. Eval: 3.7940 s/iter. Total: 3.9005 s/iter. ETA=0:08:27\n",
            "[09/04 13:58:57 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0030 s/iter. Inference: 0.1403 s/iter. Eval: 3.9470 s/iter. Total: 4.0909 s/iter. ETA=0:08:47\n",
            "[09/04 13:59:03 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0030 s/iter. Inference: 0.1395 s/iter. Eval: 4.0809 s/iter. Total: 4.2242 s/iter. ETA=0:09:00\n",
            "[09/04 13:59:10 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0030 s/iter. Inference: 0.1388 s/iter. Eval: 4.2013 s/iter. Total: 4.3439 s/iter. ETA=0:09:11\n",
            "[09/04 13:59:16 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0031 s/iter. Inference: 0.1382 s/iter. Eval: 4.3089 s/iter. Total: 4.4509 s/iter. ETA=0:09:20\n",
            "[09/04 13:59:23 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0031 s/iter. Inference: 0.1376 s/iter. Eval: 4.4055 s/iter. Total: 4.5469 s/iter. ETA=0:09:28\n",
            "[09/04 13:59:29 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0031 s/iter. Inference: 0.1371 s/iter. Eval: 4.4925 s/iter. Total: 4.6335 s/iter. ETA=0:09:34\n",
            "[09/04 13:59:35 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0031 s/iter. Inference: 0.1366 s/iter. Eval: 4.5718 s/iter. Total: 4.7124 s/iter. ETA=0:09:39\n",
            "[09/04 13:59:40 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 13:59:54 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0031 s/iter. Inference: 0.5908 s/iter. Eval: 4.5092 s/iter. Total: 5.1039 s/iter. ETA=0:10:17\n",
            "[09/04 13:59:55 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 14:00:09 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0032 s/iter. Inference: 1.0423 s/iter. Eval: 4.4693 s/iter. Total: 5.5156 s/iter. ETA=0:11:01\n",
            "[09/04 14:00:10 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 14:00:25 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0032 s/iter. Inference: 1.4556 s/iter. Eval: 4.4304 s/iter. Total: 5.8900 s/iter. ETA=0:11:40\n",
            "[09/04 14:00:32 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0032 s/iter. Inference: 1.4184 s/iter. Eval: 4.5112 s/iter. Total: 5.9337 s/iter. ETA=0:11:40\n",
            "[09/04 14:00:38 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0032 s/iter. Inference: 1.3920 s/iter. Eval: 4.5638 s/iter. Total: 5.9600 s/iter. ETA=0:11:37\n",
            "[09/04 14:00:44 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0033 s/iter. Inference: 1.3483 s/iter. Eval: 4.5826 s/iter. Total: 5.9351 s/iter. ETA=0:11:28\n",
            "[09/04 14:00:51 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0033 s/iter. Inference: 1.3222 s/iter. Eval: 4.6498 s/iter. Total: 5.9762 s/iter. ETA=0:11:27\n",
            "[09/04 14:00:58 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0033 s/iter. Inference: 1.2889 s/iter. Eval: 4.7122 s/iter. Total: 6.0053 s/iter. ETA=0:11:24\n",
            "[09/04 14:01:05 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0033 s/iter. Inference: 1.2528 s/iter. Eval: 4.7723 s/iter. Total: 6.0293 s/iter. ETA=0:11:21\n",
            "[09/04 14:01:11 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0034 s/iter. Inference: 1.2187 s/iter. Eval: 4.8271 s/iter. Total: 6.0502 s/iter. ETA=0:11:17\n",
            "[09/04 14:01:18 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0034 s/iter. Inference: 1.1867 s/iter. Eval: 4.8787 s/iter. Total: 6.0699 s/iter. ETA=0:11:13\n",
            "[09/04 14:01:25 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0035 s/iter. Inference: 1.1565 s/iter. Eval: 4.9278 s/iter. Total: 6.0888 s/iter. ETA=0:11:09\n",
            "[09/04 14:01:31 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0035 s/iter. Inference: 1.1280 s/iter. Eval: 4.9750 s/iter. Total: 6.1075 s/iter. ETA=0:11:05\n",
            "[09/04 14:01:38 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0035 s/iter. Inference: 1.1010 s/iter. Eval: 5.0207 s/iter. Total: 6.1262 s/iter. ETA=0:11:01\n",
            "[09/04 14:01:45 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0035 s/iter. Inference: 1.0754 s/iter. Eval: 5.0630 s/iter. Total: 6.1430 s/iter. ETA=0:10:57\n",
            "[09/04 14:01:51 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0035 s/iter. Inference: 1.0511 s/iter. Eval: 5.0934 s/iter. Total: 6.1491 s/iter. ETA=0:10:51\n",
            "[09/04 14:01:58 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0035 s/iter. Inference: 1.0279 s/iter. Eval: 5.1224 s/iter. Total: 6.1549 s/iter. ETA=0:10:46\n",
            "[09/04 14:02:04 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0035 s/iter. Inference: 1.0060 s/iter. Eval: 5.1492 s/iter. Total: 6.1598 s/iter. ETA=0:10:40\n",
            "[09/04 14:02:11 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0035 s/iter. Inference: 0.9850 s/iter. Eval: 5.1750 s/iter. Total: 6.1647 s/iter. ETA=0:10:34\n",
            "[09/04 14:02:17 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0035 s/iter. Inference: 0.9650 s/iter. Eval: 5.1999 s/iter. Total: 6.1696 s/iter. ETA=0:10:29\n",
            "[09/04 14:02:23 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0035 s/iter. Inference: 0.9459 s/iter. Eval: 5.2239 s/iter. Total: 6.1745 s/iter. ETA=0:10:23\n",
            "[09/04 14:02:30 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0035 s/iter. Inference: 0.9277 s/iter. Eval: 5.2465 s/iter. Total: 6.1789 s/iter. ETA=0:10:17\n",
            "[09/04 14:02:36 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0035 s/iter. Inference: 0.9103 s/iter. Eval: 5.2679 s/iter. Total: 6.1829 s/iter. ETA=0:10:12\n",
            "[09/04 14:02:42 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0035 s/iter. Inference: 0.8936 s/iter. Eval: 5.2885 s/iter. Total: 6.1868 s/iter. ETA=0:10:06\n",
            "[09/04 14:02:48 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0035 s/iter. Inference: 0.8611 s/iter. Eval: 5.1927 s/iter. Total: 6.0584 s/iter. ETA=0:09:41\n",
            "[09/04 14:02:57 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0036 s/iter. Inference: 0.8314 s/iter. Eval: 5.1465 s/iter. Total: 5.9826 s/iter. ETA=0:09:22\n",
            "[09/04 14:03:03 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0036 s/iter. Inference: 0.8179 s/iter. Eval: 5.1675 s/iter. Total: 5.9901 s/iter. ETA=0:09:17\n",
            "[09/04 14:03:12 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0036 s/iter. Inference: 0.7783 s/iter. Eval: 5.0464 s/iter. Total: 5.8294 s/iter. ETA=0:08:44\n",
            "[09/04 14:03:21 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0036 s/iter. Inference: 0.7548 s/iter. Eval: 5.0128 s/iter. Total: 5.7722 s/iter. ETA=0:08:27\n",
            "[09/04 14:03:29 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0036 s/iter. Inference: 0.7328 s/iter. Eval: 4.9806 s/iter. Total: 5.7181 s/iter. ETA=0:08:11\n",
            "[09/04 14:03:35 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0036 s/iter. Inference: 0.7124 s/iter. Eval: 4.9064 s/iter. Total: 5.6235 s/iter. ETA=0:07:52\n",
            "[09/04 14:03:43 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0036 s/iter. Inference: 0.6932 s/iter. Eval: 4.8800 s/iter. Total: 5.5779 s/iter. ETA=0:07:37\n",
            "[09/04 14:03:51 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0036 s/iter. Inference: 0.6751 s/iter. Eval: 4.8552 s/iter. Total: 5.5349 s/iter. ETA=0:07:22\n",
            "[09/04 14:04:00 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0036 s/iter. Inference: 0.6582 s/iter. Eval: 4.8321 s/iter. Total: 5.4950 s/iter. ETA=0:07:08\n",
            "[09/04 14:04:08 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0036 s/iter. Inference: 0.6422 s/iter. Eval: 4.8103 s/iter. Total: 5.4571 s/iter. ETA=0:06:54\n",
            "[09/04 14:04:16 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0036 s/iter. Inference: 0.6271 s/iter. Eval: 4.7888 s/iter. Total: 5.4206 s/iter. ETA=0:06:41\n",
            "[09/04 14:04:25 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0036 s/iter. Inference: 0.6129 s/iter. Eval: 4.7690 s/iter. Total: 5.3865 s/iter. ETA=0:06:27\n",
            "[09/04 14:04:33 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0036 s/iter. Inference: 0.5994 s/iter. Eval: 4.7496 s/iter. Total: 5.3536 s/iter. ETA=0:06:14\n",
            "[09/04 14:04:41 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0036 s/iter. Inference: 0.5866 s/iter. Eval: 4.7312 s/iter. Total: 5.3223 s/iter. ETA=0:06:01\n",
            "[09/04 14:04:50 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0036 s/iter. Inference: 0.5744 s/iter. Eval: 4.7147 s/iter. Total: 5.2937 s/iter. ETA=0:05:49\n",
            "[09/04 14:04:58 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0036 s/iter. Inference: 0.5629 s/iter. Eval: 4.6989 s/iter. Total: 5.2663 s/iter. ETA=0:05:37\n",
            "[09/04 14:05:06 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0036 s/iter. Inference: 0.5519 s/iter. Eval: 4.6833 s/iter. Total: 5.2397 s/iter. ETA=0:05:24\n",
            "[09/04 14:05:15 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0036 s/iter. Inference: 0.5414 s/iter. Eval: 4.6683 s/iter. Total: 5.2143 s/iter. ETA=0:05:12\n",
            "[09/04 14:05:23 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0035 s/iter. Inference: 0.5314 s/iter. Eval: 4.6540 s/iter. Total: 5.1899 s/iter. ETA=0:05:01\n",
            "[09/04 14:05:31 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0035 s/iter. Inference: 0.5219 s/iter. Eval: 4.6405 s/iter. Total: 5.1669 s/iter. ETA=0:04:49\n",
            "[09/04 14:05:40 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0035 s/iter. Inference: 0.5127 s/iter. Eval: 4.6278 s/iter. Total: 5.1450 s/iter. ETA=0:04:37\n",
            "[09/04 14:05:48 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0035 s/iter. Inference: 0.5040 s/iter. Eval: 4.6156 s/iter. Total: 5.1241 s/iter. ETA=0:04:26\n",
            "[09/04 14:05:56 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0035 s/iter. Inference: 0.4957 s/iter. Eval: 4.6037 s/iter. Total: 5.1038 s/iter. ETA=0:04:15\n",
            "[09/04 14:06:05 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0035 s/iter. Inference: 0.4876 s/iter. Eval: 4.5928 s/iter. Total: 5.0849 s/iter. ETA=0:04:04\n",
            "[09/04 14:06:13 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0035 s/iter. Inference: 0.4799 s/iter. Eval: 4.5818 s/iter. Total: 5.0662 s/iter. ETA=0:03:53\n",
            "[09/04 14:06:21 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0035 s/iter. Inference: 0.4725 s/iter. Eval: 4.5716 s/iter. Total: 5.0486 s/iter. ETA=0:03:42\n",
            "[09/04 14:06:30 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0035 s/iter. Inference: 0.4654 s/iter. Eval: 4.5616 s/iter. Total: 5.0314 s/iter. ETA=0:03:31\n",
            "[09/04 14:06:37 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0035 s/iter. Inference: 0.4585 s/iter. Eval: 4.5378 s/iter. Total: 5.0007 s/iter. ETA=0:03:20\n",
            "[09/04 14:06:45 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0035 s/iter. Inference: 0.4519 s/iter. Eval: 4.5288 s/iter. Total: 4.9852 s/iter. ETA=0:03:09\n",
            "[09/04 14:06:53 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0035 s/iter. Inference: 0.4456 s/iter. Eval: 4.5208 s/iter. Total: 4.9708 s/iter. ETA=0:02:58\n",
            "[09/04 14:06:59 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0035 s/iter. Inference: 0.4426 s/iter. Eval: 4.5265 s/iter. Total: 4.9735 s/iter. ETA=0:02:54\n",
            "[09/04 14:07:07 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0035 s/iter. Inference: 0.4366 s/iter. Eval: 4.5180 s/iter. Total: 4.9590 s/iter. ETA=0:02:43\n",
            "[09/04 14:07:12 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 14:07:33 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0035 s/iter. Inference: 0.5740 s/iter. Eval: 4.5181 s/iter. Total: 5.0965 s/iter. ETA=0:02:37\n",
            "[09/04 14:07:33 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/04 14:07:54 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0035 s/iter. Inference: 0.7134 s/iter. Eval: 4.5225 s/iter. Total: 5.2403 s/iter. ETA=0:02:37\n",
            "[09/04 14:08:03 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0035 s/iter. Inference: 0.7077 s/iter. Eval: 4.5153 s/iter. Total: 5.2274 s/iter. ETA=0:02:26\n",
            "[09/04 14:08:12 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0035 s/iter. Inference: 0.6976 s/iter. Eval: 4.5076 s/iter. Total: 5.2096 s/iter. ETA=0:02:15\n",
            "[09/04 14:08:20 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0035 s/iter. Inference: 0.6878 s/iter. Eval: 4.5001 s/iter. Total: 5.1923 s/iter. ETA=0:02:04\n",
            "[09/04 14:08:28 d2.evaluation.evaluator]: Inference done 128/150. Dataloading: 0.0035 s/iter. Inference: 0.6783 s/iter. Eval: 4.4931 s/iter. Total: 5.1758 s/iter. ETA=0:01:53\n",
            "[09/04 14:08:37 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0035 s/iter. Inference: 0.6692 s/iter. Eval: 4.4865 s/iter. Total: 5.1601 s/iter. ETA=0:01:43\n",
            "[09/04 14:08:45 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0035 s/iter. Inference: 0.6603 s/iter. Eval: 4.4797 s/iter. Total: 5.1444 s/iter. ETA=0:01:32\n",
            "[09/04 14:08:53 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0035 s/iter. Inference: 0.6518 s/iter. Eval: 4.4739 s/iter. Total: 5.1301 s/iter. ETA=0:01:22\n",
            "[09/04 14:09:02 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0035 s/iter. Inference: 0.6434 s/iter. Eval: 4.4676 s/iter. Total: 5.1155 s/iter. ETA=0:01:11\n",
            "[09/04 14:09:10 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0035 s/iter. Inference: 0.6354 s/iter. Eval: 4.4616 s/iter. Total: 5.1014 s/iter. ETA=0:01:01\n",
            "[09/04 14:09:20 d2.evaluation.evaluator]: Inference done 140/150. Dataloading: 0.0035 s/iter. Inference: 0.6277 s/iter. Eval: 4.4648 s/iter. Total: 5.0969 s/iter. ETA=0:00:50\n",
            "[09/04 14:09:28 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0035 s/iter. Inference: 0.6201 s/iter. Eval: 4.4588 s/iter. Total: 5.0833 s/iter. ETA=0:00:40\n",
            "[09/04 14:09:36 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0035 s/iter. Inference: 0.6128 s/iter. Eval: 4.4531 s/iter. Total: 5.0703 s/iter. ETA=0:00:30\n",
            "[09/04 14:09:45 d2.evaluation.evaluator]: Inference done 146/150. Dataloading: 0.0035 s/iter. Inference: 0.6056 s/iter. Eval: 4.4475 s/iter. Total: 5.0575 s/iter. ETA=0:00:20\n",
            "[09/04 14:09:53 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0035 s/iter. Inference: 0.5986 s/iter. Eval: 4.4396 s/iter. Total: 5.0426 s/iter. ETA=0:00:10\n",
            "[09/04 14:10:00 d2.evaluation.evaluator]: Inference done 150/150. Dataloading: 0.0035 s/iter. Inference: 0.5917 s/iter. Eval: 4.4297 s/iter. Total: 5.0258 s/iter. ETA=0:00:00\n",
            "[09/04 14:10:00 d2.evaluation.evaluator]: Total inference time: 0:12:08.789547 (5.026135 s / iter per device, on 1 devices)\n",
            "[09/04 14:10:00 d2.evaluation.evaluator]: Total inference pure compute time: 0:01:25 (0.591717 s / iter per device, on 1 devices)\n",
            "[09/04 14:10:01 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/04 14:10:01 d2.evaluation.coco_evaluation]: Saving results to output/coco_instances_results.json\n",
            "[09/04 14:10:01 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/04 14:10:01 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/04 14:10:01 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.12 seconds.\n",
            "[09/04 14:10:01 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/04 14:10:01 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/04 14:10:01 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/04 14:10:01 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.31s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/04 14:10:02 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/04 14:10:02 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.31 seconds.\n",
            "[09/04 14:10:02 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/04 14:10:02 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.064\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.082\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.143\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.085\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203\n",
            "[09/04 14:10:02 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 4.099 | 6.366  | 4.079  | 0.484 | 2.524 | 4.815 |\n",
            "[09/04 14:10:02 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 7.562 | Other      | 5.294 | Bottle     | 2.403 |\n",
            "| Bottle cap            | 4.369 | Cup        | 0.884 | Lid        | 9.296 |\n",
            "| Plastic bag + wrapper | 3.261 | Pop tab    | 7.208 | Straw      | 0.022 |\n",
            "| Cigarette             | 0.695 |            |       |            |       |\n",
            "OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0, 'AP-Can': 0.0, 'AP-Other': 0.0, 'AP-Bottle': 0.0, 'AP-Bottle cap': 0.0, 'AP-Cup': 0.0, 'AP-Lid': 0.0, 'AP-Plastic bag + wrapper': 0.0, 'AP-Pop tab': 0.0, 'AP-Straw': 0.0, 'AP-Cigarette': 0.0}), ('segm', {'AP': 4.099376593083673, 'AP50': 6.366129251588916, 'AP75': 4.078660001734661, 'APs': 0.48355762879289854, 'APm': 2.524411622095869, 'APl': 4.8151857869330374, 'AP-Can': 7.562215957614642, 'AP-Other': 5.293779761726243, 'AP-Bottle': 2.4032018171759595, 'AP-Bottle cap': 4.3686365791704205, 'AP-Cup': 0.8837358233138821, 'AP-Lid': 9.296454341541246, 'AP-Plastic bag + wrapper': 3.2614274822556064, 'AP-Pop tab': 7.207920792079207, 'AP-Straw': 0.021588201725315982, 'AP-Cigarette': 0.6948051742342132})])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}