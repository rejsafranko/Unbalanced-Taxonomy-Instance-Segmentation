{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jHOC2HC0sCNE"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iUmK767kWQlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d07a215-19e5-4997-b19c-911145cb7b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mask2Former' already exists and is not an empty directory.\n",
            "/content/Mask2Former\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-mix3wrg3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-mix3wrg3\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (9.4.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.11.0)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.23.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2024.8.24)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (4.66.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm->-r requirements.txt (line 4)) (1.3.0)\n",
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n",
            "running build\n",
            "running build_py\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "/content/Mask2Former\n"
          ]
        }
      ],
      "source": [
        "# clone and install Mask2Former\n",
        "!git clone https://github.com/facebookresearch/Mask2Former.git\n",
        "%cd Mask2Former\n",
        "!pip install -U opencv-python\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!pip install -r requirements.txt\n",
        "%cd mask2former/modeling/pixel_decoder/ops\n",
        "!python setup.py build install\n",
        "%cd ../../../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xGxctvB_Pdkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd074ce7-bb5b-4dfb-99cd-d036f434d468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-78yti_sk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-78yti_sk\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 5b72c27ae39f99db75d43f18fd1312e1ea934e60\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ccGvME_lQkbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb330aec-5b25-438f-a608-bbcbaf2f8b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n"
          ]
        }
      ],
      "source": [
        "%cd mask2former/modeling/pixel_decoder/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xP44DZ_zQooA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c2886b-d7cc-43ac-903b-55dfa803f378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build\n",
            "running build_py\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ],
      "source": [
        "!sh make.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fy0WqfqBQsqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd96abe-b21f-4d54-ecf4-9d20ec0e471b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former\n"
          ]
        }
      ],
      "source": [
        "%cd ../../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# import Mask2Former project\n",
        "from mask2former import add_maskformer2_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJwaL06RNCY",
        "outputId": "28a832df-fe6d-49be-89ef-ba2ade11645b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aQ-zjQCtRU1F"
      },
      "outputs": [],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "setup_logger(name=\"mask2former\")\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.projects.deeplab import add_deeplab_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wg5JCtyWRZOo"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7EO4nMI9Rb5P"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco10_train\", {}, data_dir_path + \"mapped_annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_val\", {}, data_dir_path + \"mapped_annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_test\", {}, data_dir_path + \"mapped_annotations_0_test.json\", data_dir_path + \"images/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y44xCY4Gu_ou"
      },
      "source": [
        "# Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IZL4O9aw3XXp"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.structures import PolygonMasks\n",
        "import copy\n",
        "import torch  # Import torch to convert images to tensors\n",
        "from argparse import ArgumentParser\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from mask2former import (\n",
        "    MaskFormerInstanceDatasetMapper,\n",
        "    InstanceSegEvaluator,\n",
        "    add_maskformer2_config,\n",
        ")\n",
        "from detectron2.evaluation import COCOEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6kaK-F3shWqR"
      },
      "outputs": [],
      "source": [
        "class Trainer(DefaultTrainer):\n",
        "    \"\"\"\n",
        "    Extension of the Trainer class adapted to MaskFormer.\n",
        "    \"\"\"\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
        "        return build_detection_train_loader(cfg, mapper=mapper)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "      if output_folder is None:\n",
        "              output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "      print(dataset_name)\n",
        "      return COCOEvaluator(dataset_name, output_dir=output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6FlsDz1hq_54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "add_deeplab_config(cfg)\n",
        "add_maskformer2_config(cfg)\n",
        "cfg.merge_from_file(\"configs/coco/instance-segmentation/maskformer2_R50_bs16_50ep.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4FdGn2TvPAn",
        "outputId": "0f84c848-fb7b-409d-cf5f-e622a3b2b12b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/02 14:37:19 d2.engine.defaults]: Model:\n",
            "MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "[09/02 14:37:19 mask2former.data.dataset_mappers.mask_former_instance_dataset_mapper]: [MaskFormerInstanceDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[09/02 14:37:21 d2.data.datasets.coco]: Loading /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json takes 1.85 seconds.\n",
            "[09/02 14:37:21 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json\n",
            "[09/02 14:37:21 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[09/02 14:37:21 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 194          |   Other    | 1397         |   Bottle   | 344          |\n",
            "|  Bottle cap   | 226          |    Cup     | 150          |    Lid     | 63           |\n",
            "| Plastic bag.. | 697          |  Pop tab   | 75           |   Straw    | 108          |\n",
            "|   Cigarette   | 457          |            |              |            |              |\n",
            "|     total     | 3711         |            |              |            |              |\n",
            "[09/02 14:37:21 d2.data.build]: Using training sampler TrainingSampler\n",
            "[09/02 14:37:21 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/02 14:37:21 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[09/02 14:37:21 d2.data.common]: Serialized dataset takes 1.77 MiB\n",
            "[09/02 14:37:21 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[09/02 14:37:21 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_3c8ec9.pkl: 176MB [00:01, 103MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING [09/02 14:37:23 mask2former.modeling.transformer_decoder.mask2former_transformer_decoder]: Weight format of MultiScaleMaskedTransformerDecoder have changed! Please upgrade your models. Applying automatic conversion now ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (81, 256) in the checkpoint but (11, 256) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "criterion.empty_weight\n",
            "sem_seg_head.predictor.class_embed.{bias, weight}\n"
          ]
        }
      ],
      "source": [
        "cfg.DATASETS.TRAIN = (\"taco10_train\",)\n",
        "cfg.DATASETS.TEST = (\"taco10_val\",)\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "cfg.INPUT.DATASET_MAPPER_NAME = \"mask_former_instance\"\n",
        "\n",
        "cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 10\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"value\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.0001\n",
        "cfg.SOLVER.MAX_ITER = 60000\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "cfg.TEST.EVAL_PERIOD = 6000\n",
        "cfg.freeze()\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAuliSkJQer3",
        "outputId": "8ac4e1fe-756e-44c8-b2b9-287bb6e07817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:42:08 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 0.9723 s/iter. Eval: 4.6391 s/iter. Total: 5.6144 s/iter. ETA=0:03:10\n",
            "[09/02 20:42:09 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:42:30 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 1.1141 s/iter. Eval: 4.6421 s/iter. Total: 5.7593 s/iter. ETA=0:03:10\n",
            "[09/02 20:42:39 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 1.1013 s/iter. Eval: 4.6315 s/iter. Total: 5.7359 s/iter. ETA=0:02:57\n",
            "[09/02 20:42:47 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.0841 s/iter. Eval: 4.6208 s/iter. Total: 5.7080 s/iter. ETA=0:02:45\n",
            "[09/02 20:42:55 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.0676 s/iter. Eval: 4.6102 s/iter. Total: 5.6809 s/iter. ETA=0:02:33\n",
            "[09/02 20:43:03 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.0516 s/iter. Eval: 4.6001 s/iter. Total: 5.6548 s/iter. ETA=0:02:21\n",
            "[09/02 20:43:12 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.0361 s/iter. Eval: 4.5904 s/iter. Total: 5.6296 s/iter. ETA=0:02:09\n",
            "[09/02 20:43:17 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.0211 s/iter. Eval: 4.5612 s/iter. Total: 5.5853 s/iter. ETA=0:01:57\n",
            "[09/02 20:43:27 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.0068 s/iter. Eval: 4.5615 s/iter. Total: 5.5714 s/iter. ETA=0:01:45\n",
            "[09/02 20:43:36 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 0.9928 s/iter. Eval: 4.5575 s/iter. Total: 5.5534 s/iter. ETA=0:01:34\n",
            "[09/02 20:43:44 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 0.9792 s/iter. Eval: 4.5493 s/iter. Total: 5.5316 s/iter. ETA=0:01:22\n",
            "[09/02 20:43:52 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 0.9660 s/iter. Eval: 4.5409 s/iter. Total: 5.5100 s/iter. ETA=0:01:11\n",
            "[09/02 20:44:00 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 0.9532 s/iter. Eval: 4.5329 s/iter. Total: 5.4891 s/iter. ETA=0:01:00\n",
            "[09/02 20:44:08 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 0.9407 s/iter. Eval: 4.5251 s/iter. Total: 5.4689 s/iter. ETA=0:00:49\n",
            "[09/02 20:44:17 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 0.9287 s/iter. Eval: 4.5176 s/iter. Total: 5.4493 s/iter. ETA=0:00:38\n",
            "[09/02 20:44:24 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0022 s/iter. Inference: 0.9169 s/iter. Eval: 4.5053 s/iter. Total: 5.4252 s/iter. ETA=0:00:27\n",
            "[09/02 20:44:32 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0022 s/iter. Inference: 0.9054 s/iter. Eval: 4.4930 s/iter. Total: 5.4015 s/iter. ETA=0:00:16\n",
            "[09/02 20:44:39 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0022 s/iter. Inference: 0.8942 s/iter. Eval: 4.4815 s/iter. Total: 5.3787 s/iter. ETA=0:00:05\n",
            "[09/02 20:44:43 d2.evaluation.evaluator]: Total inference time: 0:12:58.407534 (5.368328 s / iter per device, on 1 devices)\n",
            "[09/02 20:44:43 d2.evaluation.evaluator]: Total inference pure compute time: 0:02:08 (0.888757 s / iter per device, on 1 devices)\n",
            "[09/02 20:44:43 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/02 20:44:43 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[09/02 20:44:44 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 20:44:44 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/02 20:44:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/02 20:44:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 20:44:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/02 20:44:44 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/02 20:44:44 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.44s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 20:44:45 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/02 20:44:45 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.38 seconds.\n",
            "[09/02 20:44:45 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 20:44:45 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.039\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.059\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.145\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.231\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.156\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.291\n",
            "[09/02 20:44:45 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 3.860 | 6.040  | 4.047  | 0.117 | 2.074 | 5.867 |\n",
            "[09/02 20:44:45 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP    | category   | AP    |\n",
            "|:----------------------|:-------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 2.901  | Other      | 6.125 | Bottle     | 7.316 |\n",
            "| Bottle cap            | 6.270  | Cup        | 3.205 | Lid        | 0.054 |\n",
            "| Plastic bag + wrapper | 11.020 | Pop tab    | 0.000 | Straw      | 1.205 |\n",
            "| Cigarette             | 0.499  |            |       |            |       |\n",
            "[09/02 20:44:45 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[09/02 20:44:45 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/02 20:44:45 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 20:44:45 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/02 20:44:45 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/02 20:44:45 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 20:44:45 d2.evaluation.testing]: copypaste: 3.8596,6.0400,4.0468,0.1174,2.0740,5.8674\n",
            "[09/02 20:44:45 d2.utils.events]:  eta: 1:00:52  iter: 47999  total_loss: 25.87  loss_ce: 1.154  loss_mask: 0.1391  loss_dice: 1.152  loss_ce_0: 1.445  loss_mask_0: 0.1402  loss_dice_0: 1.201  loss_ce_1: 1.259  loss_mask_1: 0.1464  loss_dice_1: 1.34  loss_ce_2: 1.207  loss_mask_2: 0.1505  loss_dice_2: 1.287  loss_ce_3: 1.029  loss_mask_3: 0.1752  loss_dice_3: 1.329  loss_ce_4: 1.14  loss_mask_4: 0.1459  loss_dice_4: 1.23  loss_ce_5: 1.071  loss_mask_5: 0.1406  loss_dice_5: 1.482  loss_ce_6: 1.219  loss_mask_6: 0.1664  loss_dice_6: 1.336  loss_ce_7: 1.048  loss_mask_7: 0.1809  loss_dice_7: 1.185  loss_ce_8: 1.137  loss_mask_8: 0.148  loss_dice_8: 1.308    time: 0.3228  last_time: 0.3008  data_time: 0.0074  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:44:52 d2.utils.events]:  eta: 1:00:44  iter: 48019  total_loss: 21.2  loss_ce: 0.9668  loss_mask: 0.07611  loss_dice: 1.025  loss_ce_0: 1.289  loss_mask_0: 0.06911  loss_dice_0: 0.9456  loss_ce_1: 1.164  loss_mask_1: 0.09563  loss_dice_1: 1.127  loss_ce_2: 1.037  loss_mask_2: 0.07833  loss_dice_2: 0.9886  loss_ce_3: 0.9468  loss_mask_3: 0.0883  loss_dice_3: 1.039  loss_ce_4: 0.9134  loss_mask_4: 0.09711  loss_dice_4: 1.093  loss_ce_5: 0.9287  loss_mask_5: 0.0777  loss_dice_5: 0.9348  loss_ce_6: 0.9544  loss_mask_6: 0.08564  loss_dice_6: 1.044  loss_ce_7: 0.972  loss_mask_7: 0.08301  loss_dice_7: 0.993  loss_ce_8: 0.9341  loss_mask_8: 0.08841  loss_dice_8: 1.097    time: 0.3228  last_time: 0.3029  data_time: 0.0072  last_data_time: 0.0090   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:44:58 d2.utils.events]:  eta: 1:00:37  iter: 48039  total_loss: 28.23  loss_ce: 1.097  loss_mask: 0.2458  loss_dice: 1.337  loss_ce_0: 1.197  loss_mask_0: 0.2205  loss_dice_0: 1.277  loss_ce_1: 1.221  loss_mask_1: 0.2187  loss_dice_1: 1.465  loss_ce_2: 1.161  loss_mask_2: 0.2622  loss_dice_2: 1.295  loss_ce_3: 1.098  loss_mask_3: 0.1889  loss_dice_3: 1.143  loss_ce_4: 1.14  loss_mask_4: 0.2014  loss_dice_4: 1.16  loss_ce_5: 1.124  loss_mask_5: 0.2064  loss_dice_5: 1.196  loss_ce_6: 1.165  loss_mask_6: 0.209  loss_dice_6: 1.175  loss_ce_7: 1.167  loss_mask_7: 0.2052  loss_dice_7: 1.299  loss_ce_8: 1.125  loss_mask_8: 0.2309  loss_dice_8: 1.521    time: 0.3228  last_time: 0.3237  data_time: 0.0118  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:04 d2.utils.events]:  eta: 1:00:34  iter: 48059  total_loss: 27.8  loss_ce: 0.9576  loss_mask: 0.1765  loss_dice: 1.542  loss_ce_0: 1.398  loss_mask_0: 0.2446  loss_dice_0: 1.613  loss_ce_1: 1.324  loss_mask_1: 0.2238  loss_dice_1: 1.723  loss_ce_2: 1.262  loss_mask_2: 0.2339  loss_dice_2: 1.454  loss_ce_3: 0.9793  loss_mask_3: 0.2229  loss_dice_3: 1.627  loss_ce_4: 0.9169  loss_mask_4: 0.2027  loss_dice_4: 1.693  loss_ce_5: 1.01  loss_mask_5: 0.2533  loss_dice_5: 1.676  loss_ce_6: 0.944  loss_mask_6: 0.1829  loss_dice_6: 1.57  loss_ce_7: 0.9426  loss_mask_7: 0.1676  loss_dice_7: 1.516  loss_ce_8: 0.9184  loss_mask_8: 0.1769  loss_dice_8: 1.495    time: 0.3228  last_time: 0.3280  data_time: 0.0069  last_data_time: 0.0123   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:10 d2.utils.events]:  eta: 1:00:27  iter: 48079  total_loss: 26.84  loss_ce: 1.138  loss_mask: 0.09402  loss_dice: 1.219  loss_ce_0: 1.388  loss_mask_0: 0.0784  loss_dice_0: 1.06  loss_ce_1: 1.411  loss_mask_1: 0.0959  loss_dice_1: 1.173  loss_ce_2: 1.468  loss_mask_2: 0.1027  loss_dice_2: 1.248  loss_ce_3: 1.258  loss_mask_3: 0.1479  loss_dice_3: 1.237  loss_ce_4: 1.288  loss_mask_4: 0.1368  loss_dice_4: 1.151  loss_ce_5: 1.239  loss_mask_5: 0.1503  loss_dice_5: 1.092  loss_ce_6: 1.136  loss_mask_6: 0.1444  loss_dice_6: 1.17  loss_ce_7: 1.196  loss_mask_7: 0.09933  loss_dice_7: 1.236  loss_ce_8: 1.145  loss_mask_8: 0.1325  loss_dice_8: 1.114    time: 0.3228  last_time: 0.2868  data_time: 0.0063  last_data_time: 0.0027   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:17 d2.utils.events]:  eta: 1:00:23  iter: 48099  total_loss: 33.67  loss_ce: 1.236  loss_mask: 0.1794  loss_dice: 1.574  loss_ce_0: 1.431  loss_mask_0: 0.1718  loss_dice_0: 1.52  loss_ce_1: 1.416  loss_mask_1: 0.1527  loss_dice_1: 1.713  loss_ce_2: 1.342  loss_mask_2: 0.1541  loss_dice_2: 1.705  loss_ce_3: 1.26  loss_mask_3: 0.1621  loss_dice_3: 1.545  loss_ce_4: 1.3  loss_mask_4: 0.1638  loss_dice_4: 1.634  loss_ce_5: 1.288  loss_mask_5: 0.1564  loss_dice_5: 1.508  loss_ce_6: 1.175  loss_mask_6: 0.1437  loss_dice_6: 1.456  loss_ce_7: 1.181  loss_mask_7: 0.1456  loss_dice_7: 1.679  loss_ce_8: 1.285  loss_mask_8: 0.1439  loss_dice_8: 1.351    time: 0.3228  last_time: 0.3036  data_time: 0.0075  last_data_time: 0.0066   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:23 d2.utils.events]:  eta: 1:00:17  iter: 48119  total_loss: 32.76  loss_ce: 1.364  loss_mask: 0.2074  loss_dice: 1.32  loss_ce_0: 1.322  loss_mask_0: 0.2693  loss_dice_0: 1.28  loss_ce_1: 1.284  loss_mask_1: 0.3391  loss_dice_1: 1.88  loss_ce_2: 1.373  loss_mask_2: 0.27  loss_dice_2: 1.423  loss_ce_3: 1.386  loss_mask_3: 0.2129  loss_dice_3: 1.389  loss_ce_4: 1.483  loss_mask_4: 0.2029  loss_dice_4: 1.421  loss_ce_5: 1.423  loss_mask_5: 0.2376  loss_dice_5: 1.587  loss_ce_6: 1.21  loss_mask_6: 0.2094  loss_dice_6: 1.316  loss_ce_7: 1.253  loss_mask_7: 0.2102  loss_dice_7: 1.346  loss_ce_8: 1.349  loss_mask_8: 0.2075  loss_dice_8: 1.143    time: 0.3228  last_time: 0.2905  data_time: 0.0068  last_data_time: 0.0039   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:29 d2.utils.events]:  eta: 1:00:14  iter: 48139  total_loss: 30.63  loss_ce: 1.167  loss_mask: 0.1501  loss_dice: 1.718  loss_ce_0: 1.502  loss_mask_0: 0.2026  loss_dice_0: 1.638  loss_ce_1: 1.392  loss_mask_1: 0.1651  loss_dice_1: 1.849  loss_ce_2: 1.421  loss_mask_2: 0.1542  loss_dice_2: 1.808  loss_ce_3: 1.255  loss_mask_3: 0.1656  loss_dice_3: 1.654  loss_ce_4: 1.162  loss_mask_4: 0.1455  loss_dice_4: 1.544  loss_ce_5: 1.168  loss_mask_5: 0.1334  loss_dice_5: 1.47  loss_ce_6: 1.189  loss_mask_6: 0.1413  loss_dice_6: 1.605  loss_ce_7: 1.235  loss_mask_7: 0.1407  loss_dice_7: 1.631  loss_ce_8: 1.129  loss_mask_8: 0.1351  loss_dice_8: 1.583    time: 0.3228  last_time: 0.3273  data_time: 0.0072  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:35 d2.utils.events]:  eta: 1:00:08  iter: 48159  total_loss: 27.65  loss_ce: 1.115  loss_mask: 0.09327  loss_dice: 1.314  loss_ce_0: 1.225  loss_mask_0: 0.1793  loss_dice_0: 1.391  loss_ce_1: 1.269  loss_mask_1: 0.1219  loss_dice_1: 1.415  loss_ce_2: 1.246  loss_mask_2: 0.1374  loss_dice_2: 1.416  loss_ce_3: 1.098  loss_mask_3: 0.1137  loss_dice_3: 1.381  loss_ce_4: 0.9579  loss_mask_4: 0.1197  loss_dice_4: 1.468  loss_ce_5: 1.086  loss_mask_5: 0.12  loss_dice_5: 1.117  loss_ce_6: 1.204  loss_mask_6: 0.1057  loss_dice_6: 1.275  loss_ce_7: 0.9735  loss_mask_7: 0.1416  loss_dice_7: 1.42  loss_ce_8: 1.054  loss_mask_8: 0.1098  loss_dice_8: 1.325    time: 0.3228  last_time: 0.3097  data_time: 0.0071  last_data_time: 0.0118   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:42 d2.utils.events]:  eta: 0:59:59  iter: 48179  total_loss: 29.89  loss_ce: 0.9231  loss_mask: 0.2809  loss_dice: 1.307  loss_ce_0: 1.172  loss_mask_0: 0.2462  loss_dice_0: 1.416  loss_ce_1: 1.011  loss_mask_1: 0.377  loss_dice_1: 1.066  loss_ce_2: 0.9556  loss_mask_2: 0.3517  loss_dice_2: 1.348  loss_ce_3: 0.9329  loss_mask_3: 0.2617  loss_dice_3: 1.322  loss_ce_4: 0.9714  loss_mask_4: 0.2646  loss_dice_4: 1.187  loss_ce_5: 0.9325  loss_mask_5: 0.2841  loss_dice_5: 1.173  loss_ce_6: 0.9466  loss_mask_6: 0.2586  loss_dice_6: 1.324  loss_ce_7: 0.937  loss_mask_7: 0.2652  loss_dice_7: 1.162  loss_ce_8: 0.9466  loss_mask_8: 0.2602  loss_dice_8: 1.382    time: 0.3228  last_time: 0.3353  data_time: 0.0072  last_data_time: 0.0172   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:48 d2.utils.events]:  eta: 0:59:53  iter: 48199  total_loss: 28.92  loss_ce: 1.002  loss_mask: 0.1675  loss_dice: 1.635  loss_ce_0: 1.244  loss_mask_0: 0.3029  loss_dice_0: 1.266  loss_ce_1: 1.175  loss_mask_1: 0.1488  loss_dice_1: 1.613  loss_ce_2: 1.06  loss_mask_2: 0.1366  loss_dice_2: 1.527  loss_ce_3: 1.015  loss_mask_3: 0.1954  loss_dice_3: 1.571  loss_ce_4: 0.9258  loss_mask_4: 0.211  loss_dice_4: 1.883  loss_ce_5: 0.9473  loss_mask_5: 0.134  loss_dice_5: 1.617  loss_ce_6: 0.9828  loss_mask_6: 0.2038  loss_dice_6: 1.8  loss_ce_7: 0.9935  loss_mask_7: 0.1683  loss_dice_7: 1.614  loss_ce_8: 0.9644  loss_mask_8: 0.1847  loss_dice_8: 1.813    time: 0.3228  last_time: 0.3169  data_time: 0.0065  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:45:54 d2.utils.events]:  eta: 0:59:49  iter: 48219  total_loss: 27.78  loss_ce: 0.9828  loss_mask: 0.09187  loss_dice: 1.218  loss_ce_0: 1.323  loss_mask_0: 0.1176  loss_dice_0: 1.307  loss_ce_1: 1.264  loss_mask_1: 0.08984  loss_dice_1: 1.341  loss_ce_2: 1.043  loss_mask_2: 0.1245  loss_dice_2: 1.456  loss_ce_3: 1.031  loss_mask_3: 0.09303  loss_dice_3: 1.417  loss_ce_4: 1.12  loss_mask_4: 0.0918  loss_dice_4: 1.361  loss_ce_5: 0.9658  loss_mask_5: 0.1073  loss_dice_5: 1.25  loss_ce_6: 0.9073  loss_mask_6: 0.09854  loss_dice_6: 1.206  loss_ce_7: 1.017  loss_mask_7: 0.07291  loss_dice_7: 1.259  loss_ce_8: 0.9633  loss_mask_8: 0.0879  loss_dice_8: 1.378    time: 0.3228  last_time: 0.2989  data_time: 0.0070  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:00 d2.utils.events]:  eta: 0:59:43  iter: 48239  total_loss: 26.76  loss_ce: 1.004  loss_mask: 0.1176  loss_dice: 1.443  loss_ce_0: 1.102  loss_mask_0: 0.1286  loss_dice_0: 1.467  loss_ce_1: 0.9512  loss_mask_1: 0.1314  loss_dice_1: 1.463  loss_ce_2: 0.9967  loss_mask_2: 0.12  loss_dice_2: 1.389  loss_ce_3: 1.019  loss_mask_3: 0.1255  loss_dice_3: 1.233  loss_ce_4: 1.043  loss_mask_4: 0.1275  loss_dice_4: 1.375  loss_ce_5: 1.039  loss_mask_5: 0.1211  loss_dice_5: 1.376  loss_ce_6: 1.017  loss_mask_6: 0.1557  loss_dice_6: 1.354  loss_ce_7: 0.9325  loss_mask_7: 0.1131  loss_dice_7: 1.639  loss_ce_8: 0.8263  loss_mask_8: 0.1228  loss_dice_8: 1.348    time: 0.3228  last_time: 0.2962  data_time: 0.0080  last_data_time: 0.0072   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:07 d2.utils.events]:  eta: 0:59:38  iter: 48259  total_loss: 18.55  loss_ce: 0.6651  loss_mask: 0.1798  loss_dice: 0.8183  loss_ce_0: 0.7818  loss_mask_0: 0.2053  loss_dice_0: 0.7231  loss_ce_1: 0.7106  loss_mask_1: 0.1751  loss_dice_1: 0.8653  loss_ce_2: 0.7208  loss_mask_2: 0.1346  loss_dice_2: 0.6735  loss_ce_3: 0.6809  loss_mask_3: 0.1418  loss_dice_3: 0.8569  loss_ce_4: 0.7156  loss_mask_4: 0.1295  loss_dice_4: 0.6908  loss_ce_5: 0.675  loss_mask_5: 0.1636  loss_dice_5: 1.055  loss_ce_6: 0.6436  loss_mask_6: 0.1668  loss_dice_6: 0.9361  loss_ce_7: 0.6298  loss_mask_7: 0.1702  loss_dice_7: 0.9639  loss_ce_8: 0.6902  loss_mask_8: 0.1499  loss_dice_8: 0.9421    time: 0.3228  last_time: 0.2922  data_time: 0.0096  last_data_time: 0.0021   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:13 d2.utils.events]:  eta: 0:59:31  iter: 48279  total_loss: 26.51  loss_ce: 0.9833  loss_mask: 0.1526  loss_dice: 1.183  loss_ce_0: 1.366  loss_mask_0: 0.1029  loss_dice_0: 1.323  loss_ce_1: 1.534  loss_mask_1: 0.1135  loss_dice_1: 1.123  loss_ce_2: 1.293  loss_mask_2: 0.09223  loss_dice_2: 1.167  loss_ce_3: 1.084  loss_mask_3: 0.1491  loss_dice_3: 1.131  loss_ce_4: 1.03  loss_mask_4: 0.1061  loss_dice_4: 1.029  loss_ce_5: 1.04  loss_mask_5: 0.1188  loss_dice_5: 1.192  loss_ce_6: 1.028  loss_mask_6: 0.1525  loss_dice_6: 1.025  loss_ce_7: 0.9862  loss_mask_7: 0.1378  loss_dice_7: 1.003  loss_ce_8: 0.9896  loss_mask_8: 0.1504  loss_dice_8: 0.9924    time: 0.3228  last_time: 0.2934  data_time: 0.0114  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:19 d2.utils.events]:  eta: 0:59:25  iter: 48299  total_loss: 24.9  loss_ce: 1.064  loss_mask: 0.09094  loss_dice: 1.161  loss_ce_0: 1.305  loss_mask_0: 0.1471  loss_dice_0: 1.397  loss_ce_1: 1.31  loss_mask_1: 0.1108  loss_dice_1: 1.191  loss_ce_2: 1.249  loss_mask_2: 0.1473  loss_dice_2: 1.112  loss_ce_3: 0.9961  loss_mask_3: 0.1672  loss_dice_3: 1  loss_ce_4: 1.051  loss_mask_4: 0.09249  loss_dice_4: 1.037  loss_ce_5: 1.036  loss_mask_5: 0.0976  loss_dice_5: 1.263  loss_ce_6: 0.9723  loss_mask_6: 0.1083  loss_dice_6: 1.191  loss_ce_7: 1.039  loss_mask_7: 0.09258  loss_dice_7: 1.123  loss_ce_8: 1.058  loss_mask_8: 0.0999  loss_dice_8: 1.089    time: 0.3227  last_time: 0.2907  data_time: 0.0067  last_data_time: 0.0020   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:25 d2.utils.events]:  eta: 0:59:20  iter: 48319  total_loss: 34.57  loss_ce: 1.551  loss_mask: 0.1213  loss_dice: 1.74  loss_ce_0: 1.835  loss_mask_0: 0.1266  loss_dice_0: 1.621  loss_ce_1: 1.587  loss_mask_1: 0.1157  loss_dice_1: 1.901  loss_ce_2: 1.699  loss_mask_2: 0.12  loss_dice_2: 1.679  loss_ce_3: 1.56  loss_mask_3: 0.1187  loss_dice_3: 1.619  loss_ce_4: 1.649  loss_mask_4: 0.1365  loss_dice_4: 1.602  loss_ce_5: 1.677  loss_mask_5: 0.1226  loss_dice_5: 1.806  loss_ce_6: 1.506  loss_mask_6: 0.1276  loss_dice_6: 1.612  loss_ce_7: 1.394  loss_mask_7: 0.1218  loss_dice_7: 1.667  loss_ce_8: 1.64  loss_mask_8: 0.1274  loss_dice_8: 1.739    time: 0.3227  last_time: 0.2953  data_time: 0.0080  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:31 d2.utils.events]:  eta: 0:59:14  iter: 48339  total_loss: 27.07  loss_ce: 1.054  loss_mask: 0.1348  loss_dice: 1.514  loss_ce_0: 1.438  loss_mask_0: 0.1559  loss_dice_0: 1.315  loss_ce_1: 1.379  loss_mask_1: 0.1312  loss_dice_1: 1.486  loss_ce_2: 1.304  loss_mask_2: 0.1597  loss_dice_2: 1.534  loss_ce_3: 1.14  loss_mask_3: 0.1567  loss_dice_3: 1.389  loss_ce_4: 1.074  loss_mask_4: 0.1487  loss_dice_4: 1.487  loss_ce_5: 1.142  loss_mask_5: 0.1502  loss_dice_5: 1.6  loss_ce_6: 1.072  loss_mask_6: 0.1413  loss_dice_6: 1.646  loss_ce_7: 1.119  loss_mask_7: 0.1604  loss_dice_7: 1.478  loss_ce_8: 1.168  loss_mask_8: 0.1422  loss_dice_8: 1.522    time: 0.3227  last_time: 0.2958  data_time: 0.0074  last_data_time: 0.0030   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:38 d2.utils.events]:  eta: 0:59:09  iter: 48359  total_loss: 25.3  loss_ce: 0.9581  loss_mask: 0.1541  loss_dice: 1.476  loss_ce_0: 1.15  loss_mask_0: 0.1771  loss_dice_0: 1.648  loss_ce_1: 1.19  loss_mask_1: 0.1671  loss_dice_1: 1.701  loss_ce_2: 1.044  loss_mask_2: 0.1748  loss_dice_2: 1.603  loss_ce_3: 0.9747  loss_mask_3: 0.1845  loss_dice_3: 1.57  loss_ce_4: 0.8708  loss_mask_4: 0.1726  loss_dice_4: 1.554  loss_ce_5: 0.9453  loss_mask_5: 0.1561  loss_dice_5: 1.564  loss_ce_6: 0.8662  loss_mask_6: 0.1652  loss_dice_6: 1.562  loss_ce_7: 0.9345  loss_mask_7: 0.1544  loss_dice_7: 1.579  loss_ce_8: 0.9566  loss_mask_8: 0.1549  loss_dice_8: 1.553    time: 0.3227  last_time: 0.3087  data_time: 0.0110  last_data_time: 0.0146   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:44 d2.utils.events]:  eta: 0:59:08  iter: 48379  total_loss: 29.29  loss_ce: 1.15  loss_mask: 0.2503  loss_dice: 1.273  loss_ce_0: 1.479  loss_mask_0: 0.3071  loss_dice_0: 1.406  loss_ce_1: 1.361  loss_mask_1: 0.2809  loss_dice_1: 1.291  loss_ce_2: 1.24  loss_mask_2: 0.2636  loss_dice_2: 1.517  loss_ce_3: 1.133  loss_mask_3: 0.2492  loss_dice_3: 1.255  loss_ce_4: 1.169  loss_mask_4: 0.2375  loss_dice_4: 1.32  loss_ce_5: 1.224  loss_mask_5: 0.2322  loss_dice_5: 1.531  loss_ce_6: 1.139  loss_mask_6: 0.2715  loss_dice_6: 1.252  loss_ce_7: 1.149  loss_mask_7: 0.1967  loss_dice_7: 1.254  loss_ce_8: 1.156  loss_mask_8: 0.2426  loss_dice_8: 1.475    time: 0.3227  last_time: 0.3654  data_time: 0.0124  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:50 d2.utils.events]:  eta: 0:59:04  iter: 48399  total_loss: 38.02  loss_ce: 1.338  loss_mask: 0.2021  loss_dice: 1.946  loss_ce_0: 1.655  loss_mask_0: 0.182  loss_dice_0: 2.041  loss_ce_1: 1.642  loss_mask_1: 0.1645  loss_dice_1: 1.888  loss_ce_2: 1.549  loss_mask_2: 0.1912  loss_dice_2: 2.05  loss_ce_3: 1.269  loss_mask_3: 0.1903  loss_dice_3: 1.799  loss_ce_4: 1.307  loss_mask_4: 0.1931  loss_dice_4: 1.903  loss_ce_5: 1.297  loss_mask_5: 0.2102  loss_dice_5: 1.692  loss_ce_6: 1.329  loss_mask_6: 0.187  loss_dice_6: 2.081  loss_ce_7: 1.269  loss_mask_7: 0.202  loss_dice_7: 1.968  loss_ce_8: 1.309  loss_mask_8: 0.201  loss_dice_8: 1.984    time: 0.3227  last_time: 0.3044  data_time: 0.0071  last_data_time: 0.0080   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:46:57 d2.utils.events]:  eta: 0:58:55  iter: 48419  total_loss: 22.22  loss_ce: 1.049  loss_mask: 0.09515  loss_dice: 1.068  loss_ce_0: 1.261  loss_mask_0: 0.1014  loss_dice_0: 1.044  loss_ce_1: 1.244  loss_mask_1: 0.1118  loss_dice_1: 1.257  loss_ce_2: 1.045  loss_mask_2: 0.131  loss_dice_2: 1.37  loss_ce_3: 1.017  loss_mask_3: 0.1091  loss_dice_3: 1.133  loss_ce_4: 0.9974  loss_mask_4: 0.09214  loss_dice_4: 1.32  loss_ce_5: 0.9956  loss_mask_5: 0.08476  loss_dice_5: 1.294  loss_ce_6: 0.9941  loss_mask_6: 0.09823  loss_dice_6: 0.9042  loss_ce_7: 1.06  loss_mask_7: 0.09426  loss_dice_7: 0.843  loss_ce_8: 0.9905  loss_mask_8: 0.08998  loss_dice_8: 1.241    time: 0.3227  last_time: 0.2957  data_time: 0.0078  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:03 d2.utils.events]:  eta: 0:58:48  iter: 48439  total_loss: 19.77  loss_ce: 0.7983  loss_mask: 0.1456  loss_dice: 0.7351  loss_ce_0: 1.127  loss_mask_0: 0.1444  loss_dice_0: 0.6757  loss_ce_1: 1.077  loss_mask_1: 0.1426  loss_dice_1: 0.8454  loss_ce_2: 0.8703  loss_mask_2: 0.1394  loss_dice_2: 0.6341  loss_ce_3: 0.7458  loss_mask_3: 0.1304  loss_dice_3: 0.7562  loss_ce_4: 0.8357  loss_mask_4: 0.1275  loss_dice_4: 0.7038  loss_ce_5: 0.7875  loss_mask_5: 0.1415  loss_dice_5: 0.7495  loss_ce_6: 0.7243  loss_mask_6: 0.1472  loss_dice_6: 0.8149  loss_ce_7: 0.7533  loss_mask_7: 0.1566  loss_dice_7: 0.7553  loss_ce_8: 0.7467  loss_mask_8: 0.1464  loss_dice_8: 0.6478    time: 0.3227  last_time: 0.2951  data_time: 0.0175  last_data_time: 0.0020   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:09 d2.utils.events]:  eta: 0:58:44  iter: 48459  total_loss: 30.45  loss_ce: 0.9692  loss_mask: 0.1153  loss_dice: 1.34  loss_ce_0: 1.475  loss_mask_0: 0.1533  loss_dice_0: 1.161  loss_ce_1: 1.425  loss_mask_1: 0.1315  loss_dice_1: 1.343  loss_ce_2: 1.129  loss_mask_2: 0.1582  loss_dice_2: 1.629  loss_ce_3: 0.9707  loss_mask_3: 0.1549  loss_dice_3: 1.31  loss_ce_4: 0.9837  loss_mask_4: 0.1555  loss_dice_4: 1.294  loss_ce_5: 1.084  loss_mask_5: 0.1314  loss_dice_5: 1.351  loss_ce_6: 1.022  loss_mask_6: 0.1321  loss_dice_6: 1.272  loss_ce_7: 0.9869  loss_mask_7: 0.1431  loss_dice_7: 1.298  loss_ce_8: 1.031  loss_mask_8: 0.1403  loss_dice_8: 1.154    time: 0.3227  last_time: 0.3211  data_time: 0.0108  last_data_time: 0.0046   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:16 d2.utils.events]:  eta: 0:58:37  iter: 48479  total_loss: 23.88  loss_ce: 0.8802  loss_mask: 0.09339  loss_dice: 1.215  loss_ce_0: 1.465  loss_mask_0: 0.1806  loss_dice_0: 1.078  loss_ce_1: 1.55  loss_mask_1: 0.1093  loss_dice_1: 1.088  loss_ce_2: 1.147  loss_mask_2: 0.1248  loss_dice_2: 1.129  loss_ce_3: 0.9959  loss_mask_3: 0.1058  loss_dice_3: 1.201  loss_ce_4: 0.9489  loss_mask_4: 0.1062  loss_dice_4: 1.105  loss_ce_5: 0.8824  loss_mask_5: 0.1072  loss_dice_5: 1.256  loss_ce_6: 0.9625  loss_mask_6: 0.09018  loss_dice_6: 1.292  loss_ce_7: 0.9073  loss_mask_7: 0.1052  loss_dice_7: 1.281  loss_ce_8: 0.9547  loss_mask_8: 0.09255  loss_dice_8: 1.211    time: 0.3227  last_time: 0.2990  data_time: 0.0138  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:22 d2.utils.events]:  eta: 0:58:35  iter: 48499  total_loss: 20.3  loss_ce: 0.9107  loss_mask: 0.09556  loss_dice: 0.7113  loss_ce_0: 1.275  loss_mask_0: 0.1425  loss_dice_0: 0.6989  loss_ce_1: 1.123  loss_mask_1: 0.1361  loss_dice_1: 0.7693  loss_ce_2: 0.9714  loss_mask_2: 0.139  loss_dice_2: 0.7836  loss_ce_3: 0.8529  loss_mask_3: 0.1021  loss_dice_3: 0.6706  loss_ce_4: 0.8968  loss_mask_4: 0.1149  loss_dice_4: 0.7127  loss_ce_5: 0.8339  loss_mask_5: 0.1037  loss_dice_5: 0.7959  loss_ce_6: 0.8614  loss_mask_6: 0.1088  loss_dice_6: 0.7327  loss_ce_7: 0.8458  loss_mask_7: 0.1062  loss_dice_7: 0.724  loss_ce_8: 0.8188  loss_mask_8: 0.1003  loss_dice_8: 0.7537    time: 0.3227  last_time: 0.3104  data_time: 0.0069  last_data_time: 0.0156   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:28 d2.utils.events]:  eta: 0:58:27  iter: 48519  total_loss: 25.48  loss_ce: 1.027  loss_mask: 0.1675  loss_dice: 1.176  loss_ce_0: 1.27  loss_mask_0: 0.1939  loss_dice_0: 1.19  loss_ce_1: 1.194  loss_mask_1: 0.1838  loss_dice_1: 1.249  loss_ce_2: 1.364  loss_mask_2: 0.1585  loss_dice_2: 1.174  loss_ce_3: 1.111  loss_mask_3: 0.1436  loss_dice_3: 1.239  loss_ce_4: 1.113  loss_mask_4: 0.1524  loss_dice_4: 1.222  loss_ce_5: 1.106  loss_mask_5: 0.1502  loss_dice_5: 1.242  loss_ce_6: 1.028  loss_mask_6: 0.1525  loss_dice_6: 1.342  loss_ce_7: 1.046  loss_mask_7: 0.1774  loss_dice_7: 1.396  loss_ce_8: 1.044  loss_mask_8: 0.1456  loss_dice_8: 1.161    time: 0.3227  last_time: 0.3166  data_time: 0.0071  last_data_time: 0.0136   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:35 d2.utils.events]:  eta: 0:58:23  iter: 48539  total_loss: 21.83  loss_ce: 1.02  loss_mask: 0.08813  loss_dice: 0.766  loss_ce_0: 1.126  loss_mask_0: 0.1023  loss_dice_0: 1.078  loss_ce_1: 0.9602  loss_mask_1: 0.08541  loss_dice_1: 0.9524  loss_ce_2: 0.9483  loss_mask_2: 0.09354  loss_dice_2: 1.172  loss_ce_3: 1.085  loss_mask_3: 0.08498  loss_dice_3: 0.81  loss_ce_4: 1.024  loss_mask_4: 0.08717  loss_dice_4: 0.9084  loss_ce_5: 1.028  loss_mask_5: 0.09678  loss_dice_5: 0.9286  loss_ce_6: 1.048  loss_mask_6: 0.1016  loss_dice_6: 0.9741  loss_ce_7: 0.9677  loss_mask_7: 0.08032  loss_dice_7: 0.8675  loss_ce_8: 0.9587  loss_mask_8: 0.0889  loss_dice_8: 0.8741    time: 0.3227  last_time: 0.2975  data_time: 0.0098  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:41 d2.utils.events]:  eta: 0:58:17  iter: 48559  total_loss: 38.45  loss_ce: 1.524  loss_mask: 0.1786  loss_dice: 1.939  loss_ce_0: 1.643  loss_mask_0: 0.2214  loss_dice_0: 1.881  loss_ce_1: 1.8  loss_mask_1: 0.1816  loss_dice_1: 1.83  loss_ce_2: 1.752  loss_mask_2: 0.1746  loss_dice_2: 1.67  loss_ce_3: 1.583  loss_mask_3: 0.1688  loss_dice_3: 1.671  loss_ce_4: 1.628  loss_mask_4: 0.1705  loss_dice_4: 2.045  loss_ce_5: 1.563  loss_mask_5: 0.1644  loss_dice_5: 1.95  loss_ce_6: 1.494  loss_mask_6: 0.1649  loss_dice_6: 1.975  loss_ce_7: 1.362  loss_mask_7: 0.1645  loss_dice_7: 1.81  loss_ce_8: 1.506  loss_mask_8: 0.1727  loss_dice_8: 1.911    time: 0.3227  last_time: 0.3345  data_time: 0.0071  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:47 d2.utils.events]:  eta: 0:58:11  iter: 48579  total_loss: 28.21  loss_ce: 1.18  loss_mask: 0.11  loss_dice: 1.144  loss_ce_0: 1.417  loss_mask_0: 0.1883  loss_dice_0: 1.335  loss_ce_1: 1.446  loss_mask_1: 0.1133  loss_dice_1: 1.416  loss_ce_2: 1.435  loss_mask_2: 0.09964  loss_dice_2: 1.358  loss_ce_3: 1.21  loss_mask_3: 0.09529  loss_dice_3: 1.245  loss_ce_4: 1.221  loss_mask_4: 0.09912  loss_dice_4: 1.263  loss_ce_5: 1.279  loss_mask_5: 0.1016  loss_dice_5: 1.234  loss_ce_6: 1.305  loss_mask_6: 0.1207  loss_dice_6: 1.385  loss_ce_7: 1.291  loss_mask_7: 0.1024  loss_dice_7: 1.207  loss_ce_8: 1.213  loss_mask_8: 0.09993  loss_dice_8: 1.43    time: 0.3227  last_time: 0.3208  data_time: 0.0098  last_data_time: 0.0040   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:53 d2.utils.events]:  eta: 0:58:02  iter: 48599  total_loss: 28.91  loss_ce: 0.9692  loss_mask: 0.1882  loss_dice: 1.466  loss_ce_0: 1.106  loss_mask_0: 0.2203  loss_dice_0: 1.483  loss_ce_1: 1.128  loss_mask_1: 0.1339  loss_dice_1: 1.532  loss_ce_2: 1.071  loss_mask_2: 0.1752  loss_dice_2: 1.641  loss_ce_3: 0.9298  loss_mask_3: 0.1778  loss_dice_3: 1.531  loss_ce_4: 0.9281  loss_mask_4: 0.1889  loss_dice_4: 1.551  loss_ce_5: 0.9161  loss_mask_5: 0.1987  loss_dice_5: 1.433  loss_ce_6: 0.9399  loss_mask_6: 0.1747  loss_dice_6: 1.385  loss_ce_7: 0.9421  loss_mask_7: 0.1902  loss_dice_7: 1.447  loss_ce_8: 1.026  loss_mask_8: 0.1522  loss_dice_8: 1.41    time: 0.3227  last_time: 0.3024  data_time: 0.0061  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:47:59 d2.utils.events]:  eta: 0:57:55  iter: 48619  total_loss: 26.31  loss_ce: 1.202  loss_mask: 0.0989  loss_dice: 0.8882  loss_ce_0: 1.33  loss_mask_0: 0.1104  loss_dice_0: 1.173  loss_ce_1: 1.225  loss_mask_1: 0.07406  loss_dice_1: 1.282  loss_ce_2: 1.063  loss_mask_2: 0.0798  loss_dice_2: 1.118  loss_ce_3: 1.24  loss_mask_3: 0.08758  loss_dice_3: 0.8612  loss_ce_4: 1.1  loss_mask_4: 0.09615  loss_dice_4: 1.006  loss_ce_5: 1.046  loss_mask_5: 0.102  loss_dice_5: 0.7954  loss_ce_6: 1.207  loss_mask_6: 0.1042  loss_dice_6: 0.9707  loss_ce_7: 1.229  loss_mask_7: 0.1033  loss_dice_7: 0.8036  loss_ce_8: 1.092  loss_mask_8: 0.09688  loss_dice_8: 0.8685    time: 0.3227  last_time: 0.2930  data_time: 0.0070  last_data_time: 0.0037   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:06 d2.utils.events]:  eta: 0:57:48  iter: 48639  total_loss: 29.03  loss_ce: 1.316  loss_mask: 0.1011  loss_dice: 1.289  loss_ce_0: 1.345  loss_mask_0: 0.1141  loss_dice_0: 1.283  loss_ce_1: 1.306  loss_mask_1: 0.1031  loss_dice_1: 1.299  loss_ce_2: 1.397  loss_mask_2: 0.1042  loss_dice_2: 1.278  loss_ce_3: 1.312  loss_mask_3: 0.09551  loss_dice_3: 1.299  loss_ce_4: 1.218  loss_mask_4: 0.1048  loss_dice_4: 1.441  loss_ce_5: 1.257  loss_mask_5: 0.1121  loss_dice_5: 1.41  loss_ce_6: 1.386  loss_mask_6: 0.105  loss_dice_6: 1.363  loss_ce_7: 1.332  loss_mask_7: 0.1101  loss_dice_7: 1.338  loss_ce_8: 1.28  loss_mask_8: 0.1171  loss_dice_8: 1.324    time: 0.3227  last_time: 0.2991  data_time: 0.0143  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:12 d2.utils.events]:  eta: 0:57:45  iter: 48659  total_loss: 19.16  loss_ce: 0.7029  loss_mask: 0.119  loss_dice: 0.8575  loss_ce_0: 0.92  loss_mask_0: 0.1353  loss_dice_0: 1.26  loss_ce_1: 0.9561  loss_mask_1: 0.1557  loss_dice_1: 1.211  loss_ce_2: 0.8643  loss_mask_2: 0.1416  loss_dice_2: 1.135  loss_ce_3: 0.732  loss_mask_3: 0.1167  loss_dice_3: 0.8805  loss_ce_4: 0.7633  loss_mask_4: 0.1474  loss_dice_4: 0.9503  loss_ce_5: 0.7631  loss_mask_5: 0.1373  loss_dice_5: 1.195  loss_ce_6: 0.7042  loss_mask_6: 0.1471  loss_dice_6: 1.068  loss_ce_7: 0.7116  loss_mask_7: 0.1286  loss_dice_7: 1.073  loss_ce_8: 0.7359  loss_mask_8: 0.1182  loss_dice_8: 1.116    time: 0.3227  last_time: 0.3269  data_time: 0.0185  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:19 d2.utils.events]:  eta: 0:57:36  iter: 48679  total_loss: 19.18  loss_ce: 0.9964  loss_mask: 0.09839  loss_dice: 0.9433  loss_ce_0: 1.308  loss_mask_0: 0.1193  loss_dice_0: 0.8777  loss_ce_1: 1.198  loss_mask_1: 0.1093  loss_dice_1: 0.9465  loss_ce_2: 0.9199  loss_mask_2: 0.1037  loss_dice_2: 0.9749  loss_ce_3: 0.9071  loss_mask_3: 0.09969  loss_dice_3: 0.8777  loss_ce_4: 1.04  loss_mask_4: 0.09833  loss_dice_4: 0.9845  loss_ce_5: 0.9388  loss_mask_5: 0.1031  loss_dice_5: 0.8446  loss_ce_6: 0.9357  loss_mask_6: 0.09044  loss_dice_6: 0.8229  loss_ce_7: 0.971  loss_mask_7: 0.1018  loss_dice_7: 0.8186  loss_ce_8: 0.865  loss_mask_8: 0.09852  loss_dice_8: 0.7638    time: 0.3227  last_time: 0.3215  data_time: 0.0137  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:25 d2.utils.events]:  eta: 0:57:29  iter: 48699  total_loss: 26.72  loss_ce: 1.22  loss_mask: 0.1233  loss_dice: 0.8673  loss_ce_0: 1.47  loss_mask_0: 0.1314  loss_dice_0: 1.055  loss_ce_1: 1.468  loss_mask_1: 0.129  loss_dice_1: 1.066  loss_ce_2: 1.139  loss_mask_2: 0.117  loss_dice_2: 0.9105  loss_ce_3: 1.255  loss_mask_3: 0.1422  loss_dice_3: 0.9239  loss_ce_4: 1.23  loss_mask_4: 0.1232  loss_dice_4: 0.8571  loss_ce_5: 1.205  loss_mask_5: 0.1327  loss_dice_5: 0.9515  loss_ce_6: 1.075  loss_mask_6: 0.155  loss_dice_6: 1.085  loss_ce_7: 1.191  loss_mask_7: 0.1297  loss_dice_7: 0.9821  loss_ce_8: 1.098  loss_mask_8: 0.1436  loss_dice_8: 0.9749    time: 0.3227  last_time: 0.3028  data_time: 0.0075  last_data_time: 0.0086   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:31 d2.utils.events]:  eta: 0:57:21  iter: 48719  total_loss: 22.82  loss_ce: 1.214  loss_mask: 0.08411  loss_dice: 1.211  loss_ce_0: 1.568  loss_mask_0: 0.07884  loss_dice_0: 1.082  loss_ce_1: 1.413  loss_mask_1: 0.0932  loss_dice_1: 0.8684  loss_ce_2: 1.213  loss_mask_2: 0.07019  loss_dice_2: 1.063  loss_ce_3: 1.167  loss_mask_3: 0.07821  loss_dice_3: 1.222  loss_ce_4: 1.07  loss_mask_4: 0.08623  loss_dice_4: 1.14  loss_ce_5: 1.122  loss_mask_5: 0.09195  loss_dice_5: 1.409  loss_ce_6: 1.158  loss_mask_6: 0.07836  loss_dice_6: 0.9874  loss_ce_7: 1.144  loss_mask_7: 0.07887  loss_dice_7: 1.126  loss_ce_8: 1.097  loss_mask_8: 0.076  loss_dice_8: 1.107    time: 0.3227  last_time: 0.3066  data_time: 0.0174  last_data_time: 0.0082   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:38 d2.utils.events]:  eta: 0:57:18  iter: 48739  total_loss: 25.16  loss_ce: 0.9267  loss_mask: 0.1019  loss_dice: 1.386  loss_ce_0: 1.211  loss_mask_0: 0.08384  loss_dice_0: 1.313  loss_ce_1: 1.207  loss_mask_1: 0.07397  loss_dice_1: 1.174  loss_ce_2: 1.076  loss_mask_2: 0.08481  loss_dice_2: 1.579  loss_ce_3: 0.9016  loss_mask_3: 0.1022  loss_dice_3: 1.45  loss_ce_4: 0.8665  loss_mask_4: 0.09245  loss_dice_4: 1.47  loss_ce_5: 0.8484  loss_mask_5: 0.103  loss_dice_5: 1.251  loss_ce_6: 0.8931  loss_mask_6: 0.1266  loss_dice_6: 1.466  loss_ce_7: 0.8669  loss_mask_7: 0.1262  loss_dice_7: 1.459  loss_ce_8: 0.9135  loss_mask_8: 0.1105  loss_dice_8: 1.38    time: 0.3227  last_time: 0.3004  data_time: 0.0076  last_data_time: 0.0107   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:44 d2.utils.events]:  eta: 0:57:13  iter: 48759  total_loss: 28.81  loss_ce: 0.9628  loss_mask: 0.1604  loss_dice: 1.039  loss_ce_0: 1.268  loss_mask_0: 0.2312  loss_dice_0: 1.033  loss_ce_1: 1.17  loss_mask_1: 0.1743  loss_dice_1: 0.9879  loss_ce_2: 1.091  loss_mask_2: 0.1705  loss_dice_2: 1.125  loss_ce_3: 0.9984  loss_mask_3: 0.1777  loss_dice_3: 1.109  loss_ce_4: 0.9889  loss_mask_4: 0.1589  loss_dice_4: 1.195  loss_ce_5: 0.9464  loss_mask_5: 0.1758  loss_dice_5: 1.273  loss_ce_6: 0.9666  loss_mask_6: 0.1702  loss_dice_6: 1.178  loss_ce_7: 0.9677  loss_mask_7: 0.1591  loss_dice_7: 0.9862  loss_ce_8: 0.9703  loss_mask_8: 0.1624  loss_dice_8: 1.004    time: 0.3227  last_time: 0.2970  data_time: 0.0064  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:50 d2.utils.events]:  eta: 0:57:08  iter: 48779  total_loss: 26.88  loss_ce: 1.016  loss_mask: 0.1833  loss_dice: 1.23  loss_ce_0: 1.25  loss_mask_0: 0.1884  loss_dice_0: 1.249  loss_ce_1: 1.245  loss_mask_1: 0.1995  loss_dice_1: 1.306  loss_ce_2: 1.16  loss_mask_2: 0.2023  loss_dice_2: 1.357  loss_ce_3: 1.062  loss_mask_3: 0.2229  loss_dice_3: 1.251  loss_ce_4: 0.9723  loss_mask_4: 0.1939  loss_dice_4: 1.262  loss_ce_5: 1.022  loss_mask_5: 0.192  loss_dice_5: 1.253  loss_ce_6: 1.039  loss_mask_6: 0.1805  loss_dice_6: 1.188  loss_ce_7: 1.031  loss_mask_7: 0.162  loss_dice_7: 1.228  loss_ce_8: 1.04  loss_mask_8: 0.1801  loss_dice_8: 1.261    time: 0.3227  last_time: 0.2904  data_time: 0.0088  last_data_time: 0.0024   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:48:56 d2.utils.events]:  eta: 0:56:59  iter: 48799  total_loss: 21.04  loss_ce: 0.9221  loss_mask: 0.09112  loss_dice: 1.088  loss_ce_0: 1.283  loss_mask_0: 0.1187  loss_dice_0: 1.241  loss_ce_1: 1.256  loss_mask_1: 0.1073  loss_dice_1: 1.205  loss_ce_2: 1.135  loss_mask_2: 0.1044  loss_dice_2: 1.191  loss_ce_3: 1.014  loss_mask_3: 0.1016  loss_dice_3: 1.292  loss_ce_4: 1.031  loss_mask_4: 0.09734  loss_dice_4: 1.202  loss_ce_5: 0.9155  loss_mask_5: 0.08453  loss_dice_5: 1.207  loss_ce_6: 0.8865  loss_mask_6: 0.1092  loss_dice_6: 1.045  loss_ce_7: 0.9784  loss_mask_7: 0.1026  loss_dice_7: 1.123  loss_ce_8: 0.9277  loss_mask_8: 0.1148  loss_dice_8: 1.197    time: 0.3227  last_time: 0.3212  data_time: 0.0069  last_data_time: 0.0029   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:02 d2.utils.events]:  eta: 0:56:50  iter: 48819  total_loss: 30.29  loss_ce: 1.107  loss_mask: 0.2319  loss_dice: 1.227  loss_ce_0: 1.332  loss_mask_0: 0.3472  loss_dice_0: 1.334  loss_ce_1: 1.34  loss_mask_1: 0.2012  loss_dice_1: 1.421  loss_ce_2: 1.264  loss_mask_2: 0.1801  loss_dice_2: 1.426  loss_ce_3: 1.165  loss_mask_3: 0.197  loss_dice_3: 1.443  loss_ce_4: 1.109  loss_mask_4: 0.2265  loss_dice_4: 1.427  loss_ce_5: 1.172  loss_mask_5: 0.2041  loss_dice_5: 1.299  loss_ce_6: 1.122  loss_mask_6: 0.2454  loss_dice_6: 1.433  loss_ce_7: 1.095  loss_mask_7: 0.2286  loss_dice_7: 1.397  loss_ce_8: 1.139  loss_mask_8: 0.2423  loss_dice_8: 1.518    time: 0.3226  last_time: 0.3046  data_time: 0.0128  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:09 d2.utils.events]:  eta: 0:56:45  iter: 48839  total_loss: 22.6  loss_ce: 0.9544  loss_mask: 0.0657  loss_dice: 1.098  loss_ce_0: 1.244  loss_mask_0: 0.08819  loss_dice_0: 1.166  loss_ce_1: 1.062  loss_mask_1: 0.07489  loss_dice_1: 1.138  loss_ce_2: 1.059  loss_mask_2: 0.07831  loss_dice_2: 1.025  loss_ce_3: 0.881  loss_mask_3: 0.08904  loss_dice_3: 0.9121  loss_ce_4: 0.8762  loss_mask_4: 0.07563  loss_dice_4: 0.9911  loss_ce_5: 0.882  loss_mask_5: 0.07624  loss_dice_5: 0.968  loss_ce_6: 0.8242  loss_mask_6: 0.09184  loss_dice_6: 1.136  loss_ce_7: 0.8199  loss_mask_7: 0.09031  loss_dice_7: 1.235  loss_ce_8: 0.8782  loss_mask_8: 0.07447  loss_dice_8: 1.04    time: 0.3226  last_time: 0.2998  data_time: 0.0068  last_data_time: 0.0048   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:15 d2.utils.events]:  eta: 0:56:41  iter: 48859  total_loss: 27.97  loss_ce: 1.229  loss_mask: 0.1772  loss_dice: 1.845  loss_ce_0: 1.458  loss_mask_0: 0.2169  loss_dice_0: 1.708  loss_ce_1: 1.54  loss_mask_1: 0.1935  loss_dice_1: 1.655  loss_ce_2: 1.286  loss_mask_2: 0.1863  loss_dice_2: 1.536  loss_ce_3: 1.106  loss_mask_3: 0.1854  loss_dice_3: 1.584  loss_ce_4: 1.075  loss_mask_4: 0.1768  loss_dice_4: 1.707  loss_ce_5: 1.188  loss_mask_5: 0.2013  loss_dice_5: 1.686  loss_ce_6: 1.099  loss_mask_6: 0.1907  loss_dice_6: 1.574  loss_ce_7: 1.09  loss_mask_7: 0.1927  loss_dice_7: 1.673  loss_ce_8: 1.163  loss_mask_8: 0.1917  loss_dice_8: 1.734    time: 0.3226  last_time: 0.3301  data_time: 0.0073  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:21 d2.utils.events]:  eta: 0:56:32  iter: 48879  total_loss: 24.6  loss_ce: 0.9925  loss_mask: 0.09326  loss_dice: 1.346  loss_ce_0: 1.257  loss_mask_0: 0.08444  loss_dice_0: 1.337  loss_ce_1: 1.287  loss_mask_1: 0.06706  loss_dice_1: 1.279  loss_ce_2: 1.17  loss_mask_2: 0.07883  loss_dice_2: 1.187  loss_ce_3: 0.9626  loss_mask_3: 0.07903  loss_dice_3: 1.354  loss_ce_4: 0.908  loss_mask_4: 0.09296  loss_dice_4: 1.329  loss_ce_5: 0.9676  loss_mask_5: 0.09553  loss_dice_5: 1.322  loss_ce_6: 0.9436  loss_mask_6: 0.07714  loss_dice_6: 1.237  loss_ce_7: 0.9129  loss_mask_7: 0.07565  loss_dice_7: 1.37  loss_ce_8: 0.9363  loss_mask_8: 0.06798  loss_dice_8: 1.552    time: 0.3226  last_time: 0.2975  data_time: 0.0064  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:28 d2.utils.events]:  eta: 0:56:28  iter: 48899  total_loss: 31.4  loss_ce: 1.178  loss_mask: 0.1008  loss_dice: 1.611  loss_ce_0: 1.81  loss_mask_0: 0.08538  loss_dice_0: 1.729  loss_ce_1: 1.523  loss_mask_1: 0.09267  loss_dice_1: 1.765  loss_ce_2: 1.63  loss_mask_2: 0.09027  loss_dice_2: 1.85  loss_ce_3: 1.277  loss_mask_3: 0.11  loss_dice_3: 1.651  loss_ce_4: 1.197  loss_mask_4: 0.09597  loss_dice_4: 1.553  loss_ce_5: 1.253  loss_mask_5: 0.09937  loss_dice_5: 1.637  loss_ce_6: 1.302  loss_mask_6: 0.09558  loss_dice_6: 1.612  loss_ce_7: 1.288  loss_mask_7: 0.09298  loss_dice_7: 1.538  loss_ce_8: 1.472  loss_mask_8: 0.08202  loss_dice_8: 1.564    time: 0.3226  last_time: 0.2996  data_time: 0.0174  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:34 d2.utils.events]:  eta: 0:56:18  iter: 48919  total_loss: 25.53  loss_ce: 0.9511  loss_mask: 0.1467  loss_dice: 1.305  loss_ce_0: 1.339  loss_mask_0: 0.2017  loss_dice_0: 1.054  loss_ce_1: 1.389  loss_mask_1: 0.1614  loss_dice_1: 1.503  loss_ce_2: 1.013  loss_mask_2: 0.1467  loss_dice_2: 1.111  loss_ce_3: 0.884  loss_mask_3: 0.1754  loss_dice_3: 1.253  loss_ce_4: 0.9368  loss_mask_4: 0.1849  loss_dice_4: 1.201  loss_ce_5: 0.8834  loss_mask_5: 0.1771  loss_dice_5: 1.492  loss_ce_6: 0.9372  loss_mask_6: 0.1377  loss_dice_6: 1.318  loss_ce_7: 0.9469  loss_mask_7: 0.1563  loss_dice_7: 1.268  loss_ce_8: 0.9481  loss_mask_8: 0.1371  loss_dice_8: 1.216    time: 0.3226  last_time: 0.2970  data_time: 0.0063  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:40 d2.utils.events]:  eta: 0:56:13  iter: 48939  total_loss: 30.75  loss_ce: 1.147  loss_mask: 0.1213  loss_dice: 1.562  loss_ce_0: 1.491  loss_mask_0: 0.2047  loss_dice_0: 1.53  loss_ce_1: 1.215  loss_mask_1: 0.144  loss_dice_1: 1.296  loss_ce_2: 1.185  loss_mask_2: 0.1597  loss_dice_2: 1.484  loss_ce_3: 1.154  loss_mask_3: 0.1468  loss_dice_3: 1.509  loss_ce_4: 1.096  loss_mask_4: 0.143  loss_dice_4: 1.567  loss_ce_5: 1.099  loss_mask_5: 0.1269  loss_dice_5: 1.32  loss_ce_6: 1.174  loss_mask_6: 0.1313  loss_dice_6: 1.494  loss_ce_7: 1.074  loss_mask_7: 0.1315  loss_dice_7: 1.517  loss_ce_8: 1.074  loss_mask_8: 0.1248  loss_dice_8: 1.515    time: 0.3226  last_time: 0.3123  data_time: 0.0075  last_data_time: 0.0101   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:46 d2.utils.events]:  eta: 0:56:08  iter: 48959  total_loss: 25.79  loss_ce: 1.118  loss_mask: 0.08663  loss_dice: 1.111  loss_ce_0: 1.819  loss_mask_0: 0.09429  loss_dice_0: 1.135  loss_ce_1: 1.431  loss_mask_1: 0.1328  loss_dice_1: 1.425  loss_ce_2: 1.425  loss_mask_2: 0.1191  loss_dice_2: 1.415  loss_ce_3: 1.147  loss_mask_3: 0.09892  loss_dice_3: 1.17  loss_ce_4: 1.179  loss_mask_4: 0.08855  loss_dice_4: 1.208  loss_ce_5: 1.105  loss_mask_5: 0.08834  loss_dice_5: 1.284  loss_ce_6: 1.095  loss_mask_6: 0.09043  loss_dice_6: 1.403  loss_ce_7: 1.061  loss_mask_7: 0.0945  loss_dice_7: 1.139  loss_ce_8: 1.002  loss_mask_8: 0.08026  loss_dice_8: 1.124    time: 0.3226  last_time: 0.3202  data_time: 0.0106  last_data_time: 0.0026   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:52 d2.utils.events]:  eta: 0:56:02  iter: 48979  total_loss: 24.76  loss_ce: 0.9171  loss_mask: 0.1306  loss_dice: 1.141  loss_ce_0: 1.339  loss_mask_0: 0.138  loss_dice_0: 0.939  loss_ce_1: 1.368  loss_mask_1: 0.1359  loss_dice_1: 1.159  loss_ce_2: 1.153  loss_mask_2: 0.1497  loss_dice_2: 1.101  loss_ce_3: 0.9943  loss_mask_3: 0.1574  loss_dice_3: 1.12  loss_ce_4: 0.8958  loss_mask_4: 0.1499  loss_dice_4: 1.157  loss_ce_5: 0.8929  loss_mask_5: 0.1484  loss_dice_5: 1.247  loss_ce_6: 0.9161  loss_mask_6: 0.1422  loss_dice_6: 1.011  loss_ce_7: 0.8749  loss_mask_7: 0.1381  loss_dice_7: 1.084  loss_ce_8: 0.9999  loss_mask_8: 0.1419  loss_dice_8: 1.192    time: 0.3226  last_time: 0.2997  data_time: 0.0069  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:49:59 d2.utils.events]:  eta: 0:55:55  iter: 48999  total_loss: 24.78  loss_ce: 1.079  loss_mask: 0.141  loss_dice: 1.115  loss_ce_0: 1.312  loss_mask_0: 0.1816  loss_dice_0: 1.196  loss_ce_1: 1.218  loss_mask_1: 0.1806  loss_dice_1: 1.228  loss_ce_2: 1.211  loss_mask_2: 0.1995  loss_dice_2: 1.103  loss_ce_3: 1.004  loss_mask_3: 0.1557  loss_dice_3: 1.24  loss_ce_4: 1.056  loss_mask_4: 0.1431  loss_dice_4: 1.15  loss_ce_5: 1.077  loss_mask_5: 0.1408  loss_dice_5: 1.161  loss_ce_6: 1.06  loss_mask_6: 0.1567  loss_dice_6: 1.268  loss_ce_7: 1.087  loss_mask_7: 0.1436  loss_dice_7: 1.073  loss_ce_8: 1.081  loss_mask_8: 0.1398  loss_dice_8: 1.131    time: 0.3226  last_time: 0.2922  data_time: 0.0126  last_data_time: 0.0039   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:05 d2.utils.events]:  eta: 0:55:50  iter: 49019  total_loss: 19.28  loss_ce: 0.7862  loss_mask: 0.07625  loss_dice: 1.091  loss_ce_0: 1.18  loss_mask_0: 0.07698  loss_dice_0: 1.018  loss_ce_1: 1.055  loss_mask_1: 0.0859  loss_dice_1: 1.063  loss_ce_2: 0.9978  loss_mask_2: 0.08842  loss_dice_2: 1.009  loss_ce_3: 0.7503  loss_mask_3: 0.09105  loss_dice_3: 1.033  loss_ce_4: 0.776  loss_mask_4: 0.09666  loss_dice_4: 1.012  loss_ce_5: 0.7973  loss_mask_5: 0.1015  loss_dice_5: 1.079  loss_ce_6: 0.7817  loss_mask_6: 0.1157  loss_dice_6: 0.9531  loss_ce_7: 0.7683  loss_mask_7: 0.1073  loss_dice_7: 0.9727  loss_ce_8: 0.8068  loss_mask_8: 0.1303  loss_dice_8: 1.048    time: 0.3226  last_time: 0.3022  data_time: 0.0125  last_data_time: 0.0062   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:11 d2.utils.events]:  eta: 0:55:44  iter: 49039  total_loss: 28.94  loss_ce: 1.198  loss_mask: 0.1198  loss_dice: 1.175  loss_ce_0: 1.583  loss_mask_0: 0.1979  loss_dice_0: 1.426  loss_ce_1: 1.425  loss_mask_1: 0.1279  loss_dice_1: 1.338  loss_ce_2: 1.232  loss_mask_2: 0.1515  loss_dice_2: 1.365  loss_ce_3: 1.219  loss_mask_3: 0.1298  loss_dice_3: 1.099  loss_ce_4: 1.212  loss_mask_4: 0.1291  loss_dice_4: 1.296  loss_ce_5: 1.21  loss_mask_5: 0.1339  loss_dice_5: 1.218  loss_ce_6: 1.211  loss_mask_6: 0.1258  loss_dice_6: 1.209  loss_ce_7: 1.231  loss_mask_7: 0.13  loss_dice_7: 1.168  loss_ce_8: 1.232  loss_mask_8: 0.1204  loss_dice_8: 1.169    time: 0.3226  last_time: 0.3316  data_time: 0.0092  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:18 d2.utils.events]:  eta: 0:55:38  iter: 49059  total_loss: 23.5  loss_ce: 0.9165  loss_mask: 0.1027  loss_dice: 1.063  loss_ce_0: 1.144  loss_mask_0: 0.1147  loss_dice_0: 1.123  loss_ce_1: 1.129  loss_mask_1: 0.1253  loss_dice_1: 1.257  loss_ce_2: 1.089  loss_mask_2: 0.1107  loss_dice_2: 0.9732  loss_ce_3: 0.9319  loss_mask_3: 0.09826  loss_dice_3: 1.111  loss_ce_4: 0.8481  loss_mask_4: 0.09563  loss_dice_4: 0.9658  loss_ce_5: 0.9282  loss_mask_5: 0.1044  loss_dice_5: 0.9746  loss_ce_6: 0.9418  loss_mask_6: 0.09663  loss_dice_6: 0.9475  loss_ce_7: 0.876  loss_mask_7: 0.1106  loss_dice_7: 1.024  loss_ce_8: 0.9373  loss_mask_8: 0.1083  loss_dice_8: 0.9774    time: 0.3226  last_time: 0.3229  data_time: 0.0068  last_data_time: 0.0043   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:24 d2.utils.events]:  eta: 0:55:33  iter: 49079  total_loss: 24.4  loss_ce: 0.9961  loss_mask: 0.1366  loss_dice: 0.878  loss_ce_0: 1.273  loss_mask_0: 0.1577  loss_dice_0: 0.9875  loss_ce_1: 1.278  loss_mask_1: 0.1299  loss_dice_1: 0.9363  loss_ce_2: 1.017  loss_mask_2: 0.1758  loss_dice_2: 0.9141  loss_ce_3: 0.9875  loss_mask_3: 0.1379  loss_dice_3: 0.8376  loss_ce_4: 1.051  loss_mask_4: 0.1345  loss_dice_4: 0.8592  loss_ce_5: 1.055  loss_mask_5: 0.1681  loss_dice_5: 0.9344  loss_ce_6: 1.036  loss_mask_6: 0.1334  loss_dice_6: 0.8833  loss_ce_7: 1.101  loss_mask_7: 0.1297  loss_dice_7: 0.8927  loss_ce_8: 1.095  loss_mask_8: 0.1271  loss_dice_8: 0.8852    time: 0.3226  last_time: 0.3289  data_time: 0.0069  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:30 d2.utils.events]:  eta: 0:55:26  iter: 49099  total_loss: 31.01  loss_ce: 1.139  loss_mask: 0.2123  loss_dice: 1.286  loss_ce_0: 1.576  loss_mask_0: 0.247  loss_dice_0: 1.181  loss_ce_1: 1.279  loss_mask_1: 0.2049  loss_dice_1: 1.566  loss_ce_2: 1.272  loss_mask_2: 0.2088  loss_dice_2: 1.506  loss_ce_3: 1.092  loss_mask_3: 0.2787  loss_dice_3: 1.505  loss_ce_4: 1.112  loss_mask_4: 0.2149  loss_dice_4: 1.392  loss_ce_5: 1.015  loss_mask_5: 0.2122  loss_dice_5: 1.344  loss_ce_6: 1.122  loss_mask_6: 0.2343  loss_dice_6: 1.367  loss_ce_7: 1.136  loss_mask_7: 0.2374  loss_dice_7: 1.366  loss_ce_8: 1.158  loss_mask_8: 0.2244  loss_dice_8: 1.305    time: 0.3226  last_time: 0.3234  data_time: 0.0072  last_data_time: 0.0039   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:36 d2.utils.events]:  eta: 0:55:19  iter: 49119  total_loss: 23.21  loss_ce: 1.086  loss_mask: 0.1923  loss_dice: 0.7163  loss_ce_0: 1.449  loss_mask_0: 0.1944  loss_dice_0: 0.8915  loss_ce_1: 1.331  loss_mask_1: 0.1817  loss_dice_1: 0.8311  loss_ce_2: 1.217  loss_mask_2: 0.2025  loss_dice_2: 0.8045  loss_ce_3: 1.065  loss_mask_3: 0.1686  loss_dice_3: 0.7121  loss_ce_4: 1.105  loss_mask_4: 0.1817  loss_dice_4: 0.8024  loss_ce_5: 1.013  loss_mask_5: 0.1794  loss_dice_5: 0.6724  loss_ce_6: 1.062  loss_mask_6: 0.1759  loss_dice_6: 0.7033  loss_ce_7: 1.144  loss_mask_7: 0.1654  loss_dice_7: 0.7755  loss_ce_8: 1.118  loss_mask_8: 0.1736  loss_dice_8: 0.7889    time: 0.3226  last_time: 0.2966  data_time: 0.0071  last_data_time: 0.0029   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:42 d2.utils.events]:  eta: 0:55:09  iter: 49139  total_loss: 15.58  loss_ce: 0.7441  loss_mask: 0.134  loss_dice: 0.7617  loss_ce_0: 1.035  loss_mask_0: 0.1605  loss_dice_0: 0.7064  loss_ce_1: 0.9542  loss_mask_1: 0.1457  loss_dice_1: 0.8104  loss_ce_2: 0.8993  loss_mask_2: 0.1699  loss_dice_2: 1.018  loss_ce_3: 0.7421  loss_mask_3: 0.188  loss_dice_3: 0.8806  loss_ce_4: 0.7023  loss_mask_4: 0.1645  loss_dice_4: 1.092  loss_ce_5: 0.7969  loss_mask_5: 0.1552  loss_dice_5: 1.058  loss_ce_6: 0.6682  loss_mask_6: 0.1317  loss_dice_6: 0.8786  loss_ce_7: 0.7161  loss_mask_7: 0.1338  loss_dice_7: 1.103  loss_ce_8: 0.7383  loss_mask_8: 0.1425  loss_dice_8: 1.087    time: 0.3226  last_time: 0.2947  data_time: 0.0083  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:49 d2.utils.events]:  eta: 0:55:04  iter: 49159  total_loss: 32.07  loss_ce: 1.123  loss_mask: 0.1689  loss_dice: 1.373  loss_ce_0: 1.388  loss_mask_0: 0.1508  loss_dice_0: 1.596  loss_ce_1: 1.244  loss_mask_1: 0.1629  loss_dice_1: 1.554  loss_ce_2: 1.297  loss_mask_2: 0.1673  loss_dice_2: 1.655  loss_ce_3: 1.143  loss_mask_3: 0.1092  loss_dice_3: 1.441  loss_ce_4: 0.9956  loss_mask_4: 0.1365  loss_dice_4: 1.707  loss_ce_5: 1.2  loss_mask_5: 0.1145  loss_dice_5: 1.38  loss_ce_6: 0.9138  loss_mask_6: 0.1512  loss_dice_6: 1.574  loss_ce_7: 1  loss_mask_7: 0.1067  loss_dice_7: 1.528  loss_ce_8: 1.056  loss_mask_8: 0.1536  loss_dice_8: 1.351    time: 0.3226  last_time: 0.2996  data_time: 0.0072  last_data_time: 0.0074   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:50:55 d2.utils.events]:  eta: 0:54:59  iter: 49179  total_loss: 30.42  loss_ce: 0.9536  loss_mask: 0.1323  loss_dice: 1.453  loss_ce_0: 1.422  loss_mask_0: 0.1691  loss_dice_0: 1.443  loss_ce_1: 1.128  loss_mask_1: 0.1643  loss_dice_1: 1.397  loss_ce_2: 1.165  loss_mask_2: 0.1646  loss_dice_2: 1.425  loss_ce_3: 1.027  loss_mask_3: 0.1379  loss_dice_3: 1.367  loss_ce_4: 0.9511  loss_mask_4: 0.146  loss_dice_4: 1.573  loss_ce_5: 1.026  loss_mask_5: 0.1383  loss_dice_5: 1.275  loss_ce_6: 0.9816  loss_mask_6: 0.1262  loss_dice_6: 1.185  loss_ce_7: 1.005  loss_mask_7: 0.1296  loss_dice_7: 1.378  loss_ce_8: 0.9815  loss_mask_8: 0.1567  loss_dice_8: 1.439    time: 0.3226  last_time: 0.2991  data_time: 0.0158  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:02 d2.utils.events]:  eta: 0:54:55  iter: 49199  total_loss: 26.31  loss_ce: 1.207  loss_mask: 0.2296  loss_dice: 0.9681  loss_ce_0: 1.171  loss_mask_0: 0.262  loss_dice_0: 1.213  loss_ce_1: 1.148  loss_mask_1: 0.133  loss_dice_1: 1.172  loss_ce_2: 1.279  loss_mask_2: 0.24  loss_dice_2: 1.013  loss_ce_3: 1.053  loss_mask_3: 0.2308  loss_dice_3: 0.8527  loss_ce_4: 1.102  loss_mask_4: 0.2291  loss_dice_4: 0.8511  loss_ce_5: 1.073  loss_mask_5: 0.2473  loss_dice_5: 0.8913  loss_ce_6: 1.221  loss_mask_6: 0.2466  loss_dice_6: 0.9544  loss_ce_7: 1.046  loss_mask_7: 0.2421  loss_dice_7: 1.002  loss_ce_8: 1.082  loss_mask_8: 0.2185  loss_dice_8: 0.9524    time: 0.3226  last_time: 0.3125  data_time: 0.0184  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:08 d2.utils.events]:  eta: 0:54:47  iter: 49219  total_loss: 22.08  loss_ce: 0.9493  loss_mask: 0.1895  loss_dice: 0.8132  loss_ce_0: 1.118  loss_mask_0: 0.1961  loss_dice_0: 0.8093  loss_ce_1: 1.287  loss_mask_1: 0.165  loss_dice_1: 0.8801  loss_ce_2: 1.13  loss_mask_2: 0.1688  loss_dice_2: 0.7938  loss_ce_3: 0.9646  loss_mask_3: 0.1695  loss_dice_3: 0.8657  loss_ce_4: 1.036  loss_mask_4: 0.1689  loss_dice_4: 0.7397  loss_ce_5: 0.9552  loss_mask_5: 0.1952  loss_dice_5: 0.8577  loss_ce_6: 0.9656  loss_mask_6: 0.1843  loss_dice_6: 0.9669  loss_ce_7: 0.9396  loss_mask_7: 0.1852  loss_dice_7: 0.8347  loss_ce_8: 0.9436  loss_mask_8: 0.1773  loss_dice_8: 0.8336    time: 0.3226  last_time: 0.3013  data_time: 0.0094  last_data_time: 0.0085   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:14 d2.utils.events]:  eta: 0:54:40  iter: 49239  total_loss: 32.07  loss_ce: 1.125  loss_mask: 0.193  loss_dice: 1.665  loss_ce_0: 1.437  loss_mask_0: 0.2509  loss_dice_0: 1.534  loss_ce_1: 1.438  loss_mask_1: 0.2031  loss_dice_1: 1.595  loss_ce_2: 1.289  loss_mask_2: 0.1921  loss_dice_2: 1.604  loss_ce_3: 1.315  loss_mask_3: 0.1364  loss_dice_3: 1.411  loss_ce_4: 1.249  loss_mask_4: 0.1457  loss_dice_4: 1.435  loss_ce_5: 1.243  loss_mask_5: 0.1721  loss_dice_5: 1.667  loss_ce_6: 1.109  loss_mask_6: 0.2242  loss_dice_6: 1.695  loss_ce_7: 1.107  loss_mask_7: 0.2218  loss_dice_7: 1.693  loss_ce_8: 1.186  loss_mask_8: 0.1937  loss_dice_8: 1.654    time: 0.3226  last_time: 0.3001  data_time: 0.0068  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:20 d2.utils.events]:  eta: 0:54:37  iter: 49259  total_loss: 28.06  loss_ce: 1.188  loss_mask: 0.1181  loss_dice: 1.308  loss_ce_0: 1.463  loss_mask_0: 0.17  loss_dice_0: 1.259  loss_ce_1: 1.417  loss_mask_1: 0.1658  loss_dice_1: 1.221  loss_ce_2: 1.278  loss_mask_2: 0.1521  loss_dice_2: 1.337  loss_ce_3: 1.244  loss_mask_3: 0.1227  loss_dice_3: 1.299  loss_ce_4: 1.222  loss_mask_4: 0.1279  loss_dice_4: 1.227  loss_ce_5: 1.152  loss_mask_5: 0.1308  loss_dice_5: 1.352  loss_ce_6: 1.169  loss_mask_6: 0.1346  loss_dice_6: 1.231  loss_ce_7: 1.175  loss_mask_7: 0.1156  loss_dice_7: 1.228  loss_ce_8: 1.19  loss_mask_8: 0.1163  loss_dice_8: 1.256    time: 0.3226  last_time: 0.3071  data_time: 0.0077  last_data_time: 0.0135   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:27 d2.utils.events]:  eta: 0:54:31  iter: 49279  total_loss: 25.41  loss_ce: 0.7759  loss_mask: 0.3077  loss_dice: 1.168  loss_ce_0: 0.9562  loss_mask_0: 0.2047  loss_dice_0: 1.116  loss_ce_1: 0.9583  loss_mask_1: 0.2265  loss_dice_1: 1.079  loss_ce_2: 0.6865  loss_mask_2: 0.2999  loss_dice_2: 1.036  loss_ce_3: 0.7148  loss_mask_3: 0.3534  loss_dice_3: 1.409  loss_ce_4: 0.7781  loss_mask_4: 0.2851  loss_dice_4: 1.245  loss_ce_5: 0.7793  loss_mask_5: 0.2752  loss_dice_5: 1.07  loss_ce_6: 0.6977  loss_mask_6: 0.3156  loss_dice_6: 1.327  loss_ce_7: 0.7326  loss_mask_7: 0.312  loss_dice_7: 1.168  loss_ce_8: 0.7643  loss_mask_8: 0.25  loss_dice_8: 1.409    time: 0.3226  last_time: 0.2985  data_time: 0.0121  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:33 d2.utils.events]:  eta: 0:54:28  iter: 49299  total_loss: 25.31  loss_ce: 0.9821  loss_mask: 0.1236  loss_dice: 1.09  loss_ce_0: 1.202  loss_mask_0: 0.1325  loss_dice_0: 0.9598  loss_ce_1: 1.014  loss_mask_1: 0.1292  loss_dice_1: 1.198  loss_ce_2: 1.139  loss_mask_2: 0.1486  loss_dice_2: 1.203  loss_ce_3: 0.9305  loss_mask_3: 0.1369  loss_dice_3: 1.285  loss_ce_4: 0.9827  loss_mask_4: 0.1355  loss_dice_4: 0.8718  loss_ce_5: 0.9724  loss_mask_5: 0.125  loss_dice_5: 0.9043  loss_ce_6: 0.9361  loss_mask_6: 0.1315  loss_dice_6: 1.059  loss_ce_7: 0.993  loss_mask_7: 0.1284  loss_dice_7: 1.085  loss_ce_8: 0.9338  loss_mask_8: 0.124  loss_dice_8: 1.049    time: 0.3226  last_time: 0.2958  data_time: 0.0112  last_data_time: 0.0022   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:39 d2.utils.events]:  eta: 0:54:19  iter: 49319  total_loss: 29.25  loss_ce: 1.153  loss_mask: 0.1104  loss_dice: 1.403  loss_ce_0: 1.389  loss_mask_0: 0.1387  loss_dice_0: 1.513  loss_ce_1: 1.384  loss_mask_1: 0.108  loss_dice_1: 1.275  loss_ce_2: 1.298  loss_mask_2: 0.1324  loss_dice_2: 1.3  loss_ce_3: 1.198  loss_mask_3: 0.1272  loss_dice_3: 1.361  loss_ce_4: 1.166  loss_mask_4: 0.1222  loss_dice_4: 1.355  loss_ce_5: 1.228  loss_mask_5: 0.1375  loss_dice_5: 1.354  loss_ce_6: 1.231  loss_mask_6: 0.1107  loss_dice_6: 1.288  loss_ce_7: 1.214  loss_mask_7: 0.1153  loss_dice_7: 1.393  loss_ce_8: 1.154  loss_mask_8: 0.1106  loss_dice_8: 1.547    time: 0.3225  last_time: 0.3047  data_time: 0.0064  last_data_time: 0.0092   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:45 d2.utils.events]:  eta: 0:54:12  iter: 49339  total_loss: 26.2  loss_ce: 1.117  loss_mask: 0.1302  loss_dice: 1.151  loss_ce_0: 1.255  loss_mask_0: 0.2056  loss_dice_0: 1.144  loss_ce_1: 1.216  loss_mask_1: 0.125  loss_dice_1: 1.167  loss_ce_2: 1.162  loss_mask_2: 0.1204  loss_dice_2: 1.177  loss_ce_3: 1.124  loss_mask_3: 0.156  loss_dice_3: 1.129  loss_ce_4: 1.172  loss_mask_4: 0.1087  loss_dice_4: 1.1  loss_ce_5: 1.121  loss_mask_5: 0.1412  loss_dice_5: 1.143  loss_ce_6: 1.105  loss_mask_6: 0.14  loss_dice_6: 1.075  loss_ce_7: 1.057  loss_mask_7: 0.1131  loss_dice_7: 1.161  loss_ce_8: 1.101  loss_mask_8: 0.1384  loss_dice_8: 1.093    time: 0.3225  last_time: 0.3039  data_time: 0.0074  last_data_time: 0.0066   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:51 d2.utils.events]:  eta: 0:54:06  iter: 49359  total_loss: 23.51  loss_ce: 0.7343  loss_mask: 0.1783  loss_dice: 1.023  loss_ce_0: 1.064  loss_mask_0: 0.1471  loss_dice_0: 1.132  loss_ce_1: 1.083  loss_mask_1: 0.1618  loss_dice_1: 1.161  loss_ce_2: 1.068  loss_mask_2: 0.1856  loss_dice_2: 1.008  loss_ce_3: 0.7773  loss_mask_3: 0.1866  loss_dice_3: 1.174  loss_ce_4: 0.73  loss_mask_4: 0.1844  loss_dice_4: 1.367  loss_ce_5: 0.7839  loss_mask_5: 0.1895  loss_dice_5: 1.013  loss_ce_6: 0.7023  loss_mask_6: 0.1716  loss_dice_6: 1.065  loss_ce_7: 0.7878  loss_mask_7: 0.1647  loss_dice_7: 0.9686  loss_ce_8: 0.7284  loss_mask_8: 0.1664  loss_dice_8: 0.7289    time: 0.3225  last_time: 0.3590  data_time: 0.0078  last_data_time: 0.0367   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:51:58 d2.utils.events]:  eta: 0:53:56  iter: 49379  total_loss: 25.97  loss_ce: 1.113  loss_mask: 0.1717  loss_dice: 1.299  loss_ce_0: 1.241  loss_mask_0: 0.2018  loss_dice_0: 1.459  loss_ce_1: 1.349  loss_mask_1: 0.1609  loss_dice_1: 1.304  loss_ce_2: 1.194  loss_mask_2: 0.2021  loss_dice_2: 1.183  loss_ce_3: 1.123  loss_mask_3: 0.1679  loss_dice_3: 1.501  loss_ce_4: 1.102  loss_mask_4: 0.1566  loss_dice_4: 1.255  loss_ce_5: 1.059  loss_mask_5: 0.1579  loss_dice_5: 1.454  loss_ce_6: 1.141  loss_mask_6: 0.1837  loss_dice_6: 1.182  loss_ce_7: 1.129  loss_mask_7: 0.1645  loss_dice_7: 1.32  loss_ce_8: 1.201  loss_mask_8: 0.1706  loss_dice_8: 1.198    time: 0.3225  last_time: 0.3158  data_time: 0.0118  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:04 d2.utils.events]:  eta: 0:53:49  iter: 49399  total_loss: 25.74  loss_ce: 1.09  loss_mask: 0.1651  loss_dice: 1.163  loss_ce_0: 1.272  loss_mask_0: 0.1377  loss_dice_0: 1.225  loss_ce_1: 1.266  loss_mask_1: 0.1413  loss_dice_1: 1.053  loss_ce_2: 1.103  loss_mask_2: 0.1924  loss_dice_2: 1.042  loss_ce_3: 0.9643  loss_mask_3: 0.1329  loss_dice_3: 1.07  loss_ce_4: 1.046  loss_mask_4: 0.1679  loss_dice_4: 1.112  loss_ce_5: 1.092  loss_mask_5: 0.1493  loss_dice_5: 1.266  loss_ce_6: 1.07  loss_mask_6: 0.1402  loss_dice_6: 1.287  loss_ce_7: 1.088  loss_mask_7: 0.1469  loss_dice_7: 1.181  loss_ce_8: 1.032  loss_mask_8: 0.1409  loss_dice_8: 1.157    time: 0.3225  last_time: 0.3015  data_time: 0.0109  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:10 d2.utils.events]:  eta: 0:53:44  iter: 49419  total_loss: 27.76  loss_ce: 1.02  loss_mask: 0.2213  loss_dice: 0.8916  loss_ce_0: 1.177  loss_mask_0: 0.172  loss_dice_0: 0.8977  loss_ce_1: 1.153  loss_mask_1: 0.2518  loss_dice_1: 0.8491  loss_ce_2: 1.125  loss_mask_2: 0.25  loss_dice_2: 0.707  loss_ce_3: 1.148  loss_mask_3: 0.2014  loss_dice_3: 0.6866  loss_ce_4: 1.018  loss_mask_4: 0.2571  loss_dice_4: 0.9069  loss_ce_5: 0.9969  loss_mask_5: 0.2261  loss_dice_5: 0.869  loss_ce_6: 0.965  loss_mask_6: 0.215  loss_dice_6: 0.9089  loss_ce_7: 0.938  loss_mask_7: 0.2112  loss_dice_7: 0.8542  loss_ce_8: 0.9468  loss_mask_8: 0.1977  loss_dice_8: 0.8248    time: 0.3225  last_time: 0.2956  data_time: 0.0077  last_data_time: 0.0034   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:16 d2.utils.events]:  eta: 0:53:36  iter: 49439  total_loss: 23.32  loss_ce: 0.9163  loss_mask: 0.15  loss_dice: 1.091  loss_ce_0: 1.08  loss_mask_0: 0.1125  loss_dice_0: 1.01  loss_ce_1: 1.122  loss_mask_1: 0.09858  loss_dice_1: 1.202  loss_ce_2: 1.048  loss_mask_2: 0.1317  loss_dice_2: 1.372  loss_ce_3: 0.9367  loss_mask_3: 0.1379  loss_dice_3: 0.9573  loss_ce_4: 0.9726  loss_mask_4: 0.1488  loss_dice_4: 1.293  loss_ce_5: 0.9607  loss_mask_5: 0.1586  loss_dice_5: 1.039  loss_ce_6: 0.942  loss_mask_6: 0.1436  loss_dice_6: 1.062  loss_ce_7: 0.8751  loss_mask_7: 0.1448  loss_dice_7: 1.18  loss_ce_8: 0.9623  loss_mask_8: 0.1351  loss_dice_8: 1.183    time: 0.3225  last_time: 0.3114  data_time: 0.0067  last_data_time: 0.0049   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:22 d2.utils.events]:  eta: 0:53:28  iter: 49459  total_loss: 28.13  loss_ce: 0.9712  loss_mask: 0.1805  loss_dice: 1.425  loss_ce_0: 1.378  loss_mask_0: 0.152  loss_dice_0: 1.355  loss_ce_1: 1.67  loss_mask_1: 0.1709  loss_dice_1: 1.603  loss_ce_2: 1.323  loss_mask_2: 0.1736  loss_dice_2: 1.492  loss_ce_3: 0.8672  loss_mask_3: 0.1709  loss_dice_3: 1.341  loss_ce_4: 1.054  loss_mask_4: 0.1563  loss_dice_4: 1.211  loss_ce_5: 1.071  loss_mask_5: 0.1656  loss_dice_5: 1.404  loss_ce_6: 1.069  loss_mask_6: 0.1949  loss_dice_6: 1.517  loss_ce_7: 0.8828  loss_mask_7: 0.184  loss_dice_7: 1.522  loss_ce_8: 0.9072  loss_mask_8: 0.1995  loss_dice_8: 1.578    time: 0.3225  last_time: 0.3014  data_time: 0.0076  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:29 d2.utils.events]:  eta: 0:53:22  iter: 49479  total_loss: 27.57  loss_ce: 1.024  loss_mask: 0.1569  loss_dice: 1.001  loss_ce_0: 1.421  loss_mask_0: 0.1097  loss_dice_0: 1.566  loss_ce_1: 1.209  loss_mask_1: 0.1298  loss_dice_1: 1.316  loss_ce_2: 1.206  loss_mask_2: 0.1798  loss_dice_2: 1.424  loss_ce_3: 1.061  loss_mask_3: 0.1264  loss_dice_3: 1.12  loss_ce_4: 1.047  loss_mask_4: 0.144  loss_dice_4: 1.303  loss_ce_5: 1.056  loss_mask_5: 0.1285  loss_dice_5: 1.442  loss_ce_6: 1.042  loss_mask_6: 0.1926  loss_dice_6: 1.226  loss_ce_7: 1.043  loss_mask_7: 0.1466  loss_dice_7: 1.324  loss_ce_8: 0.9556  loss_mask_8: 0.1752  loss_dice_8: 1.16    time: 0.3225  last_time: 0.3230  data_time: 0.0093  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:35 d2.utils.events]:  eta: 0:53:17  iter: 49499  total_loss: 28.8  loss_ce: 1.073  loss_mask: 0.1103  loss_dice: 1.67  loss_ce_0: 1.166  loss_mask_0: 0.1246  loss_dice_0: 1.327  loss_ce_1: 1.169  loss_mask_1: 0.1349  loss_dice_1: 1.427  loss_ce_2: 1.219  loss_mask_2: 0.1234  loss_dice_2: 1.378  loss_ce_3: 1.142  loss_mask_3: 0.1414  loss_dice_3: 1.523  loss_ce_4: 1.097  loss_mask_4: 0.1383  loss_dice_4: 1.599  loss_ce_5: 1.289  loss_mask_5: 0.1343  loss_dice_5: 1.348  loss_ce_6: 1.185  loss_mask_6: 0.1233  loss_dice_6: 1.252  loss_ce_7: 1.16  loss_mask_7: 0.114  loss_dice_7: 1.361  loss_ce_8: 1.081  loss_mask_8: 0.1113  loss_dice_8: 1.526    time: 0.3225  last_time: 0.3136  data_time: 0.0116  last_data_time: 0.0131   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:41 d2.utils.events]:  eta: 0:53:13  iter: 49519  total_loss: 29.85  loss_ce: 1.275  loss_mask: 0.1617  loss_dice: 1.581  loss_ce_0: 1.501  loss_mask_0: 0.1354  loss_dice_0: 1.64  loss_ce_1: 1.409  loss_mask_1: 0.1418  loss_dice_1: 1.708  loss_ce_2: 1.385  loss_mask_2: 0.1461  loss_dice_2: 1.557  loss_ce_3: 1.364  loss_mask_3: 0.133  loss_dice_3: 1.653  loss_ce_4: 1.322  loss_mask_4: 0.127  loss_dice_4: 1.524  loss_ce_5: 1.35  loss_mask_5: 0.1518  loss_dice_5: 1.545  loss_ce_6: 1.349  loss_mask_6: 0.15  loss_dice_6: 1.712  loss_ce_7: 1.36  loss_mask_7: 0.1698  loss_dice_7: 1.721  loss_ce_8: 1.315  loss_mask_8: 0.1588  loss_dice_8: 1.676    time: 0.3225  last_time: 0.2977  data_time: 0.0071  last_data_time: 0.0025   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:48 d2.utils.events]:  eta: 0:53:06  iter: 49539  total_loss: 28.41  loss_ce: 1.264  loss_mask: 0.1109  loss_dice: 1.391  loss_ce_0: 1.669  loss_mask_0: 0.1376  loss_dice_0: 1.357  loss_ce_1: 1.69  loss_mask_1: 0.1022  loss_dice_1: 1.658  loss_ce_2: 1.508  loss_mask_2: 0.126  loss_dice_2: 1.417  loss_ce_3: 1.359  loss_mask_3: 0.1219  loss_dice_3: 1.334  loss_ce_4: 1.381  loss_mask_4: 0.1122  loss_dice_4: 1.457  loss_ce_5: 1.188  loss_mask_5: 0.1241  loss_dice_5: 1.493  loss_ce_6: 1.279  loss_mask_6: 0.1071  loss_dice_6: 1.218  loss_ce_7: 1.329  loss_mask_7: 0.1079  loss_dice_7: 1.308  loss_ce_8: 1.295  loss_mask_8: 0.1013  loss_dice_8: 1.508    time: 0.3225  last_time: 0.2936  data_time: 0.0070  last_data_time: 0.0051   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:52:54 d2.utils.events]:  eta: 0:52:59  iter: 49559  total_loss: 30.6  loss_ce: 1.327  loss_mask: 0.1747  loss_dice: 1.517  loss_ce_0: 1.663  loss_mask_0: 0.1749  loss_dice_0: 1.366  loss_ce_1: 1.588  loss_mask_1: 0.184  loss_dice_1: 1.386  loss_ce_2: 1.462  loss_mask_2: 0.1806  loss_dice_2: 1.482  loss_ce_3: 1.351  loss_mask_3: 0.1802  loss_dice_3: 1.596  loss_ce_4: 1.373  loss_mask_4: 0.1765  loss_dice_4: 1.503  loss_ce_5: 1.399  loss_mask_5: 0.1589  loss_dice_5: 1.491  loss_ce_6: 1.327  loss_mask_6: 0.1795  loss_dice_6: 1.545  loss_ce_7: 1.313  loss_mask_7: 0.1701  loss_dice_7: 1.712  loss_ce_8: 1.385  loss_mask_8: 0.1764  loss_dice_8: 1.618    time: 0.3225  last_time: 0.3039  data_time: 0.0071  last_data_time: 0.0051   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:00 d2.utils.events]:  eta: 0:52:53  iter: 49579  total_loss: 33.17  loss_ce: 1.214  loss_mask: 0.2593  loss_dice: 1.349  loss_ce_0: 1.447  loss_mask_0: 0.2465  loss_dice_0: 1.226  loss_ce_1: 1.423  loss_mask_1: 0.2628  loss_dice_1: 1.434  loss_ce_2: 1.316  loss_mask_2: 0.265  loss_dice_2: 1.506  loss_ce_3: 1.169  loss_mask_3: 0.2825  loss_dice_3: 1.416  loss_ce_4: 1.079  loss_mask_4: 0.3703  loss_dice_4: 1.669  loss_ce_5: 1.12  loss_mask_5: 0.3744  loss_dice_5: 1.656  loss_ce_6: 1.232  loss_mask_6: 0.2598  loss_dice_6: 1.326  loss_ce_7: 1.264  loss_mask_7: 0.2006  loss_dice_7: 1.43  loss_ce_8: 1.232  loss_mask_8: 0.2624  loss_dice_8: 1.628    time: 0.3225  last_time: 0.2965  data_time: 0.0067  last_data_time: 0.0040   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:06 d2.utils.events]:  eta: 0:52:50  iter: 49599  total_loss: 27.47  loss_ce: 0.9864  loss_mask: 0.08178  loss_dice: 1.451  loss_ce_0: 1.169  loss_mask_0: 0.07717  loss_dice_0: 1.583  loss_ce_1: 1.179  loss_mask_1: 0.0693  loss_dice_1: 1.4  loss_ce_2: 1.023  loss_mask_2: 0.1174  loss_dice_2: 1.557  loss_ce_3: 1.091  loss_mask_3: 0.1332  loss_dice_3: 1.363  loss_ce_4: 1.081  loss_mask_4: 0.08606  loss_dice_4: 1.386  loss_ce_5: 1.115  loss_mask_5: 0.09248  loss_dice_5: 1.492  loss_ce_6: 0.9878  loss_mask_6: 0.08041  loss_dice_6: 1.424  loss_ce_7: 1.115  loss_mask_7: 0.08216  loss_dice_7: 1.49  loss_ce_8: 1.057  loss_mask_8: 0.07252  loss_dice_8: 1.505    time: 0.3225  last_time: 0.2948  data_time: 0.0066  last_data_time: 0.0026   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:13 d2.utils.events]:  eta: 0:52:46  iter: 49619  total_loss: 28.32  loss_ce: 1.174  loss_mask: 0.2022  loss_dice: 1.592  loss_ce_0: 1.536  loss_mask_0: 0.2956  loss_dice_0: 1.53  loss_ce_1: 1.416  loss_mask_1: 0.2558  loss_dice_1: 1.718  loss_ce_2: 1.331  loss_mask_2: 0.2766  loss_dice_2: 1.82  loss_ce_3: 1.159  loss_mask_3: 0.2215  loss_dice_3: 1.466  loss_ce_4: 1.168  loss_mask_4: 0.2223  loss_dice_4: 1.427  loss_ce_5: 1.229  loss_mask_5: 0.219  loss_dice_5: 1.448  loss_ce_6: 1.188  loss_mask_6: 0.2071  loss_dice_6: 1.607  loss_ce_7: 1.189  loss_mask_7: 0.205  loss_dice_7: 1.548  loss_ce_8: 1.18  loss_mask_8: 0.1877  loss_dice_8: 1.454    time: 0.3225  last_time: 0.2946  data_time: 0.0087  last_data_time: 0.0022   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:19 d2.utils.events]:  eta: 0:52:39  iter: 49639  total_loss: 24.04  loss_ce: 1.066  loss_mask: 0.151  loss_dice: 1.042  loss_ce_0: 1.303  loss_mask_0: 0.1722  loss_dice_0: 1.086  loss_ce_1: 1.403  loss_mask_1: 0.267  loss_dice_1: 1.141  loss_ce_2: 1.245  loss_mask_2: 0.2718  loss_dice_2: 1.239  loss_ce_3: 1.075  loss_mask_3: 0.1699  loss_dice_3: 1.287  loss_ce_4: 1.013  loss_mask_4: 0.1594  loss_dice_4: 1.076  loss_ce_5: 0.9989  loss_mask_5: 0.1659  loss_dice_5: 1.201  loss_ce_6: 1.101  loss_mask_6: 0.1549  loss_dice_6: 1.249  loss_ce_7: 1.079  loss_mask_7: 0.147  loss_dice_7: 0.9685  loss_ce_8: 1.004  loss_mask_8: 0.1389  loss_dice_8: 1.075    time: 0.3225  last_time: 0.2973  data_time: 0.0085  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:25 d2.utils.events]:  eta: 0:52:31  iter: 49659  total_loss: 24.02  loss_ce: 0.867  loss_mask: 0.1408  loss_dice: 0.9939  loss_ce_0: 0.9647  loss_mask_0: 0.168  loss_dice_0: 1.02  loss_ce_1: 1.075  loss_mask_1: 0.1726  loss_dice_1: 0.9376  loss_ce_2: 1.027  loss_mask_2: 0.1466  loss_dice_2: 0.9911  loss_ce_3: 0.854  loss_mask_3: 0.162  loss_dice_3: 1.018  loss_ce_4: 0.9068  loss_mask_4: 0.1564  loss_dice_4: 0.9975  loss_ce_5: 0.8647  loss_mask_5: 0.1549  loss_dice_5: 1.023  loss_ce_6: 0.8681  loss_mask_6: 0.1444  loss_dice_6: 0.9451  loss_ce_7: 0.8687  loss_mask_7: 0.1595  loss_dice_7: 1.023  loss_ce_8: 0.8747  loss_mask_8: 0.1495  loss_dice_8: 1.026    time: 0.3225  last_time: 0.2987  data_time: 0.0063  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:31 d2.utils.events]:  eta: 0:52:25  iter: 49679  total_loss: 28.67  loss_ce: 1.004  loss_mask: 0.1591  loss_dice: 1.24  loss_ce_0: 1.462  loss_mask_0: 0.1881  loss_dice_0: 1.565  loss_ce_1: 1.299  loss_mask_1: 0.1923  loss_dice_1: 1.564  loss_ce_2: 1.272  loss_mask_2: 0.1753  loss_dice_2: 1.392  loss_ce_3: 1.079  loss_mask_3: 0.169  loss_dice_3: 1.33  loss_ce_4: 1.015  loss_mask_4: 0.1726  loss_dice_4: 1.4  loss_ce_5: 1.036  loss_mask_5: 0.1644  loss_dice_5: 1.336  loss_ce_6: 0.9887  loss_mask_6: 0.1624  loss_dice_6: 1.248  loss_ce_7: 1.012  loss_mask_7: 0.1528  loss_dice_7: 1.164  loss_ce_8: 1  loss_mask_8: 0.1581  loss_dice_8: 1.348    time: 0.3225  last_time: 0.3161  data_time: 0.0120  last_data_time: 0.0245   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:38 d2.utils.events]:  eta: 0:52:19  iter: 49699  total_loss: 28.04  loss_ce: 1.242  loss_mask: 0.1362  loss_dice: 1.386  loss_ce_0: 1.739  loss_mask_0: 0.1621  loss_dice_0: 1.387  loss_ce_1: 1.465  loss_mask_1: 0.1305  loss_dice_1: 1.664  loss_ce_2: 1.352  loss_mask_2: 0.1326  loss_dice_2: 1.655  loss_ce_3: 1.387  loss_mask_3: 0.1205  loss_dice_3: 1.805  loss_ce_4: 1.363  loss_mask_4: 0.1318  loss_dice_4: 1.6  loss_ce_5: 1.294  loss_mask_5: 0.1264  loss_dice_5: 1.709  loss_ce_6: 1.255  loss_mask_6: 0.1417  loss_dice_6: 1.545  loss_ce_7: 1.246  loss_mask_7: 0.1612  loss_dice_7: 1.296  loss_ce_8: 1.287  loss_mask_8: 0.1684  loss_dice_8: 1.759    time: 0.3225  last_time: 0.3025  data_time: 0.0067  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:44 d2.utils.events]:  eta: 0:52:12  iter: 49719  total_loss: 25.12  loss_ce: 0.9537  loss_mask: 0.1735  loss_dice: 1.494  loss_ce_0: 1.285  loss_mask_0: 0.1557  loss_dice_0: 1.697  loss_ce_1: 1.083  loss_mask_1: 0.2055  loss_dice_1: 1.668  loss_ce_2: 1.184  loss_mask_2: 0.184  loss_dice_2: 1.494  loss_ce_3: 0.9943  loss_mask_3: 0.2083  loss_dice_3: 1.402  loss_ce_4: 0.9812  loss_mask_4: 0.1777  loss_dice_4: 1.641  loss_ce_5: 0.9007  loss_mask_5: 0.1988  loss_dice_5: 1.47  loss_ce_6: 0.9655  loss_mask_6: 0.1888  loss_dice_6: 1.414  loss_ce_7: 0.9384  loss_mask_7: 0.1772  loss_dice_7: 1.686  loss_ce_8: 0.9704  loss_mask_8: 0.1842  loss_dice_8: 1.471    time: 0.3225  last_time: 0.2997  data_time: 0.0128  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:50 d2.utils.events]:  eta: 0:52:06  iter: 49739  total_loss: 23.9  loss_ce: 1.005  loss_mask: 0.1331  loss_dice: 1.119  loss_ce_0: 1.219  loss_mask_0: 0.157  loss_dice_0: 1.287  loss_ce_1: 1.103  loss_mask_1: 0.1489  loss_dice_1: 1.187  loss_ce_2: 1.197  loss_mask_2: 0.1228  loss_dice_2: 1.16  loss_ce_3: 1.063  loss_mask_3: 0.133  loss_dice_3: 1.131  loss_ce_4: 1.124  loss_mask_4: 0.1144  loss_dice_4: 1.256  loss_ce_5: 0.9707  loss_mask_5: 0.1208  loss_dice_5: 1.035  loss_ce_6: 1.061  loss_mask_6: 0.1243  loss_dice_6: 1.192  loss_ce_7: 1.005  loss_mask_7: 0.1358  loss_dice_7: 1.062  loss_ce_8: 1.147  loss_mask_8: 0.1316  loss_dice_8: 1.348    time: 0.3225  last_time: 0.3337  data_time: 0.0171  last_data_time: 0.0114   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:53:57 d2.utils.events]:  eta: 0:52:01  iter: 49759  total_loss: 32.74  loss_ce: 1.324  loss_mask: 0.082  loss_dice: 1.486  loss_ce_0: 1.44  loss_mask_0: 0.1276  loss_dice_0: 1.543  loss_ce_1: 1.374  loss_mask_1: 0.112  loss_dice_1: 1.617  loss_ce_2: 1.421  loss_mask_2: 0.1155  loss_dice_2: 1.666  loss_ce_3: 1.396  loss_mask_3: 0.09567  loss_dice_3: 1.594  loss_ce_4: 1.322  loss_mask_4: 0.09005  loss_dice_4: 1.568  loss_ce_5: 1.34  loss_mask_5: 0.09826  loss_dice_5: 1.492  loss_ce_6: 1.347  loss_mask_6: 0.0914  loss_dice_6: 1.514  loss_ce_7: 1.371  loss_mask_7: 0.09939  loss_dice_7: 1.536  loss_ce_8: 1.34  loss_mask_8: 0.1037  loss_dice_8: 1.483    time: 0.3225  last_time: 0.2962  data_time: 0.0106  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:03 d2.utils.events]:  eta: 0:51:54  iter: 49779  total_loss: 28.25  loss_ce: 1.348  loss_mask: 0.09024  loss_dice: 1.254  loss_ce_0: 1.468  loss_mask_0: 0.08783  loss_dice_0: 1.137  loss_ce_1: 1.532  loss_mask_1: 0.1031  loss_dice_1: 1.23  loss_ce_2: 1.434  loss_mask_2: 0.09337  loss_dice_2: 1.27  loss_ce_3: 1.303  loss_mask_3: 0.1105  loss_dice_3: 1.277  loss_ce_4: 1.354  loss_mask_4: 0.1133  loss_dice_4: 1.314  loss_ce_5: 1.278  loss_mask_5: 0.1121  loss_dice_5: 1.288  loss_ce_6: 1.332  loss_mask_6: 0.1067  loss_dice_6: 1.275  loss_ce_7: 1.368  loss_mask_7: 0.09467  loss_dice_7: 1.37  loss_ce_8: 1.329  loss_mask_8: 0.08999  loss_dice_8: 1.247    time: 0.3224  last_time: 0.3004  data_time: 0.0074  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:09 d2.utils.events]:  eta: 0:51:51  iter: 49799  total_loss: 22.22  loss_ce: 0.9679  loss_mask: 0.1307  loss_dice: 1.256  loss_ce_0: 1.177  loss_mask_0: 0.1912  loss_dice_0: 1.043  loss_ce_1: 1.114  loss_mask_1: 0.1693  loss_dice_1: 1.127  loss_ce_2: 1.053  loss_mask_2: 0.1503  loss_dice_2: 1.235  loss_ce_3: 0.9257  loss_mask_3: 0.1364  loss_dice_3: 1.118  loss_ce_4: 0.8031  loss_mask_4: 0.1545  loss_dice_4: 0.9642  loss_ce_5: 0.908  loss_mask_5: 0.1167  loss_dice_5: 1.089  loss_ce_6: 0.9329  loss_mask_6: 0.1284  loss_dice_6: 1.11  loss_ce_7: 0.9196  loss_mask_7: 0.1388  loss_dice_7: 1.287  loss_ce_8: 0.7891  loss_mask_8: 0.1288  loss_dice_8: 0.972    time: 0.3224  last_time: 0.3019  data_time: 0.0085  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:16 d2.utils.events]:  eta: 0:51:47  iter: 49819  total_loss: 29.38  loss_ce: 0.8647  loss_mask: 0.1519  loss_dice: 1.306  loss_ce_0: 1.253  loss_mask_0: 0.1225  loss_dice_0: 1.231  loss_ce_1: 0.9876  loss_mask_1: 0.205  loss_dice_1: 1.391  loss_ce_2: 1.047  loss_mask_2: 0.1778  loss_dice_2: 1.666  loss_ce_3: 0.8366  loss_mask_3: 0.1796  loss_dice_3: 1.471  loss_ce_4: 0.8323  loss_mask_4: 0.1649  loss_dice_4: 1.586  loss_ce_5: 0.8525  loss_mask_5: 0.2356  loss_dice_5: 1.52  loss_ce_6: 0.89  loss_mask_6: 0.1556  loss_dice_6: 1.446  loss_ce_7: 0.8471  loss_mask_7: 0.1559  loss_dice_7: 1.371  loss_ce_8: 0.866  loss_mask_8: 0.1596  loss_dice_8: 1.516    time: 0.3224  last_time: 0.3046  data_time: 0.0077  last_data_time: 0.0103   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:22 d2.utils.events]:  eta: 0:51:40  iter: 49839  total_loss: 29.45  loss_ce: 1.125  loss_mask: 0.1114  loss_dice: 1.492  loss_ce_0: 1.443  loss_mask_0: 0.1177  loss_dice_0: 1.472  loss_ce_1: 1.507  loss_mask_1: 0.1217  loss_dice_1: 1.454  loss_ce_2: 1.404  loss_mask_2: 0.1343  loss_dice_2: 1.387  loss_ce_3: 1.146  loss_mask_3: 0.1178  loss_dice_3: 1.307  loss_ce_4: 1.201  loss_mask_4: 0.1172  loss_dice_4: 1.3  loss_ce_5: 1.19  loss_mask_5: 0.1008  loss_dice_5: 1.157  loss_ce_6: 1.125  loss_mask_6: 0.1112  loss_dice_6: 1.348  loss_ce_7: 1.073  loss_mask_7: 0.1118  loss_dice_7: 1.329  loss_ce_8: 1.117  loss_mask_8: 0.103  loss_dice_8: 1.281    time: 0.3224  last_time: 0.3107  data_time: 0.0075  last_data_time: 0.0046   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:28 d2.utils.events]:  eta: 0:51:34  iter: 49859  total_loss: 22.05  loss_ce: 0.9881  loss_mask: 0.0991  loss_dice: 0.8491  loss_ce_0: 1.13  loss_mask_0: 0.106  loss_dice_0: 1.073  loss_ce_1: 1.077  loss_mask_1: 0.09476  loss_dice_1: 1.028  loss_ce_2: 0.9872  loss_mask_2: 0.0933  loss_dice_2: 0.8881  loss_ce_3: 0.8528  loss_mask_3: 0.1189  loss_dice_3: 0.8567  loss_ce_4: 0.9196  loss_mask_4: 0.09048  loss_dice_4: 0.9013  loss_ce_5: 0.8944  loss_mask_5: 0.1176  loss_dice_5: 0.7837  loss_ce_6: 0.9028  loss_mask_6: 0.08987  loss_dice_6: 1.023  loss_ce_7: 0.8649  loss_mask_7: 0.09999  loss_dice_7: 0.7911  loss_ce_8: 0.9139  loss_mask_8: 0.1177  loss_dice_8: 0.6848    time: 0.3224  last_time: 0.3281  data_time: 0.0098  last_data_time: 0.0104   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:34 d2.utils.events]:  eta: 0:51:28  iter: 49879  total_loss: 19.15  loss_ce: 0.9023  loss_mask: 0.121  loss_dice: 0.7119  loss_ce_0: 1.285  loss_mask_0: 0.1416  loss_dice_0: 0.8723  loss_ce_1: 1.073  loss_mask_1: 0.1276  loss_dice_1: 0.9044  loss_ce_2: 0.956  loss_mask_2: 0.1351  loss_dice_2: 0.9352  loss_ce_3: 0.8742  loss_mask_3: 0.1362  loss_dice_3: 0.9393  loss_ce_4: 0.9062  loss_mask_4: 0.1266  loss_dice_4: 0.9355  loss_ce_5: 0.8367  loss_mask_5: 0.1329  loss_dice_5: 0.8748  loss_ce_6: 0.8304  loss_mask_6: 0.1296  loss_dice_6: 0.9983  loss_ce_7: 0.8074  loss_mask_7: 0.1247  loss_dice_7: 0.8332  loss_ce_8: 0.8496  loss_mask_8: 0.1291  loss_dice_8: 0.875    time: 0.3224  last_time: 0.3262  data_time: 0.0101  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:41 d2.utils.events]:  eta: 0:51:22  iter: 49899  total_loss: 30.39  loss_ce: 1.467  loss_mask: 0.1965  loss_dice: 0.9362  loss_ce_0: 1.517  loss_mask_0: 0.1808  loss_dice_0: 0.9915  loss_ce_1: 1.578  loss_mask_1: 0.223  loss_dice_1: 0.9649  loss_ce_2: 1.342  loss_mask_2: 0.2305  loss_dice_2: 1.181  loss_ce_3: 1.457  loss_mask_3: 0.2252  loss_dice_3: 0.9819  loss_ce_4: 1.439  loss_mask_4: 0.2365  loss_dice_4: 1.001  loss_ce_5: 1.381  loss_mask_5: 0.2318  loss_dice_5: 0.968  loss_ce_6: 1.503  loss_mask_6: 0.232  loss_dice_6: 0.9712  loss_ce_7: 1.403  loss_mask_7: 0.229  loss_dice_7: 0.9818  loss_ce_8: 1.539  loss_mask_8: 0.2081  loss_dice_8: 0.876    time: 0.3224  last_time: 0.3031  data_time: 0.0077  last_data_time: 0.0109   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:47 d2.utils.events]:  eta: 0:51:17  iter: 49919  total_loss: 28.82  loss_ce: 1.08  loss_mask: 0.1644  loss_dice: 1.258  loss_ce_0: 1.254  loss_mask_0: 0.1278  loss_dice_0: 1.29  loss_ce_1: 1.343  loss_mask_1: 0.1235  loss_dice_1: 1.882  loss_ce_2: 1.289  loss_mask_2: 0.1163  loss_dice_2: 1.461  loss_ce_3: 1.124  loss_mask_3: 0.1386  loss_dice_3: 1.483  loss_ce_4: 1.1  loss_mask_4: 0.1078  loss_dice_4: 1.349  loss_ce_5: 1.244  loss_mask_5: 0.1196  loss_dice_5: 1.532  loss_ce_6: 1.04  loss_mask_6: 0.1387  loss_dice_6: 1.342  loss_ce_7: 1.045  loss_mask_7: 0.1548  loss_dice_7: 1.357  loss_ce_8: 1.08  loss_mask_8: 0.157  loss_dice_8: 1.382    time: 0.3224  last_time: 0.2982  data_time: 0.0077  last_data_time: 0.0051   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:53 d2.utils.events]:  eta: 0:51:12  iter: 49939  total_loss: 27.42  loss_ce: 1.054  loss_mask: 0.2036  loss_dice: 1.297  loss_ce_0: 1.344  loss_mask_0: 0.2624  loss_dice_0: 1.165  loss_ce_1: 1.272  loss_mask_1: 0.288  loss_dice_1: 1.239  loss_ce_2: 1.141  loss_mask_2: 0.2403  loss_dice_2: 1.136  loss_ce_3: 1.016  loss_mask_3: 0.2731  loss_dice_3: 1.084  loss_ce_4: 1.031  loss_mask_4: 0.2229  loss_dice_4: 1.237  loss_ce_5: 1.035  loss_mask_5: 0.2439  loss_dice_5: 1.194  loss_ce_6: 1.044  loss_mask_6: 0.2046  loss_dice_6: 1.177  loss_ce_7: 0.9723  loss_mask_7: 0.2073  loss_dice_7: 1.274  loss_ce_8: 0.9619  loss_mask_8: 0.2158  loss_dice_8: 1.22    time: 0.3224  last_time: 0.3019  data_time: 0.0068  last_data_time: 0.0092   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:54:59 d2.utils.events]:  eta: 0:51:05  iter: 49959  total_loss: 25.23  loss_ce: 0.9435  loss_mask: 0.09406  loss_dice: 0.953  loss_ce_0: 1.298  loss_mask_0: 0.1626  loss_dice_0: 0.9077  loss_ce_1: 1.253  loss_mask_1: 0.1006  loss_dice_1: 1.201  loss_ce_2: 1.039  loss_mask_2: 0.1351  loss_dice_2: 1.12  loss_ce_3: 0.8708  loss_mask_3: 0.1132  loss_dice_3: 0.9336  loss_ce_4: 0.8381  loss_mask_4: 0.1279  loss_dice_4: 0.9473  loss_ce_5: 0.8776  loss_mask_5: 0.1289  loss_dice_5: 0.8573  loss_ce_6: 0.8466  loss_mask_6: 0.1061  loss_dice_6: 0.9951  loss_ce_7: 0.8487  loss_mask_7: 0.1008  loss_dice_7: 0.9451  loss_ce_8: 0.8941  loss_mask_8: 0.09736  loss_dice_8: 0.9693    time: 0.3224  last_time: 0.3020  data_time: 0.0064  last_data_time: 0.0083   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:05 d2.utils.events]:  eta: 0:50:59  iter: 49979  total_loss: 25.32  loss_ce: 0.9528  loss_mask: 0.1881  loss_dice: 1.258  loss_ce_0: 1.337  loss_mask_0: 0.1758  loss_dice_0: 1.213  loss_ce_1: 1.268  loss_mask_1: 0.1977  loss_dice_1: 1.154  loss_ce_2: 1.198  loss_mask_2: 0.2115  loss_dice_2: 1.202  loss_ce_3: 1  loss_mask_3: 0.1619  loss_dice_3: 1.061  loss_ce_4: 1.002  loss_mask_4: 0.1689  loss_dice_4: 1.042  loss_ce_5: 1.016  loss_mask_5: 0.1482  loss_dice_5: 1.242  loss_ce_6: 0.9473  loss_mask_6: 0.149  loss_dice_6: 1.201  loss_ce_7: 0.9796  loss_mask_7: 0.1368  loss_dice_7: 1.239  loss_ce_8: 0.978  loss_mask_8: 0.1292  loss_dice_8: 0.9695    time: 0.3224  last_time: 0.3349  data_time: 0.0086  last_data_time: 0.0444   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:12 d2.utils.events]:  eta: 0:50:54  iter: 49999  total_loss: 21.28  loss_ce: 1.074  loss_mask: 0.1227  loss_dice: 1.177  loss_ce_0: 1.224  loss_mask_0: 0.2117  loss_dice_0: 0.9902  loss_ce_1: 1.19  loss_mask_1: 0.164  loss_dice_1: 1.153  loss_ce_2: 1.126  loss_mask_2: 0.1383  loss_dice_2: 0.9526  loss_ce_3: 1.076  loss_mask_3: 0.1802  loss_dice_3: 1.204  loss_ce_4: 1.079  loss_mask_4: 0.1713  loss_dice_4: 1.059  loss_ce_5: 1.056  loss_mask_5: 0.1402  loss_dice_5: 1.135  loss_ce_6: 1.077  loss_mask_6: 0.1489  loss_dice_6: 1.104  loss_ce_7: 1.034  loss_mask_7: 0.1363  loss_dice_7: 1.099  loss_ce_8: 1.046  loss_mask_8: 0.1417  loss_dice_8: 1.058    time: 0.3224  last_time: 0.3032  data_time: 0.0076  last_data_time: 0.0114   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:19 d2.utils.events]:  eta: 0:50:46  iter: 50019  total_loss: 20.85  loss_ce: 0.9436  loss_mask: 0.1683  loss_dice: 0.8056  loss_ce_0: 1.165  loss_mask_0: 0.1815  loss_dice_0: 0.8766  loss_ce_1: 1.054  loss_mask_1: 0.1936  loss_dice_1: 0.958  loss_ce_2: 1.062  loss_mask_2: 0.1958  loss_dice_2: 1.13  loss_ce_3: 0.9876  loss_mask_3: 0.1754  loss_dice_3: 0.7488  loss_ce_4: 0.9621  loss_mask_4: 0.1799  loss_dice_4: 0.8293  loss_ce_5: 0.9498  loss_mask_5: 0.1671  loss_dice_5: 0.8397  loss_ce_6: 0.8693  loss_mask_6: 0.17  loss_dice_6: 0.72  loss_ce_7: 0.8944  loss_mask_7: 0.1755  loss_dice_7: 0.8098  loss_ce_8: 0.9693  loss_mask_8: 0.1701  loss_dice_8: 0.7426    time: 0.3224  last_time: 0.2970  data_time: 0.0117  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:25 d2.utils.events]:  eta: 0:50:41  iter: 50039  total_loss: 33.76  loss_ce: 1.338  loss_mask: 0.1958  loss_dice: 1.678  loss_ce_0: 1.586  loss_mask_0: 0.1789  loss_dice_0: 1.736  loss_ce_1: 1.646  loss_mask_1: 0.194  loss_dice_1: 1.644  loss_ce_2: 1.433  loss_mask_2: 0.1724  loss_dice_2: 1.487  loss_ce_3: 1.545  loss_mask_3: 0.1868  loss_dice_3: 1.542  loss_ce_4: 1.542  loss_mask_4: 0.1742  loss_dice_4: 1.481  loss_ce_5: 1.474  loss_mask_5: 0.2196  loss_dice_5: 1.434  loss_ce_6: 1.357  loss_mask_6: 0.2269  loss_dice_6: 1.524  loss_ce_7: 1.423  loss_mask_7: 0.2278  loss_dice_7: 1.565  loss_ce_8: 1.324  loss_mask_8: 0.2024  loss_dice_8: 1.537    time: 0.3224  last_time: 0.3210  data_time: 0.0080  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:31 d2.utils.events]:  eta: 0:50:35  iter: 50059  total_loss: 28.74  loss_ce: 1.322  loss_mask: 0.186  loss_dice: 1.35  loss_ce_0: 1.666  loss_mask_0: 0.2047  loss_dice_0: 1.28  loss_ce_1: 1.602  loss_mask_1: 0.2566  loss_dice_1: 1.383  loss_ce_2: 1.587  loss_mask_2: 0.2305  loss_dice_2: 1.361  loss_ce_3: 1.437  loss_mask_3: 0.1726  loss_dice_3: 1.337  loss_ce_4: 1.355  loss_mask_4: 0.1531  loss_dice_4: 1.314  loss_ce_5: 1.321  loss_mask_5: 0.1718  loss_dice_5: 1.426  loss_ce_6: 1.328  loss_mask_6: 0.1932  loss_dice_6: 1.287  loss_ce_7: 1.314  loss_mask_7: 0.1629  loss_dice_7: 1.126  loss_ce_8: 1.329  loss_mask_8: 0.1983  loss_dice_8: 1.239    time: 0.3224  last_time: 0.3293  data_time: 0.0070  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:37 d2.utils.events]:  eta: 0:50:29  iter: 50079  total_loss: 22.87  loss_ce: 0.9345  loss_mask: 0.1329  loss_dice: 0.8055  loss_ce_0: 1.217  loss_mask_0: 0.1815  loss_dice_0: 1.046  loss_ce_1: 1.075  loss_mask_1: 0.1695  loss_dice_1: 1.051  loss_ce_2: 1.008  loss_mask_2: 0.1699  loss_dice_2: 1.011  loss_ce_3: 0.9307  loss_mask_3: 0.1695  loss_dice_3: 0.9393  loss_ce_4: 0.9536  loss_mask_4: 0.1673  loss_dice_4: 0.9252  loss_ce_5: 0.7813  loss_mask_5: 0.1961  loss_dice_5: 0.888  loss_ce_6: 0.9801  loss_mask_6: 0.1539  loss_dice_6: 0.7721  loss_ce_7: 0.8918  loss_mask_7: 0.16  loss_dice_7: 1.018  loss_ce_8: 0.9271  loss_mask_8: 0.1539  loss_dice_8: 0.7679    time: 0.3224  last_time: 0.3216  data_time: 0.0082  last_data_time: 0.0042   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:44 d2.utils.events]:  eta: 0:50:23  iter: 50099  total_loss: 28.45  loss_ce: 1.121  loss_mask: 0.1375  loss_dice: 1.494  loss_ce_0: 1.215  loss_mask_0: 0.1616  loss_dice_0: 1.03  loss_ce_1: 1.061  loss_mask_1: 0.2026  loss_dice_1: 1.362  loss_ce_2: 1.075  loss_mask_2: 0.2007  loss_dice_2: 1.265  loss_ce_3: 1.107  loss_mask_3: 0.1525  loss_dice_3: 1.233  loss_ce_4: 1.132  loss_mask_4: 0.1692  loss_dice_4: 1.534  loss_ce_5: 1.112  loss_mask_5: 0.1419  loss_dice_5: 1.505  loss_ce_6: 1.075  loss_mask_6: 0.1392  loss_dice_6: 1.272  loss_ce_7: 1.092  loss_mask_7: 0.1386  loss_dice_7: 1.425  loss_ce_8: 1.084  loss_mask_8: 0.1438  loss_dice_8: 1.248    time: 0.3224  last_time: 0.6815  data_time: 0.0305  last_data_time: 0.3724   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:50 d2.utils.events]:  eta: 0:50:18  iter: 50119  total_loss: 28.98  loss_ce: 1.179  loss_mask: 0.1137  loss_dice: 1.425  loss_ce_0: 1.326  loss_mask_0: 0.1071  loss_dice_0: 1.38  loss_ce_1: 1.283  loss_mask_1: 0.1219  loss_dice_1: 1.198  loss_ce_2: 1.228  loss_mask_2: 0.1416  loss_dice_2: 1.564  loss_ce_3: 1.189  loss_mask_3: 0.1209  loss_dice_3: 1.552  loss_ce_4: 1.103  loss_mask_4: 0.1462  loss_dice_4: 1.618  loss_ce_5: 1.088  loss_mask_5: 0.122  loss_dice_5: 1.496  loss_ce_6: 1.154  loss_mask_6: 0.1496  loss_dice_6: 1.47  loss_ce_7: 1.177  loss_mask_7: 0.128  loss_dice_7: 1.655  loss_ce_8: 1.102  loss_mask_8: 0.1356  loss_dice_8: 1.31    time: 0.3224  last_time: 0.2959  data_time: 0.0127  last_data_time: 0.0046   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:55:56 d2.utils.events]:  eta: 0:50:13  iter: 50139  total_loss: 27.35  loss_ce: 0.8721  loss_mask: 0.3315  loss_dice: 1.502  loss_ce_0: 1.281  loss_mask_0: 0.1961  loss_dice_0: 0.9315  loss_ce_1: 1.039  loss_mask_1: 0.3354  loss_dice_1: 1.08  loss_ce_2: 1.021  loss_mask_2: 0.3159  loss_dice_2: 0.9286  loss_ce_3: 0.9082  loss_mask_3: 0.3231  loss_dice_3: 1.314  loss_ce_4: 0.908  loss_mask_4: 0.3266  loss_dice_4: 1.194  loss_ce_5: 0.9055  loss_mask_5: 0.3177  loss_dice_5: 1.413  loss_ce_6: 0.8765  loss_mask_6: 0.2833  loss_dice_6: 1.447  loss_ce_7: 0.8762  loss_mask_7: 0.2923  loss_dice_7: 1.374  loss_ce_8: 0.8886  loss_mask_8: 0.3244  loss_dice_8: 0.9853    time: 0.3224  last_time: 0.3136  data_time: 0.0069  last_data_time: 0.0157   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:03 d2.utils.events]:  eta: 0:50:06  iter: 50159  total_loss: 39.84  loss_ce: 1.686  loss_mask: 0.2262  loss_dice: 1.917  loss_ce_0: 1.568  loss_mask_0: 0.2087  loss_dice_0: 1.837  loss_ce_1: 1.841  loss_mask_1: 0.2081  loss_dice_1: 2.051  loss_ce_2: 1.84  loss_mask_2: 0.2052  loss_dice_2: 1.96  loss_ce_3: 1.671  loss_mask_3: 0.2539  loss_dice_3: 1.913  loss_ce_4: 1.651  loss_mask_4: 0.2395  loss_dice_4: 2.048  loss_ce_5: 1.566  loss_mask_5: 0.2178  loss_dice_5: 2.07  loss_ce_6: 1.591  loss_mask_6: 0.2197  loss_dice_6: 2.138  loss_ce_7: 1.68  loss_mask_7: 0.2307  loss_dice_7: 2.022  loss_ce_8: 1.645  loss_mask_8: 0.2327  loss_dice_8: 2.183    time: 0.3224  last_time: 0.3026  data_time: 0.0126  last_data_time: 0.0080   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:09 d2.utils.events]:  eta: 0:49:59  iter: 50179  total_loss: 35.94  loss_ce: 1.238  loss_mask: 0.1349  loss_dice: 2.038  loss_ce_0: 1.62  loss_mask_0: 0.1649  loss_dice_0: 2  loss_ce_1: 1.502  loss_mask_1: 0.1774  loss_dice_1: 2.003  loss_ce_2: 1.37  loss_mask_2: 0.1818  loss_dice_2: 1.835  loss_ce_3: 1.223  loss_mask_3: 0.2411  loss_dice_3: 1.793  loss_ce_4: 1.231  loss_mask_4: 0.1967  loss_dice_4: 1.887  loss_ce_5: 1.204  loss_mask_5: 0.1996  loss_dice_5: 1.712  loss_ce_6: 1.135  loss_mask_6: 0.1861  loss_dice_6: 1.779  loss_ce_7: 1.19  loss_mask_7: 0.1678  loss_dice_7: 1.901  loss_ce_8: 1.176  loss_mask_8: 0.2057  loss_dice_8: 1.763    time: 0.3224  last_time: 0.2924  data_time: 0.0148  last_data_time: 0.0019   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:16 d2.utils.events]:  eta: 0:49:53  iter: 50199  total_loss: 34.91  loss_ce: 1.516  loss_mask: 0.08245  loss_dice: 1.568  loss_ce_0: 1.797  loss_mask_0: 0.1172  loss_dice_0: 1.448  loss_ce_1: 1.604  loss_mask_1: 0.09937  loss_dice_1: 1.423  loss_ce_2: 1.695  loss_mask_2: 0.08179  loss_dice_2: 1.476  loss_ce_3: 1.486  loss_mask_3: 0.0911  loss_dice_3: 1.645  loss_ce_4: 1.344  loss_mask_4: 0.1149  loss_dice_4: 1.65  loss_ce_5: 1.323  loss_mask_5: 0.08873  loss_dice_5: 1.68  loss_ce_6: 1.607  loss_mask_6: 0.09597  loss_dice_6: 1.554  loss_ce_7: 1.454  loss_mask_7: 0.09259  loss_dice_7: 1.461  loss_ce_8: 1.491  loss_mask_8: 0.07887  loss_dice_8: 1.471    time: 0.3224  last_time: 0.3063  data_time: 0.0075  last_data_time: 0.0093   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:22 d2.utils.events]:  eta: 0:49:47  iter: 50219  total_loss: 28.01  loss_ce: 0.9712  loss_mask: 0.1766  loss_dice: 1.389  loss_ce_0: 1.268  loss_mask_0: 0.2488  loss_dice_0: 1.418  loss_ce_1: 1.185  loss_mask_1: 0.166  loss_dice_1: 1.438  loss_ce_2: 1.116  loss_mask_2: 0.1554  loss_dice_2: 1.544  loss_ce_3: 1.015  loss_mask_3: 0.1942  loss_dice_3: 1.319  loss_ce_4: 0.998  loss_mask_4: 0.1643  loss_dice_4: 1.453  loss_ce_5: 0.998  loss_mask_5: 0.1281  loss_dice_5: 1.195  loss_ce_6: 1.03  loss_mask_6: 0.1342  loss_dice_6: 1.358  loss_ce_7: 0.9431  loss_mask_7: 0.1721  loss_dice_7: 1.435  loss_ce_8: 0.9182  loss_mask_8: 0.1401  loss_dice_8: 1.4    time: 0.3224  last_time: 0.2952  data_time: 0.0071  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:28 d2.utils.events]:  eta: 0:49:41  iter: 50239  total_loss: 30.24  loss_ce: 0.9753  loss_mask: 0.1711  loss_dice: 1.49  loss_ce_0: 1.367  loss_mask_0: 0.1817  loss_dice_0: 1.562  loss_ce_1: 1.355  loss_mask_1: 0.1577  loss_dice_1: 1.384  loss_ce_2: 1.212  loss_mask_2: 0.1548  loss_dice_2: 1.527  loss_ce_3: 0.8785  loss_mask_3: 0.176  loss_dice_3: 1.77  loss_ce_4: 0.9856  loss_mask_4: 0.1458  loss_dice_4: 1.491  loss_ce_5: 0.9621  loss_mask_5: 0.1951  loss_dice_5: 1.82  loss_ce_6: 1.125  loss_mask_6: 0.1786  loss_dice_6: 1.751  loss_ce_7: 1.031  loss_mask_7: 0.1744  loss_dice_7: 1.57  loss_ce_8: 1.02  loss_mask_8: 0.1708  loss_dice_8: 1.666    time: 0.3224  last_time: 0.3021  data_time: 0.0099  last_data_time: 0.0046   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:34 d2.utils.events]:  eta: 0:49:36  iter: 50259  total_loss: 30.33  loss_ce: 1.015  loss_mask: 0.101  loss_dice: 1.614  loss_ce_0: 1.404  loss_mask_0: 0.1045  loss_dice_0: 1.399  loss_ce_1: 1.258  loss_mask_1: 0.113  loss_dice_1: 1.469  loss_ce_2: 1.173  loss_mask_2: 0.1135  loss_dice_2: 1.499  loss_ce_3: 1.084  loss_mask_3: 0.08948  loss_dice_3: 1.363  loss_ce_4: 1.18  loss_mask_4: 0.1043  loss_dice_4: 1.404  loss_ce_5: 0.9912  loss_mask_5: 0.1032  loss_dice_5: 1.404  loss_ce_6: 1.149  loss_mask_6: 0.1047  loss_dice_6: 1.56  loss_ce_7: 1.08  loss_mask_7: 0.103  loss_dice_7: 1.303  loss_ce_8: 1.004  loss_mask_8: 0.08981  loss_dice_8: 1.517    time: 0.3224  last_time: 0.3341  data_time: 0.0190  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:41 d2.utils.events]:  eta: 0:49:31  iter: 50279  total_loss: 30.7  loss_ce: 1.072  loss_mask: 0.1293  loss_dice: 1.376  loss_ce_0: 1.389  loss_mask_0: 0.1115  loss_dice_0: 1.013  loss_ce_1: 1.362  loss_mask_1: 0.1348  loss_dice_1: 1.249  loss_ce_2: 1.184  loss_mask_2: 0.1264  loss_dice_2: 1.271  loss_ce_3: 1.041  loss_mask_3: 0.1156  loss_dice_3: 1.156  loss_ce_4: 1.041  loss_mask_4: 0.1248  loss_dice_4: 1.337  loss_ce_5: 1.055  loss_mask_5: 0.1159  loss_dice_5: 1.335  loss_ce_6: 0.9908  loss_mask_6: 0.1195  loss_dice_6: 1.266  loss_ce_7: 1.043  loss_mask_7: 0.118  loss_dice_7: 1.354  loss_ce_8: 1.063  loss_mask_8: 0.09999  loss_dice_8: 1.503    time: 0.3224  last_time: 0.2995  data_time: 0.0077  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:47 d2.utils.events]:  eta: 0:49:24  iter: 50299  total_loss: 27.17  loss_ce: 0.9341  loss_mask: 0.189  loss_dice: 1.335  loss_ce_0: 1.281  loss_mask_0: 0.1927  loss_dice_0: 1.608  loss_ce_1: 1.248  loss_mask_1: 0.1502  loss_dice_1: 1.251  loss_ce_2: 1.09  loss_mask_2: 0.2129  loss_dice_2: 1.426  loss_ce_3: 0.9466  loss_mask_3: 0.2438  loss_dice_3: 1.278  loss_ce_4: 0.9454  loss_mask_4: 0.2098  loss_dice_4: 1.348  loss_ce_5: 0.8707  loss_mask_5: 0.2065  loss_dice_5: 1.353  loss_ce_6: 0.8693  loss_mask_6: 0.215  loss_dice_6: 1.337  loss_ce_7: 0.9409  loss_mask_7: 0.1911  loss_dice_7: 1.115  loss_ce_8: 0.9326  loss_mask_8: 0.2143  loss_dice_8: 1.213    time: 0.3224  last_time: 0.3156  data_time: 0.0108  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:56:53 d2.utils.events]:  eta: 0:49:18  iter: 50319  total_loss: 27  loss_ce: 1.026  loss_mask: 0.1773  loss_dice: 1.481  loss_ce_0: 1.399  loss_mask_0: 0.2216  loss_dice_0: 1.411  loss_ce_1: 1.335  loss_mask_1: 0.2362  loss_dice_1: 1.1  loss_ce_2: 1.214  loss_mask_2: 0.2394  loss_dice_2: 1.454  loss_ce_3: 1.023  loss_mask_3: 0.1977  loss_dice_3: 1.445  loss_ce_4: 1.087  loss_mask_4: 0.1783  loss_dice_4: 0.9548  loss_ce_5: 1.101  loss_mask_5: 0.1736  loss_dice_5: 1.38  loss_ce_6: 1.021  loss_mask_6: 0.1917  loss_dice_6: 1.4  loss_ce_7: 1.082  loss_mask_7: 0.188  loss_dice_7: 1.262  loss_ce_8: 1.076  loss_mask_8: 0.1659  loss_dice_8: 1.416    time: 0.3224  last_time: 0.3005  data_time: 0.0089  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:00 d2.utils.events]:  eta: 0:49:14  iter: 50339  total_loss: 28.07  loss_ce: 0.9378  loss_mask: 0.2092  loss_dice: 1.091  loss_ce_0: 1.228  loss_mask_0: 0.3216  loss_dice_0: 1.144  loss_ce_1: 1.175  loss_mask_1: 0.2891  loss_dice_1: 1.138  loss_ce_2: 1.154  loss_mask_2: 0.2973  loss_dice_2: 1.41  loss_ce_3: 0.9414  loss_mask_3: 0.2063  loss_dice_3: 1.195  loss_ce_4: 1.018  loss_mask_4: 0.2119  loss_dice_4: 1.186  loss_ce_5: 1.013  loss_mask_5: 0.2187  loss_dice_5: 1.073  loss_ce_6: 0.9405  loss_mask_6: 0.2542  loss_dice_6: 1.107  loss_ce_7: 0.9644  loss_mask_7: 0.2226  loss_dice_7: 1.155  loss_ce_8: 0.9278  loss_mask_8: 0.2293  loss_dice_8: 1.026    time: 0.3224  last_time: 0.3144  data_time: 0.0088  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:06 d2.utils.events]:  eta: 0:49:09  iter: 50359  total_loss: 24.14  loss_ce: 1.127  loss_mask: 0.112  loss_dice: 1.055  loss_ce_0: 1.417  loss_mask_0: 0.1474  loss_dice_0: 1.197  loss_ce_1: 1.231  loss_mask_1: 0.1268  loss_dice_1: 1.234  loss_ce_2: 1.256  loss_mask_2: 0.1164  loss_dice_2: 1.05  loss_ce_3: 1.284  loss_mask_3: 0.1069  loss_dice_3: 0.9931  loss_ce_4: 1.145  loss_mask_4: 0.1238  loss_dice_4: 1.076  loss_ce_5: 1.021  loss_mask_5: 0.1068  loss_dice_5: 0.9772  loss_ce_6: 1.12  loss_mask_6: 0.1085  loss_dice_6: 0.8789  loss_ce_7: 1.023  loss_mask_7: 0.09545  loss_dice_7: 0.9826  loss_ce_8: 1.165  loss_mask_8: 0.09765  loss_dice_8: 0.8777    time: 0.3223  last_time: 0.3025  data_time: 0.0068  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:12 d2.utils.events]:  eta: 0:49:02  iter: 50379  total_loss: 25.56  loss_ce: 0.8472  loss_mask: 0.1283  loss_dice: 0.926  loss_ce_0: 1.3  loss_mask_0: 0.1108  loss_dice_0: 1.204  loss_ce_1: 1.185  loss_mask_1: 0.1356  loss_dice_1: 0.9838  loss_ce_2: 1.002  loss_mask_2: 0.1041  loss_dice_2: 1.12  loss_ce_3: 0.9006  loss_mask_3: 0.1242  loss_dice_3: 0.9504  loss_ce_4: 1.003  loss_mask_4: 0.1217  loss_dice_4: 1.243  loss_ce_5: 0.9021  loss_mask_5: 0.09727  loss_dice_5: 1.018  loss_ce_6: 0.7678  loss_mask_6: 0.1291  loss_dice_6: 1.034  loss_ce_7: 0.8716  loss_mask_7: 0.1109  loss_dice_7: 0.8671  loss_ce_8: 0.8001  loss_mask_8: 0.1126  loss_dice_8: 1.047    time: 0.3223  last_time: 0.3046  data_time: 0.0078  last_data_time: 0.0110   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:18 d2.utils.events]:  eta: 0:48:57  iter: 50399  total_loss: 28.58  loss_ce: 1.017  loss_mask: 0.08982  loss_dice: 1.463  loss_ce_0: 1.218  loss_mask_0: 0.1367  loss_dice_0: 1.46  loss_ce_1: 1.197  loss_mask_1: 0.1453  loss_dice_1: 1.398  loss_ce_2: 1.346  loss_mask_2: 0.1321  loss_dice_2: 1.404  loss_ce_3: 1.138  loss_mask_3: 0.09478  loss_dice_3: 1.429  loss_ce_4: 1.113  loss_mask_4: 0.09532  loss_dice_4: 1.534  loss_ce_5: 1.09  loss_mask_5: 0.09988  loss_dice_5: 1.439  loss_ce_6: 1.092  loss_mask_6: 0.09704  loss_dice_6: 1.425  loss_ce_7: 1.187  loss_mask_7: 0.08907  loss_dice_7: 1.294  loss_ce_8: 1.045  loss_mask_8: 0.08834  loss_dice_8: 1.457    time: 0.3223  last_time: 0.3145  data_time: 0.0067  last_data_time: 0.0040   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:24 d2.utils.events]:  eta: 0:48:51  iter: 50419  total_loss: 22.11  loss_ce: 0.9403  loss_mask: 0.1046  loss_dice: 1.162  loss_ce_0: 1.13  loss_mask_0: 0.1188  loss_dice_0: 1.089  loss_ce_1: 1.077  loss_mask_1: 0.1144  loss_dice_1: 0.8768  loss_ce_2: 0.9826  loss_mask_2: 0.1363  loss_dice_2: 0.9597  loss_ce_3: 0.9896  loss_mask_3: 0.1258  loss_dice_3: 0.9148  loss_ce_4: 1.021  loss_mask_4: 0.1215  loss_dice_4: 1.108  loss_ce_5: 0.9936  loss_mask_5: 0.118  loss_dice_5: 1.123  loss_ce_6: 0.9732  loss_mask_6: 0.125  loss_dice_6: 1.137  loss_ce_7: 0.8763  loss_mask_7: 0.1169  loss_dice_7: 1.085  loss_ce_8: 1.028  loss_mask_8: 0.1009  loss_dice_8: 0.7545    time: 0.3223  last_time: 0.3016  data_time: 0.0067  last_data_time: 0.0047   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:30 d2.utils.events]:  eta: 0:48:45  iter: 50439  total_loss: 27.53  loss_ce: 1.123  loss_mask: 0.1564  loss_dice: 1.564  loss_ce_0: 1.442  loss_mask_0: 0.2127  loss_dice_0: 1.49  loss_ce_1: 1.36  loss_mask_1: 0.2489  loss_dice_1: 1.471  loss_ce_2: 1.052  loss_mask_2: 0.2509  loss_dice_2: 1.456  loss_ce_3: 1.01  loss_mask_3: 0.2235  loss_dice_3: 1.478  loss_ce_4: 1.059  loss_mask_4: 0.2063  loss_dice_4: 1.461  loss_ce_5: 0.9918  loss_mask_5: 0.2088  loss_dice_5: 1.308  loss_ce_6: 0.9969  loss_mask_6: 0.2125  loss_dice_6: 1.515  loss_ce_7: 1.011  loss_mask_7: 0.2041  loss_dice_7: 1.486  loss_ce_8: 1.054  loss_mask_8: 0.1485  loss_dice_8: 1.514    time: 0.3223  last_time: 0.3226  data_time: 0.0066  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:37 d2.utils.events]:  eta: 0:48:39  iter: 50459  total_loss: 27.1  loss_ce: 1.083  loss_mask: 0.1166  loss_dice: 1.231  loss_ce_0: 1.39  loss_mask_0: 0.1332  loss_dice_0: 1.271  loss_ce_1: 1.29  loss_mask_1: 0.1349  loss_dice_1: 1.473  loss_ce_2: 1.134  loss_mask_2: 0.1415  loss_dice_2: 1.192  loss_ce_3: 0.9687  loss_mask_3: 0.1143  loss_dice_3: 1.123  loss_ce_4: 1.023  loss_mask_4: 0.1109  loss_dice_4: 1.194  loss_ce_5: 1.155  loss_mask_5: 0.1074  loss_dice_5: 1.204  loss_ce_6: 1.1  loss_mask_6: 0.1165  loss_dice_6: 1.248  loss_ce_7: 0.9655  loss_mask_7: 0.1388  loss_dice_7: 1.1  loss_ce_8: 1.063  loss_mask_8: 0.1282  loss_dice_8: 1.236    time: 0.3223  last_time: 0.2964  data_time: 0.0092  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:43 d2.utils.events]:  eta: 0:48:32  iter: 50479  total_loss: 22.93  loss_ce: 0.6609  loss_mask: 0.1114  loss_dice: 1.224  loss_ce_0: 1.144  loss_mask_0: 0.09626  loss_dice_0: 1.274  loss_ce_1: 1.04  loss_mask_1: 0.1072  loss_dice_1: 1.246  loss_ce_2: 0.8208  loss_mask_2: 0.101  loss_dice_2: 1.189  loss_ce_3: 0.7298  loss_mask_3: 0.1013  loss_dice_3: 1.187  loss_ce_4: 0.7595  loss_mask_4: 0.0923  loss_dice_4: 1.361  loss_ce_5: 0.7942  loss_mask_5: 0.1087  loss_dice_5: 1.178  loss_ce_6: 0.8214  loss_mask_6: 0.09968  loss_dice_6: 1.198  loss_ce_7: 0.6431  loss_mask_7: 0.111  loss_dice_7: 1.207  loss_ce_8: 0.6531  loss_mask_8: 0.1172  loss_dice_8: 1.192    time: 0.3223  last_time: 0.2958  data_time: 0.0072  last_data_time: 0.0047   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:49 d2.utils.events]:  eta: 0:48:27  iter: 50499  total_loss: 24.61  loss_ce: 1.081  loss_mask: 0.1109  loss_dice: 1.187  loss_ce_0: 1.307  loss_mask_0: 0.1474  loss_dice_0: 1.017  loss_ce_1: 1.242  loss_mask_1: 0.1327  loss_dice_1: 1.098  loss_ce_2: 1.217  loss_mask_2: 0.1154  loss_dice_2: 1.304  loss_ce_3: 1.134  loss_mask_3: 0.1219  loss_dice_3: 1.219  loss_ce_4: 1.057  loss_mask_4: 0.1038  loss_dice_4: 1.56  loss_ce_5: 1.041  loss_mask_5: 0.1133  loss_dice_5: 1.284  loss_ce_6: 1.077  loss_mask_6: 0.1068  loss_dice_6: 1.582  loss_ce_7: 1.096  loss_mask_7: 0.1272  loss_dice_7: 1.279  loss_ce_8: 1.072  loss_mask_8: 0.119  loss_dice_8: 1.148    time: 0.3223  last_time: 0.3047  data_time: 0.0073  last_data_time: 0.0045   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:57:56 d2.utils.events]:  eta: 0:48:20  iter: 50519  total_loss: 31.97  loss_ce: 1.175  loss_mask: 0.3069  loss_dice: 1.461  loss_ce_0: 1.679  loss_mask_0: 0.3704  loss_dice_0: 1.312  loss_ce_1: 1.45  loss_mask_1: 0.3402  loss_dice_1: 1.491  loss_ce_2: 1.43  loss_mask_2: 0.3617  loss_dice_2: 1.467  loss_ce_3: 1.154  loss_mask_3: 0.2884  loss_dice_3: 1.483  loss_ce_4: 1.117  loss_mask_4: 0.3185  loss_dice_4: 1.43  loss_ce_5: 1.111  loss_mask_5: 0.3504  loss_dice_5: 1.421  loss_ce_6: 1.185  loss_mask_6: 0.3529  loss_dice_6: 1.503  loss_ce_7: 1.186  loss_mask_7: 0.342  loss_dice_7: 1.48  loss_ce_8: 1.174  loss_mask_8: 0.3119  loss_dice_8: 1.305    time: 0.3223  last_time: 0.3125  data_time: 0.0072  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:02 d2.utils.events]:  eta: 0:48:13  iter: 50539  total_loss: 19.72  loss_ce: 0.8347  loss_mask: 0.1044  loss_dice: 1.062  loss_ce_0: 1.267  loss_mask_0: 0.1122  loss_dice_0: 1.038  loss_ce_1: 1.095  loss_mask_1: 0.08181  loss_dice_1: 1.011  loss_ce_2: 0.9453  loss_mask_2: 0.07947  loss_dice_2: 0.8865  loss_ce_3: 0.8198  loss_mask_3: 0.08787  loss_dice_3: 0.8791  loss_ce_4: 0.7933  loss_mask_4: 0.09425  loss_dice_4: 0.8215  loss_ce_5: 0.7335  loss_mask_5: 0.1034  loss_dice_5: 0.7481  loss_ce_6: 0.7699  loss_mask_6: 0.1213  loss_dice_6: 0.7908  loss_ce_7: 0.7638  loss_mask_7: 0.09383  loss_dice_7: 0.8044  loss_ce_8: 0.7543  loss_mask_8: 0.0838  loss_dice_8: 0.7362    time: 0.3223  last_time: 0.2933  data_time: 0.0068  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:08 d2.utils.events]:  eta: 0:48:04  iter: 50559  total_loss: 32.6  loss_ce: 1.298  loss_mask: 0.2955  loss_dice: 1.3  loss_ce_0: 1.387  loss_mask_0: 0.2163  loss_dice_0: 1.325  loss_ce_1: 1.43  loss_mask_1: 0.2418  loss_dice_1: 1.457  loss_ce_2: 1.395  loss_mask_2: 0.2012  loss_dice_2: 1.374  loss_ce_3: 1.17  loss_mask_3: 0.2562  loss_dice_3: 1.175  loss_ce_4: 1.224  loss_mask_4: 0.2558  loss_dice_4: 1.117  loss_ce_5: 1.112  loss_mask_5: 0.2699  loss_dice_5: 1.211  loss_ce_6: 1.21  loss_mask_6: 0.3024  loss_dice_6: 1.282  loss_ce_7: 1.055  loss_mask_7: 0.2858  loss_dice_7: 1.331  loss_ce_8: 1.14  loss_mask_8: 0.2599  loss_dice_8: 1.207    time: 0.3223  last_time: 0.3278  data_time: 0.0074  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:14 d2.utils.events]:  eta: 0:47:58  iter: 50579  total_loss: 32.2  loss_ce: 1.146  loss_mask: 0.1158  loss_dice: 1.442  loss_ce_0: 1.32  loss_mask_0: 0.1448  loss_dice_0: 1.467  loss_ce_1: 1.393  loss_mask_1: 0.2251  loss_dice_1: 1.881  loss_ce_2: 1.305  loss_mask_2: 0.2063  loss_dice_2: 1.475  loss_ce_3: 1.121  loss_mask_3: 0.137  loss_dice_3: 1.428  loss_ce_4: 1.117  loss_mask_4: 0.1356  loss_dice_4: 1.739  loss_ce_5: 1.061  loss_mask_5: 0.1397  loss_dice_5: 1.557  loss_ce_6: 1.111  loss_mask_6: 0.1037  loss_dice_6: 1.528  loss_ce_7: 1.051  loss_mask_7: 0.09739  loss_dice_7: 1.702  loss_ce_8: 1.044  loss_mask_8: 0.1238  loss_dice_8: 1.476    time: 0.3223  last_time: 0.3023  data_time: 0.0120  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:21 d2.utils.events]:  eta: 0:47:53  iter: 50599  total_loss: 28.69  loss_ce: 0.9865  loss_mask: 0.06426  loss_dice: 1.561  loss_ce_0: 1.163  loss_mask_0: 0.09344  loss_dice_0: 1.61  loss_ce_1: 1.418  loss_mask_1: 0.09948  loss_dice_1: 1.493  loss_ce_2: 1.398  loss_mask_2: 0.06515  loss_dice_2: 1.465  loss_ce_3: 1.05  loss_mask_3: 0.06211  loss_dice_3: 1.468  loss_ce_4: 1.133  loss_mask_4: 0.06953  loss_dice_4: 1.36  loss_ce_5: 1.16  loss_mask_5: 0.06127  loss_dice_5: 1.482  loss_ce_6: 0.9762  loss_mask_6: 0.06234  loss_dice_6: 1.471  loss_ce_7: 1.072  loss_mask_7: 0.0683  loss_dice_7: 1.439  loss_ce_8: 1.121  loss_mask_8: 0.07667  loss_dice_8: 1.424    time: 0.3223  last_time: 0.3006  data_time: 0.0075  last_data_time: 0.0066   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:27 d2.utils.events]:  eta: 0:47:46  iter: 50619  total_loss: 31.83  loss_ce: 1.215  loss_mask: 0.07987  loss_dice: 1.519  loss_ce_0: 1.353  loss_mask_0: 0.117  loss_dice_0: 1.543  loss_ce_1: 1.333  loss_mask_1: 0.1248  loss_dice_1: 1.596  loss_ce_2: 1.313  loss_mask_2: 0.1178  loss_dice_2: 1.49  loss_ce_3: 1.088  loss_mask_3: 0.1083  loss_dice_3: 1.591  loss_ce_4: 1.139  loss_mask_4: 0.09841  loss_dice_4: 1.373  loss_ce_5: 1.105  loss_mask_5: 0.09899  loss_dice_5: 1.514  loss_ce_6: 1.055  loss_mask_6: 0.0961  loss_dice_6: 1.665  loss_ce_7: 1.193  loss_mask_7: 0.09832  loss_dice_7: 1.437  loss_ce_8: 1.128  loss_mask_8: 0.09412  loss_dice_8: 1.56    time: 0.3223  last_time: 0.3309  data_time: 0.0073  last_data_time: 0.0079   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:34 d2.utils.events]:  eta: 0:47:42  iter: 50639  total_loss: 24.81  loss_ce: 0.943  loss_mask: 0.1756  loss_dice: 1.143  loss_ce_0: 1.151  loss_mask_0: 0.3182  loss_dice_0: 1.348  loss_ce_1: 1.16  loss_mask_1: 0.2375  loss_dice_1: 1.413  loss_ce_2: 1.048  loss_mask_2: 0.2055  loss_dice_2: 1.238  loss_ce_3: 0.9374  loss_mask_3: 0.1522  loss_dice_3: 1.334  loss_ce_4: 1.007  loss_mask_4: 0.1456  loss_dice_4: 1.346  loss_ce_5: 0.936  loss_mask_5: 0.1797  loss_dice_5: 1.006  loss_ce_6: 0.9076  loss_mask_6: 0.1771  loss_dice_6: 1.172  loss_ce_7: 0.9654  loss_mask_7: 0.1716  loss_dice_7: 1.297  loss_ce_8: 0.9703  loss_mask_8: 0.1645  loss_dice_8: 1.059    time: 0.3223  last_time: 0.3040  data_time: 0.0166  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:40 d2.utils.events]:  eta: 0:47:37  iter: 50659  total_loss: 33.18  loss_ce: 1.092  loss_mask: 0.1907  loss_dice: 1.395  loss_ce_0: 1.244  loss_mask_0: 0.3297  loss_dice_0: 1.492  loss_ce_1: 1.451  loss_mask_1: 0.2169  loss_dice_1: 1.553  loss_ce_2: 1.257  loss_mask_2: 0.2272  loss_dice_2: 1.506  loss_ce_3: 1.051  loss_mask_3: 0.2653  loss_dice_3: 1.546  loss_ce_4: 1.179  loss_mask_4: 0.2406  loss_dice_4: 1.474  loss_ce_5: 1.105  loss_mask_5: 0.2004  loss_dice_5: 1.504  loss_ce_6: 1.174  loss_mask_6: 0.1965  loss_dice_6: 1.473  loss_ce_7: 1.111  loss_mask_7: 0.204  loss_dice_7: 1.604  loss_ce_8: 1.045  loss_mask_8: 0.2075  loss_dice_8: 1.488    time: 0.3223  last_time: 0.2983  data_time: 0.0070  last_data_time: 0.0024   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:46 d2.utils.events]:  eta: 0:47:28  iter: 50679  total_loss: 28.59  loss_ce: 0.9945  loss_mask: 0.2077  loss_dice: 1.139  loss_ce_0: 1.324  loss_mask_0: 0.2998  loss_dice_0: 1.055  loss_ce_1: 1.312  loss_mask_1: 0.2806  loss_dice_1: 1.386  loss_ce_2: 1.082  loss_mask_2: 0.3293  loss_dice_2: 1.225  loss_ce_3: 1.01  loss_mask_3: 0.2413  loss_dice_3: 1.344  loss_ce_4: 0.9557  loss_mask_4: 0.2168  loss_dice_4: 1.235  loss_ce_5: 0.9999  loss_mask_5: 0.2214  loss_dice_5: 1.235  loss_ce_6: 0.9906  loss_mask_6: 0.2176  loss_dice_6: 1.121  loss_ce_7: 0.9929  loss_mask_7: 0.2631  loss_dice_7: 1.292  loss_ce_8: 0.9755  loss_mask_8: 0.2045  loss_dice_8: 1.132    time: 0.3223  last_time: 0.3279  data_time: 0.0135  last_data_time: 0.0147   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:52 d2.utils.events]:  eta: 0:47:23  iter: 50699  total_loss: 30.96  loss_ce: 0.8422  loss_mask: 0.1418  loss_dice: 1.532  loss_ce_0: 0.9852  loss_mask_0: 0.2791  loss_dice_0: 1.333  loss_ce_1: 0.8639  loss_mask_1: 0.2461  loss_dice_1: 1.38  loss_ce_2: 0.7687  loss_mask_2: 0.2089  loss_dice_2: 1.524  loss_ce_3: 0.8488  loss_mask_3: 0.2018  loss_dice_3: 1.344  loss_ce_4: 0.8694  loss_mask_4: 0.2376  loss_dice_4: 1.774  loss_ce_5: 0.8142  loss_mask_5: 0.2235  loss_dice_5: 1.535  loss_ce_6: 0.8315  loss_mask_6: 0.1375  loss_dice_6: 1.394  loss_ce_7: 0.8356  loss_mask_7: 0.1552  loss_dice_7: 1.372  loss_ce_8: 0.7764  loss_mask_8: 0.1712  loss_dice_8: 1.548    time: 0.3223  last_time: 0.3079  data_time: 0.0074  last_data_time: 0.0114   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:58:59 d2.utils.events]:  eta: 0:47:19  iter: 50719  total_loss: 32.39  loss_ce: 1.045  loss_mask: 0.2365  loss_dice: 1.453  loss_ce_0: 1.19  loss_mask_0: 0.1974  loss_dice_0: 1.384  loss_ce_1: 1.191  loss_mask_1: 0.1793  loss_dice_1: 1.624  loss_ce_2: 1.088  loss_mask_2: 0.1532  loss_dice_2: 1.681  loss_ce_3: 1.064  loss_mask_3: 0.1373  loss_dice_3: 1.783  loss_ce_4: 1.029  loss_mask_4: 0.107  loss_dice_4: 1.804  loss_ce_5: 1.065  loss_mask_5: 0.1148  loss_dice_5: 1.619  loss_ce_6: 1.049  loss_mask_6: 0.222  loss_dice_6: 1.475  loss_ce_7: 1.028  loss_mask_7: 0.2158  loss_dice_7: 1.697  loss_ce_8: 1.121  loss_mask_8: 0.2266  loss_dice_8: 1.463    time: 0.3223  last_time: 0.3534  data_time: 0.0104  last_data_time: 0.0251   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:05 d2.utils.events]:  eta: 0:47:11  iter: 50739  total_loss: 37.57  loss_ce: 1.351  loss_mask: 0.277  loss_dice: 1.735  loss_ce_0: 1.462  loss_mask_0: 0.3652  loss_dice_0: 1.58  loss_ce_1: 1.588  loss_mask_1: 0.3352  loss_dice_1: 1.824  loss_ce_2: 1.495  loss_mask_2: 0.2846  loss_dice_2: 1.98  loss_ce_3: 1.405  loss_mask_3: 0.2898  loss_dice_3: 1.864  loss_ce_4: 1.381  loss_mask_4: 0.2947  loss_dice_4: 1.878  loss_ce_5: 1.464  loss_mask_5: 0.2854  loss_dice_5: 1.867  loss_ce_6: 1.343  loss_mask_6: 0.3047  loss_dice_6: 1.705  loss_ce_7: 1.323  loss_mask_7: 0.272  loss_dice_7: 1.798  loss_ce_8: 1.309  loss_mask_8: 0.2853  loss_dice_8: 1.72    time: 0.3223  last_time: 0.3017  data_time: 0.0075  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:11 d2.utils.events]:  eta: 0:47:05  iter: 50759  total_loss: 28.46  loss_ce: 1.048  loss_mask: 0.09933  loss_dice: 1.392  loss_ce_0: 1.213  loss_mask_0: 0.1528  loss_dice_0: 1.248  loss_ce_1: 1.164  loss_mask_1: 0.1343  loss_dice_1: 1.406  loss_ce_2: 1.101  loss_mask_2: 0.1553  loss_dice_2: 1.339  loss_ce_3: 1.054  loss_mask_3: 0.1381  loss_dice_3: 1.369  loss_ce_4: 1.051  loss_mask_4: 0.1044  loss_dice_4: 1.222  loss_ce_5: 1.048  loss_mask_5: 0.1178  loss_dice_5: 1.509  loss_ce_6: 1.041  loss_mask_6: 0.112  loss_dice_6: 1.402  loss_ce_7: 1.006  loss_mask_7: 0.09879  loss_dice_7: 1.405  loss_ce_8: 1.042  loss_mask_8: 0.09452  loss_dice_8: 1.338    time: 0.3223  last_time: 0.3262  data_time: 0.0117  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:18 d2.utils.events]:  eta: 0:46:58  iter: 50779  total_loss: 29.3  loss_ce: 1.228  loss_mask: 0.153  loss_dice: 1.306  loss_ce_0: 1.539  loss_mask_0: 0.1472  loss_dice_0: 1.41  loss_ce_1: 1.496  loss_mask_1: 0.156  loss_dice_1: 1.328  loss_ce_2: 1.416  loss_mask_2: 0.1633  loss_dice_2: 1.297  loss_ce_3: 1.258  loss_mask_3: 0.137  loss_dice_3: 1.35  loss_ce_4: 1.313  loss_mask_4: 0.1286  loss_dice_4: 1.417  loss_ce_5: 1.305  loss_mask_5: 0.1549  loss_dice_5: 1.31  loss_ce_6: 1.311  loss_mask_6: 0.156  loss_dice_6: 1.308  loss_ce_7: 1.206  loss_mask_7: 0.1241  loss_dice_7: 1.377  loss_ce_8: 1.28  loss_mask_8: 0.1227  loss_dice_8: 1.481    time: 0.3223  last_time: 0.3027  data_time: 0.0120  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:24 d2.utils.events]:  eta: 0:46:52  iter: 50799  total_loss: 23.3  loss_ce: 1.052  loss_mask: 0.2239  loss_dice: 0.9035  loss_ce_0: 1.121  loss_mask_0: 0.2293  loss_dice_0: 1.163  loss_ce_1: 1.248  loss_mask_1: 0.1737  loss_dice_1: 0.9181  loss_ce_2: 1.238  loss_mask_2: 0.1833  loss_dice_2: 1.006  loss_ce_3: 1.199  loss_mask_3: 0.1868  loss_dice_3: 0.9308  loss_ce_4: 1.207  loss_mask_4: 0.1948  loss_dice_4: 0.9831  loss_ce_5: 1.144  loss_mask_5: 0.2045  loss_dice_5: 1.074  loss_ce_6: 1.104  loss_mask_6: 0.2301  loss_dice_6: 0.8854  loss_ce_7: 1.185  loss_mask_7: 0.2178  loss_dice_7: 0.9167  loss_ce_8: 1.105  loss_mask_8: 0.2221  loss_dice_8: 0.9359    time: 0.3223  last_time: 0.3052  data_time: 0.0081  last_data_time: 0.0093   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:30 d2.utils.events]:  eta: 0:46:44  iter: 50819  total_loss: 20.94  loss_ce: 0.933  loss_mask: 0.1267  loss_dice: 1.033  loss_ce_0: 1.194  loss_mask_0: 0.1522  loss_dice_0: 1.076  loss_ce_1: 1.076  loss_mask_1: 0.1323  loss_dice_1: 0.9684  loss_ce_2: 1.07  loss_mask_2: 0.1403  loss_dice_2: 0.9991  loss_ce_3: 0.9487  loss_mask_3: 0.1334  loss_dice_3: 1.002  loss_ce_4: 0.898  loss_mask_4: 0.136  loss_dice_4: 0.9758  loss_ce_5: 0.9037  loss_mask_5: 0.1471  loss_dice_5: 1.008  loss_ce_6: 0.9476  loss_mask_6: 0.135  loss_dice_6: 0.926  loss_ce_7: 0.9281  loss_mask_7: 0.1361  loss_dice_7: 1.022  loss_ce_8: 0.9726  loss_mask_8: 0.1261  loss_dice_8: 0.988    time: 0.3223  last_time: 0.3047  data_time: 0.0099  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:36 d2.utils.events]:  eta: 0:46:38  iter: 50839  total_loss: 31.59  loss_ce: 1.205  loss_mask: 0.1275  loss_dice: 1.535  loss_ce_0: 1.363  loss_mask_0: 0.1656  loss_dice_0: 1.533  loss_ce_1: 1.345  loss_mask_1: 0.1581  loss_dice_1: 1.553  loss_ce_2: 1.333  loss_mask_2: 0.1828  loss_dice_2: 1.674  loss_ce_3: 1.136  loss_mask_3: 0.1514  loss_dice_3: 1.612  loss_ce_4: 1.112  loss_mask_4: 0.1241  loss_dice_4: 1.553  loss_ce_5: 1.158  loss_mask_5: 0.1206  loss_dice_5: 1.667  loss_ce_6: 1.12  loss_mask_6: 0.1086  loss_dice_6: 1.47  loss_ce_7: 1.227  loss_mask_7: 0.1126  loss_dice_7: 1.573  loss_ce_8: 1.105  loss_mask_8: 0.127  loss_dice_8: 1.479    time: 0.3223  last_time: 0.3064  data_time: 0.0070  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:43 d2.utils.events]:  eta: 0:46:31  iter: 50859  total_loss: 24.41  loss_ce: 1.019  loss_mask: 0.1069  loss_dice: 1.18  loss_ce_0: 1.239  loss_mask_0: 0.1277  loss_dice_0: 1.046  loss_ce_1: 1.268  loss_mask_1: 0.1083  loss_dice_1: 1.135  loss_ce_2: 1.27  loss_mask_2: 0.1155  loss_dice_2: 1.354  loss_ce_3: 1.08  loss_mask_3: 0.1042  loss_dice_3: 1.021  loss_ce_4: 1.014  loss_mask_4: 0.1079  loss_dice_4: 1.3  loss_ce_5: 1.044  loss_mask_5: 0.1053  loss_dice_5: 1.175  loss_ce_6: 1.039  loss_mask_6: 0.103  loss_dice_6: 1.096  loss_ce_7: 1.027  loss_mask_7: 0.08673  loss_dice_7: 1.306  loss_ce_8: 1.067  loss_mask_8: 0.08936  loss_dice_8: 1.031    time: 0.3223  last_time: 0.3005  data_time: 0.0064  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:49 d2.utils.events]:  eta: 0:46:25  iter: 50879  total_loss: 26.62  loss_ce: 1.088  loss_mask: 0.2632  loss_dice: 1.177  loss_ce_0: 1.111  loss_mask_0: 0.2472  loss_dice_0: 1.362  loss_ce_1: 1.231  loss_mask_1: 0.2147  loss_dice_1: 1.59  loss_ce_2: 1.221  loss_mask_2: 0.2796  loss_dice_2: 1.362  loss_ce_3: 1.125  loss_mask_3: 0.2924  loss_dice_3: 1.261  loss_ce_4: 1.086  loss_mask_4: 0.2829  loss_dice_4: 1.21  loss_ce_5: 1.163  loss_mask_5: 0.2658  loss_dice_5: 1.167  loss_ce_6: 1.11  loss_mask_6: 0.242  loss_dice_6: 1.174  loss_ce_7: 1.115  loss_mask_7: 0.2372  loss_dice_7: 1.115  loss_ce_8: 1.121  loss_mask_8: 0.2558  loss_dice_8: 1.137    time: 0.3223  last_time: 0.2969  data_time: 0.0077  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 20:59:55 d2.utils.events]:  eta: 0:46:19  iter: 50899  total_loss: 30.77  loss_ce: 1.145  loss_mask: 0.1279  loss_dice: 1.185  loss_ce_0: 1.564  loss_mask_0: 0.1172  loss_dice_0: 1.53  loss_ce_1: 1.422  loss_mask_1: 0.126  loss_dice_1: 1.541  loss_ce_2: 1.188  loss_mask_2: 0.141  loss_dice_2: 1.682  loss_ce_3: 1.146  loss_mask_3: 0.1337  loss_dice_3: 1.606  loss_ce_4: 1.108  loss_mask_4: 0.1739  loss_dice_4: 1.702  loss_ce_5: 1.218  loss_mask_5: 0.1856  loss_dice_5: 1.586  loss_ce_6: 1.202  loss_mask_6: 0.1344  loss_dice_6: 1.468  loss_ce_7: 1.224  loss_mask_7: 0.1172  loss_dice_7: 1.594  loss_ce_8: 1.201  loss_mask_8: 0.1351  loss_dice_8: 1.245    time: 0.3222  last_time: 0.3014  data_time: 0.0081  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:01 d2.utils.events]:  eta: 0:46:13  iter: 50919  total_loss: 24.74  loss_ce: 1.166  loss_mask: 0.1262  loss_dice: 0.9112  loss_ce_0: 1.354  loss_mask_0: 0.1515  loss_dice_0: 0.9548  loss_ce_1: 1.322  loss_mask_1: 0.1419  loss_dice_1: 0.9476  loss_ce_2: 1.178  loss_mask_2: 0.1424  loss_dice_2: 0.8977  loss_ce_3: 1.04  loss_mask_3: 0.1316  loss_dice_3: 0.8326  loss_ce_4: 1.055  loss_mask_4: 0.1346  loss_dice_4: 0.8823  loss_ce_5: 1.185  loss_mask_5: 0.1294  loss_dice_5: 0.8244  loss_ce_6: 0.8766  loss_mask_6: 0.1293  loss_dice_6: 0.9961  loss_ce_7: 0.9408  loss_mask_7: 0.1279  loss_dice_7: 1.007  loss_ce_8: 1.193  loss_mask_8: 0.1317  loss_dice_8: 0.8315    time: 0.3222  last_time: 0.2931  data_time: 0.0068  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:08 d2.utils.events]:  eta: 0:46:07  iter: 50939  total_loss: 27.58  loss_ce: 1.01  loss_mask: 0.1166  loss_dice: 1.341  loss_ce_0: 1.382  loss_mask_0: 0.1621  loss_dice_0: 1.35  loss_ce_1: 1.281  loss_mask_1: 0.2176  loss_dice_1: 1.262  loss_ce_2: 1.216  loss_mask_2: 0.2576  loss_dice_2: 1.477  loss_ce_3: 1.06  loss_mask_3: 0.1934  loss_dice_3: 1.233  loss_ce_4: 1.034  loss_mask_4: 0.1999  loss_dice_4: 1.437  loss_ce_5: 1.027  loss_mask_5: 0.175  loss_dice_5: 1.286  loss_ce_6: 1.133  loss_mask_6: 0.1001  loss_dice_6: 1.233  loss_ce_7: 1.037  loss_mask_7: 0.1173  loss_dice_7: 1.266  loss_ce_8: 1.056  loss_mask_8: 0.1831  loss_dice_8: 1.205    time: 0.3222  last_time: 0.3284  data_time: 0.0065  last_data_time: 0.0079   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:14 d2.utils.events]:  eta: 0:46:03  iter: 50959  total_loss: 28.1  loss_ce: 1.193  loss_mask: 0.2257  loss_dice: 1.381  loss_ce_0: 1.495  loss_mask_0: 0.1793  loss_dice_0: 1.736  loss_ce_1: 1.364  loss_mask_1: 0.1635  loss_dice_1: 1.441  loss_ce_2: 1.251  loss_mask_2: 0.1934  loss_dice_2: 1.616  loss_ce_3: 1.179  loss_mask_3: 0.1911  loss_dice_3: 1.363  loss_ce_4: 1.147  loss_mask_4: 0.1848  loss_dice_4: 1.312  loss_ce_5: 1.21  loss_mask_5: 0.2244  loss_dice_5: 1.131  loss_ce_6: 1.13  loss_mask_6: 0.2276  loss_dice_6: 1.319  loss_ce_7: 1.132  loss_mask_7: 0.2288  loss_dice_7: 1.218  loss_ce_8: 1.172  loss_mask_8: 0.2558  loss_dice_8: 1.363    time: 0.3222  last_time: 0.3142  data_time: 0.0076  last_data_time: 0.0072   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:20 d2.utils.events]:  eta: 0:45:58  iter: 50979  total_loss: 33.37  loss_ce: 1.228  loss_mask: 0.2145  loss_dice: 1.862  loss_ce_0: 1.269  loss_mask_0: 0.193  loss_dice_0: 1.583  loss_ce_1: 1.293  loss_mask_1: 0.2721  loss_dice_1: 1.758  loss_ce_2: 1.354  loss_mask_2: 0.2975  loss_dice_2: 1.789  loss_ce_3: 1.281  loss_mask_3: 0.2008  loss_dice_3: 1.904  loss_ce_4: 1.249  loss_mask_4: 0.2093  loss_dice_4: 1.765  loss_ce_5: 1.254  loss_mask_5: 0.2327  loss_dice_5: 1.902  loss_ce_6: 1.256  loss_mask_6: 0.1929  loss_dice_6: 1.9  loss_ce_7: 1.281  loss_mask_7: 0.2084  loss_dice_7: 1.831  loss_ce_8: 1.296  loss_mask_8: 0.2337  loss_dice_8: 1.802    time: 0.3222  last_time: 0.3055  data_time: 0.0070  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:27 d2.utils.events]:  eta: 0:45:52  iter: 50999  total_loss: 33.52  loss_ce: 1.387  loss_mask: 0.08917  loss_dice: 1.781  loss_ce_0: 1.517  loss_mask_0: 0.1049  loss_dice_0: 1.443  loss_ce_1: 1.675  loss_mask_1: 0.08985  loss_dice_1: 1.701  loss_ce_2: 1.712  loss_mask_2: 0.1203  loss_dice_2: 1.847  loss_ce_3: 1.539  loss_mask_3: 0.1104  loss_dice_3: 1.548  loss_ce_4: 1.49  loss_mask_4: 0.09386  loss_dice_4: 1.491  loss_ce_5: 1.355  loss_mask_5: 0.0931  loss_dice_5: 1.832  loss_ce_6: 1.241  loss_mask_6: 0.09905  loss_dice_6: 1.677  loss_ce_7: 1.39  loss_mask_7: 0.1057  loss_dice_7: 1.619  loss_ce_8: 1.423  loss_mask_8: 0.09365  loss_dice_8: 1.72    time: 0.3222  last_time: 0.3044  data_time: 0.0229  last_data_time: 0.0087   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:33 d2.utils.events]:  eta: 0:45:46  iter: 51019  total_loss: 36.94  loss_ce: 1.395  loss_mask: 0.2545  loss_dice: 1.508  loss_ce_0: 1.591  loss_mask_0: 0.3358  loss_dice_0: 1.6  loss_ce_1: 1.628  loss_mask_1: 0.3983  loss_dice_1: 1.417  loss_ce_2: 1.532  loss_mask_2: 0.3194  loss_dice_2: 1.51  loss_ce_3: 1.248  loss_mask_3: 0.2962  loss_dice_3: 1.506  loss_ce_4: 1.37  loss_mask_4: 0.34  loss_dice_4: 1.59  loss_ce_5: 1.324  loss_mask_5: 0.2754  loss_dice_5: 1.675  loss_ce_6: 1.341  loss_mask_6: 0.3066  loss_dice_6: 1.29  loss_ce_7: 1.368  loss_mask_7: 0.2968  loss_dice_7: 1.369  loss_ce_8: 1.318  loss_mask_8: 0.3149  loss_dice_8: 1.482    time: 0.3222  last_time: 0.2988  data_time: 0.0104  last_data_time: 0.0093   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:39 d2.utils.events]:  eta: 0:45:37  iter: 51039  total_loss: 27.14  loss_ce: 1.229  loss_mask: 0.1827  loss_dice: 1.29  loss_ce_0: 1.374  loss_mask_0: 0.1942  loss_dice_0: 1.198  loss_ce_1: 1.419  loss_mask_1: 0.1589  loss_dice_1: 1.209  loss_ce_2: 1.294  loss_mask_2: 0.1897  loss_dice_2: 1.281  loss_ce_3: 1.249  loss_mask_3: 0.1894  loss_dice_3: 1.175  loss_ce_4: 1.234  loss_mask_4: 0.1913  loss_dice_4: 1.127  loss_ce_5: 1.251  loss_mask_5: 0.2114  loss_dice_5: 1.367  loss_ce_6: 1.297  loss_mask_6: 0.2027  loss_dice_6: 1.201  loss_ce_7: 1.102  loss_mask_7: 0.2084  loss_dice_7: 1.134  loss_ce_8: 1.233  loss_mask_8: 0.1942  loss_dice_8: 1.09    time: 0.3222  last_time: 0.3300  data_time: 0.0111  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:45 d2.utils.events]:  eta: 0:45:30  iter: 51059  total_loss: 28.28  loss_ce: 1.007  loss_mask: 0.1196  loss_dice: 1.421  loss_ce_0: 1.232  loss_mask_0: 0.1533  loss_dice_0: 1.364  loss_ce_1: 1.175  loss_mask_1: 0.1273  loss_dice_1: 1.308  loss_ce_2: 1.117  loss_mask_2: 0.1673  loss_dice_2: 1.31  loss_ce_3: 0.9916  loss_mask_3: 0.142  loss_dice_3: 1.414  loss_ce_4: 1.019  loss_mask_4: 0.1334  loss_dice_4: 1.43  loss_ce_5: 0.9986  loss_mask_5: 0.1238  loss_dice_5: 1.263  loss_ce_6: 0.9856  loss_mask_6: 0.1248  loss_dice_6: 1.421  loss_ce_7: 1.005  loss_mask_7: 0.1319  loss_dice_7: 1.569  loss_ce_8: 1.025  loss_mask_8: 0.1332  loss_dice_8: 1.682    time: 0.3222  last_time: 0.3208  data_time: 0.0062  last_data_time: 0.0042   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:52 d2.utils.events]:  eta: 0:45:27  iter: 51079  total_loss: 22.37  loss_ce: 1.149  loss_mask: 0.1242  loss_dice: 1.035  loss_ce_0: 1.297  loss_mask_0: 0.1538  loss_dice_0: 1.292  loss_ce_1: 1.195  loss_mask_1: 0.1744  loss_dice_1: 1.393  loss_ce_2: 0.9833  loss_mask_2: 0.1506  loss_dice_2: 1.415  loss_ce_3: 1.063  loss_mask_3: 0.1365  loss_dice_3: 0.9583  loss_ce_4: 1.141  loss_mask_4: 0.1282  loss_dice_4: 1.056  loss_ce_5: 1.101  loss_mask_5: 0.1327  loss_dice_5: 1.01  loss_ce_6: 1.192  loss_mask_6: 0.1176  loss_dice_6: 1.001  loss_ce_7: 1.132  loss_mask_7: 0.1176  loss_dice_7: 0.9737  loss_ce_8: 1.049  loss_mask_8: 0.1348  loss_dice_8: 1.084    time: 0.3222  last_time: 0.3065  data_time: 0.0109  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:00:58 d2.utils.events]:  eta: 0:45:19  iter: 51099  total_loss: 23.95  loss_ce: 0.9012  loss_mask: 0.19  loss_dice: 0.962  loss_ce_0: 1.124  loss_mask_0: 0.235  loss_dice_0: 1.046  loss_ce_1: 1.035  loss_mask_1: 0.2942  loss_dice_1: 1.163  loss_ce_2: 1.045  loss_mask_2: 0.2244  loss_dice_2: 0.8792  loss_ce_3: 0.7956  loss_mask_3: 0.2003  loss_dice_3: 0.9713  loss_ce_4: 0.8853  loss_mask_4: 0.2004  loss_dice_4: 1.121  loss_ce_5: 0.7825  loss_mask_5: 0.2114  loss_dice_5: 1.089  loss_ce_6: 0.9504  loss_mask_6: 0.1877  loss_dice_6: 0.9216  loss_ce_7: 0.7255  loss_mask_7: 0.1884  loss_dice_7: 0.9163  loss_ce_8: 0.8375  loss_mask_8: 0.1783  loss_dice_8: 1.03    time: 0.3222  last_time: 0.3220  data_time: 0.0072  last_data_time: 0.0050   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:04 d2.utils.events]:  eta: 0:45:12  iter: 51119  total_loss: 29.14  loss_ce: 1.315  loss_mask: 0.1528  loss_dice: 1.632  loss_ce_0: 1.331  loss_mask_0: 0.1539  loss_dice_0: 1.745  loss_ce_1: 1.318  loss_mask_1: 0.1576  loss_dice_1: 1.755  loss_ce_2: 1.302  loss_mask_2: 0.1387  loss_dice_2: 1.554  loss_ce_3: 1.184  loss_mask_3: 0.1753  loss_dice_3: 1.623  loss_ce_4: 1.186  loss_mask_4: 0.1534  loss_dice_4: 1.718  loss_ce_5: 1.213  loss_mask_5: 0.153  loss_dice_5: 1.449  loss_ce_6: 1.218  loss_mask_6: 0.1426  loss_dice_6: 1.628  loss_ce_7: 1.228  loss_mask_7: 0.1479  loss_dice_7: 1.56  loss_ce_8: 1.239  loss_mask_8: 0.1546  loss_dice_8: 1.716    time: 0.3222  last_time: 0.4083  data_time: 0.0088  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:11 d2.utils.events]:  eta: 0:45:07  iter: 51139  total_loss: 25.49  loss_ce: 0.9214  loss_mask: 0.06203  loss_dice: 1.305  loss_ce_0: 1.263  loss_mask_0: 0.09577  loss_dice_0: 1.63  loss_ce_1: 1.256  loss_mask_1: 0.08217  loss_dice_1: 1.537  loss_ce_2: 1.151  loss_mask_2: 0.1081  loss_dice_2: 1.5  loss_ce_3: 0.9401  loss_mask_3: 0.07582  loss_dice_3: 1.473  loss_ce_4: 0.8694  loss_mask_4: 0.07697  loss_dice_4: 1.456  loss_ce_5: 0.9423  loss_mask_5: 0.07222  loss_dice_5: 1.212  loss_ce_6: 0.9105  loss_mask_6: 0.0786  loss_dice_6: 1.502  loss_ce_7: 0.9117  loss_mask_7: 0.0714  loss_dice_7: 1.316  loss_ce_8: 0.8892  loss_mask_8: 0.07345  loss_dice_8: 1.577    time: 0.3222  last_time: 0.3032  data_time: 0.0074  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:17 d2.utils.events]:  eta: 0:45:00  iter: 51159  total_loss: 24.61  loss_ce: 0.8888  loss_mask: 0.09348  loss_dice: 1.11  loss_ce_0: 1.196  loss_mask_0: 0.1431  loss_dice_0: 1.144  loss_ce_1: 1.21  loss_mask_1: 0.1011  loss_dice_1: 1.135  loss_ce_2: 1.089  loss_mask_2: 0.07369  loss_dice_2: 0.9268  loss_ce_3: 0.997  loss_mask_3: 0.08976  loss_dice_3: 0.9547  loss_ce_4: 1.005  loss_mask_4: 0.09669  loss_dice_4: 0.9081  loss_ce_5: 0.8476  loss_mask_5: 0.09266  loss_dice_5: 1.051  loss_ce_6: 1.012  loss_mask_6: 0.1082  loss_dice_6: 0.9707  loss_ce_7: 0.9484  loss_mask_7: 0.09812  loss_dice_7: 0.9528  loss_ce_8: 0.9799  loss_mask_8: 0.0954  loss_dice_8: 1.094    time: 0.3222  last_time: 0.2997  data_time: 0.0074  last_data_time: 0.0048   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:23 d2.utils.events]:  eta: 0:44:54  iter: 51179  total_loss: 24.84  loss_ce: 0.8945  loss_mask: 0.1931  loss_dice: 1.195  loss_ce_0: 0.9935  loss_mask_0: 0.2669  loss_dice_0: 1.188  loss_ce_1: 1.006  loss_mask_1: 0.2369  loss_dice_1: 1.404  loss_ce_2: 1  loss_mask_2: 0.2422  loss_dice_2: 1.243  loss_ce_3: 0.957  loss_mask_3: 0.2033  loss_dice_3: 1.068  loss_ce_4: 0.9923  loss_mask_4: 0.2113  loss_dice_4: 1.171  loss_ce_5: 0.9975  loss_mask_5: 0.2095  loss_dice_5: 1.037  loss_ce_6: 0.8753  loss_mask_6: 0.1857  loss_dice_6: 1.315  loss_ce_7: 0.8781  loss_mask_7: 0.185  loss_dice_7: 1.305  loss_ce_8: 0.8939  loss_mask_8: 0.1965  loss_dice_8: 1.278    time: 0.3222  last_time: 0.2971  data_time: 0.0077  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:29 d2.utils.events]:  eta: 0:44:47  iter: 51199  total_loss: 26.98  loss_ce: 1.148  loss_mask: 0.1756  loss_dice: 1.449  loss_ce_0: 1.661  loss_mask_0: 0.1849  loss_dice_0: 1.512  loss_ce_1: 1.516  loss_mask_1: 0.2065  loss_dice_1: 1.416  loss_ce_2: 1.322  loss_mask_2: 0.1827  loss_dice_2: 1.288  loss_ce_3: 1.193  loss_mask_3: 0.2064  loss_dice_3: 1.3  loss_ce_4: 1.201  loss_mask_4: 0.1823  loss_dice_4: 1.21  loss_ce_5: 1.17  loss_mask_5: 0.1616  loss_dice_5: 1.235  loss_ce_6: 1.16  loss_mask_6: 0.1853  loss_dice_6: 1.458  loss_ce_7: 1.166  loss_mask_7: 0.186  loss_dice_7: 1.14  loss_ce_8: 1.106  loss_mask_8: 0.1746  loss_dice_8: 1.541    time: 0.3222  last_time: 0.3508  data_time: 0.0076  last_data_time: 0.0213   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:35 d2.utils.events]:  eta: 0:44:42  iter: 51219  total_loss: 26.37  loss_ce: 1.042  loss_mask: 0.112  loss_dice: 0.9945  loss_ce_0: 1.349  loss_mask_0: 0.1584  loss_dice_0: 0.9796  loss_ce_1: 1.233  loss_mask_1: 0.1309  loss_dice_1: 1.252  loss_ce_2: 1.099  loss_mask_2: 0.1356  loss_dice_2: 1.282  loss_ce_3: 1.037  loss_mask_3: 0.1293  loss_dice_3: 0.9501  loss_ce_4: 1.057  loss_mask_4: 0.1313  loss_dice_4: 0.9032  loss_ce_5: 1.056  loss_mask_5: 0.1275  loss_dice_5: 0.9883  loss_ce_6: 1.033  loss_mask_6: 0.1185  loss_dice_6: 0.9194  loss_ce_7: 1.101  loss_mask_7: 0.1224  loss_dice_7: 0.9945  loss_ce_8: 1.023  loss_mask_8: 0.1191  loss_dice_8: 0.9706    time: 0.3222  last_time: 0.3306  data_time: 0.0066  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:42 d2.utils.events]:  eta: 0:44:36  iter: 51239  total_loss: 19.36  loss_ce: 0.6589  loss_mask: 0.09398  loss_dice: 0.9916  loss_ce_0: 0.8201  loss_mask_0: 0.1375  loss_dice_0: 1.143  loss_ce_1: 0.8825  loss_mask_1: 0.1164  loss_dice_1: 1.308  loss_ce_2: 0.8655  loss_mask_2: 0.08227  loss_dice_2: 1.173  loss_ce_3: 0.6412  loss_mask_3: 0.109  loss_dice_3: 1.143  loss_ce_4: 0.6856  loss_mask_4: 0.09785  loss_dice_4: 1.112  loss_ce_5: 0.6549  loss_mask_5: 0.1051  loss_dice_5: 0.9514  loss_ce_6: 0.612  loss_mask_6: 0.1058  loss_dice_6: 0.9081  loss_ce_7: 0.6557  loss_mask_7: 0.08829  loss_dice_7: 0.8474  loss_ce_8: 0.6614  loss_mask_8: 0.09078  loss_dice_8: 1.051    time: 0.3222  last_time: 0.2968  data_time: 0.0062  last_data_time: 0.0020   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:48 d2.utils.events]:  eta: 0:44:30  iter: 51259  total_loss: 34.96  loss_ce: 1.061  loss_mask: 0.198  loss_dice: 1.59  loss_ce_0: 1.292  loss_mask_0: 0.3162  loss_dice_0: 1.78  loss_ce_1: 1.18  loss_mask_1: 0.3108  loss_dice_1: 1.749  loss_ce_2: 1.196  loss_mask_2: 0.2548  loss_dice_2: 1.898  loss_ce_3: 1.092  loss_mask_3: 0.2632  loss_dice_3: 1.581  loss_ce_4: 1.075  loss_mask_4: 0.2301  loss_dice_4: 1.438  loss_ce_5: 1.05  loss_mask_5: 0.2507  loss_dice_5: 1.584  loss_ce_6: 1.032  loss_mask_6: 0.2168  loss_dice_6: 1.45  loss_ce_7: 1.128  loss_mask_7: 0.2015  loss_dice_7: 1.521  loss_ce_8: 1.122  loss_mask_8: 0.209  loss_dice_8: 1.55    time: 0.3222  last_time: 0.3419  data_time: 0.0077  last_data_time: 0.0198   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:01:54 d2.utils.events]:  eta: 0:44:23  iter: 51279  total_loss: 33.99  loss_ce: 1.345  loss_mask: 0.1377  loss_dice: 1.54  loss_ce_0: 1.589  loss_mask_0: 0.1503  loss_dice_0: 1.507  loss_ce_1: 1.625  loss_mask_1: 0.08882  loss_dice_1: 1.364  loss_ce_2: 1.446  loss_mask_2: 0.1351  loss_dice_2: 1.589  loss_ce_3: 1.372  loss_mask_3: 0.1371  loss_dice_3: 1.497  loss_ce_4: 1.352  loss_mask_4: 0.1283  loss_dice_4: 1.42  loss_ce_5: 1.334  loss_mask_5: 0.1383  loss_dice_5: 1.446  loss_ce_6: 1.305  loss_mask_6: 0.1309  loss_dice_6: 1.595  loss_ce_7: 1.313  loss_mask_7: 0.1341  loss_dice_7: 1.293  loss_ce_8: 1.376  loss_mask_8: 0.1196  loss_dice_8: 1.542    time: 0.3222  last_time: 0.3006  data_time: 0.0064  last_data_time: 0.0051   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:00 d2.utils.events]:  eta: 0:44:18  iter: 51299  total_loss: 28.23  loss_ce: 0.9618  loss_mask: 0.1489  loss_dice: 1.559  loss_ce_0: 1.213  loss_mask_0: 0.1768  loss_dice_0: 1.437  loss_ce_1: 1.252  loss_mask_1: 0.1619  loss_dice_1: 1.433  loss_ce_2: 1.011  loss_mask_2: 0.1564  loss_dice_2: 1.569  loss_ce_3: 0.9179  loss_mask_3: 0.144  loss_dice_3: 1.476  loss_ce_4: 0.8964  loss_mask_4: 0.1422  loss_dice_4: 1.603  loss_ce_5: 0.8647  loss_mask_5: 0.147  loss_dice_5: 1.608  loss_ce_6: 0.8831  loss_mask_6: 0.1423  loss_dice_6: 1.582  loss_ce_7: 0.9199  loss_mask_7: 0.1517  loss_dice_7: 1.493  loss_ce_8: 0.8941  loss_mask_8: 0.1464  loss_dice_8: 1.492    time: 0.3222  last_time: 0.3104  data_time: 0.0075  last_data_time: 0.0081   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:07 d2.utils.events]:  eta: 0:44:12  iter: 51319  total_loss: 27.2  loss_ce: 1.206  loss_mask: 0.164  loss_dice: 1.179  loss_ce_0: 1.461  loss_mask_0: 0.1281  loss_dice_0: 1.231  loss_ce_1: 1.461  loss_mask_1: 0.1664  loss_dice_1: 1.162  loss_ce_2: 1.353  loss_mask_2: 0.1412  loss_dice_2: 1.282  loss_ce_3: 1.31  loss_mask_3: 0.1667  loss_dice_3: 1.102  loss_ce_4: 1.177  loss_mask_4: 0.1471  loss_dice_4: 1.065  loss_ce_5: 1.227  loss_mask_5: 0.1875  loss_dice_5: 1.038  loss_ce_6: 1.222  loss_mask_6: 0.1704  loss_dice_6: 1.143  loss_ce_7: 1.223  loss_mask_7: 0.1728  loss_dice_7: 1.275  loss_ce_8: 1.199  loss_mask_8: 0.1856  loss_dice_8: 1.287    time: 0.3222  last_time: 0.3203  data_time: 0.0175  last_data_time: 0.0080   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:13 d2.utils.events]:  eta: 0:44:07  iter: 51339  total_loss: 38.63  loss_ce: 1.401  loss_mask: 0.2048  loss_dice: 1.564  loss_ce_0: 1.799  loss_mask_0: 0.2251  loss_dice_0: 1.624  loss_ce_1: 1.633  loss_mask_1: 0.2574  loss_dice_1: 1.658  loss_ce_2: 1.401  loss_mask_2: 0.2022  loss_dice_2: 1.562  loss_ce_3: 1.369  loss_mask_3: 0.2285  loss_dice_3: 1.429  loss_ce_4: 1.227  loss_mask_4: 0.2228  loss_dice_4: 1.921  loss_ce_5: 1.246  loss_mask_5: 0.2473  loss_dice_5: 1.465  loss_ce_6: 1.356  loss_mask_6: 0.2458  loss_dice_6: 1.569  loss_ce_7: 1.317  loss_mask_7: 0.2479  loss_dice_7: 1.602  loss_ce_8: 1.364  loss_mask_8: 0.2152  loss_dice_8: 1.447    time: 0.3222  last_time: 0.3139  data_time: 0.0076  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:19 d2.utils.events]:  eta: 0:44:00  iter: 51359  total_loss: 16.79  loss_ce: 0.7053  loss_mask: 0.1424  loss_dice: 0.7044  loss_ce_0: 1.035  loss_mask_0: 0.1672  loss_dice_0: 1.016  loss_ce_1: 1.018  loss_mask_1: 0.1057  loss_dice_1: 1.054  loss_ce_2: 0.9304  loss_mask_2: 0.1268  loss_dice_2: 0.7738  loss_ce_3: 0.8733  loss_mask_3: 0.1503  loss_dice_3: 0.9271  loss_ce_4: 0.8168  loss_mask_4: 0.1419  loss_dice_4: 0.9702  loss_ce_5: 0.9066  loss_mask_5: 0.1445  loss_dice_5: 0.8728  loss_ce_6: 0.935  loss_mask_6: 0.1399  loss_dice_6: 0.9127  loss_ce_7: 0.8148  loss_mask_7: 0.1387  loss_dice_7: 0.7616  loss_ce_8: 0.7567  loss_mask_8: 0.126  loss_dice_8: 0.7517    time: 0.3222  last_time: 0.3292  data_time: 0.0065  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:26 d2.utils.events]:  eta: 0:43:55  iter: 51379  total_loss: 26.07  loss_ce: 1.188  loss_mask: 0.2386  loss_dice: 0.8967  loss_ce_0: 1.44  loss_mask_0: 0.2282  loss_dice_0: 1.061  loss_ce_1: 1.408  loss_mask_1: 0.2051  loss_dice_1: 1.083  loss_ce_2: 1.084  loss_mask_2: 0.1773  loss_dice_2: 1.066  loss_ce_3: 1.062  loss_mask_3: 0.2476  loss_dice_3: 0.9033  loss_ce_4: 1.035  loss_mask_4: 0.2409  loss_dice_4: 0.9267  loss_ce_5: 1.043  loss_mask_5: 0.2487  loss_dice_5: 0.9487  loss_ce_6: 1.039  loss_mask_6: 0.2526  loss_dice_6: 0.9559  loss_ce_7: 1.133  loss_mask_7: 0.2406  loss_dice_7: 0.9717  loss_ce_8: 1.182  loss_mask_8: 0.2473  loss_dice_8: 1.006    time: 0.3222  last_time: 0.3121  data_time: 0.0070  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:32 d2.utils.events]:  eta: 0:43:50  iter: 51399  total_loss: 27.63  loss_ce: 1.027  loss_mask: 0.1026  loss_dice: 1.346  loss_ce_0: 1.339  loss_mask_0: 0.1582  loss_dice_0: 1.314  loss_ce_1: 1.456  loss_mask_1: 0.1533  loss_dice_1: 1.561  loss_ce_2: 1.227  loss_mask_2: 0.1049  loss_dice_2: 1.544  loss_ce_3: 1.126  loss_mask_3: 0.1082  loss_dice_3: 1.567  loss_ce_4: 1.109  loss_mask_4: 0.1088  loss_dice_4: 1.43  loss_ce_5: 1.086  loss_mask_5: 0.1122  loss_dice_5: 1.453  loss_ce_6: 1.11  loss_mask_6: 0.1064  loss_dice_6: 1.354  loss_ce_7: 1.025  loss_mask_7: 0.1043  loss_dice_7: 1.596  loss_ce_8: 1.055  loss_mask_8: 0.1118  loss_dice_8: 1.556    time: 0.3222  last_time: 0.3367  data_time: 0.0076  last_data_time: 0.0088   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:38 d2.utils.events]:  eta: 0:43:44  iter: 51419  total_loss: 34.3  loss_ce: 1.382  loss_mask: 0.1127  loss_dice: 1.449  loss_ce_0: 1.652  loss_mask_0: 0.1144  loss_dice_0: 1.609  loss_ce_1: 1.852  loss_mask_1: 0.08694  loss_dice_1: 1.631  loss_ce_2: 1.497  loss_mask_2: 0.1232  loss_dice_2: 1.713  loss_ce_3: 1.382  loss_mask_3: 0.1001  loss_dice_3: 1.277  loss_ce_4: 1.35  loss_mask_4: 0.1271  loss_dice_4: 1.494  loss_ce_5: 1.393  loss_mask_5: 0.1158  loss_dice_5: 1.345  loss_ce_6: 1.349  loss_mask_6: 0.09784  loss_dice_6: 1.241  loss_ce_7: 1.326  loss_mask_7: 0.1219  loss_dice_7: 1.255  loss_ce_8: 1.364  loss_mask_8: 0.1042  loss_dice_8: 1.395    time: 0.3222  last_time: 0.3064  data_time: 0.0185  last_data_time: 0.0090   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:45 d2.utils.events]:  eta: 0:43:37  iter: 51439  total_loss: 30.88  loss_ce: 1.22  loss_mask: 0.2324  loss_dice: 1.267  loss_ce_0: 1.532  loss_mask_0: 0.2486  loss_dice_0: 1.301  loss_ce_1: 1.441  loss_mask_1: 0.2739  loss_dice_1: 1.416  loss_ce_2: 1.344  loss_mask_2: 0.2617  loss_dice_2: 1.308  loss_ce_3: 1.15  loss_mask_3: 0.3127  loss_dice_3: 1.154  loss_ce_4: 1.186  loss_mask_4: 0.2885  loss_dice_4: 1.321  loss_ce_5: 1.151  loss_mask_5: 0.2797  loss_dice_5: 1.284  loss_ce_6: 1.145  loss_mask_6: 0.2446  loss_dice_6: 1.298  loss_ce_7: 1.199  loss_mask_7: 0.268  loss_dice_7: 1.245  loss_ce_8: 1.162  loss_mask_8: 0.2487  loss_dice_8: 1.233    time: 0.3221  last_time: 0.3018  data_time: 0.0073  last_data_time: 0.0024   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:51 d2.utils.events]:  eta: 0:43:31  iter: 51459  total_loss: 20.04  loss_ce: 0.8584  loss_mask: 0.08497  loss_dice: 0.8033  loss_ce_0: 1.197  loss_mask_0: 0.09201  loss_dice_0: 0.7803  loss_ce_1: 1.064  loss_mask_1: 0.1036  loss_dice_1: 0.7978  loss_ce_2: 0.9656  loss_mask_2: 0.1064  loss_dice_2: 0.9661  loss_ce_3: 0.9089  loss_mask_3: 0.09264  loss_dice_3: 1.129  loss_ce_4: 0.8071  loss_mask_4: 0.1075  loss_dice_4: 1.197  loss_ce_5: 0.7259  loss_mask_5: 0.0992  loss_dice_5: 0.8243  loss_ce_6: 0.7045  loss_mask_6: 0.09258  loss_dice_6: 1.048  loss_ce_7: 0.8496  loss_mask_7: 0.09219  loss_dice_7: 0.8394  loss_ce_8: 0.756  loss_mask_8: 0.09389  loss_dice_8: 1.005    time: 0.3221  last_time: 0.2936  data_time: 0.0064  last_data_time: 0.0027   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:02:57 d2.utils.events]:  eta: 0:43:25  iter: 51479  total_loss: 24.34  loss_ce: 1.088  loss_mask: 0.1299  loss_dice: 1.082  loss_ce_0: 1.442  loss_mask_0: 0.1423  loss_dice_0: 1.445  loss_ce_1: 1.427  loss_mask_1: 0.1429  loss_dice_1: 0.9887  loss_ce_2: 1.353  loss_mask_2: 0.13  loss_dice_2: 1.338  loss_ce_3: 1.092  loss_mask_3: 0.1278  loss_dice_3: 1.048  loss_ce_4: 1.106  loss_mask_4: 0.1451  loss_dice_4: 0.8433  loss_ce_5: 1.075  loss_mask_5: 0.1406  loss_dice_5: 1.192  loss_ce_6: 1.084  loss_mask_6: 0.1443  loss_dice_6: 1.034  loss_ce_7: 1.086  loss_mask_7: 0.123  loss_dice_7: 1.27  loss_ce_8: 1.082  loss_mask_8: 0.1303  loss_dice_8: 1.104    time: 0.3221  last_time: 0.3304  data_time: 0.0075  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:03 d2.utils.events]:  eta: 0:43:17  iter: 51499  total_loss: 25.9  loss_ce: 1.02  loss_mask: 0.105  loss_dice: 1.144  loss_ce_0: 1.209  loss_mask_0: 0.166  loss_dice_0: 1.164  loss_ce_1: 1.235  loss_mask_1: 0.09986  loss_dice_1: 1.203  loss_ce_2: 1.194  loss_mask_2: 0.09233  loss_dice_2: 1.329  loss_ce_3: 1.053  loss_mask_3: 0.1081  loss_dice_3: 0.9376  loss_ce_4: 1.031  loss_mask_4: 0.08852  loss_dice_4: 1.025  loss_ce_5: 0.998  loss_mask_5: 0.1035  loss_dice_5: 1.248  loss_ce_6: 1.017  loss_mask_6: 0.1033  loss_dice_6: 1.19  loss_ce_7: 1.031  loss_mask_7: 0.1077  loss_dice_7: 1.318  loss_ce_8: 1.022  loss_mask_8: 0.09963  loss_dice_8: 1.226    time: 0.3221  last_time: 0.2941  data_time: 0.0096  last_data_time: 0.0019   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:09 d2.utils.events]:  eta: 0:43:10  iter: 51519  total_loss: 27.85  loss_ce: 1.054  loss_mask: 0.1484  loss_dice: 1.304  loss_ce_0: 1.323  loss_mask_0: 0.1633  loss_dice_0: 1.174  loss_ce_1: 1.111  loss_mask_1: 0.129  loss_dice_1: 1.095  loss_ce_2: 1.206  loss_mask_2: 0.1472  loss_dice_2: 1.252  loss_ce_3: 1.1  loss_mask_3: 0.1596  loss_dice_3: 1.224  loss_ce_4: 1.11  loss_mask_4: 0.1519  loss_dice_4: 1.136  loss_ce_5: 1.177  loss_mask_5: 0.1519  loss_dice_5: 1.211  loss_ce_6: 1.133  loss_mask_6: 0.1546  loss_dice_6: 1.281  loss_ce_7: 0.9404  loss_mask_7: 0.1466  loss_dice_7: 1.258  loss_ce_8: 1.215  loss_mask_8: 0.1486  loss_dice_8: 1.276    time: 0.3221  last_time: 0.3320  data_time: 0.0061  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:16 d2.utils.events]:  eta: 0:43:05  iter: 51539  total_loss: 20.45  loss_ce: 0.8314  loss_mask: 0.119  loss_dice: 0.7862  loss_ce_0: 1.052  loss_mask_0: 0.1447  loss_dice_0: 1.111  loss_ce_1: 1.078  loss_mask_1: 0.1255  loss_dice_1: 1.039  loss_ce_2: 0.8338  loss_mask_2: 0.1451  loss_dice_2: 1.113  loss_ce_3: 0.8643  loss_mask_3: 0.1283  loss_dice_3: 0.8224  loss_ce_4: 0.8008  loss_mask_4: 0.1237  loss_dice_4: 1.011  loss_ce_5: 0.9274  loss_mask_5: 0.1206  loss_dice_5: 0.9551  loss_ce_6: 0.8064  loss_mask_6: 0.1187  loss_dice_6: 0.8676  loss_ce_7: 0.8071  loss_mask_7: 0.1384  loss_dice_7: 0.9168  loss_ce_8: 0.8434  loss_mask_8: 0.1366  loss_dice_8: 0.9826    time: 0.3221  last_time: 0.3344  data_time: 0.0067  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:22 d2.utils.events]:  eta: 0:43:00  iter: 51559  total_loss: 26.86  loss_ce: 0.9149  loss_mask: 0.1173  loss_dice: 1.339  loss_ce_0: 1.181  loss_mask_0: 0.1161  loss_dice_0: 1.135  loss_ce_1: 1.159  loss_mask_1: 0.1137  loss_dice_1: 1.468  loss_ce_2: 1.089  loss_mask_2: 0.1361  loss_dice_2: 1.561  loss_ce_3: 0.9543  loss_mask_3: 0.1324  loss_dice_3: 1.262  loss_ce_4: 0.9494  loss_mask_4: 0.1296  loss_dice_4: 1.605  loss_ce_5: 0.9912  loss_mask_5: 0.1186  loss_dice_5: 1.239  loss_ce_6: 0.9901  loss_mask_6: 0.1139  loss_dice_6: 1.431  loss_ce_7: 0.949  loss_mask_7: 0.1376  loss_dice_7: 1.238  loss_ce_8: 0.9278  loss_mask_8: 0.1221  loss_dice_8: 1.597    time: 0.3221  last_time: 0.3057  data_time: 0.0073  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:28 d2.utils.events]:  eta: 0:42:55  iter: 51579  total_loss: 22.34  loss_ce: 0.9556  loss_mask: 0.1241  loss_dice: 0.9893  loss_ce_0: 1.232  loss_mask_0: 0.124  loss_dice_0: 1.106  loss_ce_1: 1.249  loss_mask_1: 0.1465  loss_dice_1: 1.375  loss_ce_2: 1.212  loss_mask_2: 0.1368  loss_dice_2: 1.279  loss_ce_3: 0.9453  loss_mask_3: 0.114  loss_dice_3: 1.08  loss_ce_4: 0.9404  loss_mask_4: 0.1426  loss_dice_4: 0.9448  loss_ce_5: 0.9335  loss_mask_5: 0.1255  loss_dice_5: 0.9942  loss_ce_6: 0.9068  loss_mask_6: 0.1196  loss_dice_6: 1.113  loss_ce_7: 0.9527  loss_mask_7: 0.1167  loss_dice_7: 1.029  loss_ce_8: 0.937  loss_mask_8: 0.1254  loss_dice_8: 1.091    time: 0.3221  last_time: 0.2959  data_time: 0.0102  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:34 d2.utils.events]:  eta: 0:42:49  iter: 51599  total_loss: 27.59  loss_ce: 0.8941  loss_mask: 0.1314  loss_dice: 1.428  loss_ce_0: 1.287  loss_mask_0: 0.2009  loss_dice_0: 1.345  loss_ce_1: 1.212  loss_mask_1: 0.1593  loss_dice_1: 1.374  loss_ce_2: 1.095  loss_mask_2: 0.1234  loss_dice_2: 1.348  loss_ce_3: 0.9695  loss_mask_3: 0.1523  loss_dice_3: 1.248  loss_ce_4: 0.8491  loss_mask_4: 0.1433  loss_dice_4: 1.305  loss_ce_5: 0.8495  loss_mask_5: 0.1386  loss_dice_5: 1.506  loss_ce_6: 0.8488  loss_mask_6: 0.1089  loss_dice_6: 1.061  loss_ce_7: 0.8444  loss_mask_7: 0.1157  loss_dice_7: 1.479  loss_ce_8: 0.8911  loss_mask_8: 0.1081  loss_dice_8: 1.107    time: 0.3221  last_time: 0.2999  data_time: 0.0068  last_data_time: 0.0045   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:41 d2.utils.events]:  eta: 0:42:43  iter: 51619  total_loss: 27.16  loss_ce: 1.012  loss_mask: 0.1292  loss_dice: 1.247  loss_ce_0: 1.473  loss_mask_0: 0.191  loss_dice_0: 1.589  loss_ce_1: 1.54  loss_mask_1: 0.2029  loss_dice_1: 1.46  loss_ce_2: 1.165  loss_mask_2: 0.1704  loss_dice_2: 1.476  loss_ce_3: 1.177  loss_mask_3: 0.1444  loss_dice_3: 1.374  loss_ce_4: 1.121  loss_mask_4: 0.1325  loss_dice_4: 1.4  loss_ce_5: 1.047  loss_mask_5: 0.1401  loss_dice_5: 1.372  loss_ce_6: 1.051  loss_mask_6: 0.1289  loss_dice_6: 1.44  loss_ce_7: 1.003  loss_mask_7: 0.127  loss_dice_7: 1.275  loss_ce_8: 1.161  loss_mask_8: 0.1301  loss_dice_8: 1.307    time: 0.3221  last_time: 0.2993  data_time: 0.0069  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:47 d2.utils.events]:  eta: 0:42:37  iter: 51639  total_loss: 28.72  loss_ce: 1.207  loss_mask: 0.1011  loss_dice: 1.28  loss_ce_0: 1.745  loss_mask_0: 0.1642  loss_dice_0: 1.461  loss_ce_1: 1.543  loss_mask_1: 0.1353  loss_dice_1: 1.494  loss_ce_2: 1.533  loss_mask_2: 0.1392  loss_dice_2: 1.26  loss_ce_3: 1.446  loss_mask_3: 0.1004  loss_dice_3: 1.262  loss_ce_4: 1.199  loss_mask_4: 0.08553  loss_dice_4: 1.282  loss_ce_5: 1.236  loss_mask_5: 0.0856  loss_dice_5: 1.169  loss_ce_6: 1.197  loss_mask_6: 0.1034  loss_dice_6: 1.199  loss_ce_7: 1.188  loss_mask_7: 0.0996  loss_dice_7: 1.229  loss_ce_8: 1.218  loss_mask_8: 0.1049  loss_dice_8: 1.141    time: 0.3221  last_time: 0.3104  data_time: 0.0065  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:53 d2.utils.events]:  eta: 0:42:30  iter: 51659  total_loss: 18.53  loss_ce: 0.8391  loss_mask: 0.1432  loss_dice: 0.8502  loss_ce_0: 0.9925  loss_mask_0: 0.1888  loss_dice_0: 0.7671  loss_ce_1: 1.066  loss_mask_1: 0.2006  loss_dice_1: 0.9382  loss_ce_2: 0.709  loss_mask_2: 0.1644  loss_dice_2: 1.003  loss_ce_3: 0.8489  loss_mask_3: 0.1341  loss_dice_3: 0.7432  loss_ce_4: 0.8041  loss_mask_4: 0.1708  loss_dice_4: 0.7317  loss_ce_5: 0.8196  loss_mask_5: 0.1548  loss_dice_5: 0.8333  loss_ce_6: 0.7731  loss_mask_6: 0.1471  loss_dice_6: 0.676  loss_ce_7: 0.8303  loss_mask_7: 0.1405  loss_dice_7: 0.6775  loss_ce_8: 0.8653  loss_mask_8: 0.1592  loss_dice_8: 0.7281    time: 0.3221  last_time: 0.3206  data_time: 0.0129  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:03:59 d2.utils.events]:  eta: 0:42:25  iter: 51679  total_loss: 23.73  loss_ce: 0.9469  loss_mask: 0.2531  loss_dice: 1.038  loss_ce_0: 1.213  loss_mask_0: 0.2068  loss_dice_0: 1.188  loss_ce_1: 1.006  loss_mask_1: 0.2219  loss_dice_1: 1.048  loss_ce_2: 0.9858  loss_mask_2: 0.1763  loss_dice_2: 1.227  loss_ce_3: 0.9608  loss_mask_3: 0.2531  loss_dice_3: 1.12  loss_ce_4: 1.072  loss_mask_4: 0.2486  loss_dice_4: 1.145  loss_ce_5: 0.9078  loss_mask_5: 0.2573  loss_dice_5: 1.097  loss_ce_6: 0.967  loss_mask_6: 0.2538  loss_dice_6: 1.103  loss_ce_7: 1.077  loss_mask_7: 0.2579  loss_dice_7: 1.106  loss_ce_8: 0.8761  loss_mask_8: 0.2687  loss_dice_8: 1.107    time: 0.3221  last_time: 0.2999  data_time: 0.0072  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:06 d2.utils.events]:  eta: 0:42:18  iter: 51699  total_loss: 26.32  loss_ce: 0.956  loss_mask: 0.1175  loss_dice: 1.26  loss_ce_0: 1.106  loss_mask_0: 0.1424  loss_dice_0: 1.225  loss_ce_1: 0.8904  loss_mask_1: 0.144  loss_dice_1: 1.654  loss_ce_2: 1.044  loss_mask_2: 0.1419  loss_dice_2: 1.626  loss_ce_3: 0.916  loss_mask_3: 0.1186  loss_dice_3: 1.225  loss_ce_4: 0.9476  loss_mask_4: 0.1258  loss_dice_4: 1.324  loss_ce_5: 0.9553  loss_mask_5: 0.1063  loss_dice_5: 1.236  loss_ce_6: 0.7623  loss_mask_6: 0.121  loss_dice_6: 1.573  loss_ce_7: 0.8763  loss_mask_7: 0.103  loss_dice_7: 1.473  loss_ce_8: 0.893  loss_mask_8: 0.1195  loss_dice_8: 1.471    time: 0.3221  last_time: 0.3205  data_time: 0.0072  last_data_time: 0.0138   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:12 d2.utils.events]:  eta: 0:42:11  iter: 51719  total_loss: 23.09  loss_ce: 1.082  loss_mask: 0.09304  loss_dice: 1.069  loss_ce_0: 1.139  loss_mask_0: 0.1128  loss_dice_0: 1.286  loss_ce_1: 1.196  loss_mask_1: 0.1211  loss_dice_1: 1.431  loss_ce_2: 1.178  loss_mask_2: 0.1017  loss_dice_2: 1.144  loss_ce_3: 1.058  loss_mask_3: 0.1096  loss_dice_3: 1.045  loss_ce_4: 1.06  loss_mask_4: 0.1029  loss_dice_4: 0.9186  loss_ce_5: 1.005  loss_mask_5: 0.1074  loss_dice_5: 1.118  loss_ce_6: 1.034  loss_mask_6: 0.09014  loss_dice_6: 1.149  loss_ce_7: 1  loss_mask_7: 0.1061  loss_dice_7: 0.9942  loss_ce_8: 0.9712  loss_mask_8: 0.1001  loss_dice_8: 0.9981    time: 0.3221  last_time: 0.3248  data_time: 0.0067  last_data_time: 0.0019   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:18 d2.utils.events]:  eta: 0:42:05  iter: 51739  total_loss: 26.93  loss_ce: 1.123  loss_mask: 0.1193  loss_dice: 0.8296  loss_ce_0: 1.328  loss_mask_0: 0.1785  loss_dice_0: 1.038  loss_ce_1: 1.21  loss_mask_1: 0.1436  loss_dice_1: 1.051  loss_ce_2: 1.147  loss_mask_2: 0.1595  loss_dice_2: 0.8716  loss_ce_3: 1.152  loss_mask_3: 0.1222  loss_dice_3: 1.031  loss_ce_4: 1.122  loss_mask_4: 0.1309  loss_dice_4: 0.9508  loss_ce_5: 1.085  loss_mask_5: 0.1321  loss_dice_5: 0.9193  loss_ce_6: 1.154  loss_mask_6: 0.09882  loss_dice_6: 0.9359  loss_ce_7: 1.125  loss_mask_7: 0.1199  loss_dice_7: 0.9147  loss_ce_8: 1.151  loss_mask_8: 0.1336  loss_dice_8: 0.8544    time: 0.3221  last_time: 0.3003  data_time: 0.0071  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:24 d2.utils.events]:  eta: 0:41:59  iter: 51759  total_loss: 29.94  loss_ce: 1.133  loss_mask: 0.1038  loss_dice: 1.761  loss_ce_0: 1.266  loss_mask_0: 0.2082  loss_dice_0: 1.747  loss_ce_1: 1.316  loss_mask_1: 0.2305  loss_dice_1: 1.718  loss_ce_2: 1.294  loss_mask_2: 0.1653  loss_dice_2: 1.527  loss_ce_3: 1.145  loss_mask_3: 0.1633  loss_dice_3: 1.311  loss_ce_4: 1.114  loss_mask_4: 0.1358  loss_dice_4: 1.655  loss_ce_5: 1.108  loss_mask_5: 0.1461  loss_dice_5: 1.727  loss_ce_6: 1.119  loss_mask_6: 0.1429  loss_dice_6: 1.465  loss_ce_7: 1.072  loss_mask_7: 0.1486  loss_dice_7: 1.696  loss_ce_8: 1.094  loss_mask_8: 0.1454  loss_dice_8: 1.547    time: 0.3221  last_time: 0.3251  data_time: 0.0071  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:31 d2.utils.events]:  eta: 0:41:54  iter: 51779  total_loss: 28.84  loss_ce: 1.337  loss_mask: 0.1747  loss_dice: 1.184  loss_ce_0: 1.709  loss_mask_0: 0.1939  loss_dice_0: 1.718  loss_ce_1: 1.56  loss_mask_1: 0.2247  loss_dice_1: 1.665  loss_ce_2: 1.292  loss_mask_2: 0.1997  loss_dice_2: 1.606  loss_ce_3: 1.322  loss_mask_3: 0.1661  loss_dice_3: 1.478  loss_ce_4: 1.287  loss_mask_4: 0.1953  loss_dice_4: 1.38  loss_ce_5: 1.293  loss_mask_5: 0.1766  loss_dice_5: 1.641  loss_ce_6: 1.182  loss_mask_6: 0.194  loss_dice_6: 1.264  loss_ce_7: 1.278  loss_mask_7: 0.1793  loss_dice_7: 1.505  loss_ce_8: 1.287  loss_mask_8: 0.1815  loss_dice_8: 1.548    time: 0.3221  last_time: 0.3122  data_time: 0.0083  last_data_time: 0.0125   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:37 d2.utils.events]:  eta: 0:41:47  iter: 51799  total_loss: 22.64  loss_ce: 0.6379  loss_mask: 0.1069  loss_dice: 1.375  loss_ce_0: 0.9055  loss_mask_0: 0.1213  loss_dice_0: 1.437  loss_ce_1: 0.8212  loss_mask_1: 0.1458  loss_dice_1: 1.577  loss_ce_2: 0.7619  loss_mask_2: 0.1536  loss_dice_2: 1.229  loss_ce_3: 0.6849  loss_mask_3: 0.1192  loss_dice_3: 1.355  loss_ce_4: 0.6294  loss_mask_4: 0.1256  loss_dice_4: 1.146  loss_ce_5: 0.7156  loss_mask_5: 0.1159  loss_dice_5: 1.208  loss_ce_6: 0.7135  loss_mask_6: 0.09802  loss_dice_6: 1.188  loss_ce_7: 0.5666  loss_mask_7: 0.1203  loss_dice_7: 1.247  loss_ce_8: 0.5993  loss_mask_8: 0.1339  loss_dice_8: 1.384    time: 0.3221  last_time: 0.3163  data_time: 0.0221  last_data_time: 0.0068   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:43 d2.utils.events]:  eta: 0:41:42  iter: 51819  total_loss: 26.43  loss_ce: 1.182  loss_mask: 0.08551  loss_dice: 1.287  loss_ce_0: 1.481  loss_mask_0: 0.09576  loss_dice_0: 1.296  loss_ce_1: 1.564  loss_mask_1: 0.1246  loss_dice_1: 1.302  loss_ce_2: 1.494  loss_mask_2: 0.1087  loss_dice_2: 1.234  loss_ce_3: 1.211  loss_mask_3: 0.08844  loss_dice_3: 1.308  loss_ce_4: 1.23  loss_mask_4: 0.08644  loss_dice_4: 1.297  loss_ce_5: 1.203  loss_mask_5: 0.09334  loss_dice_5: 1.297  loss_ce_6: 1.188  loss_mask_6: 0.09423  loss_dice_6: 1.302  loss_ce_7: 1.188  loss_mask_7: 0.08574  loss_dice_7: 0.9426  loss_ce_8: 1.211  loss_mask_8: 0.08642  loss_dice_8: 1.257    time: 0.3221  last_time: 0.3294  data_time: 0.0077  last_data_time: 0.0074   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:50 d2.utils.events]:  eta: 0:41:36  iter: 51839  total_loss: 23.3  loss_ce: 0.936  loss_mask: 0.09941  loss_dice: 1.226  loss_ce_0: 1.263  loss_mask_0: 0.1212  loss_dice_0: 0.9457  loss_ce_1: 1.258  loss_mask_1: 0.1248  loss_dice_1: 1.04  loss_ce_2: 1.01  loss_mask_2: 0.1309  loss_dice_2: 1.299  loss_ce_3: 0.9015  loss_mask_3: 0.1063  loss_dice_3: 0.9853  loss_ce_4: 0.9089  loss_mask_4: 0.1059  loss_dice_4: 1.014  loss_ce_5: 0.9031  loss_mask_5: 0.1021  loss_dice_5: 1.077  loss_ce_6: 0.8802  loss_mask_6: 0.09793  loss_dice_6: 1.023  loss_ce_7: 0.9222  loss_mask_7: 0.1078  loss_dice_7: 1.156  loss_ce_8: 0.878  loss_mask_8: 0.0928  loss_dice_8: 0.9775    time: 0.3221  last_time: 0.3053  data_time: 0.0072  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:04:56 d2.utils.events]:  eta: 0:41:30  iter: 51859  total_loss: 23.13  loss_ce: 0.9995  loss_mask: 0.1048  loss_dice: 1.037  loss_ce_0: 1.249  loss_mask_0: 0.1329  loss_dice_0: 1.246  loss_ce_1: 1.228  loss_mask_1: 0.1419  loss_dice_1: 1.146  loss_ce_2: 1.314  loss_mask_2: 0.1279  loss_dice_2: 0.9878  loss_ce_3: 1.046  loss_mask_3: 0.1026  loss_dice_3: 0.9205  loss_ce_4: 0.9709  loss_mask_4: 0.1113  loss_dice_4: 1.004  loss_ce_5: 0.9108  loss_mask_5: 0.1107  loss_dice_5: 1.083  loss_ce_6: 0.972  loss_mask_6: 0.1071  loss_dice_6: 1.1  loss_ce_7: 1.056  loss_mask_7: 0.1082  loss_dice_7: 0.8848  loss_ce_8: 1.059  loss_mask_8: 0.1144  loss_dice_8: 0.8752    time: 0.3221  last_time: 0.3265  data_time: 0.0077  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:02 d2.utils.events]:  eta: 0:41:23  iter: 51879  total_loss: 27.68  loss_ce: 1.144  loss_mask: 0.1589  loss_dice: 1.077  loss_ce_0: 1.39  loss_mask_0: 0.2029  loss_dice_0: 1.028  loss_ce_1: 1.35  loss_mask_1: 0.1839  loss_dice_1: 1.035  loss_ce_2: 1.428  loss_mask_2: 0.1866  loss_dice_2: 1.189  loss_ce_3: 1.214  loss_mask_3: 0.1704  loss_dice_3: 0.9891  loss_ce_4: 1.165  loss_mask_4: 0.1653  loss_dice_4: 1.169  loss_ce_5: 1.188  loss_mask_5: 0.1387  loss_dice_5: 1.241  loss_ce_6: 1.184  loss_mask_6: 0.1864  loss_dice_6: 1.076  loss_ce_7: 1.169  loss_mask_7: 0.1788  loss_dice_7: 1.106  loss_ce_8: 1.158  loss_mask_8: 0.1827  loss_dice_8: 1.109    time: 0.3221  last_time: 0.3036  data_time: 0.0110  last_data_time: 0.0073   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:08 d2.utils.events]:  eta: 0:41:17  iter: 51899  total_loss: 21.68  loss_ce: 0.9443  loss_mask: 0.09257  loss_dice: 0.9524  loss_ce_0: 1.236  loss_mask_0: 0.09383  loss_dice_0: 1.147  loss_ce_1: 1.099  loss_mask_1: 0.09922  loss_dice_1: 0.9312  loss_ce_2: 0.9656  loss_mask_2: 0.09751  loss_dice_2: 1.009  loss_ce_3: 0.9307  loss_mask_3: 0.08932  loss_dice_3: 0.9726  loss_ce_4: 1.086  loss_mask_4: 0.08415  loss_dice_4: 0.8626  loss_ce_5: 0.8826  loss_mask_5: 0.08396  loss_dice_5: 1.024  loss_ce_6: 0.8718  loss_mask_6: 0.08646  loss_dice_6: 1.085  loss_ce_7: 1.044  loss_mask_7: 0.09195  loss_dice_7: 0.9561  loss_ce_8: 1.054  loss_mask_8: 0.09207  loss_dice_8: 1.074    time: 0.3221  last_time: 0.2990  data_time: 0.0075  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:15 d2.utils.events]:  eta: 0:41:12  iter: 51919  total_loss: 18.33  loss_ce: 0.8038  loss_mask: 0.1165  loss_dice: 1.101  loss_ce_0: 0.8803  loss_mask_0: 0.2509  loss_dice_0: 0.8919  loss_ce_1: 0.8706  loss_mask_1: 0.1833  loss_dice_1: 1.066  loss_ce_2: 0.9661  loss_mask_2: 0.1406  loss_dice_2: 0.8658  loss_ce_3: 0.8712  loss_mask_3: 0.1191  loss_dice_3: 0.9511  loss_ce_4: 0.8782  loss_mask_4: 0.1252  loss_dice_4: 1.028  loss_ce_5: 0.8266  loss_mask_5: 0.1108  loss_dice_5: 0.6563  loss_ce_6: 0.8354  loss_mask_6: 0.1224  loss_dice_6: 0.9533  loss_ce_7: 0.9309  loss_mask_7: 0.116  loss_dice_7: 0.7878  loss_ce_8: 0.8527  loss_mask_8: 0.1314  loss_dice_8: 0.9771    time: 0.3221  last_time: 0.2990  data_time: 0.0118  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:21 d2.utils.events]:  eta: 0:41:05  iter: 51939  total_loss: 30.64  loss_ce: 1.434  loss_mask: 0.1667  loss_dice: 1.433  loss_ce_0: 1.861  loss_mask_0: 0.151  loss_dice_0: 1.324  loss_ce_1: 1.778  loss_mask_1: 0.1316  loss_dice_1: 1.7  loss_ce_2: 1.635  loss_mask_2: 0.1452  loss_dice_2: 1.638  loss_ce_3: 1.366  loss_mask_3: 0.1468  loss_dice_3: 1.506  loss_ce_4: 1.497  loss_mask_4: 0.15  loss_dice_4: 1.262  loss_ce_5: 1.458  loss_mask_5: 0.1457  loss_dice_5: 1.336  loss_ce_6: 1.403  loss_mask_6: 0.1442  loss_dice_6: 1.544  loss_ce_7: 1.39  loss_mask_7: 0.1567  loss_dice_7: 1.669  loss_ce_8: 1.438  loss_mask_8: 0.1613  loss_dice_8: 1.578    time: 0.3220  last_time: 0.3195  data_time: 0.0072  last_data_time: 0.0049   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:27 d2.utils.events]:  eta: 0:40:59  iter: 51959  total_loss: 34.23  loss_ce: 0.9964  loss_mask: 0.09951  loss_dice: 1.579  loss_ce_0: 1.424  loss_mask_0: 0.1187  loss_dice_0: 1.601  loss_ce_1: 1.402  loss_mask_1: 0.1134  loss_dice_1: 2.135  loss_ce_2: 1.239  loss_mask_2: 0.1033  loss_dice_2: 1.696  loss_ce_3: 1.063  loss_mask_3: 0.1026  loss_dice_3: 1.533  loss_ce_4: 1.018  loss_mask_4: 0.09802  loss_dice_4: 1.61  loss_ce_5: 1.009  loss_mask_5: 0.09597  loss_dice_5: 1.655  loss_ce_6: 1.029  loss_mask_6: 0.1101  loss_dice_6: 1.501  loss_ce_7: 0.9916  loss_mask_7: 0.1  loss_dice_7: 1.625  loss_ce_8: 1.055  loss_mask_8: 0.1066  loss_dice_8: 1.624    time: 0.3220  last_time: 0.3055  data_time: 0.0160  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:34 d2.utils.events]:  eta: 0:40:53  iter: 51979  total_loss: 23.09  loss_ce: 1.124  loss_mask: 0.09932  loss_dice: 1.153  loss_ce_0: 1.386  loss_mask_0: 0.1311  loss_dice_0: 1.23  loss_ce_1: 1.305  loss_mask_1: 0.1434  loss_dice_1: 1.26  loss_ce_2: 1.231  loss_mask_2: 0.1246  loss_dice_2: 1.307  loss_ce_3: 1.126  loss_mask_3: 0.1064  loss_dice_3: 1.133  loss_ce_4: 1.152  loss_mask_4: 0.1011  loss_dice_4: 1.164  loss_ce_5: 1.095  loss_mask_5: 0.105  loss_dice_5: 1.292  loss_ce_6: 1.141  loss_mask_6: 0.09657  loss_dice_6: 1.189  loss_ce_7: 1.109  loss_mask_7: 0.1084  loss_dice_7: 1.093  loss_ce_8: 1.114  loss_mask_8: 0.093  loss_dice_8: 1.023    time: 0.3220  last_time: 0.3251  data_time: 0.0062  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:40 d2.utils.events]:  eta: 0:40:47  iter: 51999  total_loss: 20.38  loss_ce: 0.7599  loss_mask: 0.3176  loss_dice: 0.9972  loss_ce_0: 1.088  loss_mask_0: 0.3943  loss_dice_0: 0.9524  loss_ce_1: 0.8857  loss_mask_1: 0.2905  loss_dice_1: 0.9054  loss_ce_2: 0.9449  loss_mask_2: 0.3051  loss_dice_2: 0.7876  loss_ce_3: 0.7947  loss_mask_3: 0.3225  loss_dice_3: 0.8566  loss_ce_4: 0.8005  loss_mask_4: 0.339  loss_dice_4: 0.7984  loss_ce_5: 0.7776  loss_mask_5: 0.3565  loss_dice_5: 0.9251  loss_ce_6: 0.778  loss_mask_6: 0.3263  loss_dice_6: 0.8548  loss_ce_7: 0.7843  loss_mask_7: 0.3177  loss_dice_7: 0.8301  loss_ce_8: 0.7662  loss_mask_8: 0.3137  loss_dice_8: 0.9247    time: 0.3220  last_time: 0.3094  data_time: 0.0062  last_data_time: 0.0086   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:46 d2.utils.events]:  eta: 0:40:42  iter: 52019  total_loss: 20.55  loss_ce: 0.792  loss_mask: 0.07174  loss_dice: 1.268  loss_ce_0: 1.098  loss_mask_0: 0.07868  loss_dice_0: 1.1  loss_ce_1: 1.18  loss_mask_1: 0.07306  loss_dice_1: 1.055  loss_ce_2: 1.019  loss_mask_2: 0.0824  loss_dice_2: 1.236  loss_ce_3: 0.8393  loss_mask_3: 0.07562  loss_dice_3: 1.106  loss_ce_4: 0.8343  loss_mask_4: 0.07979  loss_dice_4: 0.9908  loss_ce_5: 0.7563  loss_mask_5: 0.07513  loss_dice_5: 1.203  loss_ce_6: 0.784  loss_mask_6: 0.07644  loss_dice_6: 0.9772  loss_ce_7: 0.7923  loss_mask_7: 0.08625  loss_dice_7: 1.362  loss_ce_8: 0.8122  loss_mask_8: 0.08459  loss_dice_8: 1.013    time: 0.3220  last_time: 0.3244  data_time: 0.0067  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:52 d2.utils.events]:  eta: 0:40:37  iter: 52039  total_loss: 25.39  loss_ce: 0.8266  loss_mask: 0.05899  loss_dice: 1.315  loss_ce_0: 1.323  loss_mask_0: 0.05682  loss_dice_0: 1.645  loss_ce_1: 1.133  loss_mask_1: 0.07963  loss_dice_1: 1.463  loss_ce_2: 1.164  loss_mask_2: 0.06571  loss_dice_2: 1.312  loss_ce_3: 0.8638  loss_mask_3: 0.06932  loss_dice_3: 1.511  loss_ce_4: 0.855  loss_mask_4: 0.07464  loss_dice_4: 1.356  loss_ce_5: 0.7987  loss_mask_5: 0.0526  loss_dice_5: 1.304  loss_ce_6: 0.7894  loss_mask_6: 0.0758  loss_dice_6: 1.369  loss_ce_7: 1.02  loss_mask_7: 0.08727  loss_dice_7: 1.388  loss_ce_8: 0.7993  loss_mask_8: 0.05576  loss_dice_8: 1.333    time: 0.3220  last_time: 0.3120  data_time: 0.0068  last_data_time: 0.0080   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:05:58 d2.utils.events]:  eta: 0:40:32  iter: 52059  total_loss: 23.82  loss_ce: 1.07  loss_mask: 0.153  loss_dice: 1.114  loss_ce_0: 1.123  loss_mask_0: 0.1293  loss_dice_0: 1.329  loss_ce_1: 1.128  loss_mask_1: 0.1286  loss_dice_1: 1.335  loss_ce_2: 1.11  loss_mask_2: 0.1345  loss_dice_2: 1.176  loss_ce_3: 1.033  loss_mask_3: 0.1531  loss_dice_3: 1.152  loss_ce_4: 1.03  loss_mask_4: 0.1464  loss_dice_4: 1.228  loss_ce_5: 1.052  loss_mask_5: 0.1523  loss_dice_5: 1.349  loss_ce_6: 1.059  loss_mask_6: 0.1592  loss_dice_6: 1.061  loss_ce_7: 1.072  loss_mask_7: 0.1583  loss_dice_7: 0.9852  loss_ce_8: 1.07  loss_mask_8: 0.1469  loss_dice_8: 0.9154    time: 0.3220  last_time: 0.3258  data_time: 0.0104  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:05 d2.utils.events]:  eta: 0:40:25  iter: 52079  total_loss: 24.63  loss_ce: 0.9242  loss_mask: 0.1631  loss_dice: 1.074  loss_ce_0: 1.265  loss_mask_0: 0.2048  loss_dice_0: 1.083  loss_ce_1: 1.188  loss_mask_1: 0.16  loss_dice_1: 1.05  loss_ce_2: 1.17  loss_mask_2: 0.142  loss_dice_2: 0.9642  loss_ce_3: 0.9314  loss_mask_3: 0.1875  loss_dice_3: 0.8901  loss_ce_4: 0.9338  loss_mask_4: 0.2052  loss_dice_4: 0.8895  loss_ce_5: 0.9184  loss_mask_5: 0.2088  loss_dice_5: 0.8502  loss_ce_6: 0.9258  loss_mask_6: 0.1657  loss_dice_6: 0.9699  loss_ce_7: 0.9137  loss_mask_7: 0.1692  loss_dice_7: 0.9238  loss_ce_8: 0.9942  loss_mask_8: 0.1697  loss_dice_8: 1.234    time: 0.3220  last_time: 0.3348  data_time: 0.0066  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:11 d2.utils.events]:  eta: 0:40:18  iter: 52099  total_loss: 23.92  loss_ce: 1.038  loss_mask: 0.09453  loss_dice: 1.388  loss_ce_0: 1.258  loss_mask_0: 0.1656  loss_dice_0: 1.345  loss_ce_1: 1.338  loss_mask_1: 0.1045  loss_dice_1: 1.337  loss_ce_2: 1.307  loss_mask_2: 0.131  loss_dice_2: 1.345  loss_ce_3: 1.172  loss_mask_3: 0.1019  loss_dice_3: 1.36  loss_ce_4: 1.123  loss_mask_4: 0.09825  loss_dice_4: 1.271  loss_ce_5: 1.023  loss_mask_5: 0.1036  loss_dice_5: 1.493  loss_ce_6: 1.06  loss_mask_6: 0.09971  loss_dice_6: 1.366  loss_ce_7: 1.076  loss_mask_7: 0.09857  loss_dice_7: 1.378  loss_ce_8: 1.068  loss_mask_8: 0.09869  loss_dice_8: 1.409    time: 0.3220  last_time: 0.3007  data_time: 0.0074  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:17 d2.utils.events]:  eta: 0:40:12  iter: 52119  total_loss: 30.23  loss_ce: 1.105  loss_mask: 0.1701  loss_dice: 1.919  loss_ce_0: 1.27  loss_mask_0: 0.2081  loss_dice_0: 1.643  loss_ce_1: 1.255  loss_mask_1: 0.2067  loss_dice_1: 1.476  loss_ce_2: 1.296  loss_mask_2: 0.2137  loss_dice_2: 1.517  loss_ce_3: 1.211  loss_mask_3: 0.2281  loss_dice_3: 1.659  loss_ce_4: 1.166  loss_mask_4: 0.1993  loss_dice_4: 1.616  loss_ce_5: 1.323  loss_mask_5: 0.1969  loss_dice_5: 1.665  loss_ce_6: 1.159  loss_mask_6: 0.1665  loss_dice_6: 1.64  loss_ce_7: 1.197  loss_mask_7: 0.1692  loss_dice_7: 1.442  loss_ce_8: 1.133  loss_mask_8: 0.1882  loss_dice_8: 1.61    time: 0.3220  last_time: 0.3021  data_time: 0.0070  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:23 d2.utils.events]:  eta: 0:40:05  iter: 52139  total_loss: 26.18  loss_ce: 1.024  loss_mask: 0.1048  loss_dice: 1.24  loss_ce_0: 1.365  loss_mask_0: 0.102  loss_dice_0: 1.418  loss_ce_1: 1.257  loss_mask_1: 0.08167  loss_dice_1: 1.188  loss_ce_2: 1.151  loss_mask_2: 0.1117  loss_dice_2: 1.218  loss_ce_3: 1.134  loss_mask_3: 0.1  loss_dice_3: 1.188  loss_ce_4: 1.021  loss_mask_4: 0.08535  loss_dice_4: 1.225  loss_ce_5: 0.9738  loss_mask_5: 0.1018  loss_dice_5: 1.291  loss_ce_6: 1.011  loss_mask_6: 0.09868  loss_dice_6: 1.303  loss_ce_7: 1.034  loss_mask_7: 0.09707  loss_dice_7: 1.354  loss_ce_8: 1.007  loss_mask_8: 0.08911  loss_dice_8: 1.333    time: 0.3220  last_time: 0.2999  data_time: 0.0074  last_data_time: 0.0079   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:30 d2.utils.events]:  eta: 0:40:01  iter: 52159  total_loss: 35.96  loss_ce: 1.109  loss_mask: 0.2281  loss_dice: 1.561  loss_ce_0: 1.49  loss_mask_0: 0.2884  loss_dice_0: 1.799  loss_ce_1: 1.445  loss_mask_1: 0.2541  loss_dice_1: 1.693  loss_ce_2: 1.18  loss_mask_2: 0.2073  loss_dice_2: 1.629  loss_ce_3: 1.109  loss_mask_3: 0.2027  loss_dice_3: 1.535  loss_ce_4: 1.112  loss_mask_4: 0.2115  loss_dice_4: 1.68  loss_ce_5: 1.087  loss_mask_5: 0.2316  loss_dice_5: 1.519  loss_ce_6: 1.117  loss_mask_6: 0.2112  loss_dice_6: 1.712  loss_ce_7: 1.094  loss_mask_7: 0.239  loss_dice_7: 1.5  loss_ce_8: 1.027  loss_mask_8: 0.1803  loss_dice_8: 1.593    time: 0.3220  last_time: 0.3017  data_time: 0.0075  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:36 d2.utils.events]:  eta: 0:39:54  iter: 52179  total_loss: 25.55  loss_ce: 1.077  loss_mask: 0.146  loss_dice: 1.119  loss_ce_0: 1.276  loss_mask_0: 0.214  loss_dice_0: 1.127  loss_ce_1: 1.351  loss_mask_1: 0.1664  loss_dice_1: 0.8903  loss_ce_2: 1.301  loss_mask_2: 0.1834  loss_dice_2: 1.207  loss_ce_3: 1.143  loss_mask_3: 0.1486  loss_dice_3: 1.059  loss_ce_4: 1.216  loss_mask_4: 0.1498  loss_dice_4: 1.061  loss_ce_5: 1.11  loss_mask_5: 0.1538  loss_dice_5: 1.28  loss_ce_6: 1.083  loss_mask_6: 0.1498  loss_dice_6: 1.218  loss_ce_7: 1.147  loss_mask_7: 0.1519  loss_dice_7: 1.16  loss_ce_8: 1.067  loss_mask_8: 0.1411  loss_dice_8: 1.372    time: 0.3220  last_time: 0.3013  data_time: 0.0170  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:42 d2.utils.events]:  eta: 0:39:48  iter: 52199  total_loss: 20  loss_ce: 0.9299  loss_mask: 0.1611  loss_dice: 0.7823  loss_ce_0: 1.254  loss_mask_0: 0.2146  loss_dice_0: 0.8815  loss_ce_1: 1.178  loss_mask_1: 0.2058  loss_dice_1: 0.8366  loss_ce_2: 1.209  loss_mask_2: 0.2085  loss_dice_2: 0.7583  loss_ce_3: 0.9853  loss_mask_3: 0.1828  loss_dice_3: 0.7292  loss_ce_4: 1.029  loss_mask_4: 0.1683  loss_dice_4: 0.7418  loss_ce_5: 1.1  loss_mask_5: 0.1919  loss_dice_5: 0.8956  loss_ce_6: 0.9171  loss_mask_6: 0.1733  loss_dice_6: 0.7409  loss_ce_7: 0.9495  loss_mask_7: 0.1864  loss_dice_7: 0.6933  loss_ce_8: 0.9247  loss_mask_8: 0.1805  loss_dice_8: 0.7307    time: 0.3220  last_time: 0.2960  data_time: 0.0100  last_data_time: 0.0050   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:48 d2.utils.events]:  eta: 0:39:41  iter: 52219  total_loss: 29.98  loss_ce: 0.9621  loss_mask: 0.1417  loss_dice: 1.542  loss_ce_0: 1.28  loss_mask_0: 0.1417  loss_dice_0: 1.436  loss_ce_1: 1.228  loss_mask_1: 0.1766  loss_dice_1: 1.537  loss_ce_2: 1.168  loss_mask_2: 0.1573  loss_dice_2: 1.51  loss_ce_3: 1.036  loss_mask_3: 0.1298  loss_dice_3: 1.522  loss_ce_4: 1.023  loss_mask_4: 0.1656  loss_dice_4: 1.608  loss_ce_5: 1.045  loss_mask_5: 0.1538  loss_dice_5: 1.608  loss_ce_6: 1.035  loss_mask_6: 0.1516  loss_dice_6: 1.434  loss_ce_7: 1.124  loss_mask_7: 0.1782  loss_dice_7: 1.428  loss_ce_8: 1.002  loss_mask_8: 0.1842  loss_dice_8: 1.515    time: 0.3220  last_time: 0.3020  data_time: 0.0071  last_data_time: 0.0087   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:06:55 d2.utils.events]:  eta: 0:39:34  iter: 52239  total_loss: 21.34  loss_ce: 0.8399  loss_mask: 0.1117  loss_dice: 0.7702  loss_ce_0: 1.27  loss_mask_0: 0.0971  loss_dice_0: 0.9956  loss_ce_1: 1.067  loss_mask_1: 0.1199  loss_dice_1: 0.9734  loss_ce_2: 1.04  loss_mask_2: 0.1251  loss_dice_2: 0.8919  loss_ce_3: 0.847  loss_mask_3: 0.1105  loss_dice_3: 0.9212  loss_ce_4: 0.8935  loss_mask_4: 0.0985  loss_dice_4: 0.9553  loss_ce_5: 0.9398  loss_mask_5: 0.1051  loss_dice_5: 0.9375  loss_ce_6: 0.8288  loss_mask_6: 0.1068  loss_dice_6: 0.8656  loss_ce_7: 0.8382  loss_mask_7: 0.1016  loss_dice_7: 0.8168  loss_ce_8: 0.8451  loss_mask_8: 0.1153  loss_dice_8: 0.6639    time: 0.3220  last_time: 0.3316  data_time: 0.0070  last_data_time: 0.0024   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:01 d2.utils.events]:  eta: 0:39:28  iter: 52259  total_loss: 32.52  loss_ce: 1.178  loss_mask: 0.1698  loss_dice: 1.416  loss_ce_0: 1.457  loss_mask_0: 0.2353  loss_dice_0: 1.453  loss_ce_1: 1.365  loss_mask_1: 0.2065  loss_dice_1: 1.077  loss_ce_2: 1.244  loss_mask_2: 0.1537  loss_dice_2: 1.32  loss_ce_3: 1.314  loss_mask_3: 0.1548  loss_dice_3: 1.47  loss_ce_4: 1.274  loss_mask_4: 0.1911  loss_dice_4: 1.219  loss_ce_5: 1.278  loss_mask_5: 0.1429  loss_dice_5: 1.265  loss_ce_6: 1.239  loss_mask_6: 0.2096  loss_dice_6: 1.302  loss_ce_7: 1.171  loss_mask_7: 0.2259  loss_dice_7: 1.583  loss_ce_8: 1.212  loss_mask_8: 0.1846  loss_dice_8: 1.297    time: 0.3220  last_time: 0.3037  data_time: 0.0073  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:07 d2.utils.events]:  eta: 0:39:21  iter: 52279  total_loss: 24.83  loss_ce: 1.098  loss_mask: 0.2274  loss_dice: 1.338  loss_ce_0: 1.21  loss_mask_0: 0.1708  loss_dice_0: 1.424  loss_ce_1: 1.362  loss_mask_1: 0.2194  loss_dice_1: 1.458  loss_ce_2: 1.184  loss_mask_2: 0.216  loss_dice_2: 1.257  loss_ce_3: 1.044  loss_mask_3: 0.2703  loss_dice_3: 1.3  loss_ce_4: 1.047  loss_mask_4: 0.2622  loss_dice_4: 1.266  loss_ce_5: 0.9383  loss_mask_5: 0.2557  loss_dice_5: 1.305  loss_ce_6: 1.069  loss_mask_6: 0.2354  loss_dice_6: 1.207  loss_ce_7: 0.985  loss_mask_7: 0.2304  loss_dice_7: 1.313  loss_ce_8: 1.097  loss_mask_8: 0.2379  loss_dice_8: 1.276    time: 0.3220  last_time: 0.2999  data_time: 0.0063  last_data_time: 0.0049   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:13 d2.utils.events]:  eta: 0:39:14  iter: 52299  total_loss: 26.26  loss_ce: 1.075  loss_mask: 0.1032  loss_dice: 1.113  loss_ce_0: 1.51  loss_mask_0: 0.07858  loss_dice_0: 1.074  loss_ce_1: 1.311  loss_mask_1: 0.08779  loss_dice_1: 1.184  loss_ce_2: 1.226  loss_mask_2: 0.0821  loss_dice_2: 1.248  loss_ce_3: 1.077  loss_mask_3: 0.08282  loss_dice_3: 1.059  loss_ce_4: 1.122  loss_mask_4: 0.08032  loss_dice_4: 1.068  loss_ce_5: 1.11  loss_mask_5: 0.09123  loss_dice_5: 1.267  loss_ce_6: 1.056  loss_mask_6: 0.1118  loss_dice_6: 1.175  loss_ce_7: 1.016  loss_mask_7: 0.1216  loss_dice_7: 1.176  loss_ce_8: 1.075  loss_mask_8: 0.1235  loss_dice_8: 1.149    time: 0.3220  last_time: 0.2940  data_time: 0.0073  last_data_time: 0.0047   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:20 d2.utils.events]:  eta: 0:39:08  iter: 52319  total_loss: 26.55  loss_ce: 1.01  loss_mask: 0.125  loss_dice: 1.179  loss_ce_0: 1.46  loss_mask_0: 0.1527  loss_dice_0: 1.133  loss_ce_1: 1.344  loss_mask_1: 0.1433  loss_dice_1: 1.065  loss_ce_2: 1.166  loss_mask_2: 0.1683  loss_dice_2: 1.18  loss_ce_3: 1.013  loss_mask_3: 0.1558  loss_dice_3: 1.016  loss_ce_4: 0.8779  loss_mask_4: 0.1449  loss_dice_4: 1.038  loss_ce_5: 0.869  loss_mask_5: 0.1293  loss_dice_5: 1.155  loss_ce_6: 0.8649  loss_mask_6: 0.1359  loss_dice_6: 1.01  loss_ce_7: 1.084  loss_mask_7: 0.1467  loss_dice_7: 1.06  loss_ce_8: 1.142  loss_mask_8: 0.1373  loss_dice_8: 0.9857    time: 0.3220  last_time: 0.3093  data_time: 0.0074  last_data_time: 0.0135   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:26 d2.utils.events]:  eta: 0:39:01  iter: 52339  total_loss: 29.78  loss_ce: 0.923  loss_mask: 0.1107  loss_dice: 1.199  loss_ce_0: 1.401  loss_mask_0: 0.1154  loss_dice_0: 1.549  loss_ce_1: 1.259  loss_mask_1: 0.1117  loss_dice_1: 1.502  loss_ce_2: 1.112  loss_mask_2: 0.134  loss_dice_2: 1.52  loss_ce_3: 0.9151  loss_mask_3: 0.1139  loss_dice_3: 1.347  loss_ce_4: 0.9584  loss_mask_4: 0.1263  loss_dice_4: 1.546  loss_ce_5: 0.935  loss_mask_5: 0.1094  loss_dice_5: 1.295  loss_ce_6: 0.8888  loss_mask_6: 0.1238  loss_dice_6: 1.34  loss_ce_7: 0.929  loss_mask_7: 0.1226  loss_dice_7: 1.489  loss_ce_8: 0.937  loss_mask_8: 0.1267  loss_dice_8: 1.412    time: 0.3220  last_time: 0.3180  data_time: 0.0069  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:32 d2.utils.events]:  eta: 0:38:56  iter: 52359  total_loss: 23.07  loss_ce: 0.8013  loss_mask: 0.1049  loss_dice: 1.205  loss_ce_0: 1.342  loss_mask_0: 0.1099  loss_dice_0: 1.111  loss_ce_1: 1.026  loss_mask_1: 0.09871  loss_dice_1: 1.316  loss_ce_2: 0.9448  loss_mask_2: 0.1067  loss_dice_2: 1.198  loss_ce_3: 0.8289  loss_mask_3: 0.1099  loss_dice_3: 1.15  loss_ce_4: 0.7622  loss_mask_4: 0.1103  loss_dice_4: 1.059  loss_ce_5: 0.7878  loss_mask_5: 0.103  loss_dice_5: 1.105  loss_ce_6: 0.7516  loss_mask_6: 0.09626  loss_dice_6: 1.293  loss_ce_7: 0.7463  loss_mask_7: 0.09284  loss_dice_7: 1.143  loss_ce_8: 0.7572  loss_mask_8: 0.1031  loss_dice_8: 1.171    time: 0.3220  last_time: 0.2927  data_time: 0.0072  last_data_time: 0.0045   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:38 d2.utils.events]:  eta: 0:38:51  iter: 52379  total_loss: 27.75  loss_ce: 0.9691  loss_mask: 0.1246  loss_dice: 1.103  loss_ce_0: 1.349  loss_mask_0: 0.1082  loss_dice_0: 1.084  loss_ce_1: 1.389  loss_mask_1: 0.1055  loss_dice_1: 1.19  loss_ce_2: 1.276  loss_mask_2: 0.1446  loss_dice_2: 1.261  loss_ce_3: 1.033  loss_mask_3: 0.1168  loss_dice_3: 1.427  loss_ce_4: 1.017  loss_mask_4: 0.1268  loss_dice_4: 1.162  loss_ce_5: 0.992  loss_mask_5: 0.1142  loss_dice_5: 1.492  loss_ce_6: 0.9676  loss_mask_6: 0.1216  loss_dice_6: 1.256  loss_ce_7: 0.982  loss_mask_7: 0.1169  loss_dice_7: 1.508  loss_ce_8: 0.9709  loss_mask_8: 0.1177  loss_dice_8: 1.333    time: 0.3220  last_time: 0.3145  data_time: 0.0069  last_data_time: 0.0098   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:45 d2.utils.events]:  eta: 0:38:45  iter: 52399  total_loss: 28.42  loss_ce: 1.127  loss_mask: 0.2083  loss_dice: 1.359  loss_ce_0: 1.834  loss_mask_0: 0.1539  loss_dice_0: 1.187  loss_ce_1: 1.599  loss_mask_1: 0.1537  loss_dice_1: 1.465  loss_ce_2: 1.441  loss_mask_2: 0.1742  loss_dice_2: 1.127  loss_ce_3: 1.233  loss_mask_3: 0.1666  loss_dice_3: 1.179  loss_ce_4: 1.14  loss_mask_4: 0.1811  loss_dice_4: 1.191  loss_ce_5: 1.16  loss_mask_5: 0.1856  loss_dice_5: 1.233  loss_ce_6: 1.135  loss_mask_6: 0.1845  loss_dice_6: 1.122  loss_ce_7: 1.13  loss_mask_7: 0.1835  loss_dice_7: 1.323  loss_ce_8: 1.167  loss_mask_8: 0.1758  loss_dice_8: 1.2    time: 0.3220  last_time: 0.3086  data_time: 0.0066  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:51 d2.utils.events]:  eta: 0:38:39  iter: 52419  total_loss: 23.47  loss_ce: 1.001  loss_mask: 0.09411  loss_dice: 1.127  loss_ce_0: 1.071  loss_mask_0: 0.1199  loss_dice_0: 1.131  loss_ce_1: 1.348  loss_mask_1: 0.0987  loss_dice_1: 1.246  loss_ce_2: 1.227  loss_mask_2: 0.08947  loss_dice_2: 1.021  loss_ce_3: 1.061  loss_mask_3: 0.1062  loss_dice_3: 1.11  loss_ce_4: 1.021  loss_mask_4: 0.09728  loss_dice_4: 0.9906  loss_ce_5: 0.9727  loss_mask_5: 0.1046  loss_dice_5: 1.111  loss_ce_6: 0.9781  loss_mask_6: 0.1006  loss_dice_6: 1.039  loss_ce_7: 1.032  loss_mask_7: 0.09865  loss_dice_7: 1.141  loss_ce_8: 0.9928  loss_mask_8: 0.08905  loss_dice_8: 1.028    time: 0.3220  last_time: 0.3187  data_time: 0.0071  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:07:57 d2.utils.events]:  eta: 0:38:35  iter: 52439  total_loss: 23.93  loss_ce: 0.9741  loss_mask: 0.159  loss_dice: 0.9898  loss_ce_0: 1.119  loss_mask_0: 0.1447  loss_dice_0: 1.046  loss_ce_1: 1.116  loss_mask_1: 0.1278  loss_dice_1: 1.195  loss_ce_2: 0.9805  loss_mask_2: 0.1264  loss_dice_2: 1.047  loss_ce_3: 0.9527  loss_mask_3: 0.1551  loss_dice_3: 0.9413  loss_ce_4: 0.8242  loss_mask_4: 0.1255  loss_dice_4: 0.8377  loss_ce_5: 0.7446  loss_mask_5: 0.1306  loss_dice_5: 0.9208  loss_ce_6: 0.8136  loss_mask_6: 0.1656  loss_dice_6: 1.131  loss_ce_7: 0.8852  loss_mask_7: 0.152  loss_dice_7: 0.9363  loss_ce_8: 0.8615  loss_mask_8: 0.1464  loss_dice_8: 0.9542    time: 0.3220  last_time: 0.3092  data_time: 0.0063  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:03 d2.utils.events]:  eta: 0:38:28  iter: 52459  total_loss: 26.88  loss_ce: 1.048  loss_mask: 0.1488  loss_dice: 1.021  loss_ce_0: 1.29  loss_mask_0: 0.2757  loss_dice_0: 1.499  loss_ce_1: 1.306  loss_mask_1: 0.2142  loss_dice_1: 1.534  loss_ce_2: 1.218  loss_mask_2: 0.2167  loss_dice_2: 1.388  loss_ce_3: 1.005  loss_mask_3: 0.1396  loss_dice_3: 1.152  loss_ce_4: 1.081  loss_mask_4: 0.1343  loss_dice_4: 1.299  loss_ce_5: 1.005  loss_mask_5: 0.1293  loss_dice_5: 1.185  loss_ce_6: 1.071  loss_mask_6: 0.1633  loss_dice_6: 1.115  loss_ce_7: 1.015  loss_mask_7: 0.1191  loss_dice_7: 1.207  loss_ce_8: 1.009  loss_mask_8: 0.1312  loss_dice_8: 1.111    time: 0.3219  last_time: 0.3527  data_time: 0.0079  last_data_time: 0.0301   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:09 d2.utils.events]:  eta: 0:38:22  iter: 52479  total_loss: 30.37  loss_ce: 1.252  loss_mask: 0.2398  loss_dice: 1.495  loss_ce_0: 1.471  loss_mask_0: 0.2448  loss_dice_0: 1.345  loss_ce_1: 1.382  loss_mask_1: 0.3179  loss_dice_1: 1.408  loss_ce_2: 1.122  loss_mask_2: 0.2754  loss_dice_2: 1.396  loss_ce_3: 1.221  loss_mask_3: 0.2762  loss_dice_3: 1.428  loss_ce_4: 1.183  loss_mask_4: 0.2961  loss_dice_4: 1.247  loss_ce_5: 1.073  loss_mask_5: 0.2449  loss_dice_5: 1.286  loss_ce_6: 0.9848  loss_mask_6: 0.2695  loss_dice_6: 1.225  loss_ce_7: 1.053  loss_mask_7: 0.2659  loss_dice_7: 1.289  loss_ce_8: 1.116  loss_mask_8: 0.2685  loss_dice_8: 1.224    time: 0.3219  last_time: 0.2977  data_time: 0.0065  last_data_time: 0.0021   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:16 d2.utils.events]:  eta: 0:38:16  iter: 52499  total_loss: 25.89  loss_ce: 1.099  loss_mask: 0.1236  loss_dice: 1.091  loss_ce_0: 1.391  loss_mask_0: 0.1559  loss_dice_0: 1.295  loss_ce_1: 1.374  loss_mask_1: 0.1199  loss_dice_1: 1.165  loss_ce_2: 1.265  loss_mask_2: 0.1353  loss_dice_2: 1.079  loss_ce_3: 1.096  loss_mask_3: 0.1271  loss_dice_3: 1.202  loss_ce_4: 1.035  loss_mask_4: 0.1284  loss_dice_4: 1.218  loss_ce_5: 1.179  loss_mask_5: 0.1307  loss_dice_5: 1.186  loss_ce_6: 1.091  loss_mask_6: 0.1363  loss_dice_6: 1.226  loss_ce_7: 1.056  loss_mask_7: 0.1265  loss_dice_7: 1.186  loss_ce_8: 1.099  loss_mask_8: 0.1203  loss_dice_8: 1.273    time: 0.3219  last_time: 0.3314  data_time: 0.0063  last_data_time: 0.0073   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:22 d2.utils.events]:  eta: 0:38:10  iter: 52519  total_loss: 23.55  loss_ce: 1.052  loss_mask: 0.1633  loss_dice: 1.091  loss_ce_0: 1.314  loss_mask_0: 0.1712  loss_dice_0: 1.032  loss_ce_1: 1.218  loss_mask_1: 0.2143  loss_dice_1: 1.277  loss_ce_2: 1.252  loss_mask_2: 0.168  loss_dice_2: 0.9576  loss_ce_3: 1.096  loss_mask_3: 0.1529  loss_dice_3: 0.9011  loss_ce_4: 1.127  loss_mask_4: 0.1315  loss_dice_4: 0.9297  loss_ce_5: 1.025  loss_mask_5: 0.1443  loss_dice_5: 1.242  loss_ce_6: 1.087  loss_mask_6: 0.1445  loss_dice_6: 1.113  loss_ce_7: 1.036  loss_mask_7: 0.1468  loss_dice_7: 0.9367  loss_ce_8: 1.079  loss_mask_8: 0.1547  loss_dice_8: 0.9644    time: 0.3219  last_time: 0.3029  data_time: 0.0074  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:28 d2.utils.events]:  eta: 0:38:03  iter: 52539  total_loss: 33.82  loss_ce: 1.31  loss_mask: 0.2099  loss_dice: 1.328  loss_ce_0: 2.282  loss_mask_0: 0.2292  loss_dice_0: 1.35  loss_ce_1: 1.749  loss_mask_1: 0.2424  loss_dice_1: 1.554  loss_ce_2: 1.565  loss_mask_2: 0.1991  loss_dice_2: 1.526  loss_ce_3: 1.286  loss_mask_3: 0.1974  loss_dice_3: 1.305  loss_ce_4: 1.307  loss_mask_4: 0.1816  loss_dice_4: 1.461  loss_ce_5: 1.32  loss_mask_5: 0.1716  loss_dice_5: 1.359  loss_ce_6: 1.32  loss_mask_6: 0.217  loss_dice_6: 1.332  loss_ce_7: 1.387  loss_mask_7: 0.2665  loss_dice_7: 1.471  loss_ce_8: 1.261  loss_mask_8: 0.2238  loss_dice_8: 1.292    time: 0.3219  last_time: 0.3019  data_time: 0.0077  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:34 d2.utils.events]:  eta: 0:37:56  iter: 52559  total_loss: 24.56  loss_ce: 0.851  loss_mask: 0.1084  loss_dice: 1.205  loss_ce_0: 0.9906  loss_mask_0: 0.1181  loss_dice_0: 1.25  loss_ce_1: 1.043  loss_mask_1: 0.1374  loss_dice_1: 1.37  loss_ce_2: 0.9442  loss_mask_2: 0.135  loss_dice_2: 1.361  loss_ce_3: 0.7439  loss_mask_3: 0.1332  loss_dice_3: 1.318  loss_ce_4: 0.7576  loss_mask_4: 0.1112  loss_dice_4: 1.305  loss_ce_5: 0.7746  loss_mask_5: 0.1436  loss_dice_5: 1.324  loss_ce_6: 0.8269  loss_mask_6: 0.1019  loss_dice_6: 1.292  loss_ce_7: 0.8096  loss_mask_7: 0.105  loss_dice_7: 1.262  loss_ce_8: 0.8063  loss_mask_8: 0.1043  loss_dice_8: 1.253    time: 0.3219  last_time: 0.2965  data_time: 0.0070  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:40 d2.utils.events]:  eta: 0:37:49  iter: 52579  total_loss: 31.27  loss_ce: 1.374  loss_mask: 0.2031  loss_dice: 1.711  loss_ce_0: 1.594  loss_mask_0: 0.2816  loss_dice_0: 1.459  loss_ce_1: 1.683  loss_mask_1: 0.241  loss_dice_1: 1.765  loss_ce_2: 1.441  loss_mask_2: 0.2955  loss_dice_2: 1.632  loss_ce_3: 1.426  loss_mask_3: 0.2339  loss_dice_3: 1.675  loss_ce_4: 1.333  loss_mask_4: 0.2011  loss_dice_4: 1.424  loss_ce_5: 1.319  loss_mask_5: 0.2043  loss_dice_5: 1.497  loss_ce_6: 1.357  loss_mask_6: 0.217  loss_dice_6: 1.563  loss_ce_7: 1.32  loss_mask_7: 0.2078  loss_dice_7: 1.632  loss_ce_8: 1.389  loss_mask_8: 0.2106  loss_dice_8: 1.661    time: 0.3219  last_time: 0.2952  data_time: 0.0070  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:47 d2.utils.events]:  eta: 0:37:41  iter: 52599  total_loss: 28.56  loss_ce: 1.219  loss_mask: 0.1216  loss_dice: 1.452  loss_ce_0: 1.383  loss_mask_0: 0.1204  loss_dice_0: 1.603  loss_ce_1: 1.434  loss_mask_1: 0.1189  loss_dice_1: 1.501  loss_ce_2: 1.298  loss_mask_2: 0.1135  loss_dice_2: 1.506  loss_ce_3: 1.257  loss_mask_3: 0.108  loss_dice_3: 1.353  loss_ce_4: 1.19  loss_mask_4: 0.1104  loss_dice_4: 1.317  loss_ce_5: 1.213  loss_mask_5: 0.1272  loss_dice_5: 1.161  loss_ce_6: 1.181  loss_mask_6: 0.1318  loss_dice_6: 0.9448  loss_ce_7: 1.289  loss_mask_7: 0.1137  loss_dice_7: 1.211  loss_ce_8: 1.217  loss_mask_8: 0.1071  loss_dice_8: 1.215    time: 0.3219  last_time: 0.2986  data_time: 0.0178  last_data_time: 0.0043   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:53 d2.utils.events]:  eta: 0:37:34  iter: 52619  total_loss: 27.97  loss_ce: 1.072  loss_mask: 0.1766  loss_dice: 1.455  loss_ce_0: 1.024  loss_mask_0: 0.1891  loss_dice_0: 1.308  loss_ce_1: 0.9426  loss_mask_1: 0.2062  loss_dice_1: 1.52  loss_ce_2: 0.9283  loss_mask_2: 0.1942  loss_dice_2: 1.41  loss_ce_3: 1.026  loss_mask_3: 0.1655  loss_dice_3: 1.279  loss_ce_4: 1.049  loss_mask_4: 0.1818  loss_dice_4: 1.379  loss_ce_5: 1.006  loss_mask_5: 0.2043  loss_dice_5: 1.496  loss_ce_6: 1.004  loss_mask_6: 0.1905  loss_dice_6: 1.472  loss_ce_7: 1.021  loss_mask_7: 0.1944  loss_dice_7: 1.44  loss_ce_8: 0.9858  loss_mask_8: 0.1818  loss_dice_8: 1.391    time: 0.3219  last_time: 0.2975  data_time: 0.0179  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:08:59 d2.utils.events]:  eta: 0:37:27  iter: 52639  total_loss: 24.15  loss_ce: 0.7194  loss_mask: 0.251  loss_dice: 1.468  loss_ce_0: 1.065  loss_mask_0: 0.2069  loss_dice_0: 1.006  loss_ce_1: 1.021  loss_mask_1: 0.2814  loss_dice_1: 1.529  loss_ce_2: 0.9253  loss_mask_2: 0.2049  loss_dice_2: 1.376  loss_ce_3: 0.7436  loss_mask_3: 0.2556  loss_dice_3: 1.356  loss_ce_4: 0.7259  loss_mask_4: 0.2536  loss_dice_4: 1.343  loss_ce_5: 0.7266  loss_mask_5: 0.2282  loss_dice_5: 1.116  loss_ce_6: 0.7592  loss_mask_6: 0.2514  loss_dice_6: 1.011  loss_ce_7: 0.6989  loss_mask_7: 0.2793  loss_dice_7: 1.295  loss_ce_8: 0.7045  loss_mask_8: 0.2638  loss_dice_8: 1.074    time: 0.3219  last_time: 0.3187  data_time: 0.0069  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:05 d2.utils.events]:  eta: 0:37:22  iter: 52659  total_loss: 29.77  loss_ce: 1.034  loss_mask: 0.2003  loss_dice: 1.108  loss_ce_0: 1.268  loss_mask_0: 0.3127  loss_dice_0: 0.8906  loss_ce_1: 1.23  loss_mask_1: 0.2685  loss_dice_1: 1.305  loss_ce_2: 1.176  loss_mask_2: 0.2976  loss_dice_2: 1.506  loss_ce_3: 1.075  loss_mask_3: 0.226  loss_dice_3: 1.139  loss_ce_4: 1.064  loss_mask_4: 0.2283  loss_dice_4: 1.006  loss_ce_5: 0.97  loss_mask_5: 0.2113  loss_dice_5: 1.292  loss_ce_6: 1.011  loss_mask_6: 0.2062  loss_dice_6: 1.487  loss_ce_7: 1.098  loss_mask_7: 0.2214  loss_dice_7: 1.072  loss_ce_8: 1.091  loss_mask_8: 0.2183  loss_dice_8: 1.123    time: 0.3219  last_time: 0.2975  data_time: 0.0075  last_data_time: 0.0051   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:11 d2.utils.events]:  eta: 0:37:15  iter: 52679  total_loss: 32.49  loss_ce: 1.019  loss_mask: 0.1626  loss_dice: 1.765  loss_ce_0: 1.193  loss_mask_0: 0.2506  loss_dice_0: 1.469  loss_ce_1: 1.352  loss_mask_1: 0.2132  loss_dice_1: 1.73  loss_ce_2: 1.298  loss_mask_2: 0.1948  loss_dice_2: 1.755  loss_ce_3: 1.284  loss_mask_3: 0.1477  loss_dice_3: 1.923  loss_ce_4: 1.203  loss_mask_4: 0.1431  loss_dice_4: 1.773  loss_ce_5: 1.168  loss_mask_5: 0.1303  loss_dice_5: 1.751  loss_ce_6: 1.068  loss_mask_6: 0.1431  loss_dice_6: 1.673  loss_ce_7: 1.056  loss_mask_7: 0.1861  loss_dice_7: 1.481  loss_ce_8: 1.088  loss_mask_8: 0.1539  loss_dice_8: 1.704    time: 0.3219  last_time: 0.3016  data_time: 0.0073  last_data_time: 0.0099   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:18 d2.utils.events]:  eta: 0:37:10  iter: 52699  total_loss: 32.84  loss_ce: 1.173  loss_mask: 0.1891  loss_dice: 2.001  loss_ce_0: 1.739  loss_mask_0: 0.1548  loss_dice_0: 1.946  loss_ce_1: 1.582  loss_mask_1: 0.1751  loss_dice_1: 2.095  loss_ce_2: 1.527  loss_mask_2: 0.1896  loss_dice_2: 2.085  loss_ce_3: 1.274  loss_mask_3: 0.1662  loss_dice_3: 1.92  loss_ce_4: 1.26  loss_mask_4: 0.1756  loss_dice_4: 1.952  loss_ce_5: 1.178  loss_mask_5: 0.1573  loss_dice_5: 1.99  loss_ce_6: 1.257  loss_mask_6: 0.182  loss_dice_6: 1.954  loss_ce_7: 1.254  loss_mask_7: 0.1513  loss_dice_7: 1.972  loss_ce_8: 1.209  loss_mask_8: 0.1961  loss_dice_8: 2.045    time: 0.3219  last_time: 0.2996  data_time: 0.0073  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:24 d2.utils.events]:  eta: 0:37:04  iter: 52719  total_loss: 29.95  loss_ce: 0.9219  loss_mask: 0.212  loss_dice: 1.75  loss_ce_0: 1.183  loss_mask_0: 0.3108  loss_dice_0: 1.648  loss_ce_1: 1.22  loss_mask_1: 0.1931  loss_dice_1: 1.536  loss_ce_2: 1.204  loss_mask_2: 0.2201  loss_dice_2: 1.491  loss_ce_3: 1.037  loss_mask_3: 0.1983  loss_dice_3: 1.591  loss_ce_4: 1.021  loss_mask_4: 0.1322  loss_dice_4: 1.522  loss_ce_5: 0.9913  loss_mask_5: 0.1649  loss_dice_5: 1.649  loss_ce_6: 1.02  loss_mask_6: 0.1657  loss_dice_6: 1.488  loss_ce_7: 0.9345  loss_mask_7: 0.2106  loss_dice_7: 1.475  loss_ce_8: 0.9517  loss_mask_8: 0.206  loss_dice_8: 1.795    time: 0.3219  last_time: 0.2943  data_time: 0.0080  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:30 d2.utils.events]:  eta: 0:36:58  iter: 52739  total_loss: 33.47  loss_ce: 1.432  loss_mask: 0.1493  loss_dice: 1.347  loss_ce_0: 1.674  loss_mask_0: 0.2455  loss_dice_0: 1.562  loss_ce_1: 1.533  loss_mask_1: 0.2559  loss_dice_1: 1.372  loss_ce_2: 1.434  loss_mask_2: 0.2501  loss_dice_2: 1.448  loss_ce_3: 1.412  loss_mask_3: 0.1571  loss_dice_3: 1.535  loss_ce_4: 1.415  loss_mask_4: 0.1638  loss_dice_4: 1.457  loss_ce_5: 1.368  loss_mask_5: 0.153  loss_dice_5: 1.512  loss_ce_6: 1.351  loss_mask_6: 0.1551  loss_dice_6: 1.548  loss_ce_7: 1.345  loss_mask_7: 0.1635  loss_dice_7: 1.543  loss_ce_8: 1.401  loss_mask_8: 0.1538  loss_dice_8: 1.64    time: 0.3219  last_time: 0.3027  data_time: 0.0065  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:36 d2.utils.events]:  eta: 0:36:50  iter: 52759  total_loss: 23.82  loss_ce: 1.023  loss_mask: 0.1121  loss_dice: 1.444  loss_ce_0: 1.287  loss_mask_0: 0.1488  loss_dice_0: 1.475  loss_ce_1: 1.238  loss_mask_1: 0.1603  loss_dice_1: 1.381  loss_ce_2: 1.271  loss_mask_2: 0.1385  loss_dice_2: 1.316  loss_ce_3: 1.076  loss_mask_3: 0.1556  loss_dice_3: 1.258  loss_ce_4: 1.085  loss_mask_4: 0.147  loss_dice_4: 1.141  loss_ce_5: 1.111  loss_mask_5: 0.1507  loss_dice_5: 1.21  loss_ce_6: 1.008  loss_mask_6: 0.1283  loss_dice_6: 1.199  loss_ce_7: 1.102  loss_mask_7: 0.144  loss_dice_7: 1.185  loss_ce_8: 1.082  loss_mask_8: 0.1313  loss_dice_8: 1.212    time: 0.3219  last_time: 0.3050  data_time: 0.0082  last_data_time: 0.0088   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:43 d2.utils.events]:  eta: 0:36:43  iter: 52779  total_loss: 31.94  loss_ce: 1.013  loss_mask: 0.1277  loss_dice: 1.229  loss_ce_0: 1.178  loss_mask_0: 0.1195  loss_dice_0: 1.725  loss_ce_1: 1.255  loss_mask_1: 0.1303  loss_dice_1: 1.774  loss_ce_2: 1.124  loss_mask_2: 0.144  loss_dice_2: 1.513  loss_ce_3: 1.116  loss_mask_3: 0.1489  loss_dice_3: 1.591  loss_ce_4: 1.031  loss_mask_4: 0.1206  loss_dice_4: 1.518  loss_ce_5: 1.037  loss_mask_5: 0.1284  loss_dice_5: 1.439  loss_ce_6: 1.094  loss_mask_6: 0.1372  loss_dice_6: 1.45  loss_ce_7: 0.9925  loss_mask_7: 0.1152  loss_dice_7: 1.32  loss_ce_8: 1.099  loss_mask_8: 0.1234  loss_dice_8: 1.659    time: 0.3219  last_time: 0.3048  data_time: 0.0080  last_data_time: 0.0096   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:49 d2.utils.events]:  eta: 0:36:37  iter: 52799  total_loss: 26.02  loss_ce: 0.8965  loss_mask: 0.13  loss_dice: 1.552  loss_ce_0: 1.296  loss_mask_0: 0.1333  loss_dice_0: 1.596  loss_ce_1: 1.456  loss_mask_1: 0.1588  loss_dice_1: 1.6  loss_ce_2: 1.264  loss_mask_2: 0.1618  loss_dice_2: 1.576  loss_ce_3: 0.9109  loss_mask_3: 0.1389  loss_dice_3: 1.567  loss_ce_4: 1.022  loss_mask_4: 0.134  loss_dice_4: 1.462  loss_ce_5: 0.9026  loss_mask_5: 0.1444  loss_dice_5: 1.426  loss_ce_6: 0.8664  loss_mask_6: 0.1164  loss_dice_6: 1.449  loss_ce_7: 0.7665  loss_mask_7: 0.1518  loss_dice_7: 1.471  loss_ce_8: 0.9376  loss_mask_8: 0.14  loss_dice_8: 1.261    time: 0.3219  last_time: 0.3022  data_time: 0.0070  last_data_time: 0.0023   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:09:55 d2.utils.events]:  eta: 0:36:29  iter: 52819  total_loss: 23.87  loss_ce: 1.015  loss_mask: 0.1263  loss_dice: 1.002  loss_ce_0: 1.26  loss_mask_0: 0.1321  loss_dice_0: 1.096  loss_ce_1: 1.089  loss_mask_1: 0.1685  loss_dice_1: 1.305  loss_ce_2: 1.036  loss_mask_2: 0.1956  loss_dice_2: 1.151  loss_ce_3: 0.937  loss_mask_3: 0.1282  loss_dice_3: 1.172  loss_ce_4: 1.017  loss_mask_4: 0.127  loss_dice_4: 1.133  loss_ce_5: 0.8751  loss_mask_5: 0.1261  loss_dice_5: 1.078  loss_ce_6: 0.9466  loss_mask_6: 0.121  loss_dice_6: 1.123  loss_ce_7: 0.9623  loss_mask_7: 0.1257  loss_dice_7: 1.047  loss_ce_8: 1.109  loss_mask_8: 0.1238  loss_dice_8: 1.092    time: 0.3219  last_time: 0.3037  data_time: 0.0072  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:01 d2.utils.events]:  eta: 0:36:22  iter: 52839  total_loss: 21.61  loss_ce: 0.8043  loss_mask: 0.1435  loss_dice: 0.9981  loss_ce_0: 1.027  loss_mask_0: 0.1236  loss_dice_0: 1.033  loss_ce_1: 1.128  loss_mask_1: 0.138  loss_dice_1: 0.9171  loss_ce_2: 0.8644  loss_mask_2: 0.1568  loss_dice_2: 0.9602  loss_ce_3: 0.821  loss_mask_3: 0.14  loss_dice_3: 1.049  loss_ce_4: 0.8187  loss_mask_4: 0.1229  loss_dice_4: 0.8907  loss_ce_5: 0.835  loss_mask_5: 0.1211  loss_dice_5: 0.9127  loss_ce_6: 0.7855  loss_mask_6: 0.1396  loss_dice_6: 0.8137  loss_ce_7: 0.7831  loss_mask_7: 0.1378  loss_dice_7: 1.012  loss_ce_8: 0.8394  loss_mask_8: 0.1311  loss_dice_8: 0.8884    time: 0.3219  last_time: 0.2986  data_time: 0.0090  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:07 d2.utils.events]:  eta: 0:36:16  iter: 52859  total_loss: 22.68  loss_ce: 1.066  loss_mask: 0.09473  loss_dice: 1.137  loss_ce_0: 1.225  loss_mask_0: 0.1345  loss_dice_0: 1.051  loss_ce_1: 1.171  loss_mask_1: 0.1051  loss_dice_1: 1.268  loss_ce_2: 1.07  loss_mask_2: 0.09311  loss_dice_2: 0.9456  loss_ce_3: 1.067  loss_mask_3: 0.08677  loss_dice_3: 1.103  loss_ce_4: 1.051  loss_mask_4: 0.0942  loss_dice_4: 1.146  loss_ce_5: 1.06  loss_mask_5: 0.09769  loss_dice_5: 1.023  loss_ce_6: 1.037  loss_mask_6: 0.1214  loss_dice_6: 0.9601  loss_ce_7: 1.065  loss_mask_7: 0.1072  loss_dice_7: 1.134  loss_ce_8: 1.002  loss_mask_8: 0.09869  loss_dice_8: 0.8145    time: 0.3219  last_time: 0.3031  data_time: 0.0074  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:14 d2.utils.events]:  eta: 0:36:10  iter: 52879  total_loss: 22.31  loss_ce: 1.02  loss_mask: 0.1685  loss_dice: 0.9717  loss_ce_0: 1.272  loss_mask_0: 0.2946  loss_dice_0: 1.199  loss_ce_1: 1.128  loss_mask_1: 0.2376  loss_dice_1: 1.144  loss_ce_2: 1.187  loss_mask_2: 0.1908  loss_dice_2: 1.002  loss_ce_3: 1.031  loss_mask_3: 0.1247  loss_dice_3: 0.9091  loss_ce_4: 1.048  loss_mask_4: 0.09985  loss_dice_4: 0.9791  loss_ce_5: 0.9932  loss_mask_5: 0.1006  loss_dice_5: 0.9814  loss_ce_6: 1.076  loss_mask_6: 0.1815  loss_dice_6: 1.159  loss_ce_7: 1.147  loss_mask_7: 0.1913  loss_dice_7: 1.034  loss_ce_8: 1.039  loss_mask_8: 0.1901  loss_dice_8: 1.031    time: 0.3219  last_time: 0.2938  data_time: 0.0094  last_data_time: 0.0042   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:20 d2.utils.events]:  eta: 0:36:04  iter: 52899  total_loss: 23.52  loss_ce: 1.005  loss_mask: 0.1899  loss_dice: 1.185  loss_ce_0: 1.364  loss_mask_0: 0.2559  loss_dice_0: 1.225  loss_ce_1: 1.153  loss_mask_1: 0.1816  loss_dice_1: 1.422  loss_ce_2: 1.093  loss_mask_2: 0.1843  loss_dice_2: 1.393  loss_ce_3: 0.9698  loss_mask_3: 0.1893  loss_dice_3: 1.154  loss_ce_4: 1.008  loss_mask_4: 0.1928  loss_dice_4: 1.158  loss_ce_5: 0.9551  loss_mask_5: 0.1994  loss_dice_5: 1.033  loss_ce_6: 0.9533  loss_mask_6: 0.2001  loss_dice_6: 1.234  loss_ce_7: 1.019  loss_mask_7: 0.1893  loss_dice_7: 1.207  loss_ce_8: 0.9778  loss_mask_8: 0.1785  loss_dice_8: 1.286    time: 0.3218  last_time: 0.3105  data_time: 0.0073  last_data_time: 0.0068   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:26 d2.utils.events]:  eta: 0:35:58  iter: 52919  total_loss: 31.85  loss_ce: 1.173  loss_mask: 0.1793  loss_dice: 1.521  loss_ce_0: 1.418  loss_mask_0: 0.2261  loss_dice_0: 1.839  loss_ce_1: 1.379  loss_mask_1: 0.2984  loss_dice_1: 1.858  loss_ce_2: 1.25  loss_mask_2: 0.2785  loss_dice_2: 1.668  loss_ce_3: 1.175  loss_mask_3: 0.2428  loss_dice_3: 1.595  loss_ce_4: 1.142  loss_mask_4: 0.2976  loss_dice_4: 1.746  loss_ce_5: 1.059  loss_mask_5: 0.1941  loss_dice_5: 1.498  loss_ce_6: 1.171  loss_mask_6: 0.1857  loss_dice_6: 1.549  loss_ce_7: 1.176  loss_mask_7: 0.1987  loss_dice_7: 1.641  loss_ce_8: 1.125  loss_mask_8: 0.1877  loss_dice_8: 1.618    time: 0.3218  last_time: 0.3022  data_time: 0.0076  last_data_time: 0.0047   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:32 d2.utils.events]:  eta: 0:35:51  iter: 52939  total_loss: 22.96  loss_ce: 0.7613  loss_mask: 0.1844  loss_dice: 0.9354  loss_ce_0: 0.9688  loss_mask_0: 0.2312  loss_dice_0: 0.9264  loss_ce_1: 0.9649  loss_mask_1: 0.2444  loss_dice_1: 1.011  loss_ce_2: 0.8483  loss_mask_2: 0.1958  loss_dice_2: 0.8195  loss_ce_3: 0.7915  loss_mask_3: 0.1933  loss_dice_3: 0.9124  loss_ce_4: 0.7608  loss_mask_4: 0.1425  loss_dice_4: 0.8949  loss_ce_5: 0.7869  loss_mask_5: 0.1482  loss_dice_5: 0.8527  loss_ce_6: 0.7619  loss_mask_6: 0.1511  loss_dice_6: 0.845  loss_ce_7: 0.8023  loss_mask_7: 0.1823  loss_dice_7: 0.9387  loss_ce_8: 0.79  loss_mask_8: 0.1953  loss_dice_8: 0.9279    time: 0.3218  last_time: 0.2937  data_time: 0.0072  last_data_time: 0.0037   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:39 d2.utils.events]:  eta: 0:35:45  iter: 52959  total_loss: 27.73  loss_ce: 0.991  loss_mask: 0.1352  loss_dice: 1.32  loss_ce_0: 1.414  loss_mask_0: 0.2126  loss_dice_0: 1.505  loss_ce_1: 1.206  loss_mask_1: 0.184  loss_dice_1: 1.702  loss_ce_2: 1.147  loss_mask_2: 0.1894  loss_dice_2: 1.374  loss_ce_3: 1.072  loss_mask_3: 0.1726  loss_dice_3: 1.48  loss_ce_4: 1.045  loss_mask_4: 0.1515  loss_dice_4: 1.411  loss_ce_5: 1.043  loss_mask_5: 0.176  loss_dice_5: 1.67  loss_ce_6: 0.9385  loss_mask_6: 0.1533  loss_dice_6: 1.414  loss_ce_7: 1.008  loss_mask_7: 0.1755  loss_dice_7: 1.347  loss_ce_8: 1.019  loss_mask_8: 0.1463  loss_dice_8: 1.358    time: 0.3218  last_time: 0.3092  data_time: 0.0064  last_data_time: 0.0135   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:45 d2.utils.events]:  eta: 0:35:39  iter: 52979  total_loss: 34.71  loss_ce: 1.44  loss_mask: 0.1363  loss_dice: 1.589  loss_ce_0: 1.566  loss_mask_0: 0.1406  loss_dice_0: 1.329  loss_ce_1: 1.587  loss_mask_1: 0.1421  loss_dice_1: 1.576  loss_ce_2: 1.599  loss_mask_2: 0.1391  loss_dice_2: 1.622  loss_ce_3: 1.481  loss_mask_3: 0.1071  loss_dice_3: 1.447  loss_ce_4: 1.431  loss_mask_4: 0.1122  loss_dice_4: 1.583  loss_ce_5: 1.424  loss_mask_5: 0.1129  loss_dice_5: 1.424  loss_ce_6: 1.477  loss_mask_6: 0.1122  loss_dice_6: 1.535  loss_ce_7: 1.485  loss_mask_7: 0.108  loss_dice_7: 1.428  loss_ce_8: 1.463  loss_mask_8: 0.1154  loss_dice_8: 1.532    time: 0.3218  last_time: 0.3000  data_time: 0.0063  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:51 d2.utils.events]:  eta: 0:35:33  iter: 52999  total_loss: 29.35  loss_ce: 1.012  loss_mask: 0.1124  loss_dice: 1.558  loss_ce_0: 1.203  loss_mask_0: 0.1241  loss_dice_0: 1.69  loss_ce_1: 1.299  loss_mask_1: 0.09041  loss_dice_1: 1.733  loss_ce_2: 1.271  loss_mask_2: 0.0852  loss_dice_2: 1.564  loss_ce_3: 1.115  loss_mask_3: 0.1058  loss_dice_3: 1.572  loss_ce_4: 1.121  loss_mask_4: 0.1228  loss_dice_4: 1.533  loss_ce_5: 1.174  loss_mask_5: 0.1054  loss_dice_5: 1.461  loss_ce_6: 1.142  loss_mask_6: 0.114  loss_dice_6: 1.619  loss_ce_7: 1.052  loss_mask_7: 0.09532  loss_dice_7: 1.43  loss_ce_8: 1.106  loss_mask_8: 0.0781  loss_dice_8: 1.498    time: 0.3218  last_time: 0.3240  data_time: 0.0152  last_data_time: 0.0068   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:10:57 d2.utils.events]:  eta: 0:35:25  iter: 53019  total_loss: 23.64  loss_ce: 1.097  loss_mask: 0.1831  loss_dice: 0.8493  loss_ce_0: 1.411  loss_mask_0: 0.217  loss_dice_0: 0.7961  loss_ce_1: 1.296  loss_mask_1: 0.1616  loss_dice_1: 0.8607  loss_ce_2: 1.302  loss_mask_2: 0.1708  loss_dice_2: 0.9173  loss_ce_3: 1.19  loss_mask_3: 0.1882  loss_dice_3: 0.9346  loss_ce_4: 1.15  loss_mask_4: 0.1919  loss_dice_4: 0.8297  loss_ce_5: 1.228  loss_mask_5: 0.1969  loss_dice_5: 0.8793  loss_ce_6: 1.158  loss_mask_6: 0.1848  loss_dice_6: 0.9349  loss_ce_7: 1.037  loss_mask_7: 0.1936  loss_dice_7: 1.099  loss_ce_8: 1.072  loss_mask_8: 0.1766  loss_dice_8: 0.9147    time: 0.3218  last_time: 0.3179  data_time: 0.0061  last_data_time: 0.0045   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:04 d2.utils.events]:  eta: 0:35:19  iter: 53039  total_loss: 28.11  loss_ce: 1.095  loss_mask: 0.09569  loss_dice: 1.335  loss_ce_0: 1.479  loss_mask_0: 0.1257  loss_dice_0: 1.346  loss_ce_1: 1.45  loss_mask_1: 0.09353  loss_dice_1: 1.365  loss_ce_2: 1.304  loss_mask_2: 0.07334  loss_dice_2: 1.311  loss_ce_3: 1.058  loss_mask_3: 0.1175  loss_dice_3: 1.347  loss_ce_4: 1.129  loss_mask_4: 0.1056  loss_dice_4: 1.309  loss_ce_5: 1.044  loss_mask_5: 0.1115  loss_dice_5: 1.28  loss_ce_6: 1.147  loss_mask_6: 0.08566  loss_dice_6: 1.29  loss_ce_7: 1.1  loss_mask_7: 0.08713  loss_dice_7: 1.343  loss_ce_8: 1.149  loss_mask_8: 0.0905  loss_dice_8: 1.206    time: 0.3218  last_time: 0.2968  data_time: 0.0079  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:10 d2.utils.events]:  eta: 0:35:13  iter: 53059  total_loss: 27.94  loss_ce: 1.047  loss_mask: 0.1134  loss_dice: 1.254  loss_ce_0: 1.263  loss_mask_0: 0.1377  loss_dice_0: 1.381  loss_ce_1: 1.248  loss_mask_1: 0.1351  loss_dice_1: 1.282  loss_ce_2: 1.131  loss_mask_2: 0.132  loss_dice_2: 1.125  loss_ce_3: 0.9811  loss_mask_3: 0.1383  loss_dice_3: 1.323  loss_ce_4: 1.056  loss_mask_4: 0.09702  loss_dice_4: 0.9999  loss_ce_5: 1.006  loss_mask_5: 0.1524  loss_dice_5: 1.109  loss_ce_6: 1.016  loss_mask_6: 0.1341  loss_dice_6: 1.298  loss_ce_7: 1.034  loss_mask_7: 0.1353  loss_dice_7: 1.238  loss_ce_8: 1.096  loss_mask_8: 0.1245  loss_dice_8: 1.318    time: 0.3218  last_time: 0.2992  data_time: 0.0074  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:16 d2.utils.events]:  eta: 0:35:06  iter: 53079  total_loss: 25.46  loss_ce: 0.9256  loss_mask: 0.09385  loss_dice: 1.458  loss_ce_0: 1.068  loss_mask_0: 0.09913  loss_dice_0: 1.398  loss_ce_1: 1.107  loss_mask_1: 0.1079  loss_dice_1: 1.35  loss_ce_2: 1.114  loss_mask_2: 0.1034  loss_dice_2: 1.328  loss_ce_3: 0.9432  loss_mask_3: 0.0849  loss_dice_3: 1.327  loss_ce_4: 1.03  loss_mask_4: 0.0913  loss_dice_4: 1.47  loss_ce_5: 1.081  loss_mask_5: 0.09724  loss_dice_5: 1.441  loss_ce_6: 1.071  loss_mask_6: 0.09835  loss_dice_6: 1.522  loss_ce_7: 1.083  loss_mask_7: 0.1002  loss_dice_7: 1.406  loss_ce_8: 1.078  loss_mask_8: 0.09787  loss_dice_8: 1.503    time: 0.3218  last_time: 0.3013  data_time: 0.0122  last_data_time: 0.0080   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:23 d2.utils.events]:  eta: 0:35:01  iter: 53099  total_loss: 18.22  loss_ce: 0.9403  loss_mask: 0.09174  loss_dice: 0.9124  loss_ce_0: 1.05  loss_mask_0: 0.1629  loss_dice_0: 1.044  loss_ce_1: 1.019  loss_mask_1: 0.1106  loss_dice_1: 0.9426  loss_ce_2: 0.9806  loss_mask_2: 0.104  loss_dice_2: 0.6917  loss_ce_3: 0.9041  loss_mask_3: 0.09697  loss_dice_3: 0.799  loss_ce_4: 0.9171  loss_mask_4: 0.09523  loss_dice_4: 0.9153  loss_ce_5: 0.9801  loss_mask_5: 0.09169  loss_dice_5: 0.8032  loss_ce_6: 0.9293  loss_mask_6: 0.1015  loss_dice_6: 0.8485  loss_ce_7: 0.9311  loss_mask_7: 0.1002  loss_dice_7: 0.8778  loss_ce_8: 0.9674  loss_mask_8: 0.0929  loss_dice_8: 0.6644    time: 0.3218  last_time: 0.3213  data_time: 0.0071  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:29 d2.utils.events]:  eta: 0:34:55  iter: 53119  total_loss: 27.89  loss_ce: 1.091  loss_mask: 0.172  loss_dice: 1.486  loss_ce_0: 1.355  loss_mask_0: 0.162  loss_dice_0: 1.323  loss_ce_1: 1.335  loss_mask_1: 0.1334  loss_dice_1: 1.18  loss_ce_2: 1.23  loss_mask_2: 0.1833  loss_dice_2: 1.265  loss_ce_3: 1.142  loss_mask_3: 0.2  loss_dice_3: 1.315  loss_ce_4: 1.099  loss_mask_4: 0.1688  loss_dice_4: 1.475  loss_ce_5: 1.074  loss_mask_5: 0.1665  loss_dice_5: 1.53  loss_ce_6: 1.076  loss_mask_6: 0.1941  loss_dice_6: 1.461  loss_ce_7: 1.008  loss_mask_7: 0.1749  loss_dice_7: 1.435  loss_ce_8: 1.138  loss_mask_8: 0.1692  loss_dice_8: 1.453    time: 0.3218  last_time: 0.3062  data_time: 0.0067  last_data_time: 0.0072   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:35 d2.utils.events]:  eta: 0:34:50  iter: 53139  total_loss: 35.77  loss_ce: 1.326  loss_mask: 0.1589  loss_dice: 1.729  loss_ce_0: 1.531  loss_mask_0: 0.1423  loss_dice_0: 1.691  loss_ce_1: 1.435  loss_mask_1: 0.1904  loss_dice_1: 1.57  loss_ce_2: 1.467  loss_mask_2: 0.1777  loss_dice_2: 1.575  loss_ce_3: 1.409  loss_mask_3: 0.2106  loss_dice_3: 1.448  loss_ce_4: 1.242  loss_mask_4: 0.1892  loss_dice_4: 1.475  loss_ce_5: 1.387  loss_mask_5: 0.1633  loss_dice_5: 1.717  loss_ce_6: 1.287  loss_mask_6: 0.1871  loss_dice_6: 1.789  loss_ce_7: 1.283  loss_mask_7: 0.1637  loss_dice_7: 1.533  loss_ce_8: 1.43  loss_mask_8: 0.1684  loss_dice_8: 1.849    time: 0.3218  last_time: 0.3064  data_time: 0.0088  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:41 d2.utils.events]:  eta: 0:34:44  iter: 53159  total_loss: 38.63  loss_ce: 1.623  loss_mask: 0.1631  loss_dice: 1.896  loss_ce_0: 2.024  loss_mask_0: 0.1518  loss_dice_0: 1.999  loss_ce_1: 1.717  loss_mask_1: 0.1374  loss_dice_1: 1.966  loss_ce_2: 1.599  loss_mask_2: 0.1423  loss_dice_2: 1.974  loss_ce_3: 1.528  loss_mask_3: 0.1402  loss_dice_3: 2.071  loss_ce_4: 1.571  loss_mask_4: 0.1423  loss_dice_4: 2.188  loss_ce_5: 1.581  loss_mask_5: 0.1534  loss_dice_5: 2.17  loss_ce_6: 1.614  loss_mask_6: 0.2021  loss_dice_6: 2.104  loss_ce_7: 1.565  loss_mask_7: 0.173  loss_dice_7: 2  loss_ce_8: 1.588  loss_mask_8: 0.1468  loss_dice_8: 1.959    time: 0.3218  last_time: 0.3274  data_time: 0.0075  last_data_time: 0.0093   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:48 d2.utils.events]:  eta: 0:34:38  iter: 53179  total_loss: 32.03  loss_ce: 1.223  loss_mask: 0.1752  loss_dice: 1.673  loss_ce_0: 1.29  loss_mask_0: 0.2794  loss_dice_0: 1.738  loss_ce_1: 1.491  loss_mask_1: 0.1539  loss_dice_1: 1.656  loss_ce_2: 1.268  loss_mask_2: 0.1386  loss_dice_2: 1.718  loss_ce_3: 1.271  loss_mask_3: 0.1696  loss_dice_3: 1.742  loss_ce_4: 1.289  loss_mask_4: 0.2016  loss_dice_4: 1.798  loss_ce_5: 1.118  loss_mask_5: 0.186  loss_dice_5: 1.768  loss_ce_6: 1.281  loss_mask_6: 0.2017  loss_dice_6: 1.666  loss_ce_7: 1.163  loss_mask_7: 0.1954  loss_dice_7: 1.751  loss_ce_8: 1.265  loss_mask_8: 0.1775  loss_dice_8: 1.862    time: 0.3218  last_time: 0.3028  data_time: 0.0092  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:11:54 d2.utils.events]:  eta: 0:34:31  iter: 53199  total_loss: 30.16  loss_ce: 1.159  loss_mask: 0.1983  loss_dice: 1.594  loss_ce_0: 1.396  loss_mask_0: 0.1915  loss_dice_0: 1.44  loss_ce_1: 1.318  loss_mask_1: 0.1796  loss_dice_1: 1.497  loss_ce_2: 1.207  loss_mask_2: 0.1993  loss_dice_2: 1.339  loss_ce_3: 1.031  loss_mask_3: 0.1802  loss_dice_3: 1.351  loss_ce_4: 0.9976  loss_mask_4: 0.1756  loss_dice_4: 1.492  loss_ce_5: 0.9893  loss_mask_5: 0.1735  loss_dice_5: 1.385  loss_ce_6: 1.02  loss_mask_6: 0.1642  loss_dice_6: 1.426  loss_ce_7: 1.088  loss_mask_7: 0.1492  loss_dice_7: 1.412  loss_ce_8: 1.059  loss_mask_8: 0.1873  loss_dice_8: 1.436    time: 0.3218  last_time: 0.3259  data_time: 0.0079  last_data_time: 0.0087   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:00 d2.utils.events]:  eta: 0:34:26  iter: 53219  total_loss: 26.69  loss_ce: 1.206  loss_mask: 0.05723  loss_dice: 1.536  loss_ce_0: 1.266  loss_mask_0: 0.0736  loss_dice_0: 1.468  loss_ce_1: 1.17  loss_mask_1: 0.07098  loss_dice_1: 1.832  loss_ce_2: 1.236  loss_mask_2: 0.07182  loss_dice_2: 1.633  loss_ce_3: 1.212  loss_mask_3: 0.07138  loss_dice_3: 1.638  loss_ce_4: 1.27  loss_mask_4: 0.06255  loss_dice_4: 1.604  loss_ce_5: 1.295  loss_mask_5: 0.06475  loss_dice_5: 1.669  loss_ce_6: 1.214  loss_mask_6: 0.06913  loss_dice_6: 1.601  loss_ce_7: 1.271  loss_mask_7: 0.06341  loss_dice_7: 1.466  loss_ce_8: 1.232  loss_mask_8: 0.05626  loss_dice_8: 1.55    time: 0.3218  last_time: 0.3035  data_time: 0.0079  last_data_time: 0.0096   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:07 d2.utils.events]:  eta: 0:34:21  iter: 53239  total_loss: 25.86  loss_ce: 1.063  loss_mask: 0.19  loss_dice: 0.9509  loss_ce_0: 1.303  loss_mask_0: 0.2371  loss_dice_0: 1.106  loss_ce_1: 1.26  loss_mask_1: 0.3057  loss_dice_1: 1.149  loss_ce_2: 1.163  loss_mask_2: 0.2296  loss_dice_2: 1.209  loss_ce_3: 1.119  loss_mask_3: 0.2027  loss_dice_3: 1.138  loss_ce_4: 1.142  loss_mask_4: 0.1823  loss_dice_4: 1.07  loss_ce_5: 1.059  loss_mask_5: 0.1808  loss_dice_5: 1.125  loss_ce_6: 1.166  loss_mask_6: 0.166  loss_dice_6: 1.045  loss_ce_7: 1.022  loss_mask_7: 0.174  loss_dice_7: 1.07  loss_ce_8: 1.128  loss_mask_8: 0.184  loss_dice_8: 1.21    time: 0.3218  last_time: 0.3774  data_time: 0.0106  last_data_time: 0.0814   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:13 d2.utils.events]:  eta: 0:34:16  iter: 53259  total_loss: 32.57  loss_ce: 1.476  loss_mask: 0.1733  loss_dice: 1.452  loss_ce_0: 1.612  loss_mask_0: 0.1989  loss_dice_0: 1.661  loss_ce_1: 1.526  loss_mask_1: 0.2126  loss_dice_1: 1.676  loss_ce_2: 1.58  loss_mask_2: 0.1564  loss_dice_2: 1.43  loss_ce_3: 1.504  loss_mask_3: 0.189  loss_dice_3: 1.521  loss_ce_4: 1.459  loss_mask_4: 0.1765  loss_dice_4: 1.668  loss_ce_5: 1.439  loss_mask_5: 0.1693  loss_dice_5: 1.561  loss_ce_6: 1.425  loss_mask_6: 0.1719  loss_dice_6: 1.442  loss_ce_7: 1.427  loss_mask_7: 0.1732  loss_dice_7: 1.696  loss_ce_8: 1.405  loss_mask_8: 0.1841  loss_dice_8: 1.567    time: 0.3218  last_time: 0.3053  data_time: 0.0072  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:19 d2.utils.events]:  eta: 0:34:11  iter: 53279  total_loss: 28.55  loss_ce: 1.228  loss_mask: 0.07951  loss_dice: 1.652  loss_ce_0: 1.456  loss_mask_0: 0.1067  loss_dice_0: 1.675  loss_ce_1: 1.331  loss_mask_1: 0.09722  loss_dice_1: 1.795  loss_ce_2: 1.243  loss_mask_2: 0.09855  loss_dice_2: 1.568  loss_ce_3: 1.19  loss_mask_3: 0.09517  loss_dice_3: 1.601  loss_ce_4: 1.154  loss_mask_4: 0.09347  loss_dice_4: 1.642  loss_ce_5: 1.18  loss_mask_5: 0.09552  loss_dice_5: 1.83  loss_ce_6: 1.144  loss_mask_6: 0.08986  loss_dice_6: 1.701  loss_ce_7: 1.136  loss_mask_7: 0.09586  loss_dice_7: 1.704  loss_ce_8: 1.154  loss_mask_8: 0.09504  loss_dice_8: 1.703    time: 0.3218  last_time: 0.3235  data_time: 0.0074  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:25 d2.utils.events]:  eta: 0:34:05  iter: 53299  total_loss: 26.16  loss_ce: 1.068  loss_mask: 0.1462  loss_dice: 1.117  loss_ce_0: 1.227  loss_mask_0: 0.1129  loss_dice_0: 0.9518  loss_ce_1: 1.138  loss_mask_1: 0.1467  loss_dice_1: 1.19  loss_ce_2: 1.158  loss_mask_2: 0.1483  loss_dice_2: 0.9164  loss_ce_3: 1.134  loss_mask_3: 0.1636  loss_dice_3: 1.203  loss_ce_4: 1.048  loss_mask_4: 0.1701  loss_dice_4: 1.148  loss_ce_5: 1.048  loss_mask_5: 0.1643  loss_dice_5: 0.9518  loss_ce_6: 1.027  loss_mask_6: 0.2083  loss_dice_6: 1.118  loss_ce_7: 1.06  loss_mask_7: 0.1841  loss_dice_7: 1.126  loss_ce_8: 1.08  loss_mask_8: 0.1635  loss_dice_8: 1.021    time: 0.3218  last_time: 0.3228  data_time: 0.0068  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:32 d2.utils.events]:  eta: 0:33:59  iter: 53319  total_loss: 32.95  loss_ce: 1.03  loss_mask: 0.1638  loss_dice: 1.485  loss_ce_0: 1.323  loss_mask_0: 0.1573  loss_dice_0: 1.749  loss_ce_1: 1.337  loss_mask_1: 0.1233  loss_dice_1: 1.692  loss_ce_2: 1.235  loss_mask_2: 0.2037  loss_dice_2: 1.754  loss_ce_3: 1.045  loss_mask_3: 0.1689  loss_dice_3: 1.747  loss_ce_4: 1.061  loss_mask_4: 0.1614  loss_dice_4: 1.711  loss_ce_5: 0.9948  loss_mask_5: 0.2387  loss_dice_5: 1.745  loss_ce_6: 1.033  loss_mask_6: 0.1537  loss_dice_6: 1.508  loss_ce_7: 1.039  loss_mask_7: 0.1594  loss_dice_7: 1.562  loss_ce_8: 1.009  loss_mask_8: 0.1515  loss_dice_8: 1.534    time: 0.3218  last_time: 0.3004  data_time: 0.0077  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:38 d2.utils.events]:  eta: 0:33:52  iter: 53339  total_loss: 22.56  loss_ce: 0.8133  loss_mask: 0.1076  loss_dice: 0.9737  loss_ce_0: 1.065  loss_mask_0: 0.1122  loss_dice_0: 1.43  loss_ce_1: 0.9877  loss_mask_1: 0.09711  loss_dice_1: 1.124  loss_ce_2: 0.8362  loss_mask_2: 0.1264  loss_dice_2: 1.206  loss_ce_3: 0.7836  loss_mask_3: 0.1145  loss_dice_3: 1.259  loss_ce_4: 0.7742  loss_mask_4: 0.1  loss_dice_4: 1.254  loss_ce_5: 0.7788  loss_mask_5: 0.09991  loss_dice_5: 1.02  loss_ce_6: 0.7663  loss_mask_6: 0.1018  loss_dice_6: 1.49  loss_ce_7: 0.7651  loss_mask_7: 0.113  loss_dice_7: 0.9537  loss_ce_8: 0.7618  loss_mask_8: 0.1193  loss_dice_8: 1.063    time: 0.3218  last_time: 0.2925  data_time: 0.0081  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:44 d2.utils.events]:  eta: 0:33:45  iter: 53359  total_loss: 28.16  loss_ce: 1.512  loss_mask: 0.1148  loss_dice: 1.281  loss_ce_0: 1.678  loss_mask_0: 0.1312  loss_dice_0: 1.295  loss_ce_1: 1.543  loss_mask_1: 0.1223  loss_dice_1: 1.4  loss_ce_2: 1.51  loss_mask_2: 0.1164  loss_dice_2: 1.517  loss_ce_3: 1.335  loss_mask_3: 0.1346  loss_dice_3: 1.309  loss_ce_4: 1.306  loss_mask_4: 0.1311  loss_dice_4: 1.47  loss_ce_5: 1.43  loss_mask_5: 0.1263  loss_dice_5: 1.325  loss_ce_6: 1.448  loss_mask_6: 0.1223  loss_dice_6: 1.302  loss_ce_7: 1.465  loss_mask_7: 0.1241  loss_dice_7: 1.272  loss_ce_8: 1.468  loss_mask_8: 0.118  loss_dice_8: 1.299    time: 0.3218  last_time: 0.3078  data_time: 0.0078  last_data_time: 0.0124   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:50 d2.utils.events]:  eta: 0:33:39  iter: 53379  total_loss: 29.2  loss_ce: 1.107  loss_mask: 0.1323  loss_dice: 1.413  loss_ce_0: 1.352  loss_mask_0: 0.1306  loss_dice_0: 1.396  loss_ce_1: 1.239  loss_mask_1: 0.1445  loss_dice_1: 1.557  loss_ce_2: 1.095  loss_mask_2: 0.1573  loss_dice_2: 1.641  loss_ce_3: 1.02  loss_mask_3: 0.1552  loss_dice_3: 1.442  loss_ce_4: 1.031  loss_mask_4: 0.1915  loss_dice_4: 1.72  loss_ce_5: 1.038  loss_mask_5: 0.1514  loss_dice_5: 1.364  loss_ce_6: 1.063  loss_mask_6: 0.1909  loss_dice_6: 1.478  loss_ce_7: 1.039  loss_mask_7: 0.1605  loss_dice_7: 1.409  loss_ce_8: 1.087  loss_mask_8: 0.1644  loss_dice_8: 1.561    time: 0.3218  last_time: 0.3334  data_time: 0.0132  last_data_time: 0.0140   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:12:56 d2.utils.events]:  eta: 0:33:31  iter: 53399  total_loss: 24.24  loss_ce: 0.8601  loss_mask: 0.09956  loss_dice: 1.1  loss_ce_0: 1.326  loss_mask_0: 0.1236  loss_dice_0: 1.094  loss_ce_1: 1.292  loss_mask_1: 0.1854  loss_dice_1: 1.057  loss_ce_2: 1.196  loss_mask_2: 0.1064  loss_dice_2: 1.157  loss_ce_3: 1.016  loss_mask_3: 0.1254  loss_dice_3: 0.9027  loss_ce_4: 1.034  loss_mask_4: 0.1006  loss_dice_4: 0.9894  loss_ce_5: 0.9898  loss_mask_5: 0.1074  loss_dice_5: 1.023  loss_ce_6: 0.8937  loss_mask_6: 0.09666  loss_dice_6: 0.986  loss_ce_7: 0.9647  loss_mask_7: 0.1114  loss_dice_7: 0.9902  loss_ce_8: 1.003  loss_mask_8: 0.08981  loss_dice_8: 0.9538    time: 0.3218  last_time: 0.2981  data_time: 0.0064  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:03 d2.utils.events]:  eta: 0:33:25  iter: 53419  total_loss: 24.33  loss_ce: 0.9775  loss_mask: 0.09127  loss_dice: 1.164  loss_ce_0: 1.223  loss_mask_0: 0.09121  loss_dice_0: 1.119  loss_ce_1: 1.102  loss_mask_1: 0.1046  loss_dice_1: 1.097  loss_ce_2: 1.088  loss_mask_2: 0.1109  loss_dice_2: 1.039  loss_ce_3: 1.16  loss_mask_3: 0.09844  loss_dice_3: 0.9109  loss_ce_4: 1.044  loss_mask_4: 0.1002  loss_dice_4: 1.21  loss_ce_5: 1.076  loss_mask_5: 0.08722  loss_dice_5: 1.077  loss_ce_6: 1.11  loss_mask_6: 0.09444  loss_dice_6: 0.9818  loss_ce_7: 1.055  loss_mask_7: 0.08912  loss_dice_7: 1.115  loss_ce_8: 0.9628  loss_mask_8: 0.08332  loss_dice_8: 1.043    time: 0.3218  last_time: 0.3346  data_time: 0.0070  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:09 d2.utils.events]:  eta: 0:33:19  iter: 53439  total_loss: 29.85  loss_ce: 1.226  loss_mask: 0.2007  loss_dice: 1.595  loss_ce_0: 1.427  loss_mask_0: 0.2466  loss_dice_0: 1.576  loss_ce_1: 1.423  loss_mask_1: 0.2609  loss_dice_1: 1.67  loss_ce_2: 1.339  loss_mask_2: 0.1954  loss_dice_2: 1.587  loss_ce_3: 1.271  loss_mask_3: 0.1969  loss_dice_3: 1.529  loss_ce_4: 1.208  loss_mask_4: 0.1808  loss_dice_4: 1.501  loss_ce_5: 1.225  loss_mask_5: 0.1669  loss_dice_5: 1.654  loss_ce_6: 1.291  loss_mask_6: 0.2058  loss_dice_6: 1.641  loss_ce_7: 1.236  loss_mask_7: 0.208  loss_dice_7: 1.601  loss_ce_8: 1.304  loss_mask_8: 0.2011  loss_dice_8: 1.62    time: 0.3218  last_time: 0.3232  data_time: 0.0174  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:15 d2.utils.events]:  eta: 0:33:13  iter: 53459  total_loss: 25.99  loss_ce: 0.9277  loss_mask: 0.1146  loss_dice: 1.441  loss_ce_0: 1.209  loss_mask_0: 0.1493  loss_dice_0: 1.377  loss_ce_1: 1.165  loss_mask_1: 0.1753  loss_dice_1: 1.441  loss_ce_2: 1.108  loss_mask_2: 0.1402  loss_dice_2: 1.329  loss_ce_3: 0.9733  loss_mask_3: 0.1095  loss_dice_3: 1.315  loss_ce_4: 0.9263  loss_mask_4: 0.1111  loss_dice_4: 1.358  loss_ce_5: 1.022  loss_mask_5: 0.1096  loss_dice_5: 1.321  loss_ce_6: 0.8781  loss_mask_6: 0.1145  loss_dice_6: 1.337  loss_ce_7: 0.882  loss_mask_7: 0.1149  loss_dice_7: 1.466  loss_ce_8: 0.9177  loss_mask_8: 0.1139  loss_dice_8: 1.258    time: 0.3217  last_time: 0.3396  data_time: 0.0070  last_data_time: 0.0203   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:22 d2.utils.events]:  eta: 0:33:07  iter: 53479  total_loss: 30.68  loss_ce: 1.11  loss_mask: 0.1569  loss_dice: 1.401  loss_ce_0: 1.24  loss_mask_0: 0.1983  loss_dice_0: 1.369  loss_ce_1: 1.229  loss_mask_1: 0.1558  loss_dice_1: 1.544  loss_ce_2: 1.11  loss_mask_2: 0.1959  loss_dice_2: 1.428  loss_ce_3: 1.083  loss_mask_3: 0.1643  loss_dice_3: 1.632  loss_ce_4: 1.04  loss_mask_4: 0.1606  loss_dice_4: 1.661  loss_ce_5: 0.9964  loss_mask_5: 0.1612  loss_dice_5: 1.538  loss_ce_6: 0.9496  loss_mask_6: 0.1723  loss_dice_6: 1.535  loss_ce_7: 1.044  loss_mask_7: 0.1678  loss_dice_7: 1.535  loss_ce_8: 0.9541  loss_mask_8: 0.1793  loss_dice_8: 1.473    time: 0.3217  last_time: 0.3678  data_time: 0.0071  last_data_time: 0.0042   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:28 d2.utils.events]:  eta: 0:33:01  iter: 53499  total_loss: 35.07  loss_ce: 1.144  loss_mask: 0.1492  loss_dice: 1.961  loss_ce_0: 1.615  loss_mask_0: 0.1933  loss_dice_0: 2.116  loss_ce_1: 1.374  loss_mask_1: 0.2202  loss_dice_1: 2.051  loss_ce_2: 1.303  loss_mask_2: 0.199  loss_dice_2: 1.742  loss_ce_3: 1.176  loss_mask_3: 0.1674  loss_dice_3: 1.974  loss_ce_4: 1.187  loss_mask_4: 0.1545  loss_dice_4: 1.624  loss_ce_5: 1.099  loss_mask_5: 0.1636  loss_dice_5: 1.877  loss_ce_6: 1.146  loss_mask_6: 0.1558  loss_dice_6: 2.045  loss_ce_7: 1.146  loss_mask_7: 0.1453  loss_dice_7: 1.88  loss_ce_8: 1.16  loss_mask_8: 0.1396  loss_dice_8: 1.614    time: 0.3217  last_time: 0.3013  data_time: 0.0075  last_data_time: 0.0072   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:34 d2.utils.events]:  eta: 0:32:56  iter: 53519  total_loss: 27.87  loss_ce: 1.16  loss_mask: 0.0943  loss_dice: 1.274  loss_ce_0: 1.355  loss_mask_0: 0.09788  loss_dice_0: 1.391  loss_ce_1: 1.334  loss_mask_1: 0.08999  loss_dice_1: 1.382  loss_ce_2: 1.254  loss_mask_2: 0.08659  loss_dice_2: 1.377  loss_ce_3: 1.113  loss_mask_3: 0.1144  loss_dice_3: 1.315  loss_ce_4: 1.01  loss_mask_4: 0.0802  loss_dice_4: 1.406  loss_ce_5: 1.076  loss_mask_5: 0.08446  loss_dice_5: 1.154  loss_ce_6: 1.019  loss_mask_6: 0.1023  loss_dice_6: 1.127  loss_ce_7: 1.136  loss_mask_7: 0.09929  loss_dice_7: 1.344  loss_ce_8: 1.096  loss_mask_8: 0.08186  loss_dice_8: 1.178    time: 0.3217  last_time: 0.2998  data_time: 0.0070  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:40 d2.utils.events]:  eta: 0:32:49  iter: 53539  total_loss: 34.8  loss_ce: 1.099  loss_mask: 0.3184  loss_dice: 1.467  loss_ce_0: 1.488  loss_mask_0: 0.3287  loss_dice_0: 1.571  loss_ce_1: 1.465  loss_mask_1: 0.3432  loss_dice_1: 1.7  loss_ce_2: 1.348  loss_mask_2: 0.3289  loss_dice_2: 1.672  loss_ce_3: 1.123  loss_mask_3: 0.2888  loss_dice_3: 1.687  loss_ce_4: 1.091  loss_mask_4: 0.3177  loss_dice_4: 1.575  loss_ce_5: 1.058  loss_mask_5: 0.3395  loss_dice_5: 1.565  loss_ce_6: 1.034  loss_mask_6: 0.3111  loss_dice_6: 1.393  loss_ce_7: 1.085  loss_mask_7: 0.3142  loss_dice_7: 1.632  loss_ce_8: 1.212  loss_mask_8: 0.3199  loss_dice_8: 1.699    time: 0.3217  last_time: 0.2976  data_time: 0.0067  last_data_time: 0.0049   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:46 d2.utils.events]:  eta: 0:32:43  iter: 53559  total_loss: 28.92  loss_ce: 0.9592  loss_mask: 0.127  loss_dice: 1.234  loss_ce_0: 1.008  loss_mask_0: 0.1174  loss_dice_0: 1.048  loss_ce_1: 1.115  loss_mask_1: 0.1564  loss_dice_1: 1.41  loss_ce_2: 0.9231  loss_mask_2: 0.1396  loss_dice_2: 1.405  loss_ce_3: 0.9625  loss_mask_3: 0.135  loss_dice_3: 1.182  loss_ce_4: 0.8982  loss_mask_4: 0.1266  loss_dice_4: 1.28  loss_ce_5: 0.7887  loss_mask_5: 0.1264  loss_dice_5: 0.9974  loss_ce_6: 0.8488  loss_mask_6: 0.1333  loss_dice_6: 1.56  loss_ce_7: 0.6859  loss_mask_7: 0.1333  loss_dice_7: 1.238  loss_ce_8: 0.849  loss_mask_8: 0.1232  loss_dice_8: 1.143    time: 0.3217  last_time: 0.3076  data_time: 0.0119  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:53 d2.utils.events]:  eta: 0:32:38  iter: 53579  total_loss: 25.13  loss_ce: 1.052  loss_mask: 0.09739  loss_dice: 1.141  loss_ce_0: 1.348  loss_mask_0: 0.1049  loss_dice_0: 1.224  loss_ce_1: 1.187  loss_mask_1: 0.104  loss_dice_1: 1.242  loss_ce_2: 1.138  loss_mask_2: 0.09897  loss_dice_2: 1.208  loss_ce_3: 1.106  loss_mask_3: 0.1125  loss_dice_3: 1.141  loss_ce_4: 1.114  loss_mask_4: 0.1002  loss_dice_4: 1.035  loss_ce_5: 1.078  loss_mask_5: 0.1141  loss_dice_5: 1.069  loss_ce_6: 1.061  loss_mask_6: 0.09793  loss_dice_6: 1.131  loss_ce_7: 1.089  loss_mask_7: 0.09987  loss_dice_7: 1.199  loss_ce_8: 1.006  loss_mask_8: 0.09753  loss_dice_8: 1.039    time: 0.3217  last_time: 0.3218  data_time: 0.0125  last_data_time: 0.0046   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:13:59 d2.utils.events]:  eta: 0:32:31  iter: 53599  total_loss: 26.99  loss_ce: 1.122  loss_mask: 0.1477  loss_dice: 1.184  loss_ce_0: 1.346  loss_mask_0: 0.1088  loss_dice_0: 1.092  loss_ce_1: 1.314  loss_mask_1: 0.1129  loss_dice_1: 1.044  loss_ce_2: 1.096  loss_mask_2: 0.1357  loss_dice_2: 1.187  loss_ce_3: 1.026  loss_mask_3: 0.1254  loss_dice_3: 1.253  loss_ce_4: 1.156  loss_mask_4: 0.1399  loss_dice_4: 1.031  loss_ce_5: 1.195  loss_mask_5: 0.1299  loss_dice_5: 1.354  loss_ce_6: 1.216  loss_mask_6: 0.1468  loss_dice_6: 0.9801  loss_ce_7: 1.132  loss_mask_7: 0.1311  loss_dice_7: 1.34  loss_ce_8: 1.069  loss_mask_8: 0.1217  loss_dice_8: 1.033    time: 0.3217  last_time: 0.2945  data_time: 0.0107  last_data_time: 0.0035   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:05 d2.utils.events]:  eta: 0:32:26  iter: 53619  total_loss: 29.11  loss_ce: 0.9386  loss_mask: 0.2148  loss_dice: 1.245  loss_ce_0: 1.305  loss_mask_0: 0.2779  loss_dice_0: 1.316  loss_ce_1: 1.086  loss_mask_1: 0.2235  loss_dice_1: 1.267  loss_ce_2: 1.033  loss_mask_2: 0.2069  loss_dice_2: 1.207  loss_ce_3: 0.9083  loss_mask_3: 0.2264  loss_dice_3: 1.332  loss_ce_4: 0.9789  loss_mask_4: 0.2362  loss_dice_4: 1.349  loss_ce_5: 0.8565  loss_mask_5: 0.2351  loss_dice_5: 1.429  loss_ce_6: 0.8886  loss_mask_6: 0.2149  loss_dice_6: 1.341  loss_ce_7: 0.9251  loss_mask_7: 0.2206  loss_dice_7: 1.319  loss_ce_8: 0.9462  loss_mask_8: 0.2259  loss_dice_8: 1.25    time: 0.3217  last_time: 0.3170  data_time: 0.0074  last_data_time: 0.0115   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:12 d2.utils.events]:  eta: 0:32:20  iter: 53639  total_loss: 20.2  loss_ce: 0.6674  loss_mask: 0.1446  loss_dice: 1.441  loss_ce_0: 1.02  loss_mask_0: 0.1235  loss_dice_0: 1.401  loss_ce_1: 0.9453  loss_mask_1: 0.1393  loss_dice_1: 1.453  loss_ce_2: 0.9686  loss_mask_2: 0.1686  loss_dice_2: 1.386  loss_ce_3: 0.8013  loss_mask_3: 0.1708  loss_dice_3: 1.336  loss_ce_4: 0.8127  loss_mask_4: 0.1627  loss_dice_4: 1.453  loss_ce_5: 0.8133  loss_mask_5: 0.1399  loss_dice_5: 1.358  loss_ce_6: 0.7329  loss_mask_6: 0.1568  loss_dice_6: 1.092  loss_ce_7: 0.7655  loss_mask_7: 0.1424  loss_dice_7: 1.46  loss_ce_8: 0.7901  loss_mask_8: 0.1442  loss_dice_8: 1.46    time: 0.3217  last_time: 0.2989  data_time: 0.0139  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:18 d2.utils.events]:  eta: 0:32:14  iter: 53659  total_loss: 28.32  loss_ce: 1.286  loss_mask: 0.2593  loss_dice: 1.295  loss_ce_0: 1.4  loss_mask_0: 0.2889  loss_dice_0: 1.319  loss_ce_1: 1.612  loss_mask_1: 0.2691  loss_dice_1: 1.364  loss_ce_2: 1.501  loss_mask_2: 0.2935  loss_dice_2: 1.357  loss_ce_3: 1.264  loss_mask_3: 0.278  loss_dice_3: 1.287  loss_ce_4: 1.226  loss_mask_4: 0.2825  loss_dice_4: 1.653  loss_ce_5: 1.247  loss_mask_5: 0.2757  loss_dice_5: 1.381  loss_ce_6: 1.28  loss_mask_6: 0.2975  loss_dice_6: 1.488  loss_ce_7: 1.343  loss_mask_7: 0.2723  loss_dice_7: 1.387  loss_ce_8: 1.284  loss_mask_8: 0.2822  loss_dice_8: 1.472    time: 0.3217  last_time: 0.2941  data_time: 0.0095  last_data_time: 0.0028   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:24 d2.utils.events]:  eta: 0:32:08  iter: 53679  total_loss: 27.35  loss_ce: 1.227  loss_mask: 0.0948  loss_dice: 1.266  loss_ce_0: 1.457  loss_mask_0: 0.1548  loss_dice_0: 1.577  loss_ce_1: 1.295  loss_mask_1: 0.1544  loss_dice_1: 1.449  loss_ce_2: 1.224  loss_mask_2: 0.09627  loss_dice_2: 1.359  loss_ce_3: 1.179  loss_mask_3: 0.09212  loss_dice_3: 1.372  loss_ce_4: 1.183  loss_mask_4: 0.09276  loss_dice_4: 1.373  loss_ce_5: 1.137  loss_mask_5: 0.08275  loss_dice_5: 1.42  loss_ce_6: 1.251  loss_mask_6: 0.09055  loss_dice_6: 1.372  loss_ce_7: 1.144  loss_mask_7: 0.1091  loss_dice_7: 1.415  loss_ce_8: 1.195  loss_mask_8: 0.08288  loss_dice_8: 1.167    time: 0.3217  last_time: 0.3250  data_time: 0.0083  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:30 d2.utils.events]:  eta: 0:32:01  iter: 53699  total_loss: 25.01  loss_ce: 0.7824  loss_mask: 0.1212  loss_dice: 1.116  loss_ce_0: 1.23  loss_mask_0: 0.09841  loss_dice_0: 1.121  loss_ce_1: 1.235  loss_mask_1: 0.0947  loss_dice_1: 1.252  loss_ce_2: 1.034  loss_mask_2: 0.09468  loss_dice_2: 1.214  loss_ce_3: 0.7798  loss_mask_3: 0.1116  loss_dice_3: 1.325  loss_ce_4: 0.7877  loss_mask_4: 0.0999  loss_dice_4: 0.9935  loss_ce_5: 0.8478  loss_mask_5: 0.1037  loss_dice_5: 1.254  loss_ce_6: 0.7688  loss_mask_6: 0.09717  loss_dice_6: 1.048  loss_ce_7: 0.8267  loss_mask_7: 0.1261  loss_dice_7: 1.212  loss_ce_8: 0.814  loss_mask_8: 0.1163  loss_dice_8: 1.309    time: 0.3217  last_time: 0.3023  data_time: 0.0080  last_data_time: 0.0090   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:36 d2.utils.events]:  eta: 0:31:55  iter: 53719  total_loss: 33.41  loss_ce: 1.406  loss_mask: 0.1181  loss_dice: 1.643  loss_ce_0: 1.652  loss_mask_0: 0.1487  loss_dice_0: 1.637  loss_ce_1: 1.732  loss_mask_1: 0.1026  loss_dice_1: 1.673  loss_ce_2: 1.527  loss_mask_2: 0.1398  loss_dice_2: 1.72  loss_ce_3: 1.368  loss_mask_3: 0.1342  loss_dice_3: 1.727  loss_ce_4: 1.294  loss_mask_4: 0.1309  loss_dice_4: 1.73  loss_ce_5: 1.406  loss_mask_5: 0.1251  loss_dice_5: 1.606  loss_ce_6: 1.361  loss_mask_6: 0.123  loss_dice_6: 1.719  loss_ce_7: 1.395  loss_mask_7: 0.1104  loss_dice_7: 1.656  loss_ce_8: 1.412  loss_mask_8: 0.1153  loss_dice_8: 1.76    time: 0.3217  last_time: 0.3005  data_time: 0.0076  last_data_time: 0.0046   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:43 d2.utils.events]:  eta: 0:31:50  iter: 53739  total_loss: 27.31  loss_ce: 1.085  loss_mask: 0.1208  loss_dice: 1.444  loss_ce_0: 1.389  loss_mask_0: 0.1299  loss_dice_0: 1.35  loss_ce_1: 1.347  loss_mask_1: 0.1334  loss_dice_1: 1.329  loss_ce_2: 1.253  loss_mask_2: 0.1216  loss_dice_2: 1.449  loss_ce_3: 1.144  loss_mask_3: 0.1341  loss_dice_3: 1.34  loss_ce_4: 1.251  loss_mask_4: 0.1137  loss_dice_4: 1.374  loss_ce_5: 1.087  loss_mask_5: 0.1401  loss_dice_5: 1.404  loss_ce_6: 1.139  loss_mask_6: 0.1264  loss_dice_6: 1.525  loss_ce_7: 1.099  loss_mask_7: 0.1272  loss_dice_7: 1.469  loss_ce_8: 1.062  loss_mask_8: 0.1244  loss_dice_8: 1.483    time: 0.3217  last_time: 0.3180  data_time: 0.0070  last_data_time: 0.0048   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:49 d2.utils.events]:  eta: 0:31:45  iter: 53759  total_loss: 24.3  loss_ce: 1.022  loss_mask: 0.1277  loss_dice: 1.21  loss_ce_0: 1.203  loss_mask_0: 0.1562  loss_dice_0: 1.182  loss_ce_1: 1.239  loss_mask_1: 0.1535  loss_dice_1: 1.152  loss_ce_2: 1.204  loss_mask_2: 0.1475  loss_dice_2: 1.06  loss_ce_3: 1.003  loss_mask_3: 0.13  loss_dice_3: 1.136  loss_ce_4: 1.016  loss_mask_4: 0.1397  loss_dice_4: 1.102  loss_ce_5: 0.9732  loss_mask_5: 0.1341  loss_dice_5: 1.066  loss_ce_6: 1.129  loss_mask_6: 0.1141  loss_dice_6: 1.108  loss_ce_7: 0.9583  loss_mask_7: 0.1252  loss_dice_7: 1.185  loss_ce_8: 1.055  loss_mask_8: 0.1238  loss_dice_8: 1.111    time: 0.3217  last_time: 0.2978  data_time: 0.0222  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:14:55 d2.utils.events]:  eta: 0:31:39  iter: 53779  total_loss: 28.58  loss_ce: 1.104  loss_mask: 0.1613  loss_dice: 1.241  loss_ce_0: 1.305  loss_mask_0: 0.2107  loss_dice_0: 1.404  loss_ce_1: 1.369  loss_mask_1: 0.1533  loss_dice_1: 1.433  loss_ce_2: 1.256  loss_mask_2: 0.1548  loss_dice_2: 1.329  loss_ce_3: 1.148  loss_mask_3: 0.1845  loss_dice_3: 1.373  loss_ce_4: 1.113  loss_mask_4: 0.1937  loss_dice_4: 1.245  loss_ce_5: 0.9893  loss_mask_5: 0.2027  loss_dice_5: 1.319  loss_ce_6: 1.089  loss_mask_6: 0.1821  loss_dice_6: 1.285  loss_ce_7: 0.9959  loss_mask_7: 0.1783  loss_dice_7: 1.243  loss_ce_8: 0.9889  loss_mask_8: 0.1765  loss_dice_8: 1.326    time: 0.3217  last_time: 0.2935  data_time: 0.0074  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:02 d2.utils.events]:  eta: 0:31:33  iter: 53799  total_loss: 29.88  loss_ce: 1.273  loss_mask: 0.2252  loss_dice: 1.097  loss_ce_0: 1.298  loss_mask_0: 0.2223  loss_dice_0: 1.267  loss_ce_1: 1.295  loss_mask_1: 0.2063  loss_dice_1: 1.257  loss_ce_2: 1.466  loss_mask_2: 0.1936  loss_dice_2: 1.355  loss_ce_3: 1.198  loss_mask_3: 0.2413  loss_dice_3: 1.473  loss_ce_4: 1.201  loss_mask_4: 0.2194  loss_dice_4: 1.18  loss_ce_5: 1.284  loss_mask_5: 0.2305  loss_dice_5: 1.314  loss_ce_6: 1.192  loss_mask_6: 0.2268  loss_dice_6: 1.234  loss_ce_7: 1.208  loss_mask_7: 0.2355  loss_dice_7: 1.229  loss_ce_8: 1.152  loss_mask_8: 0.2194  loss_dice_8: 1.096    time: 0.3217  last_time: 0.2967  data_time: 0.0066  last_data_time: 0.0024   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:08 d2.utils.events]:  eta: 0:31:29  iter: 53819  total_loss: 28.02  loss_ce: 0.9573  loss_mask: 0.1554  loss_dice: 1.521  loss_ce_0: 1.112  loss_mask_0: 0.2201  loss_dice_0: 1.417  loss_ce_1: 1.072  loss_mask_1: 0.1469  loss_dice_1: 1.484  loss_ce_2: 1.103  loss_mask_2: 0.1378  loss_dice_2: 1.432  loss_ce_3: 0.875  loss_mask_3: 0.1614  loss_dice_3: 1.535  loss_ce_4: 0.8964  loss_mask_4: 0.1639  loss_dice_4: 1.499  loss_ce_5: 0.9087  loss_mask_5: 0.1422  loss_dice_5: 1.516  loss_ce_6: 0.9426  loss_mask_6: 0.1429  loss_dice_6: 1.569  loss_ce_7: 0.9461  loss_mask_7: 0.1311  loss_dice_7: 1.609  loss_ce_8: 0.9608  loss_mask_8: 0.1516  loss_dice_8: 1.628    time: 0.3217  last_time: 0.3289  data_time: 0.0138  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:14 d2.utils.events]:  eta: 0:31:23  iter: 53839  total_loss: 38.76  loss_ce: 1.521  loss_mask: 0.1215  loss_dice: 1.765  loss_ce_0: 1.548  loss_mask_0: 0.1084  loss_dice_0: 1.837  loss_ce_1: 1.539  loss_mask_1: 0.07962  loss_dice_1: 1.852  loss_ce_2: 1.577  loss_mask_2: 0.09916  loss_dice_2: 1.724  loss_ce_3: 1.604  loss_mask_3: 0.1166  loss_dice_3: 1.647  loss_ce_4: 1.596  loss_mask_4: 0.1208  loss_dice_4: 1.735  loss_ce_5: 1.54  loss_mask_5: 0.1158  loss_dice_5: 1.783  loss_ce_6: 1.565  loss_mask_6: 0.1245  loss_dice_6: 1.537  loss_ce_7: 1.506  loss_mask_7: 0.119  loss_dice_7: 1.459  loss_ce_8: 1.562  loss_mask_8: 0.1135  loss_dice_8: 1.529    time: 0.3217  last_time: 0.3261  data_time: 0.0091  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:21 d2.utils.events]:  eta: 0:31:17  iter: 53859  total_loss: 31.93  loss_ce: 1.11  loss_mask: 0.1172  loss_dice: 1.473  loss_ce_0: 1.413  loss_mask_0: 0.1226  loss_dice_0: 1.631  loss_ce_1: 1.299  loss_mask_1: 0.1229  loss_dice_1: 1.335  loss_ce_2: 1.273  loss_mask_2: 0.1242  loss_dice_2: 1.442  loss_ce_3: 1.186  loss_mask_3: 0.1431  loss_dice_3: 1.622  loss_ce_4: 1.141  loss_mask_4: 0.1502  loss_dice_4: 1.659  loss_ce_5: 1.16  loss_mask_5: 0.1368  loss_dice_5: 1.426  loss_ce_6: 1.098  loss_mask_6: 0.1338  loss_dice_6: 1.487  loss_ce_7: 1.091  loss_mask_7: 0.13  loss_dice_7: 1.6  loss_ce_8: 1.106  loss_mask_8: 0.1248  loss_dice_8: 1.548    time: 0.3217  last_time: 0.3033  data_time: 0.0103  last_data_time: 0.0094   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:27 d2.utils.events]:  eta: 0:31:11  iter: 53879  total_loss: 27.65  loss_ce: 1.069  loss_mask: 0.2037  loss_dice: 1.354  loss_ce_0: 1.206  loss_mask_0: 0.249  loss_dice_0: 1.013  loss_ce_1: 1.155  loss_mask_1: 0.2594  loss_dice_1: 1.151  loss_ce_2: 1.148  loss_mask_2: 0.2324  loss_dice_2: 1.16  loss_ce_3: 1.089  loss_mask_3: 0.2112  loss_dice_3: 1.265  loss_ce_4: 1.15  loss_mask_4: 0.2188  loss_dice_4: 1.151  loss_ce_5: 1.101  loss_mask_5: 0.2218  loss_dice_5: 1.055  loss_ce_6: 0.9862  loss_mask_6: 0.2267  loss_dice_6: 1.313  loss_ce_7: 1.021  loss_mask_7: 0.2128  loss_dice_7: 1.33  loss_ce_8: 1.03  loss_mask_8: 0.2244  loss_dice_8: 1.313    time: 0.3217  last_time: 0.3056  data_time: 0.0109  last_data_time: 0.0074   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:33 d2.utils.events]:  eta: 0:31:05  iter: 53899  total_loss: 19.53  loss_ce: 0.901  loss_mask: 0.1064  loss_dice: 0.873  loss_ce_0: 1.019  loss_mask_0: 0.09756  loss_dice_0: 0.7786  loss_ce_1: 0.9649  loss_mask_1: 0.08638  loss_dice_1: 1.056  loss_ce_2: 0.9725  loss_mask_2: 0.1247  loss_dice_2: 0.9396  loss_ce_3: 0.8404  loss_mask_3: 0.1201  loss_dice_3: 0.799  loss_ce_4: 0.8723  loss_mask_4: 0.1012  loss_dice_4: 0.8733  loss_ce_5: 0.931  loss_mask_5: 0.1185  loss_dice_5: 0.752  loss_ce_6: 0.8417  loss_mask_6: 0.07716  loss_dice_6: 0.7447  loss_ce_7: 0.9143  loss_mask_7: 0.1052  loss_dice_7: 0.7175  loss_ce_8: 0.904  loss_mask_8: 0.1012  loss_dice_8: 0.7705    time: 0.3217  last_time: 0.3222  data_time: 0.0069  last_data_time: 0.0040   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:39 d2.utils.events]:  eta: 0:30:59  iter: 53919  total_loss: 25.76  loss_ce: 1.084  loss_mask: 0.244  loss_dice: 1.232  loss_ce_0: 1.09  loss_mask_0: 0.2347  loss_dice_0: 1.164  loss_ce_1: 1.062  loss_mask_1: 0.2696  loss_dice_1: 1.253  loss_ce_2: 1.136  loss_mask_2: 0.2537  loss_dice_2: 1.278  loss_ce_3: 1.072  loss_mask_3: 0.1717  loss_dice_3: 1.154  loss_ce_4: 1.113  loss_mask_4: 0.1655  loss_dice_4: 1.341  loss_ce_5: 1.112  loss_mask_5: 0.1935  loss_dice_5: 1.161  loss_ce_6: 1.076  loss_mask_6: 0.1859  loss_dice_6: 1.255  loss_ce_7: 1.1  loss_mask_7: 0.1584  loss_dice_7: 1.253  loss_ce_8: 1.092  loss_mask_8: 0.1701  loss_dice_8: 1.21    time: 0.3217  last_time: 0.3158  data_time: 0.0096  last_data_time: 0.0139   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:46 d2.utils.events]:  eta: 0:30:53  iter: 53939  total_loss: 28.61  loss_ce: 1.015  loss_mask: 0.1128  loss_dice: 1.119  loss_ce_0: 1.457  loss_mask_0: 0.1205  loss_dice_0: 1.15  loss_ce_1: 1.369  loss_mask_1: 0.1324  loss_dice_1: 1.404  loss_ce_2: 1.285  loss_mask_2: 0.1306  loss_dice_2: 1.218  loss_ce_3: 1.018  loss_mask_3: 0.1228  loss_dice_3: 1.268  loss_ce_4: 1.107  loss_mask_4: 0.1261  loss_dice_4: 1.334  loss_ce_5: 1.129  loss_mask_5: 0.1148  loss_dice_5: 1.293  loss_ce_6: 1.129  loss_mask_6: 0.121  loss_dice_6: 1.069  loss_ce_7: 1.068  loss_mask_7: 0.1127  loss_dice_7: 1.219  loss_ce_8: 1.032  loss_mask_8: 0.1201  loss_dice_8: 1.113    time: 0.3217  last_time: 0.3072  data_time: 0.0069  last_data_time: 0.0133   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:52 d2.utils.events]:  eta: 0:30:47  iter: 53959  total_loss: 28.66  loss_ce: 1.166  loss_mask: 0.2293  loss_dice: 0.7662  loss_ce_0: 1.35  loss_mask_0: 0.3944  loss_dice_0: 0.9248  loss_ce_1: 1.396  loss_mask_1: 0.2668  loss_dice_1: 1.012  loss_ce_2: 1.341  loss_mask_2: 0.2554  loss_dice_2: 1.021  loss_ce_3: 1.195  loss_mask_3: 0.2128  loss_dice_3: 0.8918  loss_ce_4: 1.175  loss_mask_4: 0.2058  loss_dice_4: 0.873  loss_ce_5: 1.172  loss_mask_5: 0.2077  loss_dice_5: 0.9322  loss_ce_6: 1.166  loss_mask_6: 0.1865  loss_dice_6: 0.7838  loss_ce_7: 1.164  loss_mask_7: 0.204  loss_dice_7: 0.8841  loss_ce_8: 1.229  loss_mask_8: 0.1979  loss_dice_8: 0.8077    time: 0.3217  last_time: 0.2969  data_time: 0.0180  last_data_time: 0.0048   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:15:58 d2.utils.events]:  eta: 0:30:41  iter: 53979  total_loss: 31.58  loss_ce: 1.302  loss_mask: 0.2153  loss_dice: 1.51  loss_ce_0: 1.307  loss_mask_0: 0.2501  loss_dice_0: 1.453  loss_ce_1: 1.459  loss_mask_1: 0.1874  loss_dice_1: 1.265  loss_ce_2: 1.497  loss_mask_2: 0.2293  loss_dice_2: 1.407  loss_ce_3: 1.333  loss_mask_3: 0.2224  loss_dice_3: 1.57  loss_ce_4: 1.251  loss_mask_4: 0.2043  loss_dice_4: 1.434  loss_ce_5: 1.331  loss_mask_5: 0.2199  loss_dice_5: 1.571  loss_ce_6: 1.276  loss_mask_6: 0.2134  loss_dice_6: 1.491  loss_ce_7: 1.255  loss_mask_7: 0.2176  loss_dice_7: 1.497  loss_ce_8: 1.32  loss_mask_8: 0.2163  loss_dice_8: 1.511    time: 0.3217  last_time: 0.2956  data_time: 0.0069  last_data_time: 0.0019   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:16:05 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[09/02 21:16:05 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[09/02 21:16:05 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/02 21:16:05 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[09/02 21:16:05 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "taco10_val\n",
            "[09/02 21:16:05 d2.evaluation.evaluator]: Start inference on 150 batches\n",
            "[09/02 21:16:54 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0015 s/iter. Inference: 0.1047 s/iter. Eval: 3.8425 s/iter. Total: 3.9487 s/iter. ETA=0:09:08\n",
            "[09/02 21:17:01 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0016 s/iter. Inference: 0.1042 s/iter. Eval: 3.7947 s/iter. Total: 3.9008 s/iter. ETA=0:08:54\n",
            "[09/02 21:17:09 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0017 s/iter. Inference: 0.1036 s/iter. Eval: 3.7657 s/iter. Total: 3.8714 s/iter. ETA=0:08:42\n",
            "[09/02 21:17:17 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0017 s/iter. Inference: 0.1033 s/iter. Eval: 3.7482 s/iter. Total: 3.8537 s/iter. ETA=0:08:32\n",
            "[09/02 21:17:24 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0018 s/iter. Inference: 0.1032 s/iter. Eval: 3.7378 s/iter. Total: 3.8433 s/iter. ETA=0:08:23\n",
            "[09/02 21:17:32 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0018 s/iter. Inference: 0.1035 s/iter. Eval: 3.7318 s/iter. Total: 3.8377 s/iter. ETA=0:08:15\n",
            "[09/02 21:17:36 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:17:56 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0019 s/iter. Inference: 0.9885 s/iter. Eval: 3.7980 s/iter. Total: 4.7889 s/iter. ETA=0:10:08\n",
            "[09/02 21:17:57 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:18:18 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0019 s/iter. Inference: 1.7965 s/iter. Eval: 3.8605 s/iter. Total: 5.6595 s/iter. ETA=0:11:53\n",
            "[09/02 21:18:23 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0019 s/iter. Inference: 1.7398 s/iter. Eval: 3.9090 s/iter. Total: 5.6513 s/iter. ETA=0:11:46\n",
            "[09/02 21:18:24 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:18:37 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0020 s/iter. Inference: 2.1647 s/iter. Eval: 3.8841 s/iter. Total: 6.0514 s/iter. ETA=0:12:30\n",
            "[09/02 21:18:38 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:18:52 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0020 s/iter. Inference: 2.5732 s/iter. Eval: 3.8628 s/iter. Total: 6.4387 s/iter. ETA=0:13:11\n",
            "[09/02 21:18:59 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0020 s/iter. Inference: 2.4814 s/iter. Eval: 3.9784 s/iter. Total: 6.4626 s/iter. ETA=0:13:08\n",
            "[09/02 21:19:06 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0020 s/iter. Inference: 2.4071 s/iter. Eval: 4.0862 s/iter. Total: 6.4961 s/iter. ETA=0:13:06\n",
            "[09/02 21:19:07 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:19:20 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0020 s/iter. Inference: 2.7484 s/iter. Eval: 4.0493 s/iter. Total: 6.8005 s/iter. ETA=0:13:36\n",
            "[09/02 21:19:27 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0020 s/iter. Inference: 2.6604 s/iter. Eval: 4.1294 s/iter. Total: 6.7925 s/iter. ETA=0:13:28\n",
            "[09/02 21:19:33 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0021 s/iter. Inference: 2.5666 s/iter. Eval: 4.2080 s/iter. Total: 6.7774 s/iter. ETA=0:13:19\n",
            "[09/02 21:19:40 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0021 s/iter. Inference: 2.4797 s/iter. Eval: 4.2772 s/iter. Total: 6.7599 s/iter. ETA=0:13:10\n",
            "[09/02 21:19:46 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0021 s/iter. Inference: 2.3986 s/iter. Eval: 4.3419 s/iter. Total: 6.7434 s/iter. ETA=0:13:02\n",
            "[09/02 21:19:53 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0021 s/iter. Inference: 2.3280 s/iter. Eval: 4.4143 s/iter. Total: 6.7453 s/iter. ETA=0:12:55\n",
            "[09/02 21:20:01 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0021 s/iter. Inference: 2.1894 s/iter. Eval: 4.3886 s/iter. Total: 6.5809 s/iter. ETA=0:12:23\n",
            "[09/02 21:20:02 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:20:15 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0021 s/iter. Inference: 2.4665 s/iter. Eval: 4.3530 s/iter. Total: 6.8225 s/iter. ETA=0:12:44\n",
            "[09/02 21:20:16 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:20:30 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0021 s/iter. Inference: 2.7352 s/iter. Eval: 4.3197 s/iter. Total: 7.0579 s/iter. ETA=0:13:03\n",
            "[09/02 21:20:36 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0021 s/iter. Inference: 2.6702 s/iter. Eval: 4.3380 s/iter. Total: 7.0113 s/iter. ETA=0:12:51\n",
            "[09/02 21:20:36 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:20:49 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0022 s/iter. Inference: 2.8871 s/iter. Eval: 4.3081 s/iter. Total: 7.1983 s/iter. ETA=0:13:04\n",
            "[09/02 21:20:55 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0022 s/iter. Inference: 2.8219 s/iter. Eval: 4.3263 s/iter. Total: 7.1513 s/iter. ETA=0:12:52\n",
            "[09/02 21:21:02 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0022 s/iter. Inference: 2.7657 s/iter. Eval: 4.3851 s/iter. Total: 7.1539 s/iter. ETA=0:12:45\n",
            "[09/02 21:21:09 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0022 s/iter. Inference: 2.6981 s/iter. Eval: 4.4410 s/iter. Total: 7.1423 s/iter. ETA=0:12:37\n",
            "[09/02 21:21:16 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0022 s/iter. Inference: 2.6339 s/iter. Eval: 4.4942 s/iter. Total: 7.1313 s/iter. ETA=0:12:28\n",
            "[09/02 21:21:22 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0022 s/iter. Inference: 2.5729 s/iter. Eval: 4.5451 s/iter. Total: 7.1212 s/iter. ETA=0:12:20\n",
            "[09/02 21:21:29 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0022 s/iter. Inference: 2.5148 s/iter. Eval: 4.5929 s/iter. Total: 7.1108 s/iter. ETA=0:12:12\n",
            "[09/02 21:21:36 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0022 s/iter. Inference: 2.4595 s/iter. Eval: 4.6393 s/iter. Total: 7.1020 s/iter. ETA=0:12:04\n",
            "[09/02 21:21:42 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0022 s/iter. Inference: 2.4067 s/iter. Eval: 4.6833 s/iter. Total: 7.0932 s/iter. ETA=0:11:56\n",
            "[09/02 21:21:49 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0022 s/iter. Inference: 2.3562 s/iter. Eval: 4.7240 s/iter. Total: 7.0834 s/iter. ETA=0:11:48\n",
            "[09/02 21:21:56 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0022 s/iter. Inference: 2.3078 s/iter. Eval: 4.7637 s/iter. Total: 7.0747 s/iter. ETA=0:11:40\n",
            "[09/02 21:22:02 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0022 s/iter. Inference: 2.2615 s/iter. Eval: 4.8014 s/iter. Total: 7.0662 s/iter. ETA=0:11:32\n",
            "[09/02 21:22:09 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0022 s/iter. Inference: 2.2173 s/iter. Eval: 4.8375 s/iter. Total: 7.0581 s/iter. ETA=0:11:24\n",
            "[09/02 21:22:15 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0022 s/iter. Inference: 2.1747 s/iter. Eval: 4.8635 s/iter. Total: 7.0414 s/iter. ETA=0:11:15\n",
            "[09/02 21:22:22 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0022 s/iter. Inference: 2.1337 s/iter. Eval: 4.8888 s/iter. Total: 7.0258 s/iter. ETA=0:11:07\n",
            "[09/02 21:22:28 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0022 s/iter. Inference: 2.0943 s/iter. Eval: 4.9128 s/iter. Total: 7.0104 s/iter. ETA=0:10:58\n",
            "[09/02 21:22:34 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0022 s/iter. Inference: 2.0565 s/iter. Eval: 4.9368 s/iter. Total: 6.9966 s/iter. ETA=0:10:50\n",
            "[09/02 21:22:40 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0022 s/iter. Inference: 2.0205 s/iter. Eval: 4.9592 s/iter. Total: 6.9830 s/iter. ETA=0:10:42\n",
            "[09/02 21:22:47 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0022 s/iter. Inference: 1.9855 s/iter. Eval: 4.9814 s/iter. Total: 6.9702 s/iter. ETA=0:10:34\n",
            "[09/02 21:22:53 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0023 s/iter. Inference: 1.9517 s/iter. Eval: 5.0024 s/iter. Total: 6.9575 s/iter. ETA=0:10:26\n",
            "[09/02 21:22:59 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0023 s/iter. Inference: 1.9192 s/iter. Eval: 5.0228 s/iter. Total: 6.9453 s/iter. ETA=0:10:18\n",
            "[09/02 21:23:05 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0023 s/iter. Inference: 1.8877 s/iter. Eval: 5.0421 s/iter. Total: 6.9332 s/iter. ETA=0:10:10\n",
            "[09/02 21:23:12 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0023 s/iter. Inference: 1.8574 s/iter. Eval: 5.0619 s/iter. Total: 6.9227 s/iter. ETA=0:10:02\n",
            "[09/02 21:23:18 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0023 s/iter. Inference: 1.8281 s/iter. Eval: 5.0798 s/iter. Total: 6.9113 s/iter. ETA=0:09:54\n",
            "[09/02 21:23:29 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0023 s/iter. Inference: 1.7444 s/iter. Eval: 5.0020 s/iter. Total: 6.7497 s/iter. ETA=0:09:20\n",
            "[09/02 21:23:35 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0023 s/iter. Inference: 1.7187 s/iter. Eval: 5.0198 s/iter. Total: 6.7419 s/iter. ETA=0:09:12\n",
            "[09/02 21:23:41 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0023 s/iter. Inference: 1.6688 s/iter. Eval: 4.9559 s/iter. Total: 6.6281 s/iter. ETA=0:08:50\n",
            "[09/02 21:23:49 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0023 s/iter. Inference: 1.6222 s/iter. Eval: 4.9276 s/iter. Total: 6.5532 s/iter. ETA=0:08:31\n",
            "[09/02 21:23:58 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0023 s/iter. Inference: 1.5783 s/iter. Eval: 4.9013 s/iter. Total: 6.4829 s/iter. ETA=0:08:12\n",
            "[09/02 21:24:06 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0023 s/iter. Inference: 1.5369 s/iter. Eval: 4.8768 s/iter. Total: 6.4169 s/iter. ETA=0:07:54\n",
            "[09/02 21:24:14 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0023 s/iter. Inference: 1.4977 s/iter. Eval: 4.8531 s/iter. Total: 6.3540 s/iter. ETA=0:07:37\n",
            "[09/02 21:24:22 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0022 s/iter. Inference: 1.4606 s/iter. Eval: 4.8305 s/iter. Total: 6.2944 s/iter. ETA=0:07:20\n",
            "[09/02 21:24:31 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0022 s/iter. Inference: 1.4254 s/iter. Eval: 4.8089 s/iter. Total: 6.2376 s/iter. ETA=0:07:04\n",
            "[09/02 21:24:39 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0022 s/iter. Inference: 1.3921 s/iter. Eval: 4.7893 s/iter. Total: 6.1846 s/iter. ETA=0:06:48\n",
            "[09/02 21:24:47 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0022 s/iter. Inference: 1.3604 s/iter. Eval: 4.7701 s/iter. Total: 6.1338 s/iter. ETA=0:06:32\n",
            "[09/02 21:24:55 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0022 s/iter. Inference: 1.3303 s/iter. Eval: 4.7521 s/iter. Total: 6.0856 s/iter. ETA=0:06:17\n",
            "[09/02 21:25:04 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0022 s/iter. Inference: 1.3016 s/iter. Eval: 4.7345 s/iter. Total: 6.0393 s/iter. ETA=0:06:02\n",
            "[09/02 21:25:12 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0022 s/iter. Inference: 1.2741 s/iter. Eval: 4.7178 s/iter. Total: 5.9951 s/iter. ETA=0:05:47\n",
            "[09/02 21:25:20 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0022 s/iter. Inference: 1.2479 s/iter. Eval: 4.7017 s/iter. Total: 5.9528 s/iter. ETA=0:05:33\n",
            "[09/02 21:25:28 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0022 s/iter. Inference: 1.2228 s/iter. Eval: 4.6863 s/iter. Total: 5.9123 s/iter. ETA=0:05:19\n",
            "[09/02 21:25:37 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0022 s/iter. Inference: 1.1989 s/iter. Eval: 4.6716 s/iter. Total: 5.8736 s/iter. ETA=0:05:05\n",
            "[09/02 21:25:45 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 1.1759 s/iter. Eval: 4.6584 s/iter. Total: 5.8375 s/iter. ETA=0:04:51\n",
            "[09/02 21:25:53 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0022 s/iter. Inference: 1.1539 s/iter. Eval: 4.6450 s/iter. Total: 5.8021 s/iter. ETA=0:04:38\n",
            "[09/02 21:26:01 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0022 s/iter. Inference: 1.1328 s/iter. Eval: 4.6327 s/iter. Total: 5.7686 s/iter. ETA=0:04:25\n",
            "[09/02 21:26:10 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0022 s/iter. Inference: 1.1125 s/iter. Eval: 4.6203 s/iter. Total: 5.7359 s/iter. ETA=0:04:12\n",
            "[09/02 21:26:18 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0022 s/iter. Inference: 1.0930 s/iter. Eval: 4.6085 s/iter. Total: 5.7046 s/iter. ETA=0:03:59\n",
            "[09/02 21:26:26 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0022 s/iter. Inference: 1.0743 s/iter. Eval: 4.5970 s/iter. Total: 5.6743 s/iter. ETA=0:03:46\n",
            "[09/02 21:26:34 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0022 s/iter. Inference: 1.0562 s/iter. Eval: 4.5860 s/iter. Total: 5.6453 s/iter. ETA=0:03:34\n",
            "[09/02 21:26:41 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0022 s/iter. Inference: 1.0387 s/iter. Eval: 4.5634 s/iter. Total: 5.6051 s/iter. ETA=0:03:21\n",
            "[09/02 21:26:46 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:27:07 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 1.1668 s/iter. Eval: 4.5622 s/iter. Total: 5.7322 s/iter. ETA=0:03:14\n",
            "[09/02 21:27:07 d2.utils.memory]: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:27:28 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 1.3057 s/iter. Eval: 4.5660 s/iter. Total: 5.8747 s/iter. ETA=0:03:13\n",
            "[09/02 21:27:37 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 1.2894 s/iter. Eval: 4.5561 s/iter. Total: 5.8486 s/iter. ETA=0:03:01\n",
            "[09/02 21:27:45 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.2690 s/iter. Eval: 4.5474 s/iter. Total: 5.8195 s/iter. ETA=0:02:48\n",
            "[09/02 21:27:54 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.2494 s/iter. Eval: 4.5383 s/iter. Total: 5.7907 s/iter. ETA=0:02:36\n",
            "[09/02 21:28:02 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.2304 s/iter. Eval: 4.5294 s/iter. Total: 5.7629 s/iter. ETA=0:02:24\n",
            "[09/02 21:28:10 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.2120 s/iter. Eval: 4.5210 s/iter. Total: 5.7360 s/iter. ETA=0:02:11\n",
            "[09/02 21:28:16 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.1940 s/iter. Eval: 4.4928 s/iter. Total: 5.6900 s/iter. ETA=0:01:59\n",
            "[09/02 21:28:25 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.1771 s/iter. Eval: 4.4946 s/iter. Total: 5.6747 s/iter. ETA=0:01:47\n",
            "[09/02 21:28:34 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 1.1605 s/iter. Eval: 4.4915 s/iter. Total: 5.6551 s/iter. ETA=0:01:36\n",
            "[09/02 21:28:42 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 1.1443 s/iter. Eval: 4.4840 s/iter. Total: 5.6314 s/iter. ETA=0:01:24\n",
            "[09/02 21:28:51 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 1.1286 s/iter. Eval: 4.4772 s/iter. Total: 5.6088 s/iter. ETA=0:01:12\n",
            "[09/02 21:28:59 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 1.1133 s/iter. Eval: 4.4701 s/iter. Total: 5.5865 s/iter. ETA=0:01:01\n",
            "[09/02 21:29:07 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 1.0985 s/iter. Eval: 4.4632 s/iter. Total: 5.5648 s/iter. ETA=0:00:50\n",
            "[09/02 21:29:15 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 1.0842 s/iter. Eval: 4.4571 s/iter. Total: 5.5443 s/iter. ETA=0:00:38\n",
            "[09/02 21:29:23 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0022 s/iter. Inference: 1.0701 s/iter. Eval: 4.4457 s/iter. Total: 5.5189 s/iter. ETA=0:00:27\n",
            "[09/02 21:29:30 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0022 s/iter. Inference: 1.0565 s/iter. Eval: 4.4345 s/iter. Total: 5.4941 s/iter. ETA=0:00:16\n",
            "[09/02 21:29:38 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0022 s/iter. Inference: 1.0433 s/iter. Eval: 4.4238 s/iter. Total: 5.4701 s/iter. ETA=0:00:05\n",
            "[09/02 21:29:42 d2.evaluation.evaluator]: Total inference time: 0:13:11.511028 (5.458697 s / iter per device, on 1 devices)\n",
            "[09/02 21:29:42 d2.evaluation.evaluator]: Total inference pure compute time: 0:02:30 (1.036772 s / iter per device, on 1 devices)\n",
            "[09/02 21:29:42 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/02 21:29:42 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[09/02 21:29:42 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 21:29:42 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/02 21:29:42 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/02 21:29:42 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 21:29:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/02 21:29:42 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/02 21:29:42 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.33s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 21:29:43 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/02 21:29:44 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.32 seconds.\n",
            "[09/02 21:29:44 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 21:29:44 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.061\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.022\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.146\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.204\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.278\n",
            "[09/02 21:29:44 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 4.018 | 6.066  | 4.734  | 0.176 | 2.229 | 5.077 |\n",
            "[09/02 21:29:44 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 5.133 | Other      | 6.586 | Bottle     | 6.049 |\n",
            "| Bottle cap            | 4.535 | Cup        | 9.065 | Lid        | 0.024 |\n",
            "| Plastic bag + wrapper | 8.371 | Pop tab    | 0.000 | Straw      | 0.213 |\n",
            "| Cigarette             | 0.201 |            |       |            |       |\n",
            "[09/02 21:29:44 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[09/02 21:29:44 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/02 21:29:44 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 21:29:44 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/02 21:29:44 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/02 21:29:44 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 21:29:44 d2.evaluation.testing]: copypaste: 4.0178,6.0662,4.7345,0.1763,2.2286,5.0775\n",
            "[09/02 21:29:44 d2.utils.events]:  eta: 0:30:35  iter: 53999  total_loss: 30.18  loss_ce: 1.223  loss_mask: 0.2041  loss_dice: 1.582  loss_ce_0: 1.405  loss_mask_0: 0.2023  loss_dice_0: 1.67  loss_ce_1: 1.462  loss_mask_1: 0.1712  loss_dice_1: 1.415  loss_ce_2: 1.321  loss_mask_2: 0.1832  loss_dice_2: 1.556  loss_ce_3: 1.197  loss_mask_3: 0.1932  loss_dice_3: 1.529  loss_ce_4: 1.232  loss_mask_4: 0.2062  loss_dice_4: 1.503  loss_ce_5: 1.26  loss_mask_5: 0.2482  loss_dice_5: 1.283  loss_ce_6: 1.198  loss_mask_6: 0.2019  loss_dice_6: 1.399  loss_ce_7: 1.262  loss_mask_7: 0.201  loss_dice_7: 1.596  loss_ce_8: 1.242  loss_mask_8: 0.217  loss_dice_8: 1.576    time: 0.3217  last_time: 0.2998  data_time: 0.0073  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:29:50 d2.utils.events]:  eta: 0:30:31  iter: 54019  total_loss: 27.76  loss_ce: 1.031  loss_mask: 0.2211  loss_dice: 1.163  loss_ce_0: 1.134  loss_mask_0: 0.1905  loss_dice_0: 1.609  loss_ce_1: 1.265  loss_mask_1: 0.2183  loss_dice_1: 1.454  loss_ce_2: 1.113  loss_mask_2: 0.194  loss_dice_2: 1.333  loss_ce_3: 1.132  loss_mask_3: 0.1485  loss_dice_3: 1.524  loss_ce_4: 0.9452  loss_mask_4: 0.1831  loss_dice_4: 1.551  loss_ce_5: 1.139  loss_mask_5: 0.1402  loss_dice_5: 1.239  loss_ce_6: 1.142  loss_mask_6: 0.1905  loss_dice_6: 1.219  loss_ce_7: 0.9536  loss_mask_7: 0.2018  loss_dice_7: 1.566  loss_ce_8: 1.021  loss_mask_8: 0.1419  loss_dice_8: 1.573    time: 0.3217  last_time: 0.2975  data_time: 0.0108  last_data_time: 0.0066   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:29:56 d2.utils.events]:  eta: 0:30:24  iter: 54039  total_loss: 33.82  loss_ce: 1.267  loss_mask: 0.183  loss_dice: 1.568  loss_ce_0: 1.468  loss_mask_0: 0.2192  loss_dice_0: 1.527  loss_ce_1: 1.252  loss_mask_1: 0.2789  loss_dice_1: 1.578  loss_ce_2: 1.303  loss_mask_2: 0.2321  loss_dice_2: 1.586  loss_ce_3: 1.324  loss_mask_3: 0.2064  loss_dice_3: 1.556  loss_ce_4: 1.229  loss_mask_4: 0.2008  loss_dice_4: 1.547  loss_ce_5: 1.132  loss_mask_5: 0.2293  loss_dice_5: 1.524  loss_ce_6: 1.22  loss_mask_6: 0.2109  loss_dice_6: 1.647  loss_ce_7: 1.229  loss_mask_7: 0.1876  loss_dice_7: 1.518  loss_ce_8: 1.23  loss_mask_8: 0.1887  loss_dice_8: 1.563    time: 0.3217  last_time: 0.3209  data_time: 0.0070  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:03 d2.utils.events]:  eta: 0:30:18  iter: 54059  total_loss: 28.4  loss_ce: 1.084  loss_mask: 0.1152  loss_dice: 1.465  loss_ce_0: 1.093  loss_mask_0: 0.1329  loss_dice_0: 1.592  loss_ce_1: 1.215  loss_mask_1: 0.1054  loss_dice_1: 1.548  loss_ce_2: 1.025  loss_mask_2: 0.1145  loss_dice_2: 1.597  loss_ce_3: 1.05  loss_mask_3: 0.1143  loss_dice_3: 1.611  loss_ce_4: 0.9742  loss_mask_4: 0.1204  loss_dice_4: 1.647  loss_ce_5: 1.087  loss_mask_5: 0.1083  loss_dice_5: 1.619  loss_ce_6: 1.061  loss_mask_6: 0.1122  loss_dice_6: 1.482  loss_ce_7: 0.9962  loss_mask_7: 0.107  loss_dice_7: 1.52  loss_ce_8: 0.9969  loss_mask_8: 0.1132  loss_dice_8: 1.576    time: 0.3216  last_time: 0.3066  data_time: 0.0059  last_data_time: 0.0083   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:09 d2.utils.events]:  eta: 0:30:12  iter: 54079  total_loss: 33.64  loss_ce: 1.288  loss_mask: 0.1413  loss_dice: 1.204  loss_ce_0: 1.833  loss_mask_0: 0.1676  loss_dice_0: 1.456  loss_ce_1: 1.64  loss_mask_1: 0.157  loss_dice_1: 1.288  loss_ce_2: 1.431  loss_mask_2: 0.1769  loss_dice_2: 1.272  loss_ce_3: 1.38  loss_mask_3: 0.1538  loss_dice_3: 1.258  loss_ce_4: 1.353  loss_mask_4: 0.1492  loss_dice_4: 1.204  loss_ce_5: 1.397  loss_mask_5: 0.1394  loss_dice_5: 1.156  loss_ce_6: 1.306  loss_mask_6: 0.1403  loss_dice_6: 1.127  loss_ce_7: 1.312  loss_mask_7: 0.1343  loss_dice_7: 1.309  loss_ce_8: 1.267  loss_mask_8: 0.141  loss_dice_8: 1.245    time: 0.3216  last_time: 0.3256  data_time: 0.0069  last_data_time: 0.0062   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:15 d2.utils.events]:  eta: 0:30:06  iter: 54099  total_loss: 28.82  loss_ce: 1.205  loss_mask: 0.1148  loss_dice: 1.497  loss_ce_0: 1.419  loss_mask_0: 0.1795  loss_dice_0: 1.408  loss_ce_1: 1.503  loss_mask_1: 0.1151  loss_dice_1: 1.473  loss_ce_2: 1.33  loss_mask_2: 0.1195  loss_dice_2: 1.579  loss_ce_3: 1.298  loss_mask_3: 0.1258  loss_dice_3: 1.413  loss_ce_4: 1.324  loss_mask_4: 0.1148  loss_dice_4: 1.36  loss_ce_5: 1.215  loss_mask_5: 0.1131  loss_dice_5: 1.267  loss_ce_6: 1.222  loss_mask_6: 0.1229  loss_dice_6: 1.112  loss_ce_7: 1.242  loss_mask_7: 0.1248  loss_dice_7: 1.378  loss_ce_8: 1.348  loss_mask_8: 0.104  loss_dice_8: 1.273    time: 0.3216  last_time: 0.3051  data_time: 0.0100  last_data_time: 0.0092   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:22 d2.utils.events]:  eta: 0:30:00  iter: 54119  total_loss: 31.46  loss_ce: 1.023  loss_mask: 0.1332  loss_dice: 1.497  loss_ce_0: 1.409  loss_mask_0: 0.1318  loss_dice_0: 1.352  loss_ce_1: 1.313  loss_mask_1: 0.1576  loss_dice_1: 1.727  loss_ce_2: 1.21  loss_mask_2: 0.1412  loss_dice_2: 1.648  loss_ce_3: 1.087  loss_mask_3: 0.1417  loss_dice_3: 1.63  loss_ce_4: 1.068  loss_mask_4: 0.1484  loss_dice_4: 1.531  loss_ce_5: 1.03  loss_mask_5: 0.1227  loss_dice_5: 1.606  loss_ce_6: 1.018  loss_mask_6: 0.1128  loss_dice_6: 1.356  loss_ce_7: 1.014  loss_mask_7: 0.1278  loss_dice_7: 1.501  loss_ce_8: 1.033  loss_mask_8: 0.1274  loss_dice_8: 1.42    time: 0.3216  last_time: 0.3047  data_time: 0.0072  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:28 d2.utils.events]:  eta: 0:29:53  iter: 54139  total_loss: 27.75  loss_ce: 0.9955  loss_mask: 0.08432  loss_dice: 1.275  loss_ce_0: 1.16  loss_mask_0: 0.1225  loss_dice_0: 1.312  loss_ce_1: 1.086  loss_mask_1: 0.1018  loss_dice_1: 1.446  loss_ce_2: 1.096  loss_mask_2: 0.127  loss_dice_2: 1.283  loss_ce_3: 0.9622  loss_mask_3: 0.1097  loss_dice_3: 1.509  loss_ce_4: 0.9878  loss_mask_4: 0.1077  loss_dice_4: 1.465  loss_ce_5: 0.9708  loss_mask_5: 0.09204  loss_dice_5: 1.541  loss_ce_6: 0.9424  loss_mask_6: 0.09115  loss_dice_6: 1.585  loss_ce_7: 0.9726  loss_mask_7: 0.1012  loss_dice_7: 1.551  loss_ce_8: 0.9612  loss_mask_8: 0.1047  loss_dice_8: 1.316    time: 0.3216  last_time: 0.2998  data_time: 0.0077  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:34 d2.utils.events]:  eta: 0:29:46  iter: 54159  total_loss: 36.11  loss_ce: 1.541  loss_mask: 0.1302  loss_dice: 1.65  loss_ce_0: 1.877  loss_mask_0: 0.1737  loss_dice_0: 1.53  loss_ce_1: 1.702  loss_mask_1: 0.147  loss_dice_1: 1.842  loss_ce_2: 1.579  loss_mask_2: 0.1566  loss_dice_2: 1.762  loss_ce_3: 1.45  loss_mask_3: 0.1396  loss_dice_3: 1.819  loss_ce_4: 1.662  loss_mask_4: 0.1359  loss_dice_4: 1.598  loss_ce_5: 1.438  loss_mask_5: 0.135  loss_dice_5: 1.578  loss_ce_6: 1.553  loss_mask_6: 0.1302  loss_dice_6: 1.805  loss_ce_7: 1.518  loss_mask_7: 0.1251  loss_dice_7: 1.595  loss_ce_8: 1.442  loss_mask_8: 0.1248  loss_dice_8: 1.648    time: 0.3216  last_time: 0.3154  data_time: 0.0071  last_data_time: 0.0139   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:40 d2.utils.events]:  eta: 0:29:41  iter: 54179  total_loss: 29.21  loss_ce: 1.103  loss_mask: 0.1601  loss_dice: 1.345  loss_ce_0: 1.341  loss_mask_0: 0.2598  loss_dice_0: 1.393  loss_ce_1: 1.369  loss_mask_1: 0.2141  loss_dice_1: 1.268  loss_ce_2: 1.348  loss_mask_2: 0.1931  loss_dice_2: 1.35  loss_ce_3: 1.145  loss_mask_3: 0.1959  loss_dice_3: 1.338  loss_ce_4: 1.189  loss_mask_4: 0.2018  loss_dice_4: 1.328  loss_ce_5: 1.207  loss_mask_5: 0.2064  loss_dice_5: 1.329  loss_ce_6: 1.01  loss_mask_6: 0.1738  loss_dice_6: 1.497  loss_ce_7: 1.119  loss_mask_7: 0.1735  loss_dice_7: 1.315  loss_ce_8: 1.132  loss_mask_8: 0.1735  loss_dice_8: 1.368    time: 0.3216  last_time: 0.2940  data_time: 0.0081  last_data_time: 0.0036   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:47 d2.utils.events]:  eta: 0:29:35  iter: 54199  total_loss: 30.91  loss_ce: 1.196  loss_mask: 0.1606  loss_dice: 1.379  loss_ce_0: 1.723  loss_mask_0: 0.1703  loss_dice_0: 1.136  loss_ce_1: 1.481  loss_mask_1: 0.1587  loss_dice_1: 1.258  loss_ce_2: 1.443  loss_mask_2: 0.1747  loss_dice_2: 1.322  loss_ce_3: 1.162  loss_mask_3: 0.2068  loss_dice_3: 1.459  loss_ce_4: 1.227  loss_mask_4: 0.174  loss_dice_4: 1.227  loss_ce_5: 1.238  loss_mask_5: 0.1849  loss_dice_5: 1.468  loss_ce_6: 1.34  loss_mask_6: 0.1666  loss_dice_6: 1.15  loss_ce_7: 1.218  loss_mask_7: 0.1768  loss_dice_7: 1.253  loss_ce_8: 1.224  loss_mask_8: 0.1636  loss_dice_8: 1.045    time: 0.3216  last_time: 0.3014  data_time: 0.0085  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:53 d2.utils.events]:  eta: 0:29:28  iter: 54219  total_loss: 29.16  loss_ce: 1.159  loss_mask: 0.2112  loss_dice: 1.204  loss_ce_0: 1.344  loss_mask_0: 0.2135  loss_dice_0: 1.378  loss_ce_1: 1.315  loss_mask_1: 0.2224  loss_dice_1: 1.276  loss_ce_2: 1.211  loss_mask_2: 0.1679  loss_dice_2: 1.198  loss_ce_3: 1.122  loss_mask_3: 0.2024  loss_dice_3: 1.273  loss_ce_4: 1.132  loss_mask_4: 0.1798  loss_dice_4: 1.241  loss_ce_5: 1.12  loss_mask_5: 0.1855  loss_dice_5: 1.185  loss_ce_6: 1.113  loss_mask_6: 0.2393  loss_dice_6: 1.197  loss_ce_7: 1.077  loss_mask_7: 0.2246  loss_dice_7: 1.103  loss_ce_8: 1.1  loss_mask_8: 0.2216  loss_dice_8: 1.169    time: 0.3216  last_time: 0.2957  data_time: 0.0065  last_data_time: 0.0045   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:30:59 d2.utils.events]:  eta: 0:29:21  iter: 54239  total_loss: 27.43  loss_ce: 1.091  loss_mask: 0.1694  loss_dice: 1.387  loss_ce_0: 1.36  loss_mask_0: 0.1641  loss_dice_0: 1.336  loss_ce_1: 1.242  loss_mask_1: 0.1658  loss_dice_1: 1.527  loss_ce_2: 1.234  loss_mask_2: 0.1616  loss_dice_2: 1.521  loss_ce_3: 1.165  loss_mask_3: 0.1781  loss_dice_3: 1.536  loss_ce_4: 1.132  loss_mask_4: 0.1604  loss_dice_4: 1.496  loss_ce_5: 1.122  loss_mask_5: 0.1541  loss_dice_5: 1.511  loss_ce_6: 1.018  loss_mask_6: 0.163  loss_dice_6: 1.277  loss_ce_7: 1.073  loss_mask_7: 0.1783  loss_dice_7: 1.498  loss_ce_8: 1.02  loss_mask_8: 0.1598  loss_dice_8: 1.418    time: 0.3216  last_time: 0.3181  data_time: 0.0066  last_data_time: 0.0088   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:05 d2.utils.events]:  eta: 0:29:15  iter: 54259  total_loss: 26.5  loss_ce: 1.083  loss_mask: 0.1496  loss_dice: 1.171  loss_ce_0: 1.66  loss_mask_0: 0.2253  loss_dice_0: 1.044  loss_ce_1: 1.565  loss_mask_1: 0.1729  loss_dice_1: 1.008  loss_ce_2: 1.255  loss_mask_2: 0.1301  loss_dice_2: 1.201  loss_ce_3: 1.247  loss_mask_3: 0.1582  loss_dice_3: 1.175  loss_ce_4: 1.142  loss_mask_4: 0.1612  loss_dice_4: 1.129  loss_ce_5: 1.218  loss_mask_5: 0.1636  loss_dice_5: 1.097  loss_ce_6: 1.182  loss_mask_6: 0.1709  loss_dice_6: 1.095  loss_ce_7: 1.126  loss_mask_7: 0.1534  loss_dice_7: 1.063  loss_ce_8: 1.077  loss_mask_8: 0.1683  loss_dice_8: 1.046    time: 0.3216  last_time: 0.2968  data_time: 0.0115  last_data_time: 0.0050   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:12 d2.utils.events]:  eta: 0:29:08  iter: 54279  total_loss: 20.63  loss_ce: 0.669  loss_mask: 0.08968  loss_dice: 1.452  loss_ce_0: 0.9297  loss_mask_0: 0.09198  loss_dice_0: 0.8118  loss_ce_1: 0.8843  loss_mask_1: 0.09458  loss_dice_1: 0.7603  loss_ce_2: 0.7506  loss_mask_2: 0.07782  loss_dice_2: 0.8944  loss_ce_3: 0.6391  loss_mask_3: 0.08076  loss_dice_3: 1.123  loss_ce_4: 0.6619  loss_mask_4: 0.08457  loss_dice_4: 1.12  loss_ce_5: 0.6797  loss_mask_5: 0.07864  loss_dice_5: 1.341  loss_ce_6: 0.6651  loss_mask_6: 0.09059  loss_dice_6: 1.112  loss_ce_7: 0.674  loss_mask_7: 0.08779  loss_dice_7: 1.128  loss_ce_8: 0.6957  loss_mask_8: 0.08007  loss_dice_8: 0.9383    time: 0.3216  last_time: 0.2985  data_time: 0.0080  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:18 d2.utils.events]:  eta: 0:29:02  iter: 54299  total_loss: 24.41  loss_ce: 0.7717  loss_mask: 0.07837  loss_dice: 1.236  loss_ce_0: 1.022  loss_mask_0: 0.09827  loss_dice_0: 1.516  loss_ce_1: 1.069  loss_mask_1: 0.1024  loss_dice_1: 1.569  loss_ce_2: 1.045  loss_mask_2: 0.1018  loss_dice_2: 1.339  loss_ce_3: 0.8665  loss_mask_3: 0.0804  loss_dice_3: 1.397  loss_ce_4: 0.8144  loss_mask_4: 0.07062  loss_dice_4: 1.147  loss_ce_5: 0.8521  loss_mask_5: 0.1094  loss_dice_5: 1.187  loss_ce_6: 0.91  loss_mask_6: 0.08401  loss_dice_6: 1.202  loss_ce_7: 0.8967  loss_mask_7: 0.08702  loss_dice_7: 1.256  loss_ce_8: 0.8464  loss_mask_8: 0.07785  loss_dice_8: 1.385    time: 0.3216  last_time: 0.3031  data_time: 0.0064  last_data_time: 0.0105   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:24 d2.utils.events]:  eta: 0:28:55  iter: 54319  total_loss: 22.01  loss_ce: 0.7085  loss_mask: 0.1173  loss_dice: 0.977  loss_ce_0: 0.9623  loss_mask_0: 0.1088  loss_dice_0: 1.015  loss_ce_1: 0.9255  loss_mask_1: 0.1114  loss_dice_1: 0.9367  loss_ce_2: 0.8867  loss_mask_2: 0.1171  loss_dice_2: 0.7576  loss_ce_3: 0.8214  loss_mask_3: 0.08812  loss_dice_3: 0.7499  loss_ce_4: 0.8375  loss_mask_4: 0.08646  loss_dice_4: 0.7243  loss_ce_5: 0.8404  loss_mask_5: 0.1121  loss_dice_5: 0.7412  loss_ce_6: 0.8187  loss_mask_6: 0.1126  loss_dice_6: 0.7568  loss_ce_7: 0.7109  loss_mask_7: 0.1064  loss_dice_7: 0.6712  loss_ce_8: 0.7986  loss_mask_8: 0.1152  loss_dice_8: 0.7912    time: 0.3216  last_time: 0.3270  data_time: 0.0069  last_data_time: 0.0062   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:30 d2.utils.events]:  eta: 0:28:49  iter: 54339  total_loss: 28.61  loss_ce: 1.157  loss_mask: 0.1408  loss_dice: 1.255  loss_ce_0: 1.345  loss_mask_0: 0.1327  loss_dice_0: 1.275  loss_ce_1: 1.394  loss_mask_1: 0.1742  loss_dice_1: 1.483  loss_ce_2: 1.378  loss_mask_2: 0.1325  loss_dice_2: 1.332  loss_ce_3: 1.176  loss_mask_3: 0.1577  loss_dice_3: 1.229  loss_ce_4: 1.166  loss_mask_4: 0.1425  loss_dice_4: 1.415  loss_ce_5: 1.173  loss_mask_5: 0.1561  loss_dice_5: 1.244  loss_ce_6: 1.166  loss_mask_6: 0.1548  loss_dice_6: 1.099  loss_ce_7: 1.176  loss_mask_7: 0.1554  loss_dice_7: 1.338  loss_ce_8: 1.186  loss_mask_8: 0.1611  loss_dice_8: 1.236    time: 0.3216  last_time: 0.3088  data_time: 0.0122  last_data_time: 0.0083   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:37 d2.utils.events]:  eta: 0:28:44  iter: 54359  total_loss: 30.06  loss_ce: 1.204  loss_mask: 0.1302  loss_dice: 1.371  loss_ce_0: 1.51  loss_mask_0: 0.1173  loss_dice_0: 1.369  loss_ce_1: 1.455  loss_mask_1: 0.1341  loss_dice_1: 1.606  loss_ce_2: 1.204  loss_mask_2: 0.1361  loss_dice_2: 1.567  loss_ce_3: 1.209  loss_mask_3: 0.1427  loss_dice_3: 1.355  loss_ce_4: 1.225  loss_mask_4: 0.1328  loss_dice_4: 1.538  loss_ce_5: 1.193  loss_mask_5: 0.1285  loss_dice_5: 1.466  loss_ce_6: 1.114  loss_mask_6: 0.1394  loss_dice_6: 1.435  loss_ce_7: 1.161  loss_mask_7: 0.1315  loss_dice_7: 1.255  loss_ce_8: 1.292  loss_mask_8: 0.1272  loss_dice_8: 1.257    time: 0.3216  last_time: 0.3010  data_time: 0.0122  last_data_time: 0.0074   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:43 d2.utils.events]:  eta: 0:28:38  iter: 54379  total_loss: 29.39  loss_ce: 1.064  loss_mask: 0.1603  loss_dice: 1.68  loss_ce_0: 1.172  loss_mask_0: 0.1338  loss_dice_0: 1.667  loss_ce_1: 1.158  loss_mask_1: 0.1314  loss_dice_1: 1.465  loss_ce_2: 1.042  loss_mask_2: 0.1388  loss_dice_2: 1.581  loss_ce_3: 0.9706  loss_mask_3: 0.2018  loss_dice_3: 1.464  loss_ce_4: 1.075  loss_mask_4: 0.1378  loss_dice_4: 1.566  loss_ce_5: 1.1  loss_mask_5: 0.1522  loss_dice_5: 1.581  loss_ce_6: 1.063  loss_mask_6: 0.1537  loss_dice_6: 1.627  loss_ce_7: 1.016  loss_mask_7: 0.2136  loss_dice_7: 1.558  loss_ce_8: 1.058  loss_mask_8: 0.1475  loss_dice_8: 1.444    time: 0.3216  last_time: 0.2972  data_time: 0.0071  last_data_time: 0.0041   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:49 d2.utils.events]:  eta: 0:28:32  iter: 54399  total_loss: 25.6  loss_ce: 0.9127  loss_mask: 0.2205  loss_dice: 1.44  loss_ce_0: 1.218  loss_mask_0: 0.2054  loss_dice_0: 1.262  loss_ce_1: 1.18  loss_mask_1: 0.1615  loss_dice_1: 1.556  loss_ce_2: 1.015  loss_mask_2: 0.2619  loss_dice_2: 1.471  loss_ce_3: 0.9474  loss_mask_3: 0.262  loss_dice_3: 1.403  loss_ce_4: 0.9223  loss_mask_4: 0.2333  loss_dice_4: 1.541  loss_ce_5: 0.9  loss_mask_5: 0.2394  loss_dice_5: 1.494  loss_ce_6: 1.102  loss_mask_6: 0.233  loss_dice_6: 1.371  loss_ce_7: 0.9833  loss_mask_7: 0.1987  loss_dice_7: 1.352  loss_ce_8: 1.069  loss_mask_8: 0.2284  loss_dice_8: 1.415    time: 0.3216  last_time: 0.4319  data_time: 0.0119  last_data_time: 0.1123   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:31:55 d2.utils.events]:  eta: 0:28:26  iter: 54419  total_loss: 30.85  loss_ce: 1.089  loss_mask: 0.1708  loss_dice: 1.67  loss_ce_0: 1.477  loss_mask_0: 0.1819  loss_dice_0: 1.642  loss_ce_1: 1.461  loss_mask_1: 0.1761  loss_dice_1: 1.845  loss_ce_2: 1.261  loss_mask_2: 0.1816  loss_dice_2: 1.837  loss_ce_3: 1.055  loss_mask_3: 0.1403  loss_dice_3: 1.739  loss_ce_4: 1.117  loss_mask_4: 0.1772  loss_dice_4: 1.808  loss_ce_5: 1.144  loss_mask_5: 0.1573  loss_dice_5: 1.935  loss_ce_6: 1.006  loss_mask_6: 0.1737  loss_dice_6: 1.715  loss_ce_7: 1.008  loss_mask_7: 0.1732  loss_dice_7: 1.666  loss_ce_8: 1.014  loss_mask_8: 0.1707  loss_dice_8: 1.874    time: 0.3216  last_time: 0.3298  data_time: 0.0073  last_data_time: 0.0110   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:02 d2.utils.events]:  eta: 0:28:19  iter: 54439  total_loss: 22.41  loss_ce: 0.8533  loss_mask: 0.08523  loss_dice: 0.9238  loss_ce_0: 0.9667  loss_mask_0: 0.1258  loss_dice_0: 1.035  loss_ce_1: 0.9996  loss_mask_1: 0.1277  loss_dice_1: 1.154  loss_ce_2: 0.9588  loss_mask_2: 0.09859  loss_dice_2: 1.327  loss_ce_3: 0.8445  loss_mask_3: 0.111  loss_dice_3: 1.136  loss_ce_4: 0.8401  loss_mask_4: 0.1058  loss_dice_4: 0.9984  loss_ce_5: 0.8673  loss_mask_5: 0.09884  loss_dice_5: 0.8603  loss_ce_6: 0.8497  loss_mask_6: 0.1205  loss_dice_6: 0.9744  loss_ce_7: 0.8449  loss_mask_7: 0.08379  loss_dice_7: 0.8498  loss_ce_8: 0.8426  loss_mask_8: 0.1083  loss_dice_8: 0.852    time: 0.3216  last_time: 0.2961  data_time: 0.0064  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:08 d2.utils.events]:  eta: 0:28:13  iter: 54459  total_loss: 27.71  loss_ce: 1.002  loss_mask: 0.1561  loss_dice: 1.062  loss_ce_0: 1.322  loss_mask_0: 0.1892  loss_dice_0: 1.268  loss_ce_1: 1.087  loss_mask_1: 0.1756  loss_dice_1: 1.342  loss_ce_2: 1.073  loss_mask_2: 0.1852  loss_dice_2: 1.236  loss_ce_3: 0.9575  loss_mask_3: 0.2039  loss_dice_3: 1.265  loss_ce_4: 0.9518  loss_mask_4: 0.2171  loss_dice_4: 1.124  loss_ce_5: 1.016  loss_mask_5: 0.1768  loss_dice_5: 1.231  loss_ce_6: 1.028  loss_mask_6: 0.1738  loss_dice_6: 1.1  loss_ce_7: 0.9418  loss_mask_7: 0.2108  loss_dice_7: 0.9965  loss_ce_8: 0.9652  loss_mask_8: 0.1641  loss_dice_8: 1.15    time: 0.3216  last_time: 0.2901  data_time: 0.0170  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:14 d2.utils.events]:  eta: 0:28:06  iter: 54479  total_loss: 27.05  loss_ce: 1.024  loss_mask: 0.1736  loss_dice: 1.076  loss_ce_0: 1.422  loss_mask_0: 0.1683  loss_dice_0: 1.179  loss_ce_1: 1.235  loss_mask_1: 0.1269  loss_dice_1: 1.117  loss_ce_2: 1.124  loss_mask_2: 0.1754  loss_dice_2: 1.006  loss_ce_3: 1.165  loss_mask_3: 0.1528  loss_dice_3: 1.026  loss_ce_4: 1.077  loss_mask_4: 0.1646  loss_dice_4: 1.102  loss_ce_5: 1.167  loss_mask_5: 0.1649  loss_dice_5: 0.9346  loss_ce_6: 1.064  loss_mask_6: 0.1606  loss_dice_6: 0.9924  loss_ce_7: 1.05  loss_mask_7: 0.1542  loss_dice_7: 0.8987  loss_ce_8: 1.032  loss_mask_8: 0.1694  loss_dice_8: 1.086    time: 0.3216  last_time: 0.3361  data_time: 0.0061  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:20 d2.utils.events]:  eta: 0:27:59  iter: 54499  total_loss: 24.59  loss_ce: 1.056  loss_mask: 0.3092  loss_dice: 0.9968  loss_ce_0: 1.419  loss_mask_0: 0.33  loss_dice_0: 1.096  loss_ce_1: 1.251  loss_mask_1: 0.3134  loss_dice_1: 1.496  loss_ce_2: 1.179  loss_mask_2: 0.2635  loss_dice_2: 1.532  loss_ce_3: 1.076  loss_mask_3: 0.291  loss_dice_3: 1.198  loss_ce_4: 1.038  loss_mask_4: 0.2496  loss_dice_4: 1.025  loss_ce_5: 1.069  loss_mask_5: 0.2365  loss_dice_5: 1.017  loss_ce_6: 1.061  loss_mask_6: 0.3054  loss_dice_6: 1.056  loss_ce_7: 1.128  loss_mask_7: 0.3062  loss_dice_7: 0.8956  loss_ce_8: 1.057  loss_mask_8: 0.2927  loss_dice_8: 1.103    time: 0.3216  last_time: 0.3030  data_time: 0.0067  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:27 d2.utils.events]:  eta: 0:27:53  iter: 54519  total_loss: 20.95  loss_ce: 0.8645  loss_mask: 0.1055  loss_dice: 1.036  loss_ce_0: 1.082  loss_mask_0: 0.127  loss_dice_0: 1.121  loss_ce_1: 1.007  loss_mask_1: 0.09487  loss_dice_1: 1.02  loss_ce_2: 0.9116  loss_mask_2: 0.09526  loss_dice_2: 1.15  loss_ce_3: 0.9202  loss_mask_3: 0.08859  loss_dice_3: 0.9277  loss_ce_4: 0.866  loss_mask_4: 0.1148  loss_dice_4: 1.104  loss_ce_5: 0.9223  loss_mask_5: 0.1012  loss_dice_5: 1.159  loss_ce_6: 0.867  loss_mask_6: 0.09958  loss_dice_6: 0.9813  loss_ce_7: 0.8747  loss_mask_7: 0.09893  loss_dice_7: 1.093  loss_ce_8: 0.9159  loss_mask_8: 0.0891  loss_dice_8: 0.7624    time: 0.3216  last_time: 0.2951  data_time: 0.0179  last_data_time: 0.0051   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:33 d2.utils.events]:  eta: 0:27:47  iter: 54539  total_loss: 31.27  loss_ce: 1.088  loss_mask: 0.132  loss_dice: 1.627  loss_ce_0: 1.356  loss_mask_0: 0.1167  loss_dice_0: 1.72  loss_ce_1: 1.44  loss_mask_1: 0.1452  loss_dice_1: 1.635  loss_ce_2: 1.103  loss_mask_2: 0.1581  loss_dice_2: 1.651  loss_ce_3: 1.112  loss_mask_3: 0.1378  loss_dice_3: 1.547  loss_ce_4: 1.024  loss_mask_4: 0.1342  loss_dice_4: 1.541  loss_ce_5: 1.132  loss_mask_5: 0.1322  loss_dice_5: 1.533  loss_ce_6: 1.153  loss_mask_6: 0.1271  loss_dice_6: 1.533  loss_ce_7: 1.025  loss_mask_7: 0.1532  loss_dice_7: 1.571  loss_ce_8: 1.017  loss_mask_8: 0.1515  loss_dice_8: 1.526    time: 0.3216  last_time: 0.2980  data_time: 0.0083  last_data_time: 0.0037   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:39 d2.utils.events]:  eta: 0:27:42  iter: 54559  total_loss: 22.82  loss_ce: 0.7444  loss_mask: 0.1267  loss_dice: 0.9219  loss_ce_0: 1.047  loss_mask_0: 0.1267  loss_dice_0: 0.8018  loss_ce_1: 0.8401  loss_mask_1: 0.1514  loss_dice_1: 1.259  loss_ce_2: 0.7713  loss_mask_2: 0.1429  loss_dice_2: 1.237  loss_ce_3: 0.7183  loss_mask_3: 0.1341  loss_dice_3: 0.7265  loss_ce_4: 0.7586  loss_mask_4: 0.1414  loss_dice_4: 0.9219  loss_ce_5: 0.7445  loss_mask_5: 0.1492  loss_dice_5: 0.8955  loss_ce_6: 0.6335  loss_mask_6: 0.126  loss_dice_6: 0.9626  loss_ce_7: 0.6385  loss_mask_7: 0.1146  loss_dice_7: 0.7756  loss_ce_8: 0.7448  loss_mask_8: 0.1238  loss_dice_8: 0.9264    time: 0.3216  last_time: 0.2992  data_time: 0.0069  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:45 d2.utils.events]:  eta: 0:27:35  iter: 54579  total_loss: 24.31  loss_ce: 0.9714  loss_mask: 0.1255  loss_dice: 1.176  loss_ce_0: 1.34  loss_mask_0: 0.1099  loss_dice_0: 1.251  loss_ce_1: 1.222  loss_mask_1: 0.1035  loss_dice_1: 1.3  loss_ce_2: 1.038  loss_mask_2: 0.1222  loss_dice_2: 1.154  loss_ce_3: 0.9779  loss_mask_3: 0.1122  loss_dice_3: 1.263  loss_ce_4: 0.9714  loss_mask_4: 0.1221  loss_dice_4: 1.252  loss_ce_5: 0.9969  loss_mask_5: 0.1217  loss_dice_5: 1.217  loss_ce_6: 0.9588  loss_mask_6: 0.1249  loss_dice_6: 1.158  loss_ce_7: 0.9732  loss_mask_7: 0.1251  loss_dice_7: 1.103  loss_ce_8: 0.9871  loss_mask_8: 0.1272  loss_dice_8: 1.409    time: 0.3216  last_time: 0.3273  data_time: 0.0066  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:52 d2.utils.events]:  eta: 0:27:30  iter: 54599  total_loss: 30.75  loss_ce: 1.115  loss_mask: 0.1858  loss_dice: 1.354  loss_ce_0: 1.509  loss_mask_0: 0.1642  loss_dice_0: 1.374  loss_ce_1: 1.313  loss_mask_1: 0.222  loss_dice_1: 1.464  loss_ce_2: 1.249  loss_mask_2: 0.1826  loss_dice_2: 1.343  loss_ce_3: 1.261  loss_mask_3: 0.1793  loss_dice_3: 1.237  loss_ce_4: 1.083  loss_mask_4: 0.1716  loss_dice_4: 1.375  loss_ce_5: 1.061  loss_mask_5: 0.1916  loss_dice_5: 1.549  loss_ce_6: 1.122  loss_mask_6: 0.1985  loss_dice_6: 1.421  loss_ce_7: 1.093  loss_mask_7: 0.1953  loss_dice_7: 1.298  loss_ce_8: 1.112  loss_mask_8: 0.2037  loss_dice_8: 1.534    time: 0.3216  last_time: 0.3319  data_time: 0.0114  last_data_time: 0.0083   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:32:58 d2.utils.events]:  eta: 0:27:23  iter: 54619  total_loss: 28  loss_ce: 0.9864  loss_mask: 0.1793  loss_dice: 1.472  loss_ce_0: 1.203  loss_mask_0: 0.1795  loss_dice_0: 1.4  loss_ce_1: 1.27  loss_mask_1: 0.1942  loss_dice_1: 1.318  loss_ce_2: 1.151  loss_mask_2: 0.2368  loss_dice_2: 1.36  loss_ce_3: 1.078  loss_mask_3: 0.2456  loss_dice_3: 1.251  loss_ce_4: 1.067  loss_mask_4: 0.2353  loss_dice_4: 1.399  loss_ce_5: 1.031  loss_mask_5: 0.1989  loss_dice_5: 1.444  loss_ce_6: 1.029  loss_mask_6: 0.1784  loss_dice_6: 1.449  loss_ce_7: 1.105  loss_mask_7: 0.1893  loss_dice_7: 1.314  loss_ce_8: 1.079  loss_mask_8: 0.1835  loss_dice_8: 1.301    time: 0.3215  last_time: 0.3013  data_time: 0.0122  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:04 d2.utils.events]:  eta: 0:27:17  iter: 54639  total_loss: 30.29  loss_ce: 1.073  loss_mask: 0.09625  loss_dice: 1.458  loss_ce_0: 1.287  loss_mask_0: 0.08149  loss_dice_0: 1.474  loss_ce_1: 1.233  loss_mask_1: 0.09935  loss_dice_1: 1.735  loss_ce_2: 1.242  loss_mask_2: 0.09475  loss_dice_2: 1.46  loss_ce_3: 1.096  loss_mask_3: 0.1157  loss_dice_3: 1.514  loss_ce_4: 1.019  loss_mask_4: 0.1288  loss_dice_4: 1.521  loss_ce_5: 1.004  loss_mask_5: 0.1072  loss_dice_5: 1.479  loss_ce_6: 1.014  loss_mask_6: 0.1042  loss_dice_6: 1.436  loss_ce_7: 1.031  loss_mask_7: 0.1037  loss_dice_7: 1.375  loss_ce_8: 1.051  loss_mask_8: 0.107  loss_dice_8: 1.453    time: 0.3215  last_time: 0.3220  data_time: 0.0130  last_data_time: 0.0050   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:11 d2.utils.events]:  eta: 0:27:11  iter: 54659  total_loss: 27.74  loss_ce: 1.037  loss_mask: 0.176  loss_dice: 1.099  loss_ce_0: 1.506  loss_mask_0: 0.2291  loss_dice_0: 1.227  loss_ce_1: 1.317  loss_mask_1: 0.1299  loss_dice_1: 1.037  loss_ce_2: 1.318  loss_mask_2: 0.1584  loss_dice_2: 1.166  loss_ce_3: 1.128  loss_mask_3: 0.1725  loss_dice_3: 1.049  loss_ce_4: 1.122  loss_mask_4: 0.1685  loss_dice_4: 1.074  loss_ce_5: 1.059  loss_mask_5: 0.1845  loss_dice_5: 1.073  loss_ce_6: 1.055  loss_mask_6: 0.1743  loss_dice_6: 1.017  loss_ce_7: 1.042  loss_mask_7: 0.1767  loss_dice_7: 1.048  loss_ce_8: 1.105  loss_mask_8: 0.1752  loss_dice_8: 0.9815    time: 0.3215  last_time: 0.3055  data_time: 0.0126  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:17 d2.utils.events]:  eta: 0:27:05  iter: 54679  total_loss: 19.7  loss_ce: 0.8668  loss_mask: 0.07836  loss_dice: 0.6712  loss_ce_0: 1.126  loss_mask_0: 0.08156  loss_dice_0: 0.9641  loss_ce_1: 1.146  loss_mask_1: 0.06644  loss_dice_1: 0.9352  loss_ce_2: 0.9807  loss_mask_2: 0.06904  loss_dice_2: 0.9324  loss_ce_3: 0.9722  loss_mask_3: 0.07276  loss_dice_3: 0.7021  loss_ce_4: 0.8964  loss_mask_4: 0.0942  loss_dice_4: 0.8382  loss_ce_5: 0.8826  loss_mask_5: 0.06942  loss_dice_5: 0.6896  loss_ce_6: 0.9108  loss_mask_6: 0.07802  loss_dice_6: 0.7617  loss_ce_7: 0.8882  loss_mask_7: 0.07108  loss_dice_7: 0.7028  loss_ce_8: 0.9237  loss_mask_8: 0.07343  loss_dice_8: 0.7309    time: 0.3215  last_time: 0.2957  data_time: 0.0070  last_data_time: 0.0040   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:23 d2.utils.events]:  eta: 0:26:59  iter: 54699  total_loss: 21.62  loss_ce: 1.054  loss_mask: 0.2119  loss_dice: 0.8057  loss_ce_0: 1.46  loss_mask_0: 0.3353  loss_dice_0: 0.9009  loss_ce_1: 1.307  loss_mask_1: 0.2755  loss_dice_1: 1.013  loss_ce_2: 1.022  loss_mask_2: 0.2907  loss_dice_2: 0.8826  loss_ce_3: 0.9438  loss_mask_3: 0.2786  loss_dice_3: 1.02  loss_ce_4: 1.025  loss_mask_4: 0.2506  loss_dice_4: 0.919  loss_ce_5: 1.02  loss_mask_5: 0.2495  loss_dice_5: 0.8943  loss_ce_6: 0.9584  loss_mask_6: 0.2514  loss_dice_6: 0.873  loss_ce_7: 1.027  loss_mask_7: 0.2661  loss_dice_7: 1.074  loss_ce_8: 1.014  loss_mask_8: 0.2341  loss_dice_8: 0.7985    time: 0.3215  last_time: 0.3062  data_time: 0.0121  last_data_time: 0.0158   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:29 d2.utils.events]:  eta: 0:26:52  iter: 54719  total_loss: 19.16  loss_ce: 0.83  loss_mask: 0.1312  loss_dice: 0.8413  loss_ce_0: 1.053  loss_mask_0: 0.1139  loss_dice_0: 0.8348  loss_ce_1: 0.9898  loss_mask_1: 0.1004  loss_dice_1: 1.442  loss_ce_2: 0.9519  loss_mask_2: 0.1193  loss_dice_2: 1.311  loss_ce_3: 0.8226  loss_mask_3: 0.11  loss_dice_3: 1.065  loss_ce_4: 0.8858  loss_mask_4: 0.1077  loss_dice_4: 1.045  loss_ce_5: 0.8152  loss_mask_5: 0.1148  loss_dice_5: 0.9251  loss_ce_6: 0.8099  loss_mask_6: 0.1215  loss_dice_6: 0.8895  loss_ce_7: 0.8143  loss_mask_7: 0.1372  loss_dice_7: 0.9481  loss_ce_8: 0.8599  loss_mask_8: 0.1334  loss_dice_8: 0.8286    time: 0.3215  last_time: 0.3013  data_time: 0.0068  last_data_time: 0.0087   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:35 d2.utils.events]:  eta: 0:26:45  iter: 54739  total_loss: 33.2  loss_ce: 1.324  loss_mask: 0.1824  loss_dice: 1.125  loss_ce_0: 1.713  loss_mask_0: 0.2179  loss_dice_0: 1.647  loss_ce_1: 1.589  loss_mask_1: 0.1979  loss_dice_1: 1.45  loss_ce_2: 1.436  loss_mask_2: 0.1831  loss_dice_2: 1.513  loss_ce_3: 1.349  loss_mask_3: 0.2122  loss_dice_3: 1.276  loss_ce_4: 1.372  loss_mask_4: 0.2022  loss_dice_4: 1.356  loss_ce_5: 1.34  loss_mask_5: 0.1903  loss_dice_5: 1.455  loss_ce_6: 1.291  loss_mask_6: 0.199  loss_dice_6: 1.287  loss_ce_7: 1.355  loss_mask_7: 0.207  loss_dice_7: 1.155  loss_ce_8: 1.3  loss_mask_8: 0.2161  loss_dice_8: 1.461    time: 0.3215  last_time: 0.3084  data_time: 0.0149  last_data_time: 0.0092   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:42 d2.utils.events]:  eta: 0:26:39  iter: 54759  total_loss: 29.7  loss_ce: 1.191  loss_mask: 0.1173  loss_dice: 0.9874  loss_ce_0: 1.551  loss_mask_0: 0.162  loss_dice_0: 1.291  loss_ce_1: 1.414  loss_mask_1: 0.2379  loss_dice_1: 1.35  loss_ce_2: 1.295  loss_mask_2: 0.1827  loss_dice_2: 1.29  loss_ce_3: 1.215  loss_mask_3: 0.1593  loss_dice_3: 0.9784  loss_ce_4: 1.218  loss_mask_4: 0.1524  loss_dice_4: 1.028  loss_ce_5: 1.204  loss_mask_5: 0.1451  loss_dice_5: 1.014  loss_ce_6: 1.16  loss_mask_6: 0.1518  loss_dice_6: 1.075  loss_ce_7: 1.191  loss_mask_7: 0.1459  loss_dice_7: 0.8958  loss_ce_8: 1.191  loss_mask_8: 0.1317  loss_dice_8: 1.156    time: 0.3215  last_time: 0.3027  data_time: 0.0174  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:48 d2.utils.events]:  eta: 0:26:33  iter: 54779  total_loss: 26.05  loss_ce: 0.9146  loss_mask: 0.1607  loss_dice: 1.181  loss_ce_0: 0.9793  loss_mask_0: 0.2129  loss_dice_0: 1.65  loss_ce_1: 0.9626  loss_mask_1: 0.2212  loss_dice_1: 1.704  loss_ce_2: 0.8296  loss_mask_2: 0.1865  loss_dice_2: 1.667  loss_ce_3: 0.8865  loss_mask_3: 0.1692  loss_dice_3: 1.276  loss_ce_4: 0.8206  loss_mask_4: 0.1453  loss_dice_4: 1.418  loss_ce_5: 0.8084  loss_mask_5: 0.135  loss_dice_5: 1.702  loss_ce_6: 0.7953  loss_mask_6: 0.1781  loss_dice_6: 1.469  loss_ce_7: 0.8119  loss_mask_7: 0.1532  loss_dice_7: 1.476  loss_ce_8: 0.8408  loss_mask_8: 0.1411  loss_dice_8: 1.361    time: 0.3215  last_time: 0.3032  data_time: 0.0134  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:33:55 d2.utils.events]:  eta: 0:26:27  iter: 54799  total_loss: 31.73  loss_ce: 1.082  loss_mask: 0.2013  loss_dice: 1.448  loss_ce_0: 1.252  loss_mask_0: 0.261  loss_dice_0: 1.704  loss_ce_1: 1.303  loss_mask_1: 0.216  loss_dice_1: 1.377  loss_ce_2: 1.204  loss_mask_2: 0.1344  loss_dice_2: 1.556  loss_ce_3: 1.102  loss_mask_3: 0.1919  loss_dice_3: 1.73  loss_ce_4: 1.084  loss_mask_4: 0.1833  loss_dice_4: 1.468  loss_ce_5: 1.106  loss_mask_5: 0.1965  loss_dice_5: 1.643  loss_ce_6: 1.054  loss_mask_6: 0.1827  loss_dice_6: 1.539  loss_ce_7: 1.066  loss_mask_7: 0.177  loss_dice_7: 1.78  loss_ce_8: 1.017  loss_mask_8: 0.1901  loss_dice_8: 1.687    time: 0.3215  last_time: 0.3247  data_time: 0.0167  last_data_time: 0.0080   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:01 d2.utils.events]:  eta: 0:26:20  iter: 54819  total_loss: 32.15  loss_ce: 1.28  loss_mask: 0.2727  loss_dice: 1.434  loss_ce_0: 1.578  loss_mask_0: 0.4119  loss_dice_0: 1.394  loss_ce_1: 1.559  loss_mask_1: 0.296  loss_dice_1: 1.389  loss_ce_2: 1.413  loss_mask_2: 0.3017  loss_dice_2: 1.538  loss_ce_3: 1.316  loss_mask_3: 0.2996  loss_dice_3: 1.386  loss_ce_4: 1.243  loss_mask_4: 0.2312  loss_dice_4: 1.323  loss_ce_5: 1.252  loss_mask_5: 0.2434  loss_dice_5: 1.292  loss_ce_6: 1.295  loss_mask_6: 0.2514  loss_dice_6: 1.447  loss_ce_7: 1.29  loss_mask_7: 0.2223  loss_dice_7: 1.408  loss_ce_8: 1.28  loss_mask_8: 0.2272  loss_dice_8: 1.332    time: 0.3215  last_time: 0.2923  data_time: 0.0074  last_data_time: 0.0048   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:07 d2.utils.events]:  eta: 0:26:14  iter: 54839  total_loss: 35.87  loss_ce: 1.591  loss_mask: 0.1799  loss_dice: 1.726  loss_ce_0: 1.86  loss_mask_0: 0.1782  loss_dice_0: 2.008  loss_ce_1: 1.74  loss_mask_1: 0.158  loss_dice_1: 1.963  loss_ce_2: 1.71  loss_mask_2: 0.1714  loss_dice_2: 1.906  loss_ce_3: 1.572  loss_mask_3: 0.1831  loss_dice_3: 1.858  loss_ce_4: 1.538  loss_mask_4: 0.188  loss_dice_4: 1.817  loss_ce_5: 1.529  loss_mask_5: 0.1836  loss_dice_5: 1.838  loss_ce_6: 1.453  loss_mask_6: 0.1823  loss_dice_6: 1.729  loss_ce_7: 1.549  loss_mask_7: 0.1715  loss_dice_7: 1.723  loss_ce_8: 1.458  loss_mask_8: 0.1824  loss_dice_8: 1.822    time: 0.3215  last_time: 0.3032  data_time: 0.0108  last_data_time: 0.0094   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:13 d2.utils.events]:  eta: 0:26:07  iter: 54859  total_loss: 25.57  loss_ce: 0.9994  loss_mask: 0.08788  loss_dice: 1.007  loss_ce_0: 1.277  loss_mask_0: 0.1169  loss_dice_0: 1.05  loss_ce_1: 1.095  loss_mask_1: 0.0978  loss_dice_1: 1.06  loss_ce_2: 1.042  loss_mask_2: 0.0919  loss_dice_2: 0.9859  loss_ce_3: 1.012  loss_mask_3: 0.1003  loss_dice_3: 1.166  loss_ce_4: 0.9882  loss_mask_4: 0.09948  loss_dice_4: 1.095  loss_ce_5: 0.9939  loss_mask_5: 0.1031  loss_dice_5: 1.049  loss_ce_6: 0.9872  loss_mask_6: 0.1023  loss_dice_6: 1.165  loss_ce_7: 0.8678  loss_mask_7: 0.1099  loss_dice_7: 1.014  loss_ce_8: 0.9945  loss_mask_8: 0.1023  loss_dice_8: 1.221    time: 0.3215  last_time: 0.3230  data_time: 0.0066  last_data_time: 0.0095   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:20 d2.utils.events]:  eta: 0:26:00  iter: 54879  total_loss: 25.04  loss_ce: 0.7615  loss_mask: 0.1193  loss_dice: 1.296  loss_ce_0: 1.101  loss_mask_0: 0.1206  loss_dice_0: 1.411  loss_ce_1: 0.9921  loss_mask_1: 0.1202  loss_dice_1: 1.348  loss_ce_2: 0.8431  loss_mask_2: 0.1278  loss_dice_2: 1.285  loss_ce_3: 0.8207  loss_mask_3: 0.1326  loss_dice_3: 1.21  loss_ce_4: 0.7981  loss_mask_4: 0.1334  loss_dice_4: 1.217  loss_ce_5: 0.8228  loss_mask_5: 0.1238  loss_dice_5: 1.298  loss_ce_6: 0.7998  loss_mask_6: 0.1282  loss_dice_6: 1.301  loss_ce_7: 0.7935  loss_mask_7: 0.1186  loss_dice_7: 1.193  loss_ce_8: 0.7848  loss_mask_8: 0.1064  loss_dice_8: 1.269    time: 0.3215  last_time: 0.2955  data_time: 0.0069  last_data_time: 0.0029   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:26 d2.utils.events]:  eta: 0:25:54  iter: 54899  total_loss: 17.99  loss_ce: 0.8557  loss_mask: 0.1053  loss_dice: 0.818  loss_ce_0: 0.8677  loss_mask_0: 0.1001  loss_dice_0: 0.815  loss_ce_1: 0.9688  loss_mask_1: 0.109  loss_dice_1: 1.046  loss_ce_2: 0.8592  loss_mask_2: 0.1097  loss_dice_2: 0.7757  loss_ce_3: 0.8383  loss_mask_3: 0.1266  loss_dice_3: 0.7189  loss_ce_4: 0.8334  loss_mask_4: 0.1122  loss_dice_4: 0.7116  loss_ce_5: 0.8484  loss_mask_5: 0.1271  loss_dice_5: 0.7592  loss_ce_6: 0.8037  loss_mask_6: 0.1077  loss_dice_6: 0.7189  loss_ce_7: 0.8617  loss_mask_7: 0.09955  loss_dice_7: 0.8247  loss_ce_8: 0.8685  loss_mask_8: 0.1123  loss_dice_8: 0.902    time: 0.3215  last_time: 0.3201  data_time: 0.0080  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:32 d2.utils.events]:  eta: 0:25:47  iter: 54919  total_loss: 21.85  loss_ce: 0.9531  loss_mask: 0.1186  loss_dice: 1.198  loss_ce_0: 1.246  loss_mask_0: 0.1484  loss_dice_0: 1.397  loss_ce_1: 1.137  loss_mask_1: 0.1585  loss_dice_1: 1.209  loss_ce_2: 1.126  loss_mask_2: 0.1335  loss_dice_2: 1.298  loss_ce_3: 0.9474  loss_mask_3: 0.1292  loss_dice_3: 1.308  loss_ce_4: 0.9383  loss_mask_4: 0.1389  loss_dice_4: 1.211  loss_ce_5: 1.014  loss_mask_5: 0.1219  loss_dice_5: 1.289  loss_ce_6: 0.9882  loss_mask_6: 0.1161  loss_dice_6: 1.099  loss_ce_7: 0.9826  loss_mask_7: 0.1248  loss_dice_7: 1.274  loss_ce_8: 0.9493  loss_mask_8: 0.1275  loss_dice_8: 1.284    time: 0.3215  last_time: 0.3156  data_time: 0.0111  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:38 d2.utils.events]:  eta: 0:25:41  iter: 54939  total_loss: 25.04  loss_ce: 0.9039  loss_mask: 0.1285  loss_dice: 1.445  loss_ce_0: 1.221  loss_mask_0: 0.09769  loss_dice_0: 1.399  loss_ce_1: 1.177  loss_mask_1: 0.1054  loss_dice_1: 1.325  loss_ce_2: 1.141  loss_mask_2: 0.1424  loss_dice_2: 1.356  loss_ce_3: 1.038  loss_mask_3: 0.1253  loss_dice_3: 1.301  loss_ce_4: 0.8555  loss_mask_4: 0.1326  loss_dice_4: 1.471  loss_ce_5: 0.9286  loss_mask_5: 0.1348  loss_dice_5: 1.464  loss_ce_6: 0.9228  loss_mask_6: 0.1403  loss_dice_6: 1.504  loss_ce_7: 0.9506  loss_mask_7: 0.1198  loss_dice_7: 1.461  loss_ce_8: 0.9378  loss_mask_8: 0.1278  loss_dice_8: 1.465    time: 0.3215  last_time: 0.3076  data_time: 0.0089  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:44 d2.utils.events]:  eta: 0:25:35  iter: 54959  total_loss: 23.98  loss_ce: 1.011  loss_mask: 0.176  loss_dice: 1.052  loss_ce_0: 1.127  loss_mask_0: 0.1973  loss_dice_0: 0.9165  loss_ce_1: 1.22  loss_mask_1: 0.1985  loss_dice_1: 1.209  loss_ce_2: 1.045  loss_mask_2: 0.1968  loss_dice_2: 1.021  loss_ce_3: 1.015  loss_mask_3: 0.2232  loss_dice_3: 1.149  loss_ce_4: 0.9599  loss_mask_4: 0.1775  loss_dice_4: 1.058  loss_ce_5: 0.9982  loss_mask_5: 0.1812  loss_dice_5: 1.042  loss_ce_6: 0.9337  loss_mask_6: 0.2037  loss_dice_6: 1.013  loss_ce_7: 0.9613  loss_mask_7: 0.179  loss_dice_7: 1.002  loss_ce_8: 1.021  loss_mask_8: 0.1724  loss_dice_8: 1.043    time: 0.3215  last_time: 0.3288  data_time: 0.0072  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:51 d2.utils.events]:  eta: 0:25:30  iter: 54979  total_loss: 30.94  loss_ce: 1.126  loss_mask: 0.1088  loss_dice: 1.68  loss_ce_0: 1.499  loss_mask_0: 0.1088  loss_dice_0: 1.522  loss_ce_1: 1.131  loss_mask_1: 0.08534  loss_dice_1: 1.618  loss_ce_2: 1.127  loss_mask_2: 0.1099  loss_dice_2: 1.613  loss_ce_3: 1.072  loss_mask_3: 0.1055  loss_dice_3: 1.615  loss_ce_4: 1.085  loss_mask_4: 0.1103  loss_dice_4: 1.679  loss_ce_5: 1.123  loss_mask_5: 0.09724  loss_dice_5: 1.568  loss_ce_6: 1.125  loss_mask_6: 0.1025  loss_dice_6: 1.594  loss_ce_7: 1  loss_mask_7: 0.08275  loss_dice_7: 1.563  loss_ce_8: 1.011  loss_mask_8: 0.08954  loss_dice_8: 1.668    time: 0.3215  last_time: 0.3318  data_time: 0.0108  last_data_time: 0.0102   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:34:58 d2.utils.events]:  eta: 0:25:23  iter: 54999  total_loss: 27.03  loss_ce: 1.194  loss_mask: 0.1637  loss_dice: 1.383  loss_ce_0: 1.582  loss_mask_0: 0.1689  loss_dice_0: 1.409  loss_ce_1: 1.434  loss_mask_1: 0.1484  loss_dice_1: 1.446  loss_ce_2: 1.387  loss_mask_2: 0.1511  loss_dice_2: 1.387  loss_ce_3: 1.316  loss_mask_3: 0.1821  loss_dice_3: 1.307  loss_ce_4: 1.302  loss_mask_4: 0.158  loss_dice_4: 1.209  loss_ce_5: 1.22  loss_mask_5: 0.1689  loss_dice_5: 1.337  loss_ce_6: 1.201  loss_mask_6: 0.1591  loss_dice_6: 1.219  loss_ce_7: 1.218  loss_mask_7: 0.176  loss_dice_7: 1.407  loss_ce_8: 1.175  loss_mask_8: 0.1531  loss_dice_8: 1.31    time: 0.3215  last_time: 0.2984  data_time: 0.0072  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:04 d2.utils.events]:  eta: 0:25:17  iter: 55019  total_loss: 29.14  loss_ce: 1.191  loss_mask: 0.1719  loss_dice: 1.135  loss_ce_0: 1.168  loss_mask_0: 0.2077  loss_dice_0: 1.395  loss_ce_1: 1.13  loss_mask_1: 0.2409  loss_dice_1: 1.502  loss_ce_2: 1.167  loss_mask_2: 0.2376  loss_dice_2: 1.538  loss_ce_3: 1.014  loss_mask_3: 0.1494  loss_dice_3: 1.289  loss_ce_4: 1.146  loss_mask_4: 0.141  loss_dice_4: 1.131  loss_ce_5: 1.017  loss_mask_5: 0.1445  loss_dice_5: 1.39  loss_ce_6: 1.11  loss_mask_6: 0.153  loss_dice_6: 1.221  loss_ce_7: 1.066  loss_mask_7: 0.1406  loss_dice_7: 1.309  loss_ce_8: 1.114  loss_mask_8: 0.1554  loss_dice_8: 1.347    time: 0.3215  last_time: 0.3321  data_time: 0.0067  last_data_time: 0.0074   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:10 d2.utils.events]:  eta: 0:25:11  iter: 55039  total_loss: 24.54  loss_ce: 0.9602  loss_mask: 0.09143  loss_dice: 1.082  loss_ce_0: 1.143  loss_mask_0: 0.1219  loss_dice_0: 0.9568  loss_ce_1: 1.224  loss_mask_1: 0.2155  loss_dice_1: 1.404  loss_ce_2: 1.113  loss_mask_2: 0.1243  loss_dice_2: 1.298  loss_ce_3: 0.9692  loss_mask_3: 0.1087  loss_dice_3: 0.9236  loss_ce_4: 1.054  loss_mask_4: 0.09689  loss_dice_4: 1.119  loss_ce_5: 1.06  loss_mask_5: 0.09405  loss_dice_5: 1.107  loss_ce_6: 0.9892  loss_mask_6: 0.08194  loss_dice_6: 0.963  loss_ce_7: 1.023  loss_mask_7: 0.1022  loss_dice_7: 0.9611  loss_ce_8: 1.015  loss_mask_8: 0.09702  loss_dice_8: 1.106    time: 0.3215  last_time: 0.2978  data_time: 0.0065  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:16 d2.utils.events]:  eta: 0:25:04  iter: 55059  total_loss: 18.09  loss_ce: 0.8224  loss_mask: 0.1164  loss_dice: 0.6956  loss_ce_0: 1.025  loss_mask_0: 0.1339  loss_dice_0: 0.6898  loss_ce_1: 1.122  loss_mask_1: 0.1409  loss_dice_1: 0.9321  loss_ce_2: 0.948  loss_mask_2: 0.1288  loss_dice_2: 0.8734  loss_ce_3: 0.7968  loss_mask_3: 0.121  loss_dice_3: 0.7605  loss_ce_4: 0.8707  loss_mask_4: 0.1139  loss_dice_4: 0.6659  loss_ce_5: 0.8713  loss_mask_5: 0.142  loss_dice_5: 0.8489  loss_ce_6: 0.7733  loss_mask_6: 0.1091  loss_dice_6: 0.7289  loss_ce_7: 0.7567  loss_mask_7: 0.1133  loss_dice_7: 0.7159  loss_ce_8: 0.7621  loss_mask_8: 0.1114  loss_dice_8: 0.7264    time: 0.3215  last_time: 0.2992  data_time: 0.0063  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:23 d2.utils.events]:  eta: 0:24:58  iter: 55079  total_loss: 29.2  loss_ce: 1.163  loss_mask: 0.1016  loss_dice: 1.396  loss_ce_0: 1.202  loss_mask_0: 0.1104  loss_dice_0: 1.549  loss_ce_1: 1.117  loss_mask_1: 0.1209  loss_dice_1: 1.55  loss_ce_2: 1.007  loss_mask_2: 0.1063  loss_dice_2: 1.415  loss_ce_3: 1.174  loss_mask_3: 0.1068  loss_dice_3: 1.384  loss_ce_4: 1.057  loss_mask_4: 0.09305  loss_dice_4: 1.178  loss_ce_5: 1.034  loss_mask_5: 0.1078  loss_dice_5: 1.46  loss_ce_6: 1.012  loss_mask_6: 0.09347  loss_dice_6: 1.443  loss_ce_7: 1.084  loss_mask_7: 0.09037  loss_dice_7: 1.306  loss_ce_8: 1.044  loss_mask_8: 0.08966  loss_dice_8: 1.561    time: 0.3215  last_time: 0.3170  data_time: 0.0118  last_data_time: 0.0022   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:30 d2.utils.events]:  eta: 0:24:51  iter: 55099  total_loss: 24.01  loss_ce: 1.073  loss_mask: 0.1207  loss_dice: 1.179  loss_ce_0: 1.586  loss_mask_0: 0.164  loss_dice_0: 0.9978  loss_ce_1: 1.511  loss_mask_1: 0.1299  loss_dice_1: 1.137  loss_ce_2: 1.208  loss_mask_2: 0.1326  loss_dice_2: 1.194  loss_ce_3: 1.118  loss_mask_3: 0.1237  loss_dice_3: 1.197  loss_ce_4: 1.093  loss_mask_4: 0.1238  loss_dice_4: 0.838  loss_ce_5: 1.144  loss_mask_5: 0.1168  loss_dice_5: 1.01  loss_ce_6: 1.111  loss_mask_6: 0.1178  loss_dice_6: 1  loss_ce_7: 1.094  loss_mask_7: 0.1218  loss_dice_7: 1.213  loss_ce_8: 1.066  loss_mask_8: 0.1227  loss_dice_8: 1.196    time: 0.3215  last_time: 0.2971  data_time: 0.0133  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:36 d2.utils.events]:  eta: 0:24:45  iter: 55119  total_loss: 37.5  loss_ce: 1.241  loss_mask: 0.142  loss_dice: 2.082  loss_ce_0: 1.602  loss_mask_0: 0.1547  loss_dice_0: 1.397  loss_ce_1: 1.478  loss_mask_1: 0.1664  loss_dice_1: 1.919  loss_ce_2: 1.295  loss_mask_2: 0.1858  loss_dice_2: 2.043  loss_ce_3: 1.244  loss_mask_3: 0.1597  loss_dice_3: 2.048  loss_ce_4: 1.191  loss_mask_4: 0.1466  loss_dice_4: 1.886  loss_ce_5: 1.248  loss_mask_5: 0.1226  loss_dice_5: 1.873  loss_ce_6: 1.203  loss_mask_6: 0.161  loss_dice_6: 2.146  loss_ce_7: 1.209  loss_mask_7: 0.1436  loss_dice_7: 1.961  loss_ce_8: 1.252  loss_mask_8: 0.151  loss_dice_8: 1.923    time: 0.3215  last_time: 0.3002  data_time: 0.0131  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:42 d2.utils.events]:  eta: 0:24:39  iter: 55139  total_loss: 24.82  loss_ce: 0.9838  loss_mask: 0.1796  loss_dice: 0.979  loss_ce_0: 1.067  loss_mask_0: 0.1932  loss_dice_0: 1.051  loss_ce_1: 1.237  loss_mask_1: 0.1398  loss_dice_1: 1.176  loss_ce_2: 1.07  loss_mask_2: 0.167  loss_dice_2: 1.139  loss_ce_3: 1.01  loss_mask_3: 0.1919  loss_dice_3: 0.9739  loss_ce_4: 1.023  loss_mask_4: 0.1539  loss_dice_4: 1.172  loss_ce_5: 0.9562  loss_mask_5: 0.2103  loss_dice_5: 1.058  loss_ce_6: 1.071  loss_mask_6: 0.179  loss_dice_6: 0.9911  loss_ce_7: 1.055  loss_mask_7: 0.1791  loss_dice_7: 0.9463  loss_ce_8: 1.01  loss_mask_8: 0.1843  loss_dice_8: 0.9172    time: 0.3215  last_time: 0.2991  data_time: 0.0068  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:49 d2.utils.events]:  eta: 0:24:33  iter: 55159  total_loss: 35  loss_ce: 1.294  loss_mask: 0.1494  loss_dice: 1.941  loss_ce_0: 1.542  loss_mask_0: 0.2098  loss_dice_0: 1.941  loss_ce_1: 1.667  loss_mask_1: 0.146  loss_dice_1: 1.845  loss_ce_2: 1.539  loss_mask_2: 0.1495  loss_dice_2: 2.119  loss_ce_3: 1.334  loss_mask_3: 0.1551  loss_dice_3: 1.886  loss_ce_4: 1.339  loss_mask_4: 0.1396  loss_dice_4: 1.825  loss_ce_5: 1.283  loss_mask_5: 0.148  loss_dice_5: 1.879  loss_ce_6: 1.337  loss_mask_6: 0.1536  loss_dice_6: 1.712  loss_ce_7: 1.324  loss_mask_7: 0.1434  loss_dice_7: 1.803  loss_ce_8: 1.254  loss_mask_8: 0.1489  loss_dice_8: 1.822    time: 0.3215  last_time: 0.3139  data_time: 0.0300  last_data_time: 0.0044   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:35:55 d2.utils.events]:  eta: 0:24:27  iter: 55179  total_loss: 26.43  loss_ce: 0.9078  loss_mask: 0.07898  loss_dice: 1.76  loss_ce_0: 1.124  loss_mask_0: 0.07881  loss_dice_0: 1.46  loss_ce_1: 1.156  loss_mask_1: 0.06831  loss_dice_1: 1.676  loss_ce_2: 0.9424  loss_mask_2: 0.1115  loss_dice_2: 1.791  loss_ce_3: 0.8849  loss_mask_3: 0.105  loss_dice_3: 1.526  loss_ce_4: 0.8939  loss_mask_4: 0.105  loss_dice_4: 1.642  loss_ce_5: 0.8899  loss_mask_5: 0.1008  loss_dice_5: 1.639  loss_ce_6: 0.8966  loss_mask_6: 0.08901  loss_dice_6: 1.512  loss_ce_7: 0.9031  loss_mask_7: 0.07703  loss_dice_7: 1.233  loss_ce_8: 0.9134  loss_mask_8: 0.07153  loss_dice_8: 1.487    time: 0.3215  last_time: 0.3265  data_time: 0.0066  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:01 d2.utils.events]:  eta: 0:24:20  iter: 55199  total_loss: 23.85  loss_ce: 1.19  loss_mask: 0.1053  loss_dice: 0.9349  loss_ce_0: 1.207  loss_mask_0: 0.1401  loss_dice_0: 1.209  loss_ce_1: 1.306  loss_mask_1: 0.1477  loss_dice_1: 1.185  loss_ce_2: 1.206  loss_mask_2: 0.152  loss_dice_2: 1.112  loss_ce_3: 1.3  loss_mask_3: 0.1195  loss_dice_3: 0.9491  loss_ce_4: 1.307  loss_mask_4: 0.1216  loss_dice_4: 1.066  loss_ce_5: 1.339  loss_mask_5: 0.1105  loss_dice_5: 1.002  loss_ce_6: 1.195  loss_mask_6: 0.09887  loss_dice_6: 1.005  loss_ce_7: 1.23  loss_mask_7: 0.1044  loss_dice_7: 0.9711  loss_ce_8: 1.121  loss_mask_8: 0.1148  loss_dice_8: 1.071    time: 0.3215  last_time: 0.2996  data_time: 0.0092  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:08 d2.utils.events]:  eta: 0:24:15  iter: 55219  total_loss: 28.57  loss_ce: 0.886  loss_mask: 0.11  loss_dice: 1.486  loss_ce_0: 1.183  loss_mask_0: 0.1357  loss_dice_0: 1.283  loss_ce_1: 1.159  loss_mask_1: 0.1095  loss_dice_1: 1.371  loss_ce_2: 1.005  loss_mask_2: 0.121  loss_dice_2: 1.321  loss_ce_3: 0.8932  loss_mask_3: 0.1244  loss_dice_3: 1.268  loss_ce_4: 0.9023  loss_mask_4: 0.122  loss_dice_4: 1.394  loss_ce_5: 0.933  loss_mask_5: 0.09197  loss_dice_5: 1.366  loss_ce_6: 0.8764  loss_mask_6: 0.118  loss_dice_6: 1.484  loss_ce_7: 0.9314  loss_mask_7: 0.107  loss_dice_7: 1.362  loss_ce_8: 0.9216  loss_mask_8: 0.1065  loss_dice_8: 1.17    time: 0.3215  last_time: 0.3019  data_time: 0.0073  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:14 d2.utils.events]:  eta: 0:24:09  iter: 55239  total_loss: 26.17  loss_ce: 1.145  loss_mask: 0.1909  loss_dice: 1.113  loss_ce_0: 1.274  loss_mask_0: 0.223  loss_dice_0: 1.562  loss_ce_1: 1.389  loss_mask_1: 0.2622  loss_dice_1: 1.563  loss_ce_2: 1.235  loss_mask_2: 0.2435  loss_dice_2: 1.451  loss_ce_3: 1.234  loss_mask_3: 0.2062  loss_dice_3: 1.167  loss_ce_4: 1.064  loss_mask_4: 0.216  loss_dice_4: 1.514  loss_ce_5: 1.143  loss_mask_5: 0.2483  loss_dice_5: 1.303  loss_ce_6: 1.052  loss_mask_6: 0.2049  loss_dice_6: 1.667  loss_ce_7: 1.146  loss_mask_7: 0.2028  loss_dice_7: 1.295  loss_ce_8: 1.094  loss_mask_8: 0.2157  loss_dice_8: 1.183    time: 0.3215  last_time: 0.3053  data_time: 0.0124  last_data_time: 0.0095   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:20 d2.utils.events]:  eta: 0:24:03  iter: 55259  total_loss: 29.18  loss_ce: 1.248  loss_mask: 0.269  loss_dice: 1.736  loss_ce_0: 1.481  loss_mask_0: 0.3481  loss_dice_0: 1.868  loss_ce_1: 1.491  loss_mask_1: 0.2577  loss_dice_1: 1.681  loss_ce_2: 1.395  loss_mask_2: 0.2667  loss_dice_2: 1.748  loss_ce_3: 1.293  loss_mask_3: 0.2347  loss_dice_3: 1.332  loss_ce_4: 1.203  loss_mask_4: 0.2894  loss_dice_4: 1.603  loss_ce_5: 1.27  loss_mask_5: 0.2767  loss_dice_5: 1.515  loss_ce_6: 1.279  loss_mask_6: 0.2946  loss_dice_6: 1.63  loss_ce_7: 1.186  loss_mask_7: 0.2775  loss_dice_7: 1.549  loss_ce_8: 1.201  loss_mask_8: 0.2694  loss_dice_8: 1.46    time: 0.3215  last_time: 0.3121  data_time: 0.0078  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:26 d2.utils.events]:  eta: 0:23:57  iter: 55279  total_loss: 24.27  loss_ce: 1.151  loss_mask: 0.1066  loss_dice: 1.046  loss_ce_0: 1.283  loss_mask_0: 0.08738  loss_dice_0: 1.247  loss_ce_1: 1.19  loss_mask_1: 0.1042  loss_dice_1: 1.003  loss_ce_2: 1.229  loss_mask_2: 0.09463  loss_dice_2: 1.153  loss_ce_3: 1.079  loss_mask_3: 0.111  loss_dice_3: 1.074  loss_ce_4: 1.03  loss_mask_4: 0.1095  loss_dice_4: 1.074  loss_ce_5: 1.136  loss_mask_5: 0.1027  loss_dice_5: 1.008  loss_ce_6: 1.098  loss_mask_6: 0.104  loss_dice_6: 0.8753  loss_ce_7: 1.142  loss_mask_7: 0.09103  loss_dice_7: 1.025  loss_ce_8: 1.069  loss_mask_8: 0.1103  loss_dice_8: 1.164    time: 0.3214  last_time: 0.3236  data_time: 0.0060  last_data_time: 0.0041   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:33 d2.utils.events]:  eta: 0:23:51  iter: 55299  total_loss: 21.14  loss_ce: 0.9857  loss_mask: 0.06996  loss_dice: 0.9545  loss_ce_0: 1.183  loss_mask_0: 0.05492  loss_dice_0: 0.8764  loss_ce_1: 1.154  loss_mask_1: 0.05996  loss_dice_1: 1.046  loss_ce_2: 1.259  loss_mask_2: 0.0801  loss_dice_2: 0.8041  loss_ce_3: 0.9452  loss_mask_3: 0.07314  loss_dice_3: 1.146  loss_ce_4: 0.8672  loss_mask_4: 0.06428  loss_dice_4: 0.8796  loss_ce_5: 0.9627  loss_mask_5: 0.06971  loss_dice_5: 1.003  loss_ce_6: 0.9021  loss_mask_6: 0.08114  loss_dice_6: 1.044  loss_ce_7: 1.02  loss_mask_7: 0.07086  loss_dice_7: 1.132  loss_ce_8: 0.9666  loss_mask_8: 0.07103  loss_dice_8: 0.8492    time: 0.3214  last_time: 0.3209  data_time: 0.0107  last_data_time: 0.0102   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:39 d2.utils.events]:  eta: 0:23:44  iter: 55319  total_loss: 27.18  loss_ce: 0.8963  loss_mask: 0.09035  loss_dice: 1.082  loss_ce_0: 1.136  loss_mask_0: 0.1304  loss_dice_0: 1.189  loss_ce_1: 1.227  loss_mask_1: 0.1638  loss_dice_1: 1.22  loss_ce_2: 1.066  loss_mask_2: 0.1548  loss_dice_2: 1.135  loss_ce_3: 0.8998  loss_mask_3: 0.1151  loss_dice_3: 1.18  loss_ce_4: 0.9031  loss_mask_4: 0.09158  loss_dice_4: 0.8736  loss_ce_5: 0.8901  loss_mask_5: 0.1109  loss_dice_5: 1.069  loss_ce_6: 0.8967  loss_mask_6: 0.08645  loss_dice_6: 1.011  loss_ce_7: 0.891  loss_mask_7: 0.08965  loss_dice_7: 1.028  loss_ce_8: 0.8973  loss_mask_8: 0.0994  loss_dice_8: 0.9721    time: 0.3214  last_time: 0.3248  data_time: 0.0063  last_data_time: 0.0061   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:45 d2.utils.events]:  eta: 0:23:39  iter: 55339  total_loss: 20.02  loss_ce: 0.7687  loss_mask: 0.2126  loss_dice: 0.8474  loss_ce_0: 1.025  loss_mask_0: 0.3001  loss_dice_0: 0.8464  loss_ce_1: 0.9799  loss_mask_1: 0.231  loss_dice_1: 0.7629  loss_ce_2: 0.9589  loss_mask_2: 0.2193  loss_dice_2: 0.8923  loss_ce_3: 0.8285  loss_mask_3: 0.1606  loss_dice_3: 0.7746  loss_ce_4: 0.8069  loss_mask_4: 0.1865  loss_dice_4: 0.7038  loss_ce_5: 0.7624  loss_mask_5: 0.1677  loss_dice_5: 0.7815  loss_ce_6: 0.8079  loss_mask_6: 0.1948  loss_dice_6: 0.7603  loss_ce_7: 0.8076  loss_mask_7: 0.2071  loss_dice_7: 0.8266  loss_ce_8: 0.7692  loss_mask_8: 0.1952  loss_dice_8: 0.7176    time: 0.3214  last_time: 0.3149  data_time: 0.0182  last_data_time: 0.0099   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:52 d2.utils.events]:  eta: 0:23:32  iter: 55359  total_loss: 26.65  loss_ce: 1.073  loss_mask: 0.1367  loss_dice: 1.537  loss_ce_0: 1.551  loss_mask_0: 0.1381  loss_dice_0: 1.476  loss_ce_1: 1.299  loss_mask_1: 0.1597  loss_dice_1: 1.65  loss_ce_2: 1.328  loss_mask_2: 0.1599  loss_dice_2: 1.621  loss_ce_3: 1.112  loss_mask_3: 0.1583  loss_dice_3: 1.426  loss_ce_4: 1.114  loss_mask_4: 0.1381  loss_dice_4: 1.35  loss_ce_5: 1.169  loss_mask_5: 0.1288  loss_dice_5: 1.22  loss_ce_6: 1.092  loss_mask_6: 0.1381  loss_dice_6: 1.381  loss_ce_7: 1.118  loss_mask_7: 0.1424  loss_dice_7: 1.342  loss_ce_8: 1.109  loss_mask_8: 0.1371  loss_dice_8: 1.431    time: 0.3214  last_time: 0.3037  data_time: 0.0072  last_data_time: 0.0050   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:36:58 d2.utils.events]:  eta: 0:23:25  iter: 55379  total_loss: 33.03  loss_ce: 0.8776  loss_mask: 0.2072  loss_dice: 1.49  loss_ce_0: 1.304  loss_mask_0: 0.1777  loss_dice_0: 1.403  loss_ce_1: 1.122  loss_mask_1: 0.1755  loss_dice_1: 1.588  loss_ce_2: 1.239  loss_mask_2: 0.202  loss_dice_2: 1.422  loss_ce_3: 1.138  loss_mask_3: 0.1881  loss_dice_3: 1.477  loss_ce_4: 1.098  loss_mask_4: 0.2318  loss_dice_4: 1.53  loss_ce_5: 1.124  loss_mask_5: 0.1647  loss_dice_5: 1.474  loss_ce_6: 1.077  loss_mask_6: 0.2081  loss_dice_6: 1.217  loss_ce_7: 1.013  loss_mask_7: 0.1592  loss_dice_7: 1.298  loss_ce_8: 1.037  loss_mask_8: 0.1604  loss_dice_8: 1.147    time: 0.3214  last_time: 0.2987  data_time: 0.0071  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:04 d2.utils.events]:  eta: 0:23:20  iter: 55399  total_loss: 28.76  loss_ce: 1.194  loss_mask: 0.1458  loss_dice: 1.281  loss_ce_0: 1.358  loss_mask_0: 0.1587  loss_dice_0: 1.316  loss_ce_1: 1.4  loss_mask_1: 0.1494  loss_dice_1: 1.142  loss_ce_2: 1.318  loss_mask_2: 0.1348  loss_dice_2: 1.312  loss_ce_3: 1.214  loss_mask_3: 0.1313  loss_dice_3: 1.075  loss_ce_4: 1.133  loss_mask_4: 0.1413  loss_dice_4: 1.317  loss_ce_5: 1.087  loss_mask_5: 0.126  loss_dice_5: 1.436  loss_ce_6: 1.067  loss_mask_6: 0.1544  loss_dice_6: 1.528  loss_ce_7: 1.141  loss_mask_7: 0.1635  loss_dice_7: 1.485  loss_ce_8: 1.198  loss_mask_8: 0.1566  loss_dice_8: 1.504    time: 0.3214  last_time: 0.3102  data_time: 0.0130  last_data_time: 0.0093   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:10 d2.utils.events]:  eta: 0:23:13  iter: 55419  total_loss: 28.88  loss_ce: 1.275  loss_mask: 0.1426  loss_dice: 1.355  loss_ce_0: 1.517  loss_mask_0: 0.1756  loss_dice_0: 1.25  loss_ce_1: 1.386  loss_mask_1: 0.2247  loss_dice_1: 1.203  loss_ce_2: 1.362  loss_mask_2: 0.1682  loss_dice_2: 1.298  loss_ce_3: 1.275  loss_mask_3: 0.1662  loss_dice_3: 1.33  loss_ce_4: 1.317  loss_mask_4: 0.1288  loss_dice_4: 1.226  loss_ce_5: 1.278  loss_mask_5: 0.1686  loss_dice_5: 1.108  loss_ce_6: 1.259  loss_mask_6: 0.1369  loss_dice_6: 1.318  loss_ce_7: 1.306  loss_mask_7: 0.1369  loss_dice_7: 1.121  loss_ce_8: 1.179  loss_mask_8: 0.1768  loss_dice_8: 1.353    time: 0.3214  last_time: 0.3241  data_time: 0.0075  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:17 d2.utils.events]:  eta: 0:23:08  iter: 55439  total_loss: 30.4  loss_ce: 0.9585  loss_mask: 0.1685  loss_dice: 1.799  loss_ce_0: 1.398  loss_mask_0: 0.1396  loss_dice_0: 1.919  loss_ce_1: 1.366  loss_mask_1: 0.1258  loss_dice_1: 1.967  loss_ce_2: 1.138  loss_mask_2: 0.13  loss_dice_2: 1.931  loss_ce_3: 1.069  loss_mask_3: 0.1553  loss_dice_3: 1.908  loss_ce_4: 1.014  loss_mask_4: 0.1544  loss_dice_4: 2.055  loss_ce_5: 1.037  loss_mask_5: 0.1622  loss_dice_5: 2.152  loss_ce_6: 1.034  loss_mask_6: 0.155  loss_dice_6: 2.004  loss_ce_7: 1.014  loss_mask_7: 0.166  loss_dice_7: 1.995  loss_ce_8: 0.9868  loss_mask_8: 0.1594  loss_dice_8: 1.955    time: 0.3214  last_time: 0.3236  data_time: 0.0060  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:23 d2.utils.events]:  eta: 0:23:02  iter: 55459  total_loss: 27.3  loss_ce: 1.082  loss_mask: 0.2068  loss_dice: 1.037  loss_ce_0: 1.258  loss_mask_0: 0.2302  loss_dice_0: 1.218  loss_ce_1: 1.269  loss_mask_1: 0.1614  loss_dice_1: 1.068  loss_ce_2: 1.176  loss_mask_2: 0.1926  loss_dice_2: 0.9165  loss_ce_3: 1.02  loss_mask_3: 0.2255  loss_dice_3: 1.083  loss_ce_4: 1.013  loss_mask_4: 0.2189  loss_dice_4: 1.002  loss_ce_5: 1.121  loss_mask_5: 0.2503  loss_dice_5: 0.9341  loss_ce_6: 1.048  loss_mask_6: 0.2308  loss_dice_6: 1.059  loss_ce_7: 1.098  loss_mask_7: 0.2201  loss_dice_7: 0.7887  loss_ce_8: 1.08  loss_mask_8: 0.2212  loss_dice_8: 0.9837    time: 0.3214  last_time: 0.2984  data_time: 0.0067  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:29 d2.utils.events]:  eta: 0:22:56  iter: 55479  total_loss: 29.7  loss_ce: 1.038  loss_mask: 0.19  loss_dice: 1.705  loss_ce_0: 1.22  loss_mask_0: 0.216  loss_dice_0: 1.556  loss_ce_1: 1.231  loss_mask_1: 0.2091  loss_dice_1: 1.772  loss_ce_2: 1.133  loss_mask_2: 0.2104  loss_dice_2: 1.611  loss_ce_3: 1.039  loss_mask_3: 0.2256  loss_dice_3: 1.472  loss_ce_4: 1.052  loss_mask_4: 0.2049  loss_dice_4: 1.539  loss_ce_5: 1.013  loss_mask_5: 0.2238  loss_dice_5: 1.619  loss_ce_6: 0.9781  loss_mask_6: 0.1876  loss_dice_6: 1.461  loss_ce_7: 0.9955  loss_mask_7: 0.1856  loss_dice_7: 1.565  loss_ce_8: 1.048  loss_mask_8: 0.1771  loss_dice_8: 1.581    time: 0.3214  last_time: 0.3242  data_time: 0.0064  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:35 d2.utils.events]:  eta: 0:22:51  iter: 55499  total_loss: 28.45  loss_ce: 1.285  loss_mask: 0.1119  loss_dice: 1.495  loss_ce_0: 1.302  loss_mask_0: 0.0973  loss_dice_0: 1.521  loss_ce_1: 1.368  loss_mask_1: 0.146  loss_dice_1: 1.725  loss_ce_2: 1.175  loss_mask_2: 0.1201  loss_dice_2: 1.625  loss_ce_3: 1.174  loss_mask_3: 0.09959  loss_dice_3: 1.353  loss_ce_4: 1.051  loss_mask_4: 0.1082  loss_dice_4: 1.509  loss_ce_5: 1.049  loss_mask_5: 0.1074  loss_dice_5: 1.434  loss_ce_6: 1.061  loss_mask_6: 0.1093  loss_dice_6: 1.471  loss_ce_7: 1.054  loss_mask_7: 0.1143  loss_dice_7: 1.592  loss_ce_8: 1.05  loss_mask_8: 0.1115  loss_dice_8: 1.345    time: 0.3214  last_time: 0.3042  data_time: 0.0074  last_data_time: 0.0077   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:41 d2.utils.events]:  eta: 0:22:45  iter: 55519  total_loss: 26.2  loss_ce: 0.9687  loss_mask: 0.1299  loss_dice: 1.258  loss_ce_0: 1.196  loss_mask_0: 0.1599  loss_dice_0: 1.364  loss_ce_1: 1.234  loss_mask_1: 0.149  loss_dice_1: 1.296  loss_ce_2: 1.049  loss_mask_2: 0.1559  loss_dice_2: 1.13  loss_ce_3: 0.9541  loss_mask_3: 0.1441  loss_dice_3: 1.35  loss_ce_4: 1.021  loss_mask_4: 0.147  loss_dice_4: 1.206  loss_ce_5: 0.9739  loss_mask_5: 0.1403  loss_dice_5: 1.233  loss_ce_6: 0.9688  loss_mask_6: 0.1277  loss_dice_6: 1.178  loss_ce_7: 0.9817  loss_mask_7: 0.1262  loss_dice_7: 1.359  loss_ce_8: 0.9744  loss_mask_8: 0.128  loss_dice_8: 1.277    time: 0.3214  last_time: 0.3005  data_time: 0.0066  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:48 d2.utils.events]:  eta: 0:22:39  iter: 55539  total_loss: 23.78  loss_ce: 1.088  loss_mask: 0.1574  loss_dice: 1.123  loss_ce_0: 1.388  loss_mask_0: 0.2184  loss_dice_0: 1.197  loss_ce_1: 1.252  loss_mask_1: 0.1847  loss_dice_1: 1.089  loss_ce_2: 1.143  loss_mask_2: 0.1733  loss_dice_2: 1.094  loss_ce_3: 1.084  loss_mask_3: 0.1722  loss_dice_3: 0.9483  loss_ce_4: 1.07  loss_mask_4: 0.1611  loss_dice_4: 1.039  loss_ce_5: 1.072  loss_mask_5: 0.1565  loss_dice_5: 0.8636  loss_ce_6: 1.078  loss_mask_6: 0.1645  loss_dice_6: 1.019  loss_ce_7: 1.034  loss_mask_7: 0.1479  loss_dice_7: 1.117  loss_ce_8: 1.087  loss_mask_8: 0.152  loss_dice_8: 1.07    time: 0.3214  last_time: 0.3011  data_time: 0.0066  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:37:54 d2.utils.events]:  eta: 0:22:33  iter: 55559  total_loss: 22.67  loss_ce: 0.8644  loss_mask: 0.1151  loss_dice: 1.047  loss_ce_0: 1.151  loss_mask_0: 0.1536  loss_dice_0: 1.028  loss_ce_1: 1.103  loss_mask_1: 0.1044  loss_dice_1: 1.153  loss_ce_2: 0.9532  loss_mask_2: 0.1362  loss_dice_2: 1.293  loss_ce_3: 0.8406  loss_mask_3: 0.1194  loss_dice_3: 1.026  loss_ce_4: 0.8236  loss_mask_4: 0.1098  loss_dice_4: 1.106  loss_ce_5: 0.8051  loss_mask_5: 0.1363  loss_dice_5: 0.9853  loss_ce_6: 0.8733  loss_mask_6: 0.1173  loss_dice_6: 1.001  loss_ce_7: 0.8502  loss_mask_7: 0.1175  loss_dice_7: 1.006  loss_ce_8: 0.8634  loss_mask_8: 0.1114  loss_dice_8: 0.9646    time: 0.3214  last_time: 0.3003  data_time: 0.0090  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:00 d2.utils.events]:  eta: 0:22:27  iter: 55579  total_loss: 26.29  loss_ce: 0.8635  loss_mask: 0.1557  loss_dice: 0.9921  loss_ce_0: 1.14  loss_mask_0: 0.1942  loss_dice_0: 0.9909  loss_ce_1: 1.097  loss_mask_1: 0.1996  loss_dice_1: 1.13  loss_ce_2: 0.9895  loss_mask_2: 0.2021  loss_dice_2: 1.05  loss_ce_3: 0.9546  loss_mask_3: 0.1641  loss_dice_3: 1.099  loss_ce_4: 0.8243  loss_mask_4: 0.1695  loss_dice_4: 1.058  loss_ce_5: 0.9575  loss_mask_5: 0.1598  loss_dice_5: 1.142  loss_ce_6: 0.8767  loss_mask_6: 0.1663  loss_dice_6: 1.184  loss_ce_7: 0.8799  loss_mask_7: 0.1794  loss_dice_7: 1.21  loss_ce_8: 0.8758  loss_mask_8: 0.1684  loss_dice_8: 1.039    time: 0.3214  last_time: 0.2900  data_time: 0.0060  last_data_time: 0.0023   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:07 d2.utils.events]:  eta: 0:22:20  iter: 55599  total_loss: 22.97  loss_ce: 0.9205  loss_mask: 0.1307  loss_dice: 1.146  loss_ce_0: 1.257  loss_mask_0: 0.1671  loss_dice_0: 1.211  loss_ce_1: 1.202  loss_mask_1: 0.1511  loss_dice_1: 1.372  loss_ce_2: 1.109  loss_mask_2: 0.1731  loss_dice_2: 1.234  loss_ce_3: 1.041  loss_mask_3: 0.1369  loss_dice_3: 1.109  loss_ce_4: 1.065  loss_mask_4: 0.1376  loss_dice_4: 1.062  loss_ce_5: 0.9395  loss_mask_5: 0.1375  loss_dice_5: 0.9176  loss_ce_6: 0.8846  loss_mask_6: 0.1386  loss_dice_6: 0.9517  loss_ce_7: 0.9869  loss_mask_7: 0.1417  loss_dice_7: 1.11  loss_ce_8: 1.023  loss_mask_8: 0.1478  loss_dice_8: 1.107    time: 0.3214  last_time: 0.2988  data_time: 0.0172  last_data_time: 0.0073   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:13 d2.utils.events]:  eta: 0:22:14  iter: 55619  total_loss: 32.04  loss_ce: 1.204  loss_mask: 0.1394  loss_dice: 1.429  loss_ce_0: 1.346  loss_mask_0: 0.2306  loss_dice_0: 1.399  loss_ce_1: 1.24  loss_mask_1: 0.1485  loss_dice_1: 1.566  loss_ce_2: 1.275  loss_mask_2: 0.1689  loss_dice_2: 1.35  loss_ce_3: 1.233  loss_mask_3: 0.2181  loss_dice_3: 1.453  loss_ce_4: 1.185  loss_mask_4: 0.1848  loss_dice_4: 1.459  loss_ce_5: 1.218  loss_mask_5: 0.1915  loss_dice_5: 1.394  loss_ce_6: 1.211  loss_mask_6: 0.1307  loss_dice_6: 1.409  loss_ce_7: 1.173  loss_mask_7: 0.1437  loss_dice_7: 1.276  loss_ce_8: 1.139  loss_mask_8: 0.1436  loss_dice_8: 1.407    time: 0.3214  last_time: 0.3328  data_time: 0.0071  last_data_time: 0.0145   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:19 d2.utils.events]:  eta: 0:22:08  iter: 55639  total_loss: 28.17  loss_ce: 1.038  loss_mask: 0.1045  loss_dice: 1.267  loss_ce_0: 1.414  loss_mask_0: 0.1092  loss_dice_0: 1.218  loss_ce_1: 1.305  loss_mask_1: 0.1115  loss_dice_1: 1.371  loss_ce_2: 1.119  loss_mask_2: 0.104  loss_dice_2: 1.248  loss_ce_3: 1.018  loss_mask_3: 0.1197  loss_dice_3: 1.186  loss_ce_4: 1.025  loss_mask_4: 0.1134  loss_dice_4: 1.494  loss_ce_5: 0.9686  loss_mask_5: 0.1184  loss_dice_5: 1.127  loss_ce_6: 1.008  loss_mask_6: 0.1119  loss_dice_6: 1.166  loss_ce_7: 0.9934  loss_mask_7: 0.1055  loss_dice_7: 1.266  loss_ce_8: 1.021  loss_mask_8: 0.1051  loss_dice_8: 1.26    time: 0.3214  last_time: 0.3312  data_time: 0.0076  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:25 d2.utils.events]:  eta: 0:22:02  iter: 55659  total_loss: 29.4  loss_ce: 1  loss_mask: 0.178  loss_dice: 1.62  loss_ce_0: 1.317  loss_mask_0: 0.2259  loss_dice_0: 1.666  loss_ce_1: 1.281  loss_mask_1: 0.2432  loss_dice_1: 1.494  loss_ce_2: 1.189  loss_mask_2: 0.1827  loss_dice_2: 1.621  loss_ce_3: 1.001  loss_mask_3: 0.1815  loss_dice_3: 1.483  loss_ce_4: 1.022  loss_mask_4: 0.1658  loss_dice_4: 1.486  loss_ce_5: 1.102  loss_mask_5: 0.1816  loss_dice_5: 1.525  loss_ce_6: 0.9782  loss_mask_6: 0.1921  loss_dice_6: 1.513  loss_ce_7: 0.955  loss_mask_7: 0.1764  loss_dice_7: 1.509  loss_ce_8: 0.9637  loss_mask_8: 0.1769  loss_dice_8: 1.447    time: 0.3214  last_time: 0.3203  data_time: 0.0145  last_data_time: 0.0041   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:32 d2.utils.events]:  eta: 0:21:56  iter: 55679  total_loss: 27.15  loss_ce: 0.8398  loss_mask: 0.1202  loss_dice: 1.313  loss_ce_0: 1.028  loss_mask_0: 0.1845  loss_dice_0: 1.27  loss_ce_1: 0.9808  loss_mask_1: 0.1627  loss_dice_1: 1.496  loss_ce_2: 0.9054  loss_mask_2: 0.1428  loss_dice_2: 1.323  loss_ce_3: 0.8162  loss_mask_3: 0.1484  loss_dice_3: 1.421  loss_ce_4: 0.902  loss_mask_4: 0.1527  loss_dice_4: 1.301  loss_ce_5: 0.842  loss_mask_5: 0.144  loss_dice_5: 1.34  loss_ce_6: 0.8413  loss_mask_6: 0.1346  loss_dice_6: 1.27  loss_ce_7: 0.8401  loss_mask_7: 0.1223  loss_dice_7: 1.45  loss_ce_8: 0.8321  loss_mask_8: 0.1528  loss_dice_8: 1.451    time: 0.3214  last_time: 0.3193  data_time: 0.0150  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:38 d2.utils.events]:  eta: 0:21:49  iter: 55699  total_loss: 28.32  loss_ce: 1.097  loss_mask: 0.0838  loss_dice: 1.211  loss_ce_0: 1.501  loss_mask_0: 0.1276  loss_dice_0: 1.263  loss_ce_1: 1.294  loss_mask_1: 0.1092  loss_dice_1: 1.271  loss_ce_2: 1.38  loss_mask_2: 0.1003  loss_dice_2: 1.482  loss_ce_3: 1.08  loss_mask_3: 0.1002  loss_dice_3: 1.347  loss_ce_4: 1.157  loss_mask_4: 0.08976  loss_dice_4: 1.298  loss_ce_5: 1.14  loss_mask_5: 0.08807  loss_dice_5: 1.379  loss_ce_6: 1.108  loss_mask_6: 0.08738  loss_dice_6: 1.312  loss_ce_7: 1.208  loss_mask_7: 0.08857  loss_dice_7: 1.262  loss_ce_8: 1.041  loss_mask_8: 0.09228  loss_dice_8: 1.324    time: 0.3214  last_time: 0.2999  data_time: 0.0064  last_data_time: 0.0060   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:44 d2.utils.events]:  eta: 0:21:43  iter: 55719  total_loss: 24.19  loss_ce: 0.9802  loss_mask: 0.1351  loss_dice: 0.936  loss_ce_0: 1.306  loss_mask_0: 0.1057  loss_dice_0: 1.487  loss_ce_1: 1.111  loss_mask_1: 0.1174  loss_dice_1: 1.108  loss_ce_2: 0.9642  loss_mask_2: 0.1538  loss_dice_2: 1.342  loss_ce_3: 0.9236  loss_mask_3: 0.149  loss_dice_3: 1.135  loss_ce_4: 0.9306  loss_mask_4: 0.1246  loss_dice_4: 1.167  loss_ce_5: 1.065  loss_mask_5: 0.1389  loss_dice_5: 1.05  loss_ce_6: 1.091  loss_mask_6: 0.1266  loss_dice_6: 1.063  loss_ce_7: 0.844  loss_mask_7: 0.1446  loss_dice_7: 1.148  loss_ce_8: 1.03  loss_mask_8: 0.1301  loss_dice_8: 0.9917    time: 0.3214  last_time: 0.3457  data_time: 0.0140  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:50 d2.utils.events]:  eta: 0:21:38  iter: 55739  total_loss: 29.64  loss_ce: 1.215  loss_mask: 0.09289  loss_dice: 1.22  loss_ce_0: 1.298  loss_mask_0: 0.1049  loss_dice_0: 1.472  loss_ce_1: 1.472  loss_mask_1: 0.1108  loss_dice_1: 1.403  loss_ce_2: 1.31  loss_mask_2: 0.1245  loss_dice_2: 1.367  loss_ce_3: 1.267  loss_mask_3: 0.1046  loss_dice_3: 1.358  loss_ce_4: 1.282  loss_mask_4: 0.09688  loss_dice_4: 1.432  loss_ce_5: 1.221  loss_mask_5: 0.07682  loss_dice_5: 1.413  loss_ce_6: 1.114  loss_mask_6: 0.08563  loss_dice_6: 1.36  loss_ce_7: 1.208  loss_mask_7: 0.09114  loss_dice_7: 1.326  loss_ce_8: 1.077  loss_mask_8: 0.109  loss_dice_8: 1.153    time: 0.3214  last_time: 0.3186  data_time: 0.0089  last_data_time: 0.0128   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:38:57 d2.utils.events]:  eta: 0:21:32  iter: 55759  total_loss: 27.01  loss_ce: 1.106  loss_mask: 0.1513  loss_dice: 0.9856  loss_ce_0: 1.261  loss_mask_0: 0.1731  loss_dice_0: 1.004  loss_ce_1: 1.167  loss_mask_1: 0.1597  loss_dice_1: 1.184  loss_ce_2: 1.113  loss_mask_2: 0.1496  loss_dice_2: 1.096  loss_ce_3: 1.109  loss_mask_3: 0.1775  loss_dice_3: 0.9712  loss_ce_4: 1.045  loss_mask_4: 0.1548  loss_dice_4: 1.207  loss_ce_5: 1.065  loss_mask_5: 0.1516  loss_dice_5: 1.113  loss_ce_6: 1.092  loss_mask_6: 0.1693  loss_dice_6: 1.227  loss_ce_7: 1.101  loss_mask_7: 0.1515  loss_dice_7: 1.152  loss_ce_8: 1.107  loss_mask_8: 0.1485  loss_dice_8: 0.9303    time: 0.3214  last_time: 0.3238  data_time: 0.0103  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:03 d2.utils.events]:  eta: 0:21:26  iter: 55779  total_loss: 20.73  loss_ce: 0.7504  loss_mask: 0.1423  loss_dice: 0.8003  loss_ce_0: 1.04  loss_mask_0: 0.09794  loss_dice_0: 0.8276  loss_ce_1: 0.848  loss_mask_1: 0.1582  loss_dice_1: 1.194  loss_ce_2: 0.8192  loss_mask_2: 0.2044  loss_dice_2: 0.9999  loss_ce_3: 0.7633  loss_mask_3: 0.1714  loss_dice_3: 0.9008  loss_ce_4: 0.7978  loss_mask_4: 0.1262  loss_dice_4: 1.114  loss_ce_5: 0.7962  loss_mask_5: 0.1411  loss_dice_5: 1.143  loss_ce_6: 0.7498  loss_mask_6: 0.1269  loss_dice_6: 1.123  loss_ce_7: 0.787  loss_mask_7: 0.1451  loss_dice_7: 1.045  loss_ce_8: 0.7375  loss_mask_8: 0.1447  loss_dice_8: 0.7611    time: 0.3214  last_time: 0.2961  data_time: 0.0071  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:09 d2.utils.events]:  eta: 0:21:20  iter: 55799  total_loss: 25  loss_ce: 1.012  loss_mask: 0.1034  loss_dice: 1.346  loss_ce_0: 1.318  loss_mask_0: 0.1167  loss_dice_0: 1.064  loss_ce_1: 1.145  loss_mask_1: 0.1251  loss_dice_1: 1.338  loss_ce_2: 1.06  loss_mask_2: 0.1174  loss_dice_2: 1.368  loss_ce_3: 0.911  loss_mask_3: 0.1214  loss_dice_3: 1.35  loss_ce_4: 0.9256  loss_mask_4: 0.1169  loss_dice_4: 1.401  loss_ce_5: 0.9473  loss_mask_5: 0.1118  loss_dice_5: 1.254  loss_ce_6: 0.9241  loss_mask_6: 0.1092  loss_dice_6: 1.312  loss_ce_7: 0.9285  loss_mask_7: 0.1099  loss_dice_7: 1.147  loss_ce_8: 0.8823  loss_mask_8: 0.1186  loss_dice_8: 1.194    time: 0.3214  last_time: 0.2943  data_time: 0.0066  last_data_time: 0.0056   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:15 d2.utils.events]:  eta: 0:21:13  iter: 55819  total_loss: 18.25  loss_ce: 0.7414  loss_mask: 0.1354  loss_dice: 0.9972  loss_ce_0: 0.8372  loss_mask_0: 0.1151  loss_dice_0: 0.9057  loss_ce_1: 0.9026  loss_mask_1: 0.1443  loss_dice_1: 0.9464  loss_ce_2: 0.8198  loss_mask_2: 0.1707  loss_dice_2: 1.059  loss_ce_3: 0.7391  loss_mask_3: 0.1336  loss_dice_3: 0.8132  loss_ce_4: 0.7478  loss_mask_4: 0.1329  loss_dice_4: 0.8003  loss_ce_5: 0.7773  loss_mask_5: 0.1346  loss_dice_5: 0.76  loss_ce_6: 0.8134  loss_mask_6: 0.1299  loss_dice_6: 0.9999  loss_ce_7: 0.7472  loss_mask_7: 0.1353  loss_dice_7: 0.9529  loss_ce_8: 0.7604  loss_mask_8: 0.1294  loss_dice_8: 0.8504    time: 0.3214  last_time: 0.2957  data_time: 0.0103  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:21 d2.utils.events]:  eta: 0:21:06  iter: 55839  total_loss: 22.45  loss_ce: 0.907  loss_mask: 0.1093  loss_dice: 0.9093  loss_ce_0: 1.137  loss_mask_0: 0.1138  loss_dice_0: 0.8659  loss_ce_1: 1.097  loss_mask_1: 0.1194  loss_dice_1: 0.9653  loss_ce_2: 0.981  loss_mask_2: 0.1359  loss_dice_2: 0.9732  loss_ce_3: 0.9216  loss_mask_3: 0.1175  loss_dice_3: 0.8853  loss_ce_4: 0.8632  loss_mask_4: 0.1214  loss_dice_4: 0.8764  loss_ce_5: 0.9725  loss_mask_5: 0.12  loss_dice_5: 0.9002  loss_ce_6: 0.9043  loss_mask_6: 0.1222  loss_dice_6: 0.9428  loss_ce_7: 0.8627  loss_mask_7: 0.1127  loss_dice_7: 0.8714  loss_ce_8: 0.9684  loss_mask_8: 0.1018  loss_dice_8: 0.8392    time: 0.3214  last_time: 0.3002  data_time: 0.0068  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:28 d2.utils.events]:  eta: 0:21:00  iter: 55859  total_loss: 29.77  loss_ce: 0.9588  loss_mask: 0.2585  loss_dice: 1.178  loss_ce_0: 1.38  loss_mask_0: 0.3378  loss_dice_0: 1.202  loss_ce_1: 1.182  loss_mask_1: 0.3546  loss_dice_1: 1.241  loss_ce_2: 1.181  loss_mask_2: 0.2542  loss_dice_2: 1.253  loss_ce_3: 0.935  loss_mask_3: 0.2718  loss_dice_3: 1.066  loss_ce_4: 0.9171  loss_mask_4: 0.2491  loss_dice_4: 1.106  loss_ce_5: 0.9014  loss_mask_5: 0.2592  loss_dice_5: 1.273  loss_ce_6: 0.8706  loss_mask_6: 0.2679  loss_dice_6: 1.153  loss_ce_7: 1.022  loss_mask_7: 0.2914  loss_dice_7: 1.13  loss_ce_8: 0.8962  loss_mask_8: 0.2373  loss_dice_8: 1.149    time: 0.3213  last_time: 0.2930  data_time: 0.0068  last_data_time: 0.0052   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:34 d2.utils.events]:  eta: 0:20:53  iter: 55879  total_loss: 28.47  loss_ce: 1.092  loss_mask: 0.1246  loss_dice: 1.347  loss_ce_0: 1.519  loss_mask_0: 0.0931  loss_dice_0: 1.328  loss_ce_1: 1.35  loss_mask_1: 0.1466  loss_dice_1: 1.088  loss_ce_2: 1.199  loss_mask_2: 0.1486  loss_dice_2: 1.393  loss_ce_3: 1.142  loss_mask_3: 0.1024  loss_dice_3: 1.358  loss_ce_4: 1.178  loss_mask_4: 0.14  loss_dice_4: 1.291  loss_ce_5: 1.01  loss_mask_5: 0.1037  loss_dice_5: 1.448  loss_ce_6: 1.05  loss_mask_6: 0.1236  loss_dice_6: 1.517  loss_ce_7: 1.115  loss_mask_7: 0.09497  loss_dice_7: 1.266  loss_ce_8: 1.034  loss_mask_8: 0.1131  loss_dice_8: 1.308    time: 0.3213  last_time: 0.3166  data_time: 0.0078  last_data_time: 0.0039   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:40 d2.utils.events]:  eta: 0:20:47  iter: 55899  total_loss: 26.33  loss_ce: 0.9706  loss_mask: 0.2135  loss_dice: 1.341  loss_ce_0: 1.298  loss_mask_0: 0.2226  loss_dice_0: 1.374  loss_ce_1: 1.173  loss_mask_1: 0.2274  loss_dice_1: 1.437  loss_ce_2: 1.121  loss_mask_2: 0.2283  loss_dice_2: 1.303  loss_ce_3: 1.054  loss_mask_3: 0.1563  loss_dice_3: 1.259  loss_ce_4: 1.052  loss_mask_4: 0.1453  loss_dice_4: 1.196  loss_ce_5: 1.036  loss_mask_5: 0.1559  loss_dice_5: 1.093  loss_ce_6: 0.9831  loss_mask_6: 0.1619  loss_dice_6: 1.378  loss_ce_7: 1.004  loss_mask_7: 0.2182  loss_dice_7: 1.331  loss_ce_8: 0.9224  loss_mask_8: 0.2415  loss_dice_8: 1.397    time: 0.3213  last_time: 0.3011  data_time: 0.0068  last_data_time: 0.0062   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:46 d2.utils.events]:  eta: 0:20:42  iter: 55919  total_loss: 36.67  loss_ce: 1.374  loss_mask: 0.1023  loss_dice: 1.77  loss_ce_0: 1.76  loss_mask_0: 0.1527  loss_dice_0: 1.707  loss_ce_1: 1.67  loss_mask_1: 0.1615  loss_dice_1: 1.73  loss_ce_2: 1.559  loss_mask_2: 0.1601  loss_dice_2: 1.707  loss_ce_3: 1.426  loss_mask_3: 0.1176  loss_dice_3: 1.751  loss_ce_4: 1.43  loss_mask_4: 0.1116  loss_dice_4: 1.728  loss_ce_5: 1.362  loss_mask_5: 0.1176  loss_dice_5: 1.686  loss_ce_6: 1.346  loss_mask_6: 0.1103  loss_dice_6: 1.8  loss_ce_7: 1.339  loss_mask_7: 0.1069  loss_dice_7: 1.704  loss_ce_8: 1.345  loss_mask_8: 0.09493  loss_dice_8: 1.625    time: 0.3213  last_time: 0.3016  data_time: 0.0078  last_data_time: 0.0073   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:53 d2.utils.events]:  eta: 0:20:36  iter: 55939  total_loss: 24.76  loss_ce: 1.037  loss_mask: 0.06793  loss_dice: 1.314  loss_ce_0: 1.337  loss_mask_0: 0.05266  loss_dice_0: 1.242  loss_ce_1: 1.304  loss_mask_1: 0.05564  loss_dice_1: 1.287  loss_ce_2: 1.081  loss_mask_2: 0.0618  loss_dice_2: 1.3  loss_ce_3: 1.113  loss_mask_3: 0.0686  loss_dice_3: 1.369  loss_ce_4: 1.062  loss_mask_4: 0.06019  loss_dice_4: 1.189  loss_ce_5: 1.087  loss_mask_5: 0.06154  loss_dice_5: 1.18  loss_ce_6: 1.045  loss_mask_6: 0.05451  loss_dice_6: 1.409  loss_ce_7: 0.9947  loss_mask_7: 0.05818  loss_dice_7: 1.34  loss_ce_8: 1.015  loss_mask_8: 0.05568  loss_dice_8: 1.151    time: 0.3213  last_time: 0.3081  data_time: 0.0098  last_data_time: 0.0121   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:39:59 d2.utils.events]:  eta: 0:20:31  iter: 55959  total_loss: 28.62  loss_ce: 1.079  loss_mask: 0.1242  loss_dice: 1.263  loss_ce_0: 1.23  loss_mask_0: 0.161  loss_dice_0: 1.43  loss_ce_1: 1.128  loss_mask_1: 0.1259  loss_dice_1: 1.473  loss_ce_2: 1.241  loss_mask_2: 0.1463  loss_dice_2: 1.318  loss_ce_3: 1.161  loss_mask_3: 0.1261  loss_dice_3: 1.243  loss_ce_4: 1.106  loss_mask_4: 0.1123  loss_dice_4: 1.336  loss_ce_5: 1.042  loss_mask_5: 0.1416  loss_dice_5: 1.434  loss_ce_6: 1.222  loss_mask_6: 0.1405  loss_dice_6: 1.317  loss_ce_7: 1.001  loss_mask_7: 0.1218  loss_dice_7: 1.476  loss_ce_8: 1.139  loss_mask_8: 0.1322  loss_dice_8: 1.288    time: 0.3213  last_time: 0.4021  data_time: 0.0068  last_data_time: 0.0063   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:05 d2.utils.events]:  eta: 0:20:24  iter: 55979  total_loss: 28.94  loss_ce: 0.8007  loss_mask: 0.08094  loss_dice: 1.352  loss_ce_0: 1.214  loss_mask_0: 0.08666  loss_dice_0: 1.627  loss_ce_1: 1.126  loss_mask_1: 0.08468  loss_dice_1: 1.387  loss_ce_2: 0.8819  loss_mask_2: 0.08341  loss_dice_2: 1.627  loss_ce_3: 0.8135  loss_mask_3: 0.08481  loss_dice_3: 1.449  loss_ce_4: 0.8735  loss_mask_4: 0.07618  loss_dice_4: 1.485  loss_ce_5: 0.828  loss_mask_5: 0.07452  loss_dice_5: 1.444  loss_ce_6: 0.7692  loss_mask_6: 0.07719  loss_dice_6: 1.517  loss_ce_7: 0.8182  loss_mask_7: 0.09883  loss_dice_7: 1.55  loss_ce_8: 0.833  loss_mask_8: 0.0806  loss_dice_8: 1.43    time: 0.3213  last_time: 0.3081  data_time: 0.0073  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:11 d2.utils.events]:  eta: 0:20:19  iter: 55999  total_loss: 23.21  loss_ce: 0.8181  loss_mask: 0.1513  loss_dice: 1.274  loss_ce_0: 1.078  loss_mask_0: 0.2345  loss_dice_0: 1.274  loss_ce_1: 1.07  loss_mask_1: 0.2051  loss_dice_1: 1.392  loss_ce_2: 1.022  loss_mask_2: 0.1796  loss_dice_2: 1.227  loss_ce_3: 0.9133  loss_mask_3: 0.1568  loss_dice_3: 1.261  loss_ce_4: 0.8347  loss_mask_4: 0.167  loss_dice_4: 1.234  loss_ce_5: 0.9395  loss_mask_5: 0.1514  loss_dice_5: 1.148  loss_ce_6: 0.8014  loss_mask_6: 0.1796  loss_dice_6: 1.148  loss_ce_7: 0.776  loss_mask_7: 0.1472  loss_dice_7: 1.08  loss_ce_8: 0.8103  loss_mask_8: 0.1546  loss_dice_8: 1.259    time: 0.3213  last_time: 0.2971  data_time: 0.0074  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:18 d2.utils.events]:  eta: 0:20:13  iter: 56019  total_loss: 26.62  loss_ce: 1.038  loss_mask: 0.1308  loss_dice: 1.434  loss_ce_0: 1.112  loss_mask_0: 0.1351  loss_dice_0: 1.638  loss_ce_1: 1.177  loss_mask_1: 0.1412  loss_dice_1: 1.518  loss_ce_2: 1.196  loss_mask_2: 0.1326  loss_dice_2: 1.366  loss_ce_3: 1.033  loss_mask_3: 0.136  loss_dice_3: 1.401  loss_ce_4: 0.8514  loss_mask_4: 0.1224  loss_dice_4: 1.163  loss_ce_5: 0.908  loss_mask_5: 0.1268  loss_dice_5: 1.244  loss_ce_6: 1.055  loss_mask_6: 0.1421  loss_dice_6: 1.287  loss_ce_7: 1.031  loss_mask_7: 0.122  loss_dice_7: 1.346  loss_ce_8: 0.8962  loss_mask_8: 0.115  loss_dice_8: 1.322    time: 0.3213  last_time: 0.3224  data_time: 0.0077  last_data_time: 0.0072   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:24 d2.utils.events]:  eta: 0:20:07  iter: 56039  total_loss: 25.19  loss_ce: 0.8473  loss_mask: 0.1234  loss_dice: 1.235  loss_ce_0: 1.335  loss_mask_0: 0.1074  loss_dice_0: 1.273  loss_ce_1: 1.283  loss_mask_1: 0.1021  loss_dice_1: 1.192  loss_ce_2: 1.194  loss_mask_2: 0.1162  loss_dice_2: 1.225  loss_ce_3: 0.8597  loss_mask_3: 0.1077  loss_dice_3: 1.381  loss_ce_4: 0.9759  loss_mask_4: 0.1064  loss_dice_4: 1.117  loss_ce_5: 0.9902  loss_mask_5: 0.09942  loss_dice_5: 1.161  loss_ce_6: 0.9248  loss_mask_6: 0.1085  loss_dice_6: 1.123  loss_ce_7: 0.839  loss_mask_7: 0.1086  loss_dice_7: 1.217  loss_ce_8: 0.9588  loss_mask_8: 0.1066  loss_dice_8: 1.099    time: 0.3213  last_time: 0.3040  data_time: 0.0074  last_data_time: 0.0090   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:30 d2.utils.events]:  eta: 0:20:01  iter: 56059  total_loss: 21.97  loss_ce: 0.9299  loss_mask: 0.1198  loss_dice: 0.8532  loss_ce_0: 1.385  loss_mask_0: 0.1441  loss_dice_0: 1.019  loss_ce_1: 1.259  loss_mask_1: 0.13  loss_dice_1: 0.899  loss_ce_2: 1.199  loss_mask_2: 0.1351  loss_dice_2: 0.9029  loss_ce_3: 0.9578  loss_mask_3: 0.1351  loss_dice_3: 0.7925  loss_ce_4: 0.9917  loss_mask_4: 0.1329  loss_dice_4: 0.8005  loss_ce_5: 0.8938  loss_mask_5: 0.1313  loss_dice_5: 0.8342  loss_ce_6: 0.9147  loss_mask_6: 0.1234  loss_dice_6: 0.7725  loss_ce_7: 0.9129  loss_mask_7: 0.1194  loss_dice_7: 0.7876  loss_ce_8: 0.905  loss_mask_8: 0.12  loss_dice_8: 0.9468    time: 0.3213  last_time: 0.3275  data_time: 0.0067  last_data_time: 0.0076   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:36 d2.utils.events]:  eta: 0:19:55  iter: 56079  total_loss: 25.3  loss_ce: 1.075  loss_mask: 0.1233  loss_dice: 1.343  loss_ce_0: 1.388  loss_mask_0: 0.1164  loss_dice_0: 1.403  loss_ce_1: 1.201  loss_mask_1: 0.1108  loss_dice_1: 1.381  loss_ce_2: 1.166  loss_mask_2: 0.1576  loss_dice_2: 1.37  loss_ce_3: 1.109  loss_mask_3: 0.1652  loss_dice_3: 1.579  loss_ce_4: 1.116  loss_mask_4: 0.1355  loss_dice_4: 1.446  loss_ce_5: 1.1  loss_mask_5: 0.1261  loss_dice_5: 1.483  loss_ce_6: 1.081  loss_mask_6: 0.12  loss_dice_6: 1.29  loss_ce_7: 1.064  loss_mask_7: 0.125  loss_dice_7: 1.486  loss_ce_8: 1.07  loss_mask_8: 0.128  loss_dice_8: 1.367    time: 0.3213  last_time: 0.3147  data_time: 0.0115  last_data_time: 0.0133   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:43 d2.utils.events]:  eta: 0:19:49  iter: 56099  total_loss: 26.49  loss_ce: 0.9876  loss_mask: 0.09662  loss_dice: 1.35  loss_ce_0: 1.37  loss_mask_0: 0.08667  loss_dice_0: 1.462  loss_ce_1: 1.185  loss_mask_1: 0.15  loss_dice_1: 1.715  loss_ce_2: 1.135  loss_mask_2: 0.1271  loss_dice_2: 1.611  loss_ce_3: 1.03  loss_mask_3: 0.1191  loss_dice_3: 1.447  loss_ce_4: 0.978  loss_mask_4: 0.1132  loss_dice_4: 1.524  loss_ce_5: 0.9951  loss_mask_5: 0.1361  loss_dice_5: 1.518  loss_ce_6: 0.9548  loss_mask_6: 0.1218  loss_dice_6: 1.313  loss_ce_7: 0.9965  loss_mask_7: 0.1112  loss_dice_7: 1.292  loss_ce_8: 1.003  loss_mask_8: 0.1018  loss_dice_8: 1.308    time: 0.3213  last_time: 0.3214  data_time: 0.0121  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:49 d2.utils.events]:  eta: 0:19:43  iter: 56119  total_loss: 23.68  loss_ce: 1.105  loss_mask: 0.1325  loss_dice: 1.092  loss_ce_0: 1.418  loss_mask_0: 0.1667  loss_dice_0: 0.9229  loss_ce_1: 1.361  loss_mask_1: 0.1426  loss_dice_1: 1.313  loss_ce_2: 1.173  loss_mask_2: 0.1217  loss_dice_2: 1.055  loss_ce_3: 1.088  loss_mask_3: 0.1315  loss_dice_3: 1.028  loss_ce_4: 1.08  loss_mask_4: 0.128  loss_dice_4: 0.8768  loss_ce_5: 1.09  loss_mask_5: 0.1014  loss_dice_5: 0.9712  loss_ce_6: 1.109  loss_mask_6: 0.1378  loss_dice_6: 0.872  loss_ce_7: 1.151  loss_mask_7: 0.1349  loss_dice_7: 0.8273  loss_ce_8: 1.134  loss_mask_8: 0.1151  loss_dice_8: 0.8182    time: 0.3213  last_time: 0.3303  data_time: 0.0072  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:40:55 d2.utils.events]:  eta: 0:19:37  iter: 56139  total_loss: 27.05  loss_ce: 1.096  loss_mask: 0.172  loss_dice: 1.273  loss_ce_0: 1.389  loss_mask_0: 0.2413  loss_dice_0: 1.414  loss_ce_1: 1.275  loss_mask_1: 0.2086  loss_dice_1: 1.551  loss_ce_2: 1.102  loss_mask_2: 0.2279  loss_dice_2: 1.478  loss_ce_3: 1.089  loss_mask_3: 0.1515  loss_dice_3: 1.33  loss_ce_4: 1.061  loss_mask_4: 0.1565  loss_dice_4: 1.391  loss_ce_5: 0.9958  loss_mask_5: 0.1605  loss_dice_5: 1.368  loss_ce_6: 1.091  loss_mask_6: 0.1983  loss_dice_6: 1.543  loss_ce_7: 1.014  loss_mask_7: 0.1809  loss_dice_7: 1.343  loss_ce_8: 0.9928  loss_mask_8: 0.1913  loss_dice_8: 1.353    time: 0.3213  last_time: 0.3231  data_time: 0.0067  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:01 d2.utils.events]:  eta: 0:19:30  iter: 56159  total_loss: 19.41  loss_ce: 0.8787  loss_mask: 0.1045  loss_dice: 0.8419  loss_ce_0: 1.386  loss_mask_0: 0.1364  loss_dice_0: 0.7844  loss_ce_1: 1.217  loss_mask_1: 0.1391  loss_dice_1: 0.6947  loss_ce_2: 0.9388  loss_mask_2: 0.1328  loss_dice_2: 0.9127  loss_ce_3: 0.9983  loss_mask_3: 0.1098  loss_dice_3: 0.7935  loss_ce_4: 0.8913  loss_mask_4: 0.1231  loss_dice_4: 0.94  loss_ce_5: 0.9303  loss_mask_5: 0.08293  loss_dice_5: 0.8518  loss_ce_6: 0.7759  loss_mask_6: 0.1034  loss_dice_6: 0.8594  loss_ce_7: 0.7887  loss_mask_7: 0.1014  loss_dice_7: 0.6916  loss_ce_8: 0.769  loss_mask_8: 0.1021  loss_dice_8: 0.6486    time: 0.3213  last_time: 0.3009  data_time: 0.0071  last_data_time: 0.0082   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:08 d2.utils.events]:  eta: 0:19:24  iter: 56179  total_loss: 34.99  loss_ce: 1.056  loss_mask: 0.2585  loss_dice: 1.446  loss_ce_0: 1.353  loss_mask_0: 0.2313  loss_dice_0: 1.338  loss_ce_1: 1.299  loss_mask_1: 0.2908  loss_dice_1: 1.511  loss_ce_2: 1.184  loss_mask_2: 0.3784  loss_dice_2: 1.234  loss_ce_3: 1.023  loss_mask_3: 0.3091  loss_dice_3: 1.478  loss_ce_4: 1.053  loss_mask_4: 0.2956  loss_dice_4: 1.349  loss_ce_5: 1.067  loss_mask_5: 0.3012  loss_dice_5: 1.203  loss_ce_6: 1.163  loss_mask_6: 0.2774  loss_dice_6: 1.339  loss_ce_7: 1.103  loss_mask_7: 0.2526  loss_dice_7: 1.29  loss_ce_8: 1.075  loss_mask_8: 0.2601  loss_dice_8: 1.302    time: 0.3213  last_time: 0.3030  data_time: 0.0074  last_data_time: 0.0067   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:14 d2.utils.events]:  eta: 0:19:18  iter: 56199  total_loss: 28.19  loss_ce: 1.014  loss_mask: 0.1656  loss_dice: 1.295  loss_ce_0: 1.136  loss_mask_0: 0.1806  loss_dice_0: 1.403  loss_ce_1: 1.234  loss_mask_1: 0.1842  loss_dice_1: 1.357  loss_ce_2: 1.121  loss_mask_2: 0.1841  loss_dice_2: 1.527  loss_ce_3: 0.979  loss_mask_3: 0.1718  loss_dice_3: 1.284  loss_ce_4: 1.034  loss_mask_4: 0.1636  loss_dice_4: 1.256  loss_ce_5: 0.9813  loss_mask_5: 0.2004  loss_dice_5: 1.374  loss_ce_6: 1.1  loss_mask_6: 0.1728  loss_dice_6: 1.353  loss_ce_7: 1.003  loss_mask_7: 0.1829  loss_dice_7: 1.383  loss_ce_8: 0.9347  loss_mask_8: 0.1629  loss_dice_8: 1.212    time: 0.3213  last_time: 0.2935  data_time: 0.0065  last_data_time: 0.0050   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:20 d2.utils.events]:  eta: 0:19:12  iter: 56219  total_loss: 29.36  loss_ce: 0.9959  loss_mask: 0.1356  loss_dice: 1.282  loss_ce_0: 1.595  loss_mask_0: 0.1175  loss_dice_0: 1.116  loss_ce_1: 1.537  loss_mask_1: 0.128  loss_dice_1: 1.239  loss_ce_2: 1.17  loss_mask_2: 0.1361  loss_dice_2: 1.126  loss_ce_3: 1.027  loss_mask_3: 0.1209  loss_dice_3: 1.288  loss_ce_4: 1.038  loss_mask_4: 0.1176  loss_dice_4: 1.171  loss_ce_5: 1.07  loss_mask_5: 0.109  loss_dice_5: 1.299  loss_ce_6: 0.9818  loss_mask_6: 0.1177  loss_dice_6: 1.394  loss_ce_7: 1.041  loss_mask_7: 0.115  loss_dice_7: 1.248  loss_ce_8: 1.067  loss_mask_8: 0.1181  loss_dice_8: 1.253    time: 0.3213  last_time: 0.3636  data_time: 0.0115  last_data_time: 0.0412   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:26 d2.utils.events]:  eta: 0:19:05  iter: 56239  total_loss: 27.5  loss_ce: 0.8588  loss_mask: 0.1753  loss_dice: 1.349  loss_ce_0: 1.118  loss_mask_0: 0.1613  loss_dice_0: 1.688  loss_ce_1: 1.152  loss_mask_1: 0.1319  loss_dice_1: 1.226  loss_ce_2: 0.9604  loss_mask_2: 0.1368  loss_dice_2: 1.288  loss_ce_3: 0.8965  loss_mask_3: 0.1747  loss_dice_3: 1.34  loss_ce_4: 0.9401  loss_mask_4: 0.1702  loss_dice_4: 1.385  loss_ce_5: 0.8496  loss_mask_5: 0.155  loss_dice_5: 1.309  loss_ce_6: 0.9548  loss_mask_6: 0.1644  loss_dice_6: 1.268  loss_ce_7: 0.8581  loss_mask_7: 0.1674  loss_dice_7: 1.315  loss_ce_8: 0.8952  loss_mask_8: 0.1695  loss_dice_8: 1.342    time: 0.3213  last_time: 0.3098  data_time: 0.0067  last_data_time: 0.0142   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:33 d2.utils.events]:  eta: 0:18:58  iter: 56259  total_loss: 33.96  loss_ce: 1.554  loss_mask: 0.1511  loss_dice: 1.586  loss_ce_0: 1.689  loss_mask_0: 0.1895  loss_dice_0: 1.506  loss_ce_1: 1.691  loss_mask_1: 0.1377  loss_dice_1: 1.671  loss_ce_2: 1.618  loss_mask_2: 0.1086  loss_dice_2: 1.426  loss_ce_3: 1.583  loss_mask_3: 0.1513  loss_dice_3: 1.402  loss_ce_4: 1.651  loss_mask_4: 0.1703  loss_dice_4: 1.653  loss_ce_5: 1.694  loss_mask_5: 0.1331  loss_dice_5: 1.185  loss_ce_6: 1.576  loss_mask_6: 0.1984  loss_dice_6: 1.227  loss_ce_7: 1.535  loss_mask_7: 0.1433  loss_dice_7: 1.453  loss_ce_8: 1.614  loss_mask_8: 0.1402  loss_dice_8: 1.561    time: 0.3213  last_time: 0.2967  data_time: 0.0071  last_data_time: 0.0029   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:39 d2.utils.events]:  eta: 0:18:52  iter: 56279  total_loss: 29.74  loss_ce: 0.9289  loss_mask: 0.223  loss_dice: 1.461  loss_ce_0: 1.093  loss_mask_0: 0.1844  loss_dice_0: 1.524  loss_ce_1: 1.042  loss_mask_1: 0.1689  loss_dice_1: 1.659  loss_ce_2: 0.9682  loss_mask_2: 0.194  loss_dice_2: 1.774  loss_ce_3: 0.8208  loss_mask_3: 0.2333  loss_dice_3: 1.493  loss_ce_4: 0.8764  loss_mask_4: 0.218  loss_dice_4: 1.537  loss_ce_5: 0.8706  loss_mask_5: 0.2359  loss_dice_5: 1.572  loss_ce_6: 0.9167  loss_mask_6: 0.2314  loss_dice_6: 1.494  loss_ce_7: 0.9173  loss_mask_7: 0.2106  loss_dice_7: 1.464  loss_ce_8: 0.8738  loss_mask_8: 0.1961  loss_dice_8: 1.596    time: 0.3213  last_time: 0.3054  data_time: 0.0073  last_data_time: 0.0137   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:45 d2.utils.events]:  eta: 0:18:46  iter: 56299  total_loss: 33.51  loss_ce: 1.174  loss_mask: 0.2815  loss_dice: 1.729  loss_ce_0: 1.198  loss_mask_0: 0.2953  loss_dice_0: 1.674  loss_ce_1: 1.29  loss_mask_1: 0.3232  loss_dice_1: 1.655  loss_ce_2: 1.104  loss_mask_2: 0.2919  loss_dice_2: 1.683  loss_ce_3: 1.164  loss_mask_3: 0.2552  loss_dice_3: 1.609  loss_ce_4: 1.052  loss_mask_4: 0.2506  loss_dice_4: 1.483  loss_ce_5: 1.157  loss_mask_5: 0.298  loss_dice_5: 1.829  loss_ce_6: 1.177  loss_mask_6: 0.2875  loss_dice_6: 1.651  loss_ce_7: 1.094  loss_mask_7: 0.3065  loss_dice_7: 1.541  loss_ce_8: 1.131  loss_mask_8: 0.291  loss_dice_8: 1.523    time: 0.3213  last_time: 0.4420  data_time: 0.0126  last_data_time: 0.1273   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:51 d2.utils.events]:  eta: 0:18:41  iter: 56319  total_loss: 26.21  loss_ce: 1.152  loss_mask: 0.1501  loss_dice: 1.104  loss_ce_0: 1.525  loss_mask_0: 0.1191  loss_dice_0: 0.8829  loss_ce_1: 1.434  loss_mask_1: 0.1705  loss_dice_1: 0.9449  loss_ce_2: 1.329  loss_mask_2: 0.1273  loss_dice_2: 1.012  loss_ce_3: 1.188  loss_mask_3: 0.1593  loss_dice_3: 0.7757  loss_ce_4: 1.192  loss_mask_4: 0.1633  loss_dice_4: 1.104  loss_ce_5: 1.238  loss_mask_5: 0.1541  loss_dice_5: 1.029  loss_ce_6: 1.219  loss_mask_6: 0.1457  loss_dice_6: 1.084  loss_ce_7: 1.18  loss_mask_7: 0.13  loss_dice_7: 1.066  loss_ce_8: 1.205  loss_mask_8: 0.1395  loss_dice_8: 1.068    time: 0.3213  last_time: 0.3241  data_time: 0.0067  last_data_time: 0.0065   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:41:58 d2.utils.events]:  eta: 0:18:34  iter: 56339  total_loss: 26.53  loss_ce: 0.9948  loss_mask: 0.2805  loss_dice: 0.9203  loss_ce_0: 1.257  loss_mask_0: 0.3237  loss_dice_0: 0.9929  loss_ce_1: 1.291  loss_mask_1: 0.3048  loss_dice_1: 1.022  loss_ce_2: 1.187  loss_mask_2: 0.2856  loss_dice_2: 0.9189  loss_ce_3: 1.184  loss_mask_3: 0.3005  loss_dice_3: 0.9672  loss_ce_4: 1.095  loss_mask_4: 0.293  loss_dice_4: 1.015  loss_ce_5: 1.162  loss_mask_5: 0.2724  loss_dice_5: 1.054  loss_ce_6: 1.148  loss_mask_6: 0.2961  loss_dice_6: 0.9578  loss_ce_7: 1.125  loss_mask_7: 0.2895  loss_dice_7: 1.049  loss_ce_8: 1.182  loss_mask_8: 0.3036  loss_dice_8: 1.074    time: 0.3213  last_time: 0.2962  data_time: 0.0068  last_data_time: 0.0075   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:04 d2.utils.events]:  eta: 0:18:28  iter: 56359  total_loss: 30.43  loss_ce: 1.204  loss_mask: 0.1172  loss_dice: 1.171  loss_ce_0: 1.522  loss_mask_0: 0.1304  loss_dice_0: 1.244  loss_ce_1: 1.311  loss_mask_1: 0.1421  loss_dice_1: 1.217  loss_ce_2: 1.246  loss_mask_2: 0.1424  loss_dice_2: 1.392  loss_ce_3: 1.228  loss_mask_3: 0.1775  loss_dice_3: 1.351  loss_ce_4: 1.124  loss_mask_4: 0.1707  loss_dice_4: 1.254  loss_ce_5: 1.101  loss_mask_5: 0.169  loss_dice_5: 1.456  loss_ce_6: 1.213  loss_mask_6: 0.1368  loss_dice_6: 1.218  loss_ce_7: 1.102  loss_mask_7: 0.1612  loss_dice_7: 1.296  loss_ce_8: 1.104  loss_mask_8: 0.1669  loss_dice_8: 1.232    time: 0.3213  last_time: 0.3230  data_time: 0.0068  last_data_time: 0.0070   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:10 d2.utils.events]:  eta: 0:18:22  iter: 56379  total_loss: 21.73  loss_ce: 0.9348  loss_mask: 0.1307  loss_dice: 0.985  loss_ce_0: 1.003  loss_mask_0: 0.1568  loss_dice_0: 1.09  loss_ce_1: 1.064  loss_mask_1: 0.176  loss_dice_1: 1.245  loss_ce_2: 1.006  loss_mask_2: 0.1229  loss_dice_2: 1.127  loss_ce_3: 0.848  loss_mask_3: 0.1366  loss_dice_3: 1.021  loss_ce_4: 0.764  loss_mask_4: 0.119  loss_dice_4: 1.061  loss_ce_5: 0.7872  loss_mask_5: 0.1336  loss_dice_5: 0.9206  loss_ce_6: 0.7654  loss_mask_6: 0.1596  loss_dice_6: 0.8517  loss_ce_7: 0.8136  loss_mask_7: 0.1426  loss_dice_7: 1.106  loss_ce_8: 0.788  loss_mask_8: 0.1418  loss_dice_8: 1.025    time: 0.3213  last_time: 0.3010  data_time: 0.0066  last_data_time: 0.0074   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:16 d2.utils.events]:  eta: 0:18:15  iter: 56399  total_loss: 19.82  loss_ce: 0.9373  loss_mask: 0.1175  loss_dice: 0.7731  loss_ce_0: 1.137  loss_mask_0: 0.1111  loss_dice_0: 0.7036  loss_ce_1: 1.035  loss_mask_1: 0.1148  loss_dice_1: 0.7559  loss_ce_2: 0.922  loss_mask_2: 0.1128  loss_dice_2: 0.7893  loss_ce_3: 0.9176  loss_mask_3: 0.1117  loss_dice_3: 0.7943  loss_ce_4: 0.9333  loss_mask_4: 0.1087  loss_dice_4: 0.8068  loss_ce_5: 0.9383  loss_mask_5: 0.1103  loss_dice_5: 0.8042  loss_ce_6: 0.9146  loss_mask_6: 0.1179  loss_dice_6: 0.7865  loss_ce_7: 0.9564  loss_mask_7: 0.1254  loss_dice_7: 0.7263  loss_ce_8: 0.9187  loss_mask_8: 0.1119  loss_dice_8: 0.6147    time: 0.3213  last_time: 0.3256  data_time: 0.0068  last_data_time: 0.0062   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:22 d2.utils.events]:  eta: 0:18:09  iter: 56419  total_loss: 17.36  loss_ce: 0.7481  loss_mask: 0.2002  loss_dice: 0.8463  loss_ce_0: 0.9129  loss_mask_0: 0.2385  loss_dice_0: 0.6964  loss_ce_1: 0.8991  loss_mask_1: 0.2344  loss_dice_1: 0.8795  loss_ce_2: 0.8891  loss_mask_2: 0.2341  loss_dice_2: 0.7924  loss_ce_3: 0.8113  loss_mask_3: 0.1559  loss_dice_3: 0.7365  loss_ce_4: 0.7935  loss_mask_4: 0.2044  loss_dice_4: 0.8447  loss_ce_5: 0.7598  loss_mask_5: 0.1509  loss_dice_5: 0.6926  loss_ce_6: 0.7447  loss_mask_6: 0.1522  loss_dice_6: 0.6721  loss_ce_7: 0.7554  loss_mask_7: 0.164  loss_dice_7: 0.7317  loss_ce_8: 0.8155  loss_mask_8: 0.207  loss_dice_8: 0.5722    time: 0.3212  last_time: 0.3225  data_time: 0.0071  last_data_time: 0.0058   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:28 d2.utils.events]:  eta: 0:18:03  iter: 56439  total_loss: 33.85  loss_ce: 1.337  loss_mask: 0.1875  loss_dice: 1.569  loss_ce_0: 1.455  loss_mask_0: 0.2362  loss_dice_0: 1.645  loss_ce_1: 1.33  loss_mask_1: 0.1752  loss_dice_1: 1.674  loss_ce_2: 1.433  loss_mask_2: 0.1899  loss_dice_2: 1.675  loss_ce_3: 1.283  loss_mask_3: 0.2298  loss_dice_3: 1.713  loss_ce_4: 1.359  loss_mask_4: 0.2243  loss_dice_4: 1.606  loss_ce_5: 1.382  loss_mask_5: 0.2156  loss_dice_5: 1.575  loss_ce_6: 1.339  loss_mask_6: 0.1843  loss_dice_6: 1.686  loss_ce_7: 1.282  loss_mask_7: 0.1764  loss_dice_7: 1.478  loss_ce_8: 1.346  loss_mask_8: 0.1933  loss_dice_8: 1.529    time: 0.3212  last_time: 0.3305  data_time: 0.0074  last_data_time: 0.0137   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:35 d2.utils.events]:  eta: 0:17:57  iter: 56459  total_loss: 27.3  loss_ce: 1.078  loss_mask: 0.09497  loss_dice: 1.174  loss_ce_0: 1.423  loss_mask_0: 0.09087  loss_dice_0: 1.224  loss_ce_1: 1.417  loss_mask_1: 0.08869  loss_dice_1: 1.387  loss_ce_2: 1.266  loss_mask_2: 0.07944  loss_dice_2: 1.293  loss_ce_3: 1.117  loss_mask_3: 0.07737  loss_dice_3: 1.228  loss_ce_4: 0.9863  loss_mask_4: 0.09738  loss_dice_4: 1.358  loss_ce_5: 1.117  loss_mask_5: 0.09655  loss_dice_5: 1.266  loss_ce_6: 1.126  loss_mask_6: 0.1016  loss_dice_6: 1.339  loss_ce_7: 1.071  loss_mask_7: 0.09313  loss_dice_7: 1.292  loss_ce_8: 1.071  loss_mask_8: 0.08835  loss_dice_8: 1.257    time: 0.3212  last_time: 0.2945  data_time: 0.0065  last_data_time: 0.0040   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:41 d2.utils.events]:  eta: 0:17:52  iter: 56479  total_loss: 23.84  loss_ce: 1.065  loss_mask: 0.1469  loss_dice: 0.9661  loss_ce_0: 1.111  loss_mask_0: 0.1209  loss_dice_0: 1.311  loss_ce_1: 1.195  loss_mask_1: 0.1366  loss_dice_1: 1.197  loss_ce_2: 1.138  loss_mask_2: 0.1193  loss_dice_2: 1.212  loss_ce_3: 1.047  loss_mask_3: 0.1344  loss_dice_3: 1.054  loss_ce_4: 1.08  loss_mask_4: 0.1221  loss_dice_4: 1.054  loss_ce_5: 1.05  loss_mask_5: 0.1388  loss_dice_5: 0.9523  loss_ce_6: 1.082  loss_mask_6: 0.1287  loss_dice_6: 0.8445  loss_ce_7: 1.066  loss_mask_7: 0.1386  loss_dice_7: 1.175  loss_ce_8: 1.064  loss_mask_8: 0.1269  loss_dice_8: 1.091    time: 0.3212  last_time: 0.3356  data_time: 0.0075  last_data_time: 0.0136   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:47 d2.utils.events]:  eta: 0:17:45  iter: 56499  total_loss: 22.52  loss_ce: 0.9981  loss_mask: 0.1239  loss_dice: 0.9478  loss_ce_0: 1.064  loss_mask_0: 0.1341  loss_dice_0: 0.8944  loss_ce_1: 1.122  loss_mask_1: 0.2171  loss_dice_1: 1  loss_ce_2: 0.9386  loss_mask_2: 0.1828  loss_dice_2: 1.219  loss_ce_3: 0.8582  loss_mask_3: 0.151  loss_dice_3: 1.134  loss_ce_4: 1.038  loss_mask_4: 0.1365  loss_dice_4: 1.171  loss_ce_5: 1.012  loss_mask_5: 0.129  loss_dice_5: 1.195  loss_ce_6: 1.01  loss_mask_6: 0.1248  loss_dice_6: 1.006  loss_ce_7: 0.9537  loss_mask_7: 0.1175  loss_dice_7: 1.022  loss_ce_8: 0.857  loss_mask_8: 0.115  loss_dice_8: 1.088    time: 0.3212  last_time: 0.3036  data_time: 0.0070  last_data_time: 0.0104   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:42:54 d2.utils.events]:  eta: 0:17:38  iter: 56519  total_loss: 28.75  loss_ce: 0.9183  loss_mask: 0.1043  loss_dice: 1.526  loss_ce_0: 1.16  loss_mask_0: 0.1517  loss_dice_0: 1.553  loss_ce_1: 1.28  loss_mask_1: 0.1308  loss_dice_1: 1.479  loss_ce_2: 1.061  loss_mask_2: 0.1329  loss_dice_2: 1.379  loss_ce_3: 0.9608  loss_mask_3: 0.1155  loss_dice_3: 1.556  loss_ce_4: 0.9519  loss_mask_4: 0.1517  loss_dice_4: 1.392  loss_ce_5: 0.9184  loss_mask_5: 0.1344  loss_dice_5: 1.425  loss_ce_6: 0.9475  loss_mask_6: 0.1142  loss_dice_6: 1.429  loss_ce_7: 0.9343  loss_mask_7: 0.1075  loss_dice_7: 1.476  loss_ce_8: 0.9678  loss_mask_8: 0.101  loss_dice_8: 1.389    time: 0.3212  last_time: 0.2959  data_time: 0.0072  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:00 d2.utils.events]:  eta: 0:17:33  iter: 56539  total_loss: 31.28  loss_ce: 1.301  loss_mask: 0.1892  loss_dice: 1.654  loss_ce_0: 1.774  loss_mask_0: 0.2224  loss_dice_0: 1.631  loss_ce_1: 1.61  loss_mask_1: 0.1802  loss_dice_1: 1.457  loss_ce_2: 1.45  loss_mask_2: 0.1874  loss_dice_2: 1.563  loss_ce_3: 1.288  loss_mask_3: 0.2074  loss_dice_3: 1.54  loss_ce_4: 1.347  loss_mask_4: 0.1983  loss_dice_4: 1.643  loss_ce_5: 1.469  loss_mask_5: 0.1849  loss_dice_5: 1.491  loss_ce_6: 1.334  loss_mask_6: 0.1968  loss_dice_6: 1.465  loss_ce_7: 1.235  loss_mask_7: 0.1779  loss_dice_7: 1.579  loss_ce_8: 1.377  loss_mask_8: 0.181  loss_dice_8: 1.607    time: 0.3212  last_time: 0.3287  data_time: 0.0070  last_data_time: 0.0049   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:06 d2.utils.events]:  eta: 0:17:26  iter: 56559  total_loss: 26.46  loss_ce: 1.178  loss_mask: 0.2042  loss_dice: 1.354  loss_ce_0: 1.148  loss_mask_0: 0.1852  loss_dice_0: 1.221  loss_ce_1: 1.357  loss_mask_1: 0.2018  loss_dice_1: 1.539  loss_ce_2: 1.218  loss_mask_2: 0.1808  loss_dice_2: 1.453  loss_ce_3: 1.09  loss_mask_3: 0.1668  loss_dice_3: 1.212  loss_ce_4: 1.187  loss_mask_4: 0.1613  loss_dice_4: 1.47  loss_ce_5: 1.191  loss_mask_5: 0.1858  loss_dice_5: 1.083  loss_ce_6: 1.207  loss_mask_6: 0.1748  loss_dice_6: 1.369  loss_ce_7: 1.107  loss_mask_7: 0.175  loss_dice_7: 1.378  loss_ce_8: 1.117  loss_mask_8: 0.1867  loss_dice_8: 1.299    time: 0.3212  last_time: 0.3244  data_time: 0.0062  last_data_time: 0.0038   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:12 d2.utils.events]:  eta: 0:17:20  iter: 56579  total_loss: 27.01  loss_ce: 0.8766  loss_mask: 0.1171  loss_dice: 1.507  loss_ce_0: 1.06  loss_mask_0: 0.1513  loss_dice_0: 1.569  loss_ce_1: 1.153  loss_mask_1: 0.1542  loss_dice_1: 1.59  loss_ce_2: 0.9197  loss_mask_2: 0.09896  loss_dice_2: 1.526  loss_ce_3: 0.88  loss_mask_3: 0.1025  loss_dice_3: 1.569  loss_ce_4: 0.9377  loss_mask_4: 0.116  loss_dice_4: 1.468  loss_ce_5: 0.8605  loss_mask_5: 0.148  loss_dice_5: 1.605  loss_ce_6: 0.8888  loss_mask_6: 0.1165  loss_dice_6: 1.518  loss_ce_7: 0.9693  loss_mask_7: 0.1022  loss_dice_7: 1.254  loss_ce_8: 1.017  loss_mask_8: 0.111  loss_dice_8: 1.584    time: 0.3212  last_time: 0.3090  data_time: 0.0095  last_data_time: 0.0021   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:18 d2.utils.events]:  eta: 0:17:13  iter: 56599  total_loss: 25.48  loss_ce: 1.059  loss_mask: 0.1396  loss_dice: 0.87  loss_ce_0: 1.58  loss_mask_0: 0.1134  loss_dice_0: 1.027  loss_ce_1: 1.523  loss_mask_1: 0.1482  loss_dice_1: 1.379  loss_ce_2: 1.285  loss_mask_2: 0.1371  loss_dice_2: 1.104  loss_ce_3: 1.283  loss_mask_3: 0.1291  loss_dice_3: 1.022  loss_ce_4: 1.198  loss_mask_4: 0.1537  loss_dice_4: 0.989  loss_ce_5: 1.253  loss_mask_5: 0.1377  loss_dice_5: 1.169  loss_ce_6: 1.095  loss_mask_6: 0.1469  loss_dice_6: 0.9188  loss_ce_7: 1.071  loss_mask_7: 0.1922  loss_dice_7: 0.9586  loss_ce_8: 1.143  loss_mask_8: 0.1848  loss_dice_8: 0.8561    time: 0.3212  last_time: 0.3073  data_time: 0.0073  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:25 d2.utils.events]:  eta: 0:17:07  iter: 56619  total_loss: 29.56  loss_ce: 1.167  loss_mask: 0.1083  loss_dice: 1.616  loss_ce_0: 1.505  loss_mask_0: 0.151  loss_dice_0: 1.709  loss_ce_1: 1.319  loss_mask_1: 0.1433  loss_dice_1: 1.76  loss_ce_2: 1.3  loss_mask_2: 0.09653  loss_dice_2: 1.642  loss_ce_3: 1.17  loss_mask_3: 0.1129  loss_dice_3: 1.701  loss_ce_4: 1.163  loss_mask_4: 0.1151  loss_dice_4: 1.67  loss_ce_5: 1.13  loss_mask_5: 0.109  loss_dice_5: 1.621  loss_ce_6: 1.054  loss_mask_6: 0.1168  loss_dice_6: 1.699  loss_ce_7: 1.132  loss_mask_7: 0.1279  loss_dice_7: 1.615  loss_ce_8: 1.228  loss_mask_8: 0.1085  loss_dice_8: 1.421    time: 0.3212  last_time: 0.3094  data_time: 0.0074  last_data_time: 0.0175   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:31 d2.utils.events]:  eta: 0:17:01  iter: 56639  total_loss: 23.53  loss_ce: 0.8395  loss_mask: 0.1486  loss_dice: 1.133  loss_ce_0: 1.002  loss_mask_0: 0.1217  loss_dice_0: 1.246  loss_ce_1: 0.9768  loss_mask_1: 0.1754  loss_dice_1: 1.375  loss_ce_2: 0.8938  loss_mask_2: 0.1598  loss_dice_2: 1.257  loss_ce_3: 0.8272  loss_mask_3: 0.1434  loss_dice_3: 1.177  loss_ce_4: 0.8562  loss_mask_4: 0.1313  loss_dice_4: 1.06  loss_ce_5: 0.9138  loss_mask_5: 0.1386  loss_dice_5: 1.182  loss_ce_6: 0.9343  loss_mask_6: 0.1464  loss_dice_6: 1.225  loss_ce_7: 0.8328  loss_mask_7: 0.1377  loss_dice_7: 1.135  loss_ce_8: 0.8548  loss_mask_8: 0.1552  loss_dice_8: 1.191    time: 0.3212  last_time: 0.2969  data_time: 0.0063  last_data_time: 0.0054   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:37 d2.utils.events]:  eta: 0:16:55  iter: 56659  total_loss: 34.46  loss_ce: 1.327  loss_mask: 0.1506  loss_dice: 1.75  loss_ce_0: 1.622  loss_mask_0: 0.1094  loss_dice_0: 1.655  loss_ce_1: 1.579  loss_mask_1: 0.1359  loss_dice_1: 1.995  loss_ce_2: 1.541  loss_mask_2: 0.1126  loss_dice_2: 1.9  loss_ce_3: 1.344  loss_mask_3: 0.1145  loss_dice_3: 1.736  loss_ce_4: 1.274  loss_mask_4: 0.1276  loss_dice_4: 1.736  loss_ce_5: 1.384  loss_mask_5: 0.1409  loss_dice_5: 1.722  loss_ce_6: 1.428  loss_mask_6: 0.1324  loss_dice_6: 1.787  loss_ce_7: 1.323  loss_mask_7: 0.1392  loss_dice_7: 1.905  loss_ce_8: 1.366  loss_mask_8: 0.1475  loss_dice_8: 1.605    time: 0.3212  last_time: 0.3304  data_time: 0.0077  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:43 d2.utils.events]:  eta: 0:16:49  iter: 56679  total_loss: 34.32  loss_ce: 1.308  loss_mask: 0.2813  loss_dice: 1.926  loss_ce_0: 1.682  loss_mask_0: 0.3062  loss_dice_0: 2.059  loss_ce_1: 1.343  loss_mask_1: 0.2809  loss_dice_1: 1.907  loss_ce_2: 1.267  loss_mask_2: 0.2372  loss_dice_2: 1.884  loss_ce_3: 1.206  loss_mask_3: 0.3142  loss_dice_3: 1.849  loss_ce_4: 1.298  loss_mask_4: 0.3169  loss_dice_4: 1.932  loss_ce_5: 1.382  loss_mask_5: 0.3196  loss_dice_5: 1.971  loss_ce_6: 1.243  loss_mask_6: 0.2901  loss_dice_6: 1.887  loss_ce_7: 1.235  loss_mask_7: 0.322  loss_dice_7: 1.906  loss_ce_8: 1.367  loss_mask_8: 0.2858  loss_dice_8: 1.869    time: 0.3212  last_time: 0.3128  data_time: 0.0084  last_data_time: 0.0138   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:49 d2.utils.events]:  eta: 0:16:44  iter: 56699  total_loss: 28.07  loss_ce: 0.9499  loss_mask: 0.117  loss_dice: 1.571  loss_ce_0: 1.244  loss_mask_0: 0.1155  loss_dice_0: 1.573  loss_ce_1: 1.258  loss_mask_1: 0.1341  loss_dice_1: 1.691  loss_ce_2: 1.258  loss_mask_2: 0.1323  loss_dice_2: 1.446  loss_ce_3: 1.019  loss_mask_3: 0.1211  loss_dice_3: 1.541  loss_ce_4: 1.077  loss_mask_4: 0.1084  loss_dice_4: 1.617  loss_ce_5: 1.021  loss_mask_5: 0.1125  loss_dice_5: 1.411  loss_ce_6: 1.079  loss_mask_6: 0.1102  loss_dice_6: 1.383  loss_ce_7: 0.9771  loss_mask_7: 0.1197  loss_dice_7: 1.547  loss_ce_8: 0.9847  loss_mask_8: 0.1219  loss_dice_8: 1.561    time: 0.3212  last_time: 0.3287  data_time: 0.0081  last_data_time: 0.0068   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:43:56 d2.utils.events]:  eta: 0:16:38  iter: 56719  total_loss: 27.21  loss_ce: 1.004  loss_mask: 0.1105  loss_dice: 1.272  loss_ce_0: 1.256  loss_mask_0: 0.1152  loss_dice_0: 1.45  loss_ce_1: 1.213  loss_mask_1: 0.1216  loss_dice_1: 1.436  loss_ce_2: 1.24  loss_mask_2: 0.1202  loss_dice_2: 1.344  loss_ce_3: 1.07  loss_mask_3: 0.1098  loss_dice_3: 1.528  loss_ce_4: 1.011  loss_mask_4: 0.1111  loss_dice_4: 1.263  loss_ce_5: 1.019  loss_mask_5: 0.1039  loss_dice_5: 1.356  loss_ce_6: 1.025  loss_mask_6: 0.1127  loss_dice_6: 1.279  loss_ce_7: 1.081  loss_mask_7: 0.1087  loss_dice_7: 1.359  loss_ce_8: 1.003  loss_mask_8: 0.1183  loss_dice_8: 1.287    time: 0.3212  last_time: 0.3029  data_time: 0.0086  last_data_time: 0.0059   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:02 d2.utils.events]:  eta: 0:16:31  iter: 56739  total_loss: 26.74  loss_ce: 0.9288  loss_mask: 0.1236  loss_dice: 1.126  loss_ce_0: 1.584  loss_mask_0: 0.1404  loss_dice_0: 1.164  loss_ce_1: 1.472  loss_mask_1: 0.1298  loss_dice_1: 1.066  loss_ce_2: 1.246  loss_mask_2: 0.1334  loss_dice_2: 1.27  loss_ce_3: 1.15  loss_mask_3: 0.1156  loss_dice_3: 1.328  loss_ce_4: 0.9732  loss_mask_4: 0.1184  loss_dice_4: 1.131  loss_ce_5: 0.9767  loss_mask_5: 0.127  loss_dice_5: 1.171  loss_ce_6: 1.002  loss_mask_6: 0.1155  loss_dice_6: 1.384  loss_ce_7: 1.01  loss_mask_7: 0.1087  loss_dice_7: 1.199  loss_ce_8: 1.11  loss_mask_8: 0.1433  loss_dice_8: 1.383    time: 0.3212  last_time: 0.2970  data_time: 0.0071  last_data_time: 0.0039   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:08 d2.utils.events]:  eta: 0:16:25  iter: 56759  total_loss: 30.1  loss_ce: 1.275  loss_mask: 0.1707  loss_dice: 1.339  loss_ce_0: 1.722  loss_mask_0: 0.1497  loss_dice_0: 1.274  loss_ce_1: 1.611  loss_mask_1: 0.1347  loss_dice_1: 1.278  loss_ce_2: 1.492  loss_mask_2: 0.1937  loss_dice_2: 1.559  loss_ce_3: 1.296  loss_mask_3: 0.1602  loss_dice_3: 1.247  loss_ce_4: 1.29  loss_mask_4: 0.1568  loss_dice_4: 1.46  loss_ce_5: 1.253  loss_mask_5: 0.1685  loss_dice_5: 1.492  loss_ce_6: 1.302  loss_mask_6: 0.1678  loss_dice_6: 1.736  loss_ce_7: 1.302  loss_mask_7: 0.1467  loss_dice_7: 1.52  loss_ce_8: 1.32  loss_mask_8: 0.1605  loss_dice_8: 1.589    time: 0.3212  last_time: 0.3401  data_time: 0.0070  last_data_time: 0.0184   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:15 d2.utils.events]:  eta: 0:16:20  iter: 56779  total_loss: 24.95  loss_ce: 0.923  loss_mask: 0.1396  loss_dice: 1.072  loss_ce_0: 1.23  loss_mask_0: 0.1238  loss_dice_0: 1.269  loss_ce_1: 1.099  loss_mask_1: 0.1208  loss_dice_1: 1.39  loss_ce_2: 1.088  loss_mask_2: 0.114  loss_dice_2: 1.226  loss_ce_3: 0.8446  loss_mask_3: 0.1263  loss_dice_3: 0.9679  loss_ce_4: 0.9023  loss_mask_4: 0.1201  loss_dice_4: 1.003  loss_ce_5: 0.9186  loss_mask_5: 0.1383  loss_dice_5: 1.094  loss_ce_6: 0.9664  loss_mask_6: 0.1296  loss_dice_6: 1.158  loss_ce_7: 0.9097  loss_mask_7: 0.1418  loss_dice_7: 0.9248  loss_ce_8: 0.9792  loss_mask_8: 0.1608  loss_dice_8: 0.9874    time: 0.3212  last_time: 0.3037  data_time: 0.0073  last_data_time: 0.0096   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:21 d2.utils.events]:  eta: 0:16:14  iter: 56799  total_loss: 27.11  loss_ce: 1.282  loss_mask: 0.1106  loss_dice: 1.248  loss_ce_0: 1.48  loss_mask_0: 0.1187  loss_dice_0: 1.413  loss_ce_1: 1.504  loss_mask_1: 0.1627  loss_dice_1: 1.294  loss_ce_2: 1.453  loss_mask_2: 0.1643  loss_dice_2: 1.412  loss_ce_3: 1.187  loss_mask_3: 0.1468  loss_dice_3: 1.246  loss_ce_4: 1.153  loss_mask_4: 0.1317  loss_dice_4: 1.196  loss_ce_5: 1.241  loss_mask_5: 0.1405  loss_dice_5: 1.213  loss_ce_6: 1.154  loss_mask_6: 0.1157  loss_dice_6: 1.112  loss_ce_7: 1.147  loss_mask_7: 0.1619  loss_dice_7: 1.276  loss_ce_8: 1.133  loss_mask_8: 0.1124  loss_dice_8: 1.404    time: 0.3212  last_time: 0.3226  data_time: 0.0071  last_data_time: 0.0057   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:27 d2.utils.events]:  eta: 0:16:08  iter: 56819  total_loss: 27.08  loss_ce: 0.9391  loss_mask: 0.2383  loss_dice: 1.099  loss_ce_0: 1.437  loss_mask_0: 0.2046  loss_dice_0: 1.006  loss_ce_1: 1.224  loss_mask_1: 0.3529  loss_dice_1: 1.258  loss_ce_2: 1.043  loss_mask_2: 0.3349  loss_dice_2: 1.261  loss_ce_3: 0.95  loss_mask_3: 0.26  loss_dice_3: 1.331  loss_ce_4: 0.981  loss_mask_4: 0.2579  loss_dice_4: 1.233  loss_ce_5: 0.9177  loss_mask_5: 0.2656  loss_dice_5: 1.151  loss_ce_6: 0.936  loss_mask_6: 0.2512  loss_dice_6: 0.9399  loss_ce_7: 0.9062  loss_mask_7: 0.2539  loss_dice_7: 1.297  loss_ce_8: 0.9104  loss_mask_8: 0.2459  loss_dice_8: 1.244    time: 0.3212  last_time: 0.3058  data_time: 0.0067  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:33 d2.utils.events]:  eta: 0:16:02  iter: 56839  total_loss: 24.18  loss_ce: 0.8621  loss_mask: 0.108  loss_dice: 0.7358  loss_ce_0: 0.9738  loss_mask_0: 0.1711  loss_dice_0: 1.096  loss_ce_1: 1.051  loss_mask_1: 0.1155  loss_dice_1: 1.125  loss_ce_2: 0.8734  loss_mask_2: 0.1443  loss_dice_2: 0.9691  loss_ce_3: 0.8205  loss_mask_3: 0.1435  loss_dice_3: 0.9817  loss_ce_4: 0.8145  loss_mask_4: 0.117  loss_dice_4: 1.123  loss_ce_5: 0.8444  loss_mask_5: 0.1629  loss_dice_5: 1.022  loss_ce_6: 0.7797  loss_mask_6: 0.1198  loss_dice_6: 1.041  loss_ce_7: 0.7155  loss_mask_7: 0.1374  loss_dice_7: 0.9107  loss_ce_8: 0.77  loss_mask_8: 0.1284  loss_dice_8: 1.014    time: 0.3212  last_time: 0.3113  data_time: 0.0114  last_data_time: 0.0071   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:40 d2.utils.events]:  eta: 0:15:56  iter: 56859  total_loss: 26.6  loss_ce: 0.9745  loss_mask: 0.2664  loss_dice: 1.06  loss_ce_0: 1.002  loss_mask_0: 0.205  loss_dice_0: 1.059  loss_ce_1: 0.9036  loss_mask_1: 0.2256  loss_dice_1: 1.377  loss_ce_2: 0.7998  loss_mask_2: 0.2003  loss_dice_2: 1.286  loss_ce_3: 0.9442  loss_mask_3: 0.2457  loss_dice_3: 1.055  loss_ce_4: 0.915  loss_mask_4: 0.2158  loss_dice_4: 1.017  loss_ce_5: 0.9309  loss_mask_5: 0.2562  loss_dice_5: 1.153  loss_ce_6: 0.9253  loss_mask_6: 0.2329  loss_dice_6: 1.2  loss_ce_7: 0.9038  loss_mask_7: 0.2502  loss_dice_7: 1.023  loss_ce_8: 0.9739  loss_mask_8: 0.2625  loss_dice_8: 1.055    time: 0.3212  last_time: 0.4603  data_time: 0.0135  last_data_time: 0.1527   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:46 d2.utils.events]:  eta: 0:15:50  iter: 56879  total_loss: 27.57  loss_ce: 0.9302  loss_mask: 0.138  loss_dice: 1.131  loss_ce_0: 1.478  loss_mask_0: 0.174  loss_dice_0: 1.306  loss_ce_1: 1.274  loss_mask_1: 0.1742  loss_dice_1: 1.485  loss_ce_2: 1.184  loss_mask_2: 0.1532  loss_dice_2: 1.366  loss_ce_3: 1.111  loss_mask_3: 0.129  loss_dice_3: 1.146  loss_ce_4: 0.954  loss_mask_4: 0.1309  loss_dice_4: 1.417  loss_ce_5: 1.018  loss_mask_5: 0.1487  loss_dice_5: 1.366  loss_ce_6: 1.153  loss_mask_6: 0.1296  loss_dice_6: 1.255  loss_ce_7: 0.9859  loss_mask_7: 0.1345  loss_dice_7: 1.241  loss_ce_8: 0.949  loss_mask_8: 0.132  loss_dice_8: 1.264    time: 0.3212  last_time: 0.3247  data_time: 0.0063  last_data_time: 0.0053   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:52 d2.utils.events]:  eta: 0:15:44  iter: 56899  total_loss: 23.68  loss_ce: 0.9715  loss_mask: 0.2221  loss_dice: 0.9259  loss_ce_0: 1.298  loss_mask_0: 0.218  loss_dice_0: 0.9548  loss_ce_1: 1.243  loss_mask_1: 0.2195  loss_dice_1: 1.057  loss_ce_2: 1.132  loss_mask_2: 0.274  loss_dice_2: 0.98  loss_ce_3: 1.093  loss_mask_3: 0.2424  loss_dice_3: 0.7977  loss_ce_4: 1.049  loss_mask_4: 0.2377  loss_dice_4: 0.9976  loss_ce_5: 1.077  loss_mask_5: 0.2547  loss_dice_5: 0.9677  loss_ce_6: 1.002  loss_mask_6: 0.2347  loss_dice_6: 1.006  loss_ce_7: 0.9837  loss_mask_7: 0.2518  loss_dice_7: 0.8914  loss_ce_8: 1.042  loss_mask_8: 0.2142  loss_dice_8: 0.897    time: 0.3212  last_time: 0.3195  data_time: 0.0080  last_data_time: 0.0105   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:44:58 d2.utils.events]:  eta: 0:15:38  iter: 56919  total_loss: 32.31  loss_ce: 0.9929  loss_mask: 0.1427  loss_dice: 1.788  loss_ce_0: 1.43  loss_mask_0: 0.1044  loss_dice_0: 1.752  loss_ce_1: 1.528  loss_mask_1: 0.1138  loss_dice_1: 1.658  loss_ce_2: 1.244  loss_mask_2: 0.1113  loss_dice_2: 1.795  loss_ce_3: 1.193  loss_mask_3: 0.1624  loss_dice_3: 1.439  loss_ce_4: 1.251  loss_mask_4: 0.123  loss_dice_4: 1.603  loss_ce_5: 1.188  loss_mask_5: 0.1022  loss_dice_5: 1.511  loss_ce_6: 1.034  loss_mask_6: 0.123  loss_dice_6: 1.625  loss_ce_7: 1.049  loss_mask_7: 0.1052  loss_dice_7: 1.624  loss_ce_8: 0.9969  loss_mask_8: 0.1397  loss_dice_8: 1.476    time: 0.3212  last_time: 0.3280  data_time: 0.0070  last_data_time: 0.0091   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:05 d2.utils.events]:  eta: 0:15:32  iter: 56939  total_loss: 27.87  loss_ce: 0.9781  loss_mask: 0.1607  loss_dice: 1.632  loss_ce_0: 1.254  loss_mask_0: 0.1549  loss_dice_0: 1.337  loss_ce_1: 1.059  loss_mask_1: 0.1644  loss_dice_1: 1.745  loss_ce_2: 1.058  loss_mask_2: 0.1708  loss_dice_2: 1.716  loss_ce_3: 0.9217  loss_mask_3: 0.1932  loss_dice_3: 1.573  loss_ce_4: 0.8917  loss_mask_4: 0.1757  loss_dice_4: 1.631  loss_ce_5: 1.037  loss_mask_5: 0.1634  loss_dice_5: 1.525  loss_ce_6: 0.8649  loss_mask_6: 0.1548  loss_dice_6: 1.537  loss_ce_7: 0.8588  loss_mask_7: 0.175  loss_dice_7: 1.561  loss_ce_8: 0.9309  loss_mask_8: 0.1617  loss_dice_8: 1.584    time: 0.3212  last_time: 0.3055  data_time: 0.0069  last_data_time: 0.0113   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:11 d2.utils.events]:  eta: 0:15:26  iter: 56959  total_loss: 24.63  loss_ce: 0.9199  loss_mask: 0.1953  loss_dice: 1.373  loss_ce_0: 1.237  loss_mask_0: 0.1948  loss_dice_0: 1.332  loss_ce_1: 1.143  loss_mask_1: 0.1937  loss_dice_1: 1.423  loss_ce_2: 1.019  loss_mask_2: 0.2198  loss_dice_2: 1.39  loss_ce_3: 0.9022  loss_mask_3: 0.1804  loss_dice_3: 1.261  loss_ce_4: 0.9497  loss_mask_4: 0.1676  loss_dice_4: 1.173  loss_ce_5: 0.9284  loss_mask_5: 0.1762  loss_dice_5: 0.9989  loss_ce_6: 0.9053  loss_mask_6: 0.1973  loss_dice_6: 1.043  loss_ce_7: 0.9197  loss_mask_7: 0.2016  loss_dice_7: 1.196  loss_ce_8: 0.9151  loss_mask_8: 0.2187  loss_dice_8: 1.384    time: 0.3212  last_time: 0.3026  data_time: 0.0177  last_data_time: 0.0084   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:17 d2.utils.events]:  eta: 0:15:20  iter: 56979  total_loss: 20.18  loss_ce: 0.7493  loss_mask: 0.1336  loss_dice: 0.8807  loss_ce_0: 0.9692  loss_mask_0: 0.12  loss_dice_0: 0.8277  loss_ce_1: 1.021  loss_mask_1: 0.132  loss_dice_1: 0.923  loss_ce_2: 0.9278  loss_mask_2: 0.1549  loss_dice_2: 1.033  loss_ce_3: 0.8022  loss_mask_3: 0.12  loss_dice_3: 0.8315  loss_ce_4: 0.8307  loss_mask_4: 0.1209  loss_dice_4: 0.8019  loss_ce_5: 0.7935  loss_mask_5: 0.1332  loss_dice_5: 0.9163  loss_ce_6: 0.728  loss_mask_6: 0.1308  loss_dice_6: 0.9468  loss_ce_7: 0.8309  loss_mask_7: 0.1075  loss_dice_7: 0.9919  loss_ce_8: 0.769  loss_mask_8: 0.1409  loss_dice_8: 0.8623    time: 0.3212  last_time: 0.3228  data_time: 0.0069  last_data_time: 0.0049   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:23 d2.utils.events]:  eta: 0:15:14  iter: 56999  total_loss: 34.96  loss_ce: 1.331  loss_mask: 0.2289  loss_dice: 1.763  loss_ce_0: 1.497  loss_mask_0: 0.2191  loss_dice_0: 1.725  loss_ce_1: 1.517  loss_mask_1: 0.2524  loss_dice_1: 1.891  loss_ce_2: 1.51  loss_mask_2: 0.2178  loss_dice_2: 1.596  loss_ce_3: 1.476  loss_mask_3: 0.1958  loss_dice_3: 1.763  loss_ce_4: 1.47  loss_mask_4: 0.1903  loss_dice_4: 1.753  loss_ce_5: 1.448  loss_mask_5: 0.2113  loss_dice_5: 1.753  loss_ce_6: 1.423  loss_mask_6: 0.2501  loss_dice_6: 1.776  loss_ce_7: 1.433  loss_mask_7: 0.2335  loss_dice_7: 1.745  loss_ce_8: 1.477  loss_mask_8: 0.2226  loss_dice_8: 1.801    time: 0.3211  last_time: 0.2963  data_time: 0.0079  last_data_time: 0.0068   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:30 d2.utils.events]:  eta: 0:15:07  iter: 57019  total_loss: 29.53  loss_ce: 0.9889  loss_mask: 0.1084  loss_dice: 1.402  loss_ce_0: 1.177  loss_mask_0: 0.0752  loss_dice_0: 1.614  loss_ce_1: 1.058  loss_mask_1: 0.1154  loss_dice_1: 1.582  loss_ce_2: 1.015  loss_mask_2: 0.09898  loss_dice_2: 1.397  loss_ce_3: 1.105  loss_mask_3: 0.09911  loss_dice_3: 1.347  loss_ce_4: 1.038  loss_mask_4: 0.09781  loss_dice_4: 1.357  loss_ce_5: 1.069  loss_mask_5: 0.1079  loss_dice_5: 1.42  loss_ce_6: 1.048  loss_mask_6: 0.102  loss_dice_6: 1.49  loss_ce_7: 1.024  loss_mask_7: 0.1145  loss_dice_7: 1.396  loss_ce_8: 1.003  loss_mask_8: 0.103  loss_dice_8: 1.416    time: 0.3211  last_time: 0.2949  data_time: 0.0068  last_data_time: 0.0042   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:36 d2.utils.events]:  eta: 0:15:01  iter: 57039  total_loss: 28.86  loss_ce: 1.207  loss_mask: 0.09166  loss_dice: 1.511  loss_ce_0: 1.361  loss_mask_0: 0.1508  loss_dice_0: 1.541  loss_ce_1: 1.145  loss_mask_1: 0.1407  loss_dice_1: 1.514  loss_ce_2: 1.123  loss_mask_2: 0.1337  loss_dice_2: 1.517  loss_ce_3: 1.232  loss_mask_3: 0.1163  loss_dice_3: 1.368  loss_ce_4: 1.275  loss_mask_4: 0.1123  loss_dice_4: 1.495  loss_ce_5: 1.161  loss_mask_5: 0.1198  loss_dice_5: 1.532  loss_ce_6: 1.118  loss_mask_6: 0.1347  loss_dice_6: 1.476  loss_ce_7: 1.117  loss_mask_7: 0.1184  loss_dice_7: 1.417  loss_ce_8: 1.267  loss_mask_8: 0.1031  loss_dice_8: 1.452    time: 0.3211  last_time: 0.3001  data_time: 0.0070  last_data_time: 0.0069   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:42 d2.utils.events]:  eta: 0:14:55  iter: 57059  total_loss: 30.59  loss_ce: 1.613  loss_mask: 0.1142  loss_dice: 1.146  loss_ce_0: 1.897  loss_mask_0: 0.147  loss_dice_0: 1.48  loss_ce_1: 1.889  loss_mask_1: 0.1174  loss_dice_1: 1.568  loss_ce_2: 1.534  loss_mask_2: 0.132  loss_dice_2: 1.472  loss_ce_3: 1.633  loss_mask_3: 0.1175  loss_dice_3: 1.367  loss_ce_4: 1.578  loss_mask_4: 0.1202  loss_dice_4: 1.237  loss_ce_5: 1.6  loss_mask_5: 0.1313  loss_dice_5: 1.467  loss_ce_6: 1.663  loss_mask_6: 0.1135  loss_dice_6: 1.492  loss_ce_7: 1.617  loss_mask_7: 0.1298  loss_dice_7: 1.636  loss_ce_8: 1.493  loss_mask_8: 0.1241  loss_dice_8: 1.342    time: 0.3211  last_time: 0.3021  data_time: 0.0095  last_data_time: 0.0094   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:48 d2.utils.events]:  eta: 0:14:50  iter: 57079  total_loss: 23.27  loss_ce: 1.085  loss_mask: 0.09621  loss_dice: 1.148  loss_ce_0: 1.425  loss_mask_0: 0.123  loss_dice_0: 1.18  loss_ce_1: 1.273  loss_mask_1: 0.09672  loss_dice_1: 1.204  loss_ce_2: 1.303  loss_mask_2: 0.1129  loss_dice_2: 1.144  loss_ce_3: 1.079  loss_mask_3: 0.09883  loss_dice_3: 1.04  loss_ce_4: 1.02  loss_mask_4: 0.09517  loss_dice_4: 1.055  loss_ce_5: 1.031  loss_mask_5: 0.09458  loss_dice_5: 0.9092  loss_ce_6: 1.016  loss_mask_6: 0.09544  loss_dice_6: 1.209  loss_ce_7: 1.05  loss_mask_7: 0.09335  loss_dice_7: 1.078  loss_ce_8: 1.053  loss_mask_8: 0.1105  loss_dice_8: 0.9523    time: 0.3211  last_time: 0.3190  data_time: 0.0075  last_data_time: 0.0128   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:45:55 d2.utils.events]:  eta: 0:14:44  iter: 57099  total_loss: 26.85  loss_ce: 1.058  loss_mask: 0.1223  loss_dice: 1.622  loss_ce_0: 1.521  loss_mask_0: 0.1026  loss_dice_0: 1.714  loss_ce_1: 1.282  loss_mask_1: 0.1329  loss_dice_1: 1.605  loss_ce_2: 1.234  loss_mask_2: 0.1266  loss_dice_2: 1.327  loss_ce_3: 1.066  loss_mask_3: 0.1222  loss_dice_3: 1.647  loss_ce_4: 1.193  loss_mask_4: 0.1284  loss_dice_4: 1.521  loss_ce_5: 1.205  loss_mask_5: 0.1356  loss_dice_5: 1.608  loss_ce_6: 1.037  loss_mask_6: 0.1031  loss_dice_6: 1.541  loss_ce_7: 1.119  loss_mask_7: 0.1189  loss_dice_7: 1.491  loss_ce_8: 1.064  loss_mask_8: 0.1057  loss_dice_8: 1.671    time: 0.3211  last_time: 0.3445  data_time: 0.0080  last_data_time: 0.0158   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:46:01 d2.utils.events]:  eta: 0:14:37  iter: 57119  total_loss: 27.56  loss_ce: 0.9422  loss_mask: 0.1587  loss_dice: 1.476  loss_ce_0: 1.113  loss_mask_0: 0.1554  loss_dice_0: 1.222  loss_ce_1: 1.29  loss_mask_1: 0.1645  loss_dice_1: 1.234  loss_ce_2: 1.242  loss_mask_2: 0.1303  loss_dice_2: 1.339  loss_ce_3: 1.144  loss_mask_3: 0.234  loss_dice_3: 1.328  loss_ce_4: 1.032  loss_mask_4: 0.1175  loss_dice_4: 1.354  loss_ce_5: 1.131  loss_mask_5: 0.1168  loss_dice_5: 1.339  loss_ce_6: 1.082  loss_mask_6: 0.1671  loss_dice_6: 1.339  loss_ce_7: 1.077  loss_mask_7: 0.116  loss_dice_7: 1.409  loss_ce_8: 1.034  loss_mask_8: 0.1757  loss_dice_8: 1.348    time: 0.3211  last_time: 0.3001  data_time: 0.0070  last_data_time: 0.0064   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:46:07 d2.utils.events]:  eta: 0:14:31  iter: 57139  total_loss: 24.22  loss_ce: 0.9761  loss_mask: 0.1291  loss_dice: 1.02  loss_ce_0: 1.214  loss_mask_0: 0.1394  loss_dice_0: 0.7865  loss_ce_1: 1.123  loss_mask_1: 0.1979  loss_dice_1: 0.9659  loss_ce_2: 1.032  loss_mask_2: 0.1617  loss_dice_2: 0.8311  loss_ce_3: 0.9837  loss_mask_3: 0.1071  loss_dice_3: 0.7563  loss_ce_4: 0.9598  loss_mask_4: 0.1108  loss_dice_4: 1.054  loss_ce_5: 0.9713  loss_mask_5: 0.109  loss_dice_5: 1.007  loss_ce_6: 0.9631  loss_mask_6: 0.1179  loss_dice_6: 1.18  loss_ce_7: 0.9222  loss_mask_7: 0.1364  loss_dice_7: 1.006  loss_ce_8: 0.9262  loss_mask_8: 0.1232  loss_dice_8: 0.8819    time: 0.3211  last_time: 0.3524  data_time: 0.0072  last_data_time: 0.0183   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:46:13 d2.utils.events]:  eta: 0:14:26  iter: 57159  total_loss: 26.18  loss_ce: 0.9925  loss_mask: 0.1035  loss_dice: 1.34  loss_ce_0: 1.235  loss_mask_0: 0.1267  loss_dice_0: 1.168  loss_ce_1: 1.259  loss_mask_1: 0.1127  loss_dice_1: 1.234  loss_ce_2: 1.086  loss_mask_2: 0.1525  loss_dice_2: 1.418  loss_ce_3: 0.9732  loss_mask_3: 0.1225  loss_dice_3: 1.446  loss_ce_4: 1.021  loss_mask_4: 0.1034  loss_dice_4: 1.429  loss_ce_5: 0.8948  loss_mask_5: 0.1034  loss_dice_5: 1.505  loss_ce_6: 1.014  loss_mask_6: 0.1068  loss_dice_6: 1.403  loss_ce_7: 1.026  loss_mask_7: 0.1067  loss_dice_7: 1.556  loss_ce_8: 1.016  loss_mask_8: 0.1161  loss_dice_8: 1.545    time: 0.3211  last_time: 0.2912  data_time: 0.0076  last_data_time: 0.0026   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:46:20 d2.utils.events]:  eta: 0:14:20  iter: 57179  total_loss: 26.32  loss_ce: 0.9933  loss_mask: 0.2252  loss_dice: 1.052  loss_ce_0: 1.286  loss_mask_0: 0.1413  loss_dice_0: 1.147  loss_ce_1: 1.225  loss_mask_1: 0.2953  loss_dice_1: 1.33  loss_ce_2: 1.096  loss_mask_2: 0.1914  loss_dice_2: 1.074  loss_ce_3: 0.9843  loss_mask_3: 0.2267  loss_dice_3: 0.9501  loss_ce_4: 1.002  loss_mask_4: 0.2436  loss_dice_4: 1.143  loss_ce_5: 1.01  loss_mask_5: 0.2127  loss_dice_5: 1.216  loss_ce_6: 0.9711  loss_mask_6: 0.2133  loss_dice_6: 1.063  loss_ce_7: 0.9709  loss_mask_7: 0.2113  loss_dice_7: 1.044  loss_ce_8: 0.9864  loss_mask_8: 0.2501  loss_dice_8: 1.144    time: 0.3211  last_time: 0.3126  data_time: 0.0072  last_data_time: 0.0047   lr: 0.0001  max_mem: 37650M\n",
            "[09/02 21:46:26 d2.utils.events]:  eta: 0:14:14  iter: 57199  total_loss: 26.34  loss_ce: 1.101  loss_mask: 0.2557  loss_dice: 1.021  loss_ce_0: 1.267  loss_mask_0: 0.2599  loss_dice_0: 1.215  loss_ce_1: 1.266  loss_mask_1: 0.2104  loss_dice_1: 1.063  loss_ce_2: 1.212  loss_mask_2: 0.2537  loss_dice_2: 1.112  loss_ce_3: 1.018  loss_mask_3: 0.2121  loss_dice_3: 1.105  loss_ce_4: 0.9935  loss_mask_4: 0.225  loss_dice_4: 1.057  loss_ce_5: 1.067  loss_mask_5: 0.2339  loss_dice_5: 0.9638  loss_ce_6: 1.151  loss_mask_6: 0.1979  loss_dice_6: 0.9802  loss_ce_7: 1.111  loss_mask_7: 0.2551  loss_dice_7: 1.026  loss_ce_8: 1.109  loss_mask_8: 0.2202  loss_dice_8: 1.097    time: 0.3211  last_time: 0.2920  data_time: 0.0067  last_data_time: 0.0055   lr: 0.0001  max_mem: 37650M\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}