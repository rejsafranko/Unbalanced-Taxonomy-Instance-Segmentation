{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw-R4vGpxEHg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IklOkpH8yVxW"
      },
      "outputs": [],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "%pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!git clone https://github.com/bytedance/fc-clip.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylL4Ns0J0fpB"
      },
      "outputs": [],
      "source": [
        "%cd fc-clip\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B_WtaSz1dJN"
      },
      "outputs": [],
      "source": [
        "%cd fcclip/modeling/pixel_decoder/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWqtS_QK1f8i"
      },
      "outputs": [],
      "source": [
        "!sh make.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofqDRU0V1fj-"
      },
      "outputs": [],
      "source": [
        "%cd ../../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0KrzDcv3CCB"
      },
      "outputs": [],
      "source": [
        "%pip install open-clip-torch==2.24.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpbzjAi35B8U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import detectron2\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.data.datasets import load_coco_json\n",
        "from detectron2.evaluation import inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from fcclip import (\n",
        "    InstanceSegEvaluator,\n",
        "    MaskFormerInstanceDatasetMapper,\n",
        "    add_maskformer2_config,\n",
        "    add_fcclip_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPTS = [\n",
        "    \"A discarded {} found on the ground.\",\n",
        "    \"A {} lying among scattered trash.\",\n",
        "    \"A {} left as litter in the street.\",\n",
        "    \"A photo of a {} thrown away in a public space.\",\n",
        "    \"A crumpled {} lying near other garbage.\",\n",
        "    \"A {} mixed with other debris on the ground.\",\n",
        "    \"A broken {} discarded in the environment.\",\n",
        "    \"This is a {} among other littered objects.\",\n",
        "    \"A small {} found discarded in a cluttered area.\",\n",
        "    \"A {} partially covered by other trash in the scene.\",\n",
        "    \"A large {} lying near a pile of garbage.\",\n",
        "    \"A weathered {} discarded on the side of the road.\",\n",
        "    \"A crushed {} among other waste on the street.\",\n",
        "    \"A {} that has been thrown away and left as litter.\",\n",
        "    \"A {} carelessly discarded in a public park.\",\n",
        "    \"A {} found among a variety of litter in this scene.\",\n",
        "    \"A {} thrown into the corner of a littered area.\",\n",
        "    \"This is a {} abandoned as trash in the environment.\",\n",
        "    \"A {} lying in a heap of litter, partially hidden.\",\n",
        "    \"A photo of a {} found on the sidewalk among other debris.\"\n",
        "]"
      ],
      "metadata": {
        "id": "uQ1YxuTa68ws"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MOT76Nk_7bvx"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AJxEgkzZZkGY"
      },
      "outputs": [],
      "source": [
        "def register_coco_dataset(name, json_file, image_root):\n",
        "    detectron2.data.DatasetCatalog.register(\n",
        "        name,\n",
        "        lambda: load_coco_json(json_file, image_root, name),\n",
        "    )\n",
        "    detectron2.data.MetadataCatalog.get(name).set(\n",
        "        thing_classes=[\n",
        "            \"Can\",\n",
        "            \"Other\",\n",
        "            \"Bottle\",\n",
        "            \"Bottle cap\",\n",
        "            \"Cup\",\n",
        "            \"Lid\",\n",
        "            \"Plastic bag + wrapper\",\n",
        "            \"Pop tab\",\n",
        "            \"Straw\",\n",
        "            \"Cigarette\",\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gkfTEBMK6iK8"
      },
      "outputs": [],
      "source": [
        "def configure_model():\n",
        "    cfg = detectron2.config.get_cfg()\n",
        "    add_deeplab_config(cfg)\n",
        "    add_maskformer2_config(cfg)\n",
        "    add_fcclip_config(cfg)\n",
        "    cfg.merge_from_file(\"configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_coco.yaml\")\n",
        "    cfg.MODEL.WEIGHTS = (\n",
        "        \"/content/drive/MyDrive/instseg/fcclip_cocopan.pth\"\n",
        "    )\n",
        "    cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON = True\n",
        "    cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON = False\n",
        "    cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON = False\n",
        "    cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 10\n",
        "    cfg.DATASETS.TEST = (\"taco10_test\",)\n",
        "    cfg.MODEL.DEVICE = \"cuda\"\n",
        "    cfg.freeze()\n",
        "    return cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sYXKsBBhaP8g"
      },
      "outputs": [],
      "source": [
        "register_coco_dataset(\n",
        "        name=\"taco10_test\",\n",
        "        json_file=f\"{data_dir_path}mapped_annotations_0_test.json\",\n",
        "        image_root=f\"{data_dir_path}images/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Pntoo51Kzs1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a47849-066c-4486-af4b-05b96ad11651"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(name='taco10_test',\n",
              "          thing_classes=['Can',\n",
              "                         'Other',\n",
              "                         'Bottle',\n",
              "                         'Bottle cap',\n",
              "                         'Cup',\n",
              "                         'Lid',\n",
              "                         'Plastic bag + wrapper',\n",
              "                         'Pop tab',\n",
              "                         'Straw',\n",
              "                         'Cigarette'],\n",
              "          json_file='/content/drive/MyDrive/instseg/data/mapped_annotations_0_test.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "detectron2.data.MetadataCatalog.get(\"taco10_test\").set(\n",
        "    json_file=f\"{data_dir_path}mapped_annotations_0_test.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = configure_model()"
      ],
      "metadata": {
        "id": "hcU8CI9D1cYe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW_PLpHh1iCa",
        "outputId": "66fcb6b8-3278-41c4-c1f3-467177df04ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/open_clip/factory.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
            "/usr/local/lib/python3.10/dist-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (134,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "backbone.clip_model.ln_final.{bias, weight}\n",
            "backbone.clip_model.token_embedding.weight\n",
            "backbone.clip_model.transformer.resblocks.0.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.0.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.0.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.0.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.0.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.1.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.1.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.1.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.1.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.1.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.10.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.10.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.10.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.10.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.10.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.10.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.11.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.11.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.11.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.11.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.11.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.11.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.12.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.12.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.12.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.12.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.12.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.12.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.13.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.13.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.13.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.13.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.13.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.13.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.14.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.14.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.14.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.14.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.14.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.14.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.15.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.15.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.15.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.15.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.15.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.15.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.2.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.2.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.2.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.2.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.2.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.3.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.3.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.3.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.3.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.3.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.4.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.4.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.4.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.4.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.4.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.5.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.5.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.5.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.5.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.5.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.6.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.6.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.6.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.6.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.6.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.6.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.7.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.7.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.7.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.7.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.7.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.7.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.8.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.8.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.8.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.8.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.8.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.8.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.9.attn.out_proj.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.9.attn.{in_proj_bias, in_proj_weight}\n",
            "backbone.clip_model.transformer.resblocks.9.ln_1.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.9.ln_2.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.9.mlp.c_fc.{bias, weight}\n",
            "backbone.clip_model.transformer.resblocks.9.mlp.c_proj.{bias, weight}\n",
            "backbone.clip_model.visual.head.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.head.mlp.fc2.weight\n",
            "backbone.clip_model.visual.trunk.head.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.0.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.0.gamma\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.0.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.0.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.0.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.1.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.1.gamma\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.1.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.1.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.1.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.2.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.2.gamma\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.2.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.2.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.0.blocks.2.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.0.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.0.gamma\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.0.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.0.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.0.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.1.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.1.gamma\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.1.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.1.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.1.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.2.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.2.gamma\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.2.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.2.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.blocks.2.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.downsample.0.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.1.downsample.1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.0.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.0.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.0.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.0.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.0.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.1.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.1.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.1.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.1.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.1.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.10.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.10.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.10.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.10.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.10.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.11.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.11.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.11.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.11.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.11.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.12.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.12.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.12.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.12.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.12.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.13.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.13.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.13.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.13.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.13.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.14.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.14.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.14.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.14.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.14.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.15.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.15.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.15.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.15.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.15.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.16.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.16.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.16.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.16.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.16.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.17.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.17.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.17.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.17.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.17.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.18.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.18.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.18.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.18.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.18.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.19.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.19.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.19.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.19.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.19.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.2.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.2.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.2.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.2.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.2.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.20.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.20.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.20.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.20.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.20.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.21.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.21.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.21.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.21.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.21.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.22.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.22.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.22.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.22.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.22.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.23.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.23.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.23.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.23.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.23.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.24.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.24.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.24.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.24.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.24.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.25.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.25.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.25.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.25.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.25.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.26.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.26.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.26.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.26.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.26.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.3.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.3.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.3.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.3.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.3.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.4.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.4.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.4.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.4.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.4.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.5.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.5.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.5.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.5.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.5.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.6.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.6.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.6.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.6.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.6.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.7.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.7.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.7.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.7.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.7.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.8.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.8.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.8.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.8.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.8.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.9.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.9.gamma\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.9.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.9.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.blocks.9.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.downsample.0.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.2.downsample.1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.0.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.0.gamma\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.0.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.0.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.0.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.1.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.1.gamma\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.1.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.1.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.1.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.2.conv_dw.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.2.gamma\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.2.mlp.fc1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.2.mlp.fc2.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.blocks.2.norm.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.downsample.0.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stages.3.downsample.1.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stem.0.{bias, weight}\n",
            "backbone.clip_model.visual.trunk.stem.1.{bias, weight}\n",
            "backbone.clip_model.{logit_scale, positional_embedding, text_projection}\n",
            "criterion.empty_weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = InstanceSegEvaluator(\"taco10_test\",output_dir=\"./output\")"
      ],
      "metadata": {
        "id": "qT7tMy3h1uMm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = build_detection_test_loader(cfg, dataset_name=\"taco10_test\", mapper=MaskFormerInstanceDatasetMapper(cfg, is_train=False))"
      ],
      "metadata": {
        "id": "vOmJnaEi1-HX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = inference_on_dataset(predictor.model, test_loader, evaluator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01L-tdt42ATd",
        "outputId": "0faabcc9-05c9-4494-d660-028d7ca4a0b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "Loading and preparing results...\n",
            "DONE (t=0.68s)\n",
            "creating index...\n",
            "index created!\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.096\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.051\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.105\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.085\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.109\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.169\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluation_results[\"segm\"])"
      ],
      "metadata": {
        "id": "9Gugs4KZCccs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b33447-1266-40b3-92d8-b2ca30b39be3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'AP': 5.715046577724339, 'AP50': 9.616931490558422, 'AP75': 5.102638184253004, 'APs': 0.0, 'APm': 10.538036380737848, 'APl': 8.537141711059494, 'AP-Can': 3.7312812252885212, 'AP-Other': 1.1523077820832237, 'AP-Bottle': 26.328826029125587, 'AP-Bottle cap': 1.3813294476712445, 'AP-Cup': 17.55552176070476, 'AP-Lid': 0.935888339441007, 'AP-Plastic bag + wrapper': 4.9035436464667415, 'AP-Pop tab': 0.0, 'AP-Straw': 0.07265863557122612, 'AP-Cigarette': 1.089108910891089}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}