{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUVUvZEL0GHx",
        "outputId": "2f478aab-fa0f-42cd-9d1d-0fbffd718810"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/multimodal_mapped_annotations_0_test.json\""
      ],
      "metadata": {
        "id": "dLx1LST20KqO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5PYm3l6r5ki",
        "outputId": "8daeb8f2-a7b6-4e88-a8d0-e4b2cc62bb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch==2.26.1 in /usr/local/lib/python3.10/dist-packages (2.26.1)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (0.19.0+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (2024.5.15)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (6.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (0.23.5)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.26.1) (1.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch==2.26.1) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch==2.26.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch==2.26.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch==2.26.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch==2.26.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch==2.26.1) (2024.6.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch==2.26.1) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch==2.26.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch==2.26.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch==2.26.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->open_clip_torch==2.26.1) (0.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch==2.26.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch==2.26.1) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch==2.26.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch==2.26.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch==2.26.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch==2.26.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch==2.26.1) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open_clip_torch==2.26.1) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install open_clip_torch==2.26.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "import PIL.Image\n",
        "import open_clip\n",
        "import sklearn.metrics\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "qw87kOVVzpDn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_text_prompts(path: str) -> Tuple[List[str], dict]:\n",
        "    with open(path) as f:\n",
        "        data = json.load(f)\n",
        "    categories = {category[\"id\"]: category[\"name\"] for category in data[\"categories\"]}\n",
        "    return list(categories.values()), data"
      ],
      "metadata": {
        "id": "oVyUwF9SznH3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_model(\n",
        "    model_id: str,\n",
        ") -> Tuple[\n",
        "    open_clip.CLIP, open_clip.transform.Compose, open_clip.tokenizer.SimpleTokenizer\n",
        "]:\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "        model_id, pretrained=\"laion2b_s34b_b79k\"\n",
        "    )\n",
        "    model.eval()\n",
        "    tokenizer = open_clip.get_tokenizer(model_id)\n",
        "    return model, preprocess, tokenizer"
      ],
      "metadata": {
        "id": "kwvHyrZRzoby"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(\n",
        "    data: dict,\n",
        "    model: open_clip.CLIP,\n",
        "    preprocess: open_clip.tokenizer.SimpleTokenizer,\n",
        "    text: torch.LongTensor,\n",
        "):\n",
        "    ground_truths = []\n",
        "    predictions = []\n",
        "\n",
        "    images = data[\"images\"]\n",
        "\n",
        "    for annotation in tqdm(data[\"annotations\"]):\n",
        "        image_id = annotation[\"image_id\"]\n",
        "        image_path = f\"/content/drive/MyDrive/instseg/data/images/{next((image for image in images if image['id'] == image_id), None)['file_name']}\"\n",
        "        image_label = annotation[\"category_id\"]\n",
        "\n",
        "        image = preprocess(PIL.Image.open(image_path).convert(\"RGB\")).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = model.encode_image(image)\n",
        "            text_features = model.encode_text(text)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "            predicted_label = text_probs.argmax(dim=-1).item()\n",
        "\n",
        "        ground_truths.append(image_label)\n",
        "        predictions.append(predicted_label)\n",
        "\n",
        "    return ground_truths, predictions"
      ],
      "metadata": {
        "id": "xkk3M1RCzu2o"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(true_labels: List[int], predicted_labels: List[int]):\n",
        "    accuracy = sklearn.metrics.accuracy_score(true_labels, predicted_labels) * 100\n",
        "    precision = sklearn.metrics.precision_score(true_labels, predicted_labels, average='weighted') * 100\n",
        "    recall = sklearn.metrics.recall_score(true_labels, predicted_labels, average='weighted') * 100\n",
        "    f1 = sklearn.metrics.f1_score(true_labels, predicted_labels, average='weighted') * 100\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}%\")\n",
        "    print(f\"Recall: {recall:.2f}%\")\n",
        "    print(f\"F1 Score: {f1:.2f}%\")"
      ],
      "metadata": {
        "id": "l-EGikBkzv8y"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "    text_prompts, taco_data = prepare_text_prompts(data_dir_path)\n",
        "    model, preprocess, tokenizer = configure_model(\"ViT-B-32\")\n",
        "    text = tokenizer(text_prompts)\n",
        "    ground_truths, predictions = inference(taco_data, model, preprocess, text)\n",
        "    calculate_metrics(true_labels=ground_truths, predicted_labels=predictions)"
      ],
      "metadata": {
        "id": "OpZxy74FzxM9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKy4o7zpAJBF",
        "outputId": "32102ef3-5cbd-489c-a511-4f720815e132"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
            "  0%|          | 0/569 [00:00<?, ?it/s]<ipython-input-47-f973a09bb23c>:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
            "100%|██████████| 569/569 [03:30<00:00,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 6.85%\n",
            "Precision: 8.68%\n",
            "Recall: 6.85%\n",
            "F1 Score: 6.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}