{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jHOC2HC0sCNE"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iUmK767kWQlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cb3ad2-35e0-4e75-a1d6-e25998146f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mask2Former' already exists and is not an empty directory.\n",
            "/content/Mask2Former\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-bb9min67\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-bb9min67\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (9.4.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.11.0)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.23.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2024.8.24)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (4.66.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm->-r requirements.txt (line 4)) (1.3.0)\n",
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n",
            "running build\n",
            "running build_py\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "/content/Mask2Former\n"
          ]
        }
      ],
      "source": [
        "# clone and install Mask2Former\n",
        "!git clone https://github.com/facebookresearch/Mask2Former.git\n",
        "%cd Mask2Former\n",
        "!pip install -U opencv-python\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "!pip install -r requirements.txt\n",
        "%cd mask2former/modeling/pixel_decoder/ops\n",
        "!python setup.py build install\n",
        "%cd ../../../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xGxctvB_Pdkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bac075a-9ed8-4728-e2c3-7078e5d8b62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-1mlyhrf8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-1mlyhrf8\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 5b72c27ae39f99db75d43f18fd1312e1ea934e60\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ccGvME_lQkbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0fafa6-0112-414d-d3c6-b150f2771744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former/mask2former/modeling/pixel_decoder/ops\n"
          ]
        }
      ],
      "source": [
        "%cd mask2former/modeling/pixel_decoder/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xP44DZ_zQooA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416b3098-59da-4392-b705-8176a5f2d87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build\n",
            "running build_py\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ],
      "source": [
        "!sh make.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fy0WqfqBQsqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0ae3b2-9555-4f73-ca77-36c5bfbfe03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Mask2Former\n"
          ]
        }
      ],
      "source": [
        "%cd ../../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# import Mask2Former project\n",
        "from mask2former import add_maskformer2_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvJwaL06RNCY",
        "outputId": "8a12f907-30c4-4391-9c56-98edc58dd82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aQ-zjQCtRU1F"
      },
      "outputs": [],
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "setup_logger(name=\"mask2former\")\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.projects.deeplab import add_deeplab_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wg5JCtyWRZOo"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7EO4nMI9Rb5P"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco10_train\", {}, data_dir_path + \"mapped_annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_val\", {}, data_dir_path + \"mapped_annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_test\", {}, data_dir_path + \"mapped_annotations_0_test.json\", data_dir_path + \"images/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y44xCY4Gu_ou"
      },
      "source": [
        "# Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IZL4O9aw3XXp"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.structures import PolygonMasks\n",
        "import copy\n",
        "import torch  # Import torch to convert images to tensors\n",
        "from argparse import ArgumentParser\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from mask2former import (\n",
        "    MaskFormerInstanceDatasetMapper,\n",
        "    InstanceSegEvaluator,\n",
        "    add_maskformer2_config,\n",
        ")\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "import os\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "import torch.nn as nn\n",
        "from torch import cat\n",
        "import torch.nn.functional as F\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import (\n",
        "    MetadataCatalog,\n",
        "    build_detection_test_loader,\n",
        "    build_detection_train_loader,\n",
        ")\n",
        "from detectron2.engine import default_argument_parser, default_setup, default_writers, launch\n",
        "from detectron2.evaluation import (\n",
        "    COCOEvaluator,\n",
        "    DatasetEvaluators,\n",
        "    inference_on_dataset,\n",
        "    print_csv_format,\n",
        ")\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
        "from detectron2.utils.events import EventStorage\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "from typing import List\n",
        "import fvcore.nn.weight_init as weight_init\n",
        "\n",
        "from detectron2.config import configurable\n",
        "from detectron2.layers import Conv2d, ConvTranspose2d, ShapeSpec, cat, get_norm\n",
        "from detectron2.layers.wrappers import move_device_like\n",
        "from detectron2.structures import Instances\n",
        "from detectron2.utils.events import get_event_storage\n",
        "from detectron2.modeling import ROI_MASK_HEAD_REGISTRY, ROI_HEADS_REGISTRY\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
        "from detectron2.modeling.roi_heads import StandardROIHeads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "from detectron2.utils.logger import create_small_table\n",
        "from tabulate import tabulate\n",
        "\n",
        "class CustomCOCOEvaluator(COCOEvaluator):\n",
        "  def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n",
        "        metrics = {\n",
        "            \"bbox\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
        "            \"segm\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
        "            \"keypoints\": [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"],\n",
        "        }[iou_type]\n",
        "\n",
        "        if coco_eval is None:\n",
        "            self._logger.warn(\"No predictions from the model!\")\n",
        "            return {metric: float(\"nan\") for metric in metrics}\n",
        "\n",
        "        # the standard metrics\n",
        "        results = {\n",
        "            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else \"nan\")\n",
        "            for idx, metric in enumerate(metrics)\n",
        "        }\n",
        "        self._logger.info(\n",
        "            \"Evaluation results for {}: \\n\".format(iou_type) + create_small_table(results)\n",
        "        )\n",
        "        if not np.isfinite(sum(results.values())):\n",
        "            self._logger.info(\"Some metrics cannot be computed and is shown as NaN.\")\n",
        "\n",
        "        if class_names is None or len(class_names) <= 1:\n",
        "            return results\n",
        "        # Compute per-category AP\n",
        "        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa\n",
        "        precisions = coco_eval.eval[\"precision\"]\n",
        "        recalls = coco_eval.eval[\"recall\"]\n",
        "        # precision has dims (iou, recall, cls, area range, max dets)\n",
        "        assert len(class_names) == precisions.shape[2]\n",
        "\n",
        "        results_per_category = []\n",
        "        ar_per_category = []\n",
        "        for idx, name in enumerate(class_names):\n",
        "            # area range index 0: all area ranges\n",
        "            # max dets index -1: typically 100 per image\n",
        "            precision = precisions[:, :, idx, 0, -1]\n",
        "            recall = recalls[:, idx, 0, -1]\n",
        "            precision = precision[precision > -1]\n",
        "            recall = recall[recall > -1]\n",
        "            ap = np.mean(precision) if precision.size else float(\"nan\")\n",
        "            ar = np.mean(recall) if recall.size else float(\"nan\")\n",
        "            results_per_category.append((\"{}\".format(name), float(ap * 100)))\n",
        "            ar_per_category.append((\"{}\".format(name), float(ar * 100)))\n",
        "\n",
        "        # tabulate it\n",
        "        N_COLS = min(6, len(results_per_category) * 2)\n",
        "        results_flatten = list(itertools.chain(*results_per_category))\n",
        "        results_2d = itertools.zip_longest(*[results_flatten[i::N_COLS] for i in range(N_COLS)])\n",
        "        table = tabulate(\n",
        "            results_2d,\n",
        "            tablefmt=\"pipe\",\n",
        "            floatfmt=\".3f\",\n",
        "            headers=[\"category\", \"AP\"] * (N_COLS // 2),\n",
        "            numalign=\"left\",\n",
        "        )\n",
        "        self._logger.info(\"Per-category {} AP: \\n\".format(iou_type) + table)\n",
        "\n",
        "        results.update({\"AP-\" + name: ap for name, ap in results_per_category})\n",
        "\n",
        "        # tabulate AR\n",
        "        ar_flatten = list(itertools.chain(*ar_per_category))\n",
        "        ar_2d = itertools.zip_longest(*[ar_flatten[i::N_COLS] for i in range(N_COLS)])\n",
        "        ar_table = tabulate(\n",
        "            ar_2d,\n",
        "            tablefmt=\"pipe\",\n",
        "            floatfmt=\".3f\",\n",
        "            headers=[\"category\", \"AR\"] * (N_COLS // 2),\n",
        "            numalign=\"left\",\n",
        "        )\n",
        "        self._logger.info(\"Per-category {} AR: \\n\".format(iou_type) + ar_table)\n",
        "\n",
        "        results.update({\"AR-\" + name: ar for name, ar in ar_per_category})\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "OxeO4BHxz45k"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6kaK-F3shWqR"
      },
      "outputs": [],
      "source": [
        "def get_evaluator(cfg, dataset_name, output_folder=None):\n",
        "    return CustomCOCOEvaluator(dataset_name, output_dir=output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup():\n",
        "  cfg = get_cfg()\n",
        "  add_deeplab_config(cfg)\n",
        "  add_maskformer2_config(cfg)\n",
        "  cfg.merge_from_file(\"configs/coco/instance-segmentation/maskformer2_R50_bs16_50ep.yaml\")\n",
        "  cfg.DATASETS.TRAIN = (\"taco10_train\",)\n",
        "  cfg.DATASETS.TEST = (\"taco10_val\",)\n",
        "  cfg.DATALOADER.NUM_WORKERS = 2\n",
        "  cfg.INPUT.DATASET_MAPPER_NAME = \"mask_former_instance\"\n",
        "  cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 10\n",
        "  cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl\"\n",
        "  cfg.MODEL.META_ARCHITECTURE = \"CustomMaskFormer\"\n",
        "  cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"value\"\n",
        "  cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
        "  cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "  cfg.SOLVER.BASE_LR = 0.0001\n",
        "  cfg.SOLVER.MAX_ITER = 60000\n",
        "  cfg.SOLVER.STEPS = []\n",
        "  cfg.OUTPUT_DIR = \"./output\"\n",
        "  cfg.TEST.EVAL_PERIOD = 6000\n",
        "  cfg.freeze()\n",
        "  os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "  return cfg"
      ],
      "metadata": {
        "id": "PVqHLxrDvMEE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_test(cfg, model):\n",
        "    all_results = OrderedDict()\n",
        "    all_recalls = OrderedDict()\n",
        "    for dataset_name in cfg.DATASETS.TEST:\n",
        "        data_loader = build_detection_test_loader(cfg, dataset_name)\n",
        "        evaluator = get_evaluator(\n",
        "            cfg, dataset_name, os.path.join(cfg.OUTPUT_DIR, \"inference\", dataset_name)\n",
        "        )\n",
        "        results_i = inference_on_dataset(model, data_loader, evaluator)\n",
        "        results = {}\n",
        "        recalls = {}\n",
        "        for task, metrics in results_i.items():\n",
        "            results[task] = {}\n",
        "            recalls[task] = {}\n",
        "\n",
        "            for metric_name, metric_value in metrics.items():\n",
        "                if metric_name.startswith(\"AP-\") or metric_name in [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]:\n",
        "                    results[task][metric_name] = metric_value\n",
        "                elif metric_name.startswith(\"AR-\"):\n",
        "                    recalls[task][metric_name] = metric_value\n",
        "\n",
        "        all_results[dataset_name] = results\n",
        "        all_recalls[dataset_name] = recalls\n",
        "\n",
        "        if comm.is_main_process():\n",
        "            print(\"Evaluation results for {} in csv format:\".format(dataset_name))\n",
        "            print_csv_format(results_i)\n",
        "        # Ručni ispis AP metrika\n",
        "        print(\"Results (AP metrics):\")\n",
        "        for task, metrics in results.items():\n",
        "          print(f\"Task: {task}\")\n",
        "          print(\"Metric, Value\")\n",
        "          for metric_name, metric_value in metrics.items():\n",
        "            print(f\"{metric_name}, {metric_value:.4f}\")\n",
        "\n",
        "        print(\"Recalls (AR metrics):\")\n",
        "        for task, metrics in recalls.items():\n",
        "          print(f\"Task: {task}\")\n",
        "          print(\"Metric, Value\")\n",
        "          for metric_name, metric_value in metrics.items():\n",
        "            print(f\"{metric_name}, {metric_value:.4f}\")\n",
        "\n",
        "    if len(all_results) == 1:\n",
        "        all_results = list(all_results.values())[0]\n",
        "        all_recalls = list(all_recalls.values())[0]\n",
        "    segm_recalls = all_recalls[\"segm\"]\n",
        "    recalls_values = list(segm_recalls.values())\n",
        "    recalls_tensor = torch.tensor(recalls_values) / 100  # Pretvori u tensor i podeli sa 100\n",
        "\n",
        "    return all_results, recalls_tensor"
      ],
      "metadata": {
        "id": "uvNLs7_zvkzL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_train(cfg, model, resume=False):\n",
        "    model.train()\n",
        "    optimizer = build_optimizer(cfg, model)\n",
        "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
        "\n",
        "    checkpointer = DetectionCheckpointer(\n",
        "        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n",
        "    )\n",
        "    start_iter = (\n",
        "        checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1\n",
        "    )\n",
        "    max_iter = cfg.SOLVER.MAX_ITER\n",
        "\n",
        "    periodic_checkpointer = PeriodicCheckpointer(\n",
        "        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    writers = default_writers(cfg.OUTPUT_DIR, max_iter) if comm.is_main_process() else []\n",
        "    mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
        "    data_loader = build_detection_train_loader(cfg, mapper=mapper)\n",
        "    with EventStorage(start_iter) as storage:\n",
        "        for data, iteration in zip(data_loader, range(start_iter, max_iter)):\n",
        "            storage.iter = iteration\n",
        "\n",
        "            # Izvršavanje modela i izračun gubitka\n",
        "            loss_dict = model(data)\n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            # Ažuriranje i zapisivanje gubitka\n",
        "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            if comm.is_main_process():\n",
        "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
        "            if (iteration + 1) % 600 == 0:\n",
        "              print(f\"Epoch {(iteration + 1) / 600} - Losses: {loss_dict}\")\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
        "            scheduler.step()\n",
        "\n",
        "            # Evaluacija i ažuriranje težina gubitka\n",
        "            if (\n",
        "                cfg.TEST.EVAL_PERIOD > 0\n",
        "                and (iteration + 1) % cfg.TEST.EVAL_PERIOD == 0\n",
        "                and iteration != max_iter\n",
        "            ):\n",
        "                eval_results, recalls = do_test(cfg, model)\n",
        "\n",
        "                # Ažuriranje težina na temelju evaluacijskog recall-a po klasama\n",
        "                model.criterion.loss_balancer.update_weights(recalls)\n",
        "\n",
        "                comm.synchronize()\n",
        "\n",
        "            if iteration - start_iter > 5 and (\n",
        "                (iteration + 1) % 20 == 0 or iteration == max_iter - 1\n",
        "            ):\n",
        "                for writer in writers:\n",
        "                    writer.write()\n",
        "            periodic_checkpointer.step(iteration)"
      ],
      "metadata": {
        "id": "frP085NFvoKD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Mask2FormerLossBalancer:\n",
        "    def __init__(self, class_frequencies, total_samples, eos_file=0.1):\n",
        "        self.class_frequencies = class_frequencies\n",
        "        self.total_samples = total_samples\n",
        "        class_frequencies_tensor = torch.tensor(\n",
        "            list(self.class_frequencies.values()), dtype=torch.float32\n",
        "        )\n",
        "        self.class_weights = self.total_samples / class_frequencies_tensor\n",
        "        self.eos_file = eos_file\n",
        "        self.class_weights = torch.cat(\n",
        "            [self.class_weights, torch.tensor([self.eos_file])]\n",
        "        )\n",
        "\n",
        "    def update_weights(self, recalls):\n",
        "        \"\"\"\n",
        "        Ova metoda ažurira težine na temelju trenutnog `recall` za svaku klasu.\n",
        "        \"\"\"\n",
        "        class_frequencies_tensor = torch.tensor(\n",
        "            list(self.class_frequencies.values()), dtype=torch.float32\n",
        "        )\n",
        "        base_weights = self.total_samples / class_frequencies_tensor\n",
        "        recall_factors = 1.0 - recalls\n",
        "        class_weights = (\n",
        "            self.class_weights.clone()\n",
        "        )  # Clone to avoid modifying the original tensor.\n",
        "        class_weights[\n",
        "            :-1\n",
        "        ] *= recall_factors  # Apply the recall factor only to object classes.\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "\n",
        "    def loss_function(self, predictions, targets):\n",
        "        if torch.isnan(predictions).any() or torch.isinf(predictions).any():\n",
        "            raise ValueError(\"Predictions contain NaN or Inf values.\")\n",
        "        if torch.isnan(targets).any() or torch.isinf(targets).any():\n",
        "            raise ValueError(\"Targets contain NaN or Inf values.\")\n",
        "\n",
        "        dynamic_weights = self.class_weights\n",
        "        dynamic_weights = dynamic_weights.to(targets.device)\n",
        "        class_weights = dynamic_weights[targets.long()]\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            predictions, targets, weight=class_weights, reduction=\"none\"\n",
        "        )\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "KZotxfCgwCWX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# Modified by Bowen Cheng from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n",
        "\"\"\"\n",
        "MaskFormer criterion.\n",
        "\"\"\"\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from detectron2.utils.comm import get_world_size\n",
        "from detectron2.projects.point_rend.point_features import (\n",
        "    get_uncertain_point_coords_with_randomness,\n",
        "    point_sample,\n",
        ")\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# Modified by Bowen Cheng from https://github.com/facebookresearch/detr/blob/master/util/misc.py\n",
        "\"\"\"\n",
        "Misc functions, including distributed helpers.\n",
        "\n",
        "Mostly copy-paste from torchvision references.\n",
        "\"\"\"\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torchvision\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], : img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError(\"not supported\")\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(\n",
        "            torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)\n",
        "        ).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def dice_loss(\n",
        "        inputs: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        num_masks: float,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Compute the DICE loss, similar to generalized IOU for masks\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "    \"\"\"\n",
        "    inputs = inputs.sigmoid()\n",
        "    inputs = inputs.flatten(1)\n",
        "    numerator = 2 * (inputs * targets).sum(-1)\n",
        "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
        "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
        "    return loss.sum() / num_masks\n",
        "\n",
        "\n",
        "dice_loss_jit = torch.jit.script(\n",
        "    dice_loss\n",
        ")  # type: torch.jit.ScriptModule\n",
        "\n",
        "\n",
        "def sigmoid_ce_loss(\n",
        "        inputs: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        num_masks: float,\n",
        "        loss_balancer: Mask2FormerLossBalancer\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "    Returns:\n",
        "        Loss tensor\n",
        "    \"\"\"\n",
        "    loss = loss_balancer.loss_function(inputs, targets)\n",
        "    loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "\n",
        "    return loss.mean(1).sum() / num_masks\n",
        "\n",
        "\n",
        "\n",
        "def calculate_uncertainty(logits):\n",
        "    \"\"\"\n",
        "    We estimate uncerainty as L1 distance between 0.0 and the logit prediction in 'logits' for the\n",
        "        foreground class in `classes`.\n",
        "    Args:\n",
        "        logits (Tensor): A tensor of shape (R, 1, ...) for class-specific or\n",
        "            class-agnostic, where R is the total number of predicted masks in all images and C is\n",
        "            the number of foreground classes. The values are logits.\n",
        "    Returns:\n",
        "        scores (Tensor): A tensor of shape (R, 1, ...) that contains uncertainty scores with\n",
        "            the most uncertain locations having the highest uncertainty score.\n",
        "    \"\"\"\n",
        "    assert logits.shape[1] == 1\n",
        "    gt_class_logits = logits.clone()\n",
        "    return -(torch.abs(gt_class_logits))\n",
        "\n",
        "\n",
        "class CustomSetCriterion(nn.Module):\n",
        "    \"\"\"This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses,\n",
        "                 num_points, oversample_ratio, importance_sample_ratio):\n",
        "        \"\"\"Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_weight)\n",
        "\n",
        "        # pointwise mask loss parameters\n",
        "        self.num_points = num_points\n",
        "        self.oversample_ratio = oversample_ratio\n",
        "        self.importance_sample_ratio = importance_sample_ratio\n",
        "        self.loss_balancer = Mask2FormerLossBalancer(class_frequencies={\n",
        "                \"Can\": 194,\n",
        "                \"Other\": 1397,\n",
        "                \"Bottle\": 344,\n",
        "                \"Bottle cap\": 226,\n",
        "                \"Cup\": 150,\n",
        "                \"Lid\": 63,\n",
        "                \"Plastic bag\": 697,\n",
        "                \"Pop tab\": 75,\n",
        "                \"Straw\": 108,\n",
        "                \"Cigarette\": 457,\n",
        "            },\n",
        "            total_samples=3711)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_masks):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert \"pred_logits\" in outputs\n",
        "        src_logits = outputs[\"pred_logits\"].float()\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(\n",
        "            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n",
        "        )\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {\"loss_ce\": loss_ce}\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_masks):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "        targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # No need to upsample predictions as we are using normalized coordinates :)\n",
        "        # N x 1 x H x W\n",
        "        src_masks = src_masks[:, None]\n",
        "        target_masks = target_masks[:, None]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # sample point_coords\n",
        "            point_coords = get_uncertain_point_coords_with_randomness(\n",
        "                src_masks,\n",
        "                lambda logits: calculate_uncertainty(logits),\n",
        "                self.num_points,\n",
        "                self.oversample_ratio,\n",
        "                self.importance_sample_ratio,\n",
        "            )\n",
        "            # get gt labels\n",
        "            point_labels = point_sample(\n",
        "                target_masks,\n",
        "                point_coords,\n",
        "                align_corners=False,\n",
        "            ).squeeze(1)\n",
        "\n",
        "        point_logits = point_sample(\n",
        "            src_masks,\n",
        "            point_coords,\n",
        "            align_corners=False,\n",
        "        ).squeeze(1)\n",
        "\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_ce_loss(point_logits, point_labels, num_masks, self.loss_balancer),\n",
        "            \"loss_dice\": dice_loss_jit(point_logits, point_labels, num_masks),\n",
        "        }\n",
        "\n",
        "        del src_masks\n",
        "        del target_masks\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_masks):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'masks': self.loss_masks,\n",
        "        }\n",
        "        assert loss in loss_map, f\"do you really want to compute {loss} loss?\"\n",
        "        return loss_map[loss](outputs, targets, indices, num_masks)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"aux_outputs\"}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_masks = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_masks = torch.as_tensor(\n",
        "            [num_masks], dtype=torch.float, device=next(iter(outputs.values())).device\n",
        "        )\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_masks)\n",
        "        num_masks = torch.clamp(num_masks / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_masks))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if \"aux_outputs\" in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs[\"aux_outputs\"]):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_masks)\n",
        "                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def __repr__(self):\n",
        "        head = \"Criterion \" + self.__class__.__name__\n",
        "        body = [\n",
        "            \"matcher: {}\".format(self.matcher.__repr__(_repr_indent=8)),\n",
        "            \"losses: {}\".format(self.losses),\n",
        "            \"weight_dict: {}\".format(self.weight_dict),\n",
        "            \"num_classes: {}\".format(self.num_classes),\n",
        "            \"eos_coef: {}\".format(self.eos_coef),\n",
        "            \"num_points: {}\".format(self.num_points),\n",
        "            \"oversample_ratio: {}\".format(self.oversample_ratio),\n",
        "            \"importance_sample_ratio: {}\".format(self.importance_sample_ratio),\n",
        "        ]\n",
        "        _repr_indent = 4\n",
        "        lines = [head] + [\" \" * _repr_indent + line for line in body]\n",
        "        return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "P0JoamKmv9Pd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from detectron2.config import configurable\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.modeling import META_ARCH_REGISTRY, build_backbone, build_sem_seg_head\n",
        "from detectron2.modeling.backbone import Backbone\n",
        "from detectron2.modeling.postprocessing import sem_seg_postprocess\n",
        "from detectron2.structures import Boxes, ImageList, Instances, BitMasks\n",
        "from detectron2.utils.memory import retry_if_cuda_oom\n",
        "from mask2former.modeling.criterion import SetCriterion\n",
        "from mask2former.modeling.matcher import HungarianMatcher\n",
        "\n",
        "@META_ARCH_REGISTRY.register()\n",
        "class CustomMaskFormer(nn.Module):\n",
        "    \"\"\"\n",
        "    Main class for mask classification semantic segmentation architectures.\n",
        "    \"\"\"\n",
        "\n",
        "    @configurable\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        backbone: Backbone,\n",
        "        sem_seg_head: nn.Module,\n",
        "        criterion: nn.Module,\n",
        "        num_queries: int,\n",
        "        object_mask_threshold: float,\n",
        "        overlap_threshold: float,\n",
        "        metadata,\n",
        "        size_divisibility: int,\n",
        "        sem_seg_postprocess_before_inference: bool,\n",
        "        pixel_mean: Tuple[float],\n",
        "        pixel_std: Tuple[float],\n",
        "        # inference\n",
        "        semantic_on: bool,\n",
        "        panoptic_on: bool,\n",
        "        instance_on: bool,\n",
        "        test_topk_per_image: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            backbone: a backbone module, must follow detectron2's backbone interface\n",
        "            sem_seg_head: a module that predicts semantic segmentation from backbone features\n",
        "            criterion: a module that defines the loss\n",
        "            num_queries: int, number of queries\n",
        "            object_mask_threshold: float, threshold to filter query based on classification score\n",
        "                for panoptic segmentation inference\n",
        "            overlap_threshold: overlap threshold used in general inference for panoptic segmentation\n",
        "            metadata: dataset meta, get `thing` and `stuff` category names for panoptic\n",
        "                segmentation inference\n",
        "            size_divisibility: Some backbones require the input height and width to be divisible by a\n",
        "                specific integer. We can use this to override such requirement.\n",
        "            sem_seg_postprocess_before_inference: whether to resize the prediction back\n",
        "                to original input size before semantic segmentation inference or after.\n",
        "                For high-resolution dataset like Mapillary, resizing predictions before\n",
        "                inference will cause OOM error.\n",
        "            pixel_mean, pixel_std: list or tuple with #channels element, representing\n",
        "                the per-channel mean and std to be used to normalize the input image\n",
        "            semantic_on: bool, whether to output semantic segmentation prediction\n",
        "            instance_on: bool, whether to output instance segmentation prediction\n",
        "            panoptic_on: bool, whether to output panoptic segmentation prediction\n",
        "            test_topk_per_image: int, instance segmentation parameter, keep topk instances per image\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.sem_seg_head = sem_seg_head\n",
        "        self.criterion = criterion\n",
        "        self.num_queries = num_queries\n",
        "        self.overlap_threshold = overlap_threshold\n",
        "        self.object_mask_threshold = object_mask_threshold\n",
        "        self.metadata = metadata\n",
        "        if size_divisibility < 0:\n",
        "            # use backbone size_divisibility if not set\n",
        "            size_divisibility = self.backbone.size_divisibility\n",
        "        self.size_divisibility = size_divisibility\n",
        "        self.sem_seg_postprocess_before_inference = sem_seg_postprocess_before_inference\n",
        "        self.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
        "        self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
        "\n",
        "        # additional args\n",
        "        self.semantic_on = semantic_on\n",
        "        self.instance_on = instance_on\n",
        "        self.panoptic_on = panoptic_on\n",
        "        self.test_topk_per_image = test_topk_per_image\n",
        "\n",
        "        if not self.semantic_on:\n",
        "            assert self.sem_seg_postprocess_before_inference\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg):\n",
        "        backbone = build_backbone(cfg)\n",
        "        sem_seg_head = build_sem_seg_head(cfg, backbone.output_shape())\n",
        "\n",
        "        # Loss parameters:\n",
        "        deep_supervision = cfg.MODEL.MASK_FORMER.DEEP_SUPERVISION\n",
        "        no_object_weight = cfg.MODEL.MASK_FORMER.NO_OBJECT_WEIGHT\n",
        "\n",
        "        # loss weights\n",
        "        class_weight = cfg.MODEL.MASK_FORMER.CLASS_WEIGHT\n",
        "        dice_weight = cfg.MODEL.MASK_FORMER.DICE_WEIGHT\n",
        "        mask_weight = cfg.MODEL.MASK_FORMER.MASK_WEIGHT\n",
        "\n",
        "        # building criterion\n",
        "        matcher = HungarianMatcher(\n",
        "            cost_class=class_weight,\n",
        "            cost_mask=mask_weight,\n",
        "            cost_dice=dice_weight,\n",
        "            num_points=cfg.MODEL.MASK_FORMER.TRAIN_NUM_POINTS,\n",
        "        )\n",
        "\n",
        "        weight_dict = {\"loss_ce\": class_weight, \"loss_mask\": mask_weight, \"loss_dice\": dice_weight}\n",
        "\n",
        "        if deep_supervision:\n",
        "            dec_layers = cfg.MODEL.MASK_FORMER.DEC_LAYERS\n",
        "            aux_weight_dict = {}\n",
        "            for i in range(dec_layers - 1):\n",
        "                aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
        "            weight_dict.update(aux_weight_dict)\n",
        "\n",
        "        losses = [\"labels\", \"masks\"]\n",
        "\n",
        "        criterion = CustomSetCriterion(\n",
        "            sem_seg_head.num_classes,\n",
        "            matcher=matcher,\n",
        "            weight_dict=weight_dict,\n",
        "            eos_coef=no_object_weight,\n",
        "            losses=losses,\n",
        "            num_points=cfg.MODEL.MASK_FORMER.TRAIN_NUM_POINTS,\n",
        "            oversample_ratio=cfg.MODEL.MASK_FORMER.OVERSAMPLE_RATIO,\n",
        "            importance_sample_ratio=cfg.MODEL.MASK_FORMER.IMPORTANCE_SAMPLE_RATIO,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"backbone\": backbone,\n",
        "            \"sem_seg_head\": sem_seg_head,\n",
        "            \"criterion\": criterion,\n",
        "            \"num_queries\": cfg.MODEL.MASK_FORMER.NUM_OBJECT_QUERIES,\n",
        "            \"object_mask_threshold\": cfg.MODEL.MASK_FORMER.TEST.OBJECT_MASK_THRESHOLD,\n",
        "            \"overlap_threshold\": cfg.MODEL.MASK_FORMER.TEST.OVERLAP_THRESHOLD,\n",
        "            \"metadata\": MetadataCatalog.get(cfg.DATASETS.TRAIN[0]),\n",
        "            \"size_divisibility\": cfg.MODEL.MASK_FORMER.SIZE_DIVISIBILITY,\n",
        "            \"sem_seg_postprocess_before_inference\": (\n",
        "                cfg.MODEL.MASK_FORMER.TEST.SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\n",
        "                or cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON\n",
        "                or cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON\n",
        "            ),\n",
        "            \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n",
        "            \"pixel_std\": cfg.MODEL.PIXEL_STD,\n",
        "            # inference\n",
        "            \"semantic_on\": cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON,\n",
        "            \"instance_on\": cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON,\n",
        "            \"panoptic_on\": cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON,\n",
        "            \"test_topk_per_image\": cfg.TEST.DETECTIONS_PER_IMAGE,\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.pixel_mean.device\n",
        "\n",
        "    def forward(self, batched_inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batched_inputs: a list, batched outputs of :class:`DatasetMapper`.\n",
        "                Each item in the list contains the inputs for one image.\n",
        "                For now, each item in the list is a dict that contains:\n",
        "                   * \"image\": Tensor, image in (C, H, W) format.\n",
        "                   * \"instances\": per-region ground truth\n",
        "                   * Other information that's included in the original dicts, such as:\n",
        "                     \"height\", \"width\" (int): the output resolution of the model (may be different\n",
        "                     from input resolution), used in inference.\n",
        "        Returns:\n",
        "            list[dict]:\n",
        "                each dict has the results for one image. The dict contains the following keys:\n",
        "\n",
        "                * \"sem_seg\":\n",
        "                    A Tensor that represents the\n",
        "                    per-pixel segmentation prediced by the head.\n",
        "                    The prediction has shape KxHxW that represents the logits of\n",
        "                    each class for each pixel.\n",
        "                * \"panoptic_seg\":\n",
        "                    A tuple that represent panoptic output\n",
        "                    panoptic_seg (Tensor): of shape (height, width) where the values are ids for each segment.\n",
        "                    segments_info (list[dict]): Describe each segment in `panoptic_seg`.\n",
        "                        Each dict contains keys \"id\", \"category_id\", \"isthing\".\n",
        "        \"\"\"\n",
        "        images = [x[\"image\"].to(self.device) for x in batched_inputs]\n",
        "        images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
        "        images = ImageList.from_tensors(images, self.size_divisibility)\n",
        "\n",
        "        features = self.backbone(images.tensor)\n",
        "        outputs = self.sem_seg_head(features)\n",
        "\n",
        "        if self.training:\n",
        "            # mask classification target\n",
        "            if \"instances\" in batched_inputs[0]:\n",
        "                gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
        "                targets = self.prepare_targets(gt_instances, images)\n",
        "            else:\n",
        "                targets = None\n",
        "\n",
        "            # bipartite matching-based loss\n",
        "            losses = self.criterion(outputs, targets)\n",
        "\n",
        "            for k in list(losses.keys()):\n",
        "                if k in self.criterion.weight_dict:\n",
        "                    losses[k] *= self.criterion.weight_dict[k]\n",
        "                else:\n",
        "                    # remove this loss if not specified in `weight_dict`\n",
        "                    losses.pop(k)\n",
        "            return losses\n",
        "        else:\n",
        "            mask_cls_results = outputs[\"pred_logits\"]\n",
        "            mask_pred_results = outputs[\"pred_masks\"]\n",
        "            # upsample masks\n",
        "            mask_pred_results = F.interpolate(\n",
        "                mask_pred_results,\n",
        "                size=(images.tensor.shape[-2], images.tensor.shape[-1]),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "\n",
        "            del outputs\n",
        "\n",
        "            processed_results = []\n",
        "            for mask_cls_result, mask_pred_result, input_per_image, image_size in zip(\n",
        "                mask_cls_results, mask_pred_results, batched_inputs, images.image_sizes\n",
        "            ):\n",
        "                height = input_per_image.get(\"height\", image_size[0])\n",
        "                width = input_per_image.get(\"width\", image_size[1])\n",
        "                processed_results.append({})\n",
        "\n",
        "                if self.sem_seg_postprocess_before_inference:\n",
        "                    mask_pred_result = retry_if_cuda_oom(sem_seg_postprocess)(\n",
        "                        mask_pred_result, image_size, height, width\n",
        "                    )\n",
        "                    mask_cls_result = mask_cls_result.to(mask_pred_result)\n",
        "\n",
        "                # semantic segmentation inference\n",
        "                if self.semantic_on:\n",
        "                    r = retry_if_cuda_oom(self.semantic_inference)(mask_cls_result, mask_pred_result)\n",
        "                    if not self.sem_seg_postprocess_before_inference:\n",
        "                        r = retry_if_cuda_oom(sem_seg_postprocess)(r, image_size, height, width)\n",
        "                    processed_results[-1][\"sem_seg\"] = r\n",
        "\n",
        "                # panoptic segmentation inference\n",
        "                if self.panoptic_on:\n",
        "                    panoptic_r = retry_if_cuda_oom(self.panoptic_inference)(mask_cls_result, mask_pred_result)\n",
        "                    processed_results[-1][\"panoptic_seg\"] = panoptic_r\n",
        "\n",
        "                # instance segmentation inference\n",
        "                if self.instance_on:\n",
        "                    instance_r = retry_if_cuda_oom(self.instance_inference)(mask_cls_result, mask_pred_result)\n",
        "                    processed_results[-1][\"instances\"] = instance_r\n",
        "\n",
        "            return processed_results\n",
        "\n",
        "    def prepare_targets(self, targets, images):\n",
        "        h_pad, w_pad = images.tensor.shape[-2:]\n",
        "        new_targets = []\n",
        "        for targets_per_image in targets:\n",
        "            # pad gt\n",
        "            gt_masks = targets_per_image.gt_masks\n",
        "            padded_masks = torch.zeros((gt_masks.shape[0], h_pad, w_pad), dtype=gt_masks.dtype, device=gt_masks.device)\n",
        "            padded_masks[:, : gt_masks.shape[1], : gt_masks.shape[2]] = gt_masks\n",
        "            new_targets.append(\n",
        "                {\n",
        "                    \"labels\": targets_per_image.gt_classes,\n",
        "                    \"masks\": padded_masks,\n",
        "                }\n",
        "            )\n",
        "        return new_targets\n",
        "\n",
        "    def semantic_inference(self, mask_cls, mask_pred):\n",
        "        mask_cls = F.softmax(mask_cls, dim=-1)[..., :-1]\n",
        "        mask_pred = mask_pred.sigmoid()\n",
        "        semseg = torch.einsum(\"qc,qhw->chw\", mask_cls, mask_pred)\n",
        "        return semseg\n",
        "\n",
        "    def panoptic_inference(self, mask_cls, mask_pred):\n",
        "        scores, labels = F.softmax(mask_cls, dim=-1).max(-1)\n",
        "        mask_pred = mask_pred.sigmoid()\n",
        "\n",
        "        keep = labels.ne(self.sem_seg_head.num_classes) & (scores > self.object_mask_threshold)\n",
        "        cur_scores = scores[keep]\n",
        "        cur_classes = labels[keep]\n",
        "        cur_masks = mask_pred[keep]\n",
        "        cur_mask_cls = mask_cls[keep]\n",
        "        cur_mask_cls = cur_mask_cls[:, :-1]\n",
        "\n",
        "        cur_prob_masks = cur_scores.view(-1, 1, 1) * cur_masks\n",
        "\n",
        "        h, w = cur_masks.shape[-2:]\n",
        "        panoptic_seg = torch.zeros((h, w), dtype=torch.int32, device=cur_masks.device)\n",
        "        segments_info = []\n",
        "\n",
        "        current_segment_id = 0\n",
        "\n",
        "        if cur_masks.shape[0] == 0:\n",
        "            # We didn't detect any mask :(\n",
        "            return panoptic_seg, segments_info\n",
        "        else:\n",
        "            # take argmax\n",
        "            cur_mask_ids = cur_prob_masks.argmax(0)\n",
        "            stuff_memory_list = {}\n",
        "            for k in range(cur_classes.shape[0]):\n",
        "                pred_class = cur_classes[k].item()\n",
        "                isthing = pred_class in self.metadata.thing_dataset_id_to_contiguous_id.values()\n",
        "                mask_area = (cur_mask_ids == k).sum().item()\n",
        "                original_area = (cur_masks[k] >= 0.5).sum().item()\n",
        "                mask = (cur_mask_ids == k) & (cur_masks[k] >= 0.5)\n",
        "\n",
        "                if mask_area > 0 and original_area > 0 and mask.sum().item() > 0:\n",
        "                    if mask_area / original_area < self.overlap_threshold:\n",
        "                        continue\n",
        "\n",
        "                    # merge stuff regions\n",
        "                    if not isthing:\n",
        "                        if int(pred_class) in stuff_memory_list.keys():\n",
        "                            panoptic_seg[mask] = stuff_memory_list[int(pred_class)]\n",
        "                            continue\n",
        "                        else:\n",
        "                            stuff_memory_list[int(pred_class)] = current_segment_id + 1\n",
        "\n",
        "                    current_segment_id += 1\n",
        "                    panoptic_seg[mask] = current_segment_id\n",
        "\n",
        "                    segments_info.append(\n",
        "                        {\n",
        "                            \"id\": current_segment_id,\n",
        "                            \"isthing\": bool(isthing),\n",
        "                            \"category_id\": int(pred_class),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            return panoptic_seg, segments_info\n",
        "\n",
        "    def instance_inference(self, mask_cls, mask_pred):\n",
        "        # mask_pred is already processed to have the same shape as original input\n",
        "        image_size = mask_pred.shape[-2:]\n",
        "\n",
        "        # [Q, K]\n",
        "        scores = F.softmax(mask_cls, dim=-1)[:, :-1]\n",
        "        labels = torch.arange(self.sem_seg_head.num_classes, device=self.device).unsqueeze(0).repeat(self.num_queries, 1).flatten(0, 1)\n",
        "        # scores_per_image, topk_indices = scores.flatten(0, 1).topk(self.num_queries, sorted=False)\n",
        "        scores_per_image, topk_indices = scores.flatten(0, 1).topk(self.test_topk_per_image, sorted=False)\n",
        "        labels_per_image = labels[topk_indices]\n",
        "\n",
        "        topk_indices = topk_indices // self.sem_seg_head.num_classes\n",
        "        # mask_pred = mask_pred.unsqueeze(1).repeat(1, self.sem_seg_head.num_classes, 1).flatten(0, 1)\n",
        "        mask_pred = mask_pred[topk_indices]\n",
        "\n",
        "        # if this is panoptic segmentation, we only keep the \"thing\" classes\n",
        "        if self.panoptic_on:\n",
        "            keep = torch.zeros_like(scores_per_image).bool()\n",
        "            for i, lab in enumerate(labels_per_image):\n",
        "                keep[i] = lab in self.metadata.thing_dataset_id_to_contiguous_id.values()\n",
        "\n",
        "            scores_per_image = scores_per_image[keep]\n",
        "            labels_per_image = labels_per_image[keep]\n",
        "            mask_pred = mask_pred[keep]\n",
        "\n",
        "        result = Instances(image_size)\n",
        "        # mask (before sigmoid)\n",
        "        result.pred_masks = (mask_pred > 0).float()\n",
        "        result.pred_boxes = Boxes(torch.zeros(mask_pred.size(0), 4))\n",
        "        # Uncomment the following to get boxes from masks (this is slow)\n",
        "        # result.pred_boxes = BitMasks(mask_pred > 0).get_bounding_boxes()\n",
        "\n",
        "        # calculate average mask prob\n",
        "        mask_scores_per_image = (mask_pred.sigmoid().flatten(1) * result.pred_masks.flatten(1)).sum(1) / (result.pred_masks.flatten(1).sum(1) + 1e-6)\n",
        "        result.scores = scores_per_image * mask_scores_per_image\n",
        "        result.pred_classes = labels_per_image\n",
        "        return result"
      ],
      "metadata": {
        "id": "VaiwBNkK5r0P"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = setup()"
      ],
      "metadata": {
        "id": "2Shepo3gvuAm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(cfg)"
      ],
      "metadata": {
        "id": "MTSSvzW828Od"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_train(cfg, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Ei2UECL2mXt",
        "outputId": "2c735e52-3906-481a-9590-79c52b69f4ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/02 19:00:36 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl ...\n",
            "WARNING [09/02 19:00:36 mask2former.modeling.transformer_decoder.mask2former_transformer_decoder]: Weight format of MultiScaleMaskedTransformerDecoder have changed! Please upgrade your models. Applying automatic conversion now ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (81, 256) in the checkpoint but (11, 256) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "criterion.empty_weight\n",
            "sem_seg_head.predictor.class_embed.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:48:25 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0023 s/iter. Inference: 3.8651 s/iter. Eval: 4.0683 s/iter. Total: 7.9368 s/iter. ETA=0:13:21\n",
            "[09/02 20:48:32 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0024 s/iter. Inference: 3.7879 s/iter. Eval: 4.1244 s/iter. Total: 7.9157 s/iter. ETA=0:13:11\n",
            "[09/02 20:48:33 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:48:44 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0024 s/iter. Inference: 3.8944 s/iter. Eval: 4.0908 s/iter. Total: 7.9886 s/iter. ETA=0:13:10\n",
            "[09/02 20:48:51 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0024 s/iter. Inference: 3.8198 s/iter. Eval: 4.1443 s/iter. Total: 7.9675 s/iter. ETA=0:13:00\n",
            "[09/02 20:48:51 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:49:03 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0023 s/iter. Inference: 3.9369 s/iter. Eval: 4.1116 s/iter. Total: 8.0519 s/iter. ETA=0:13:01\n",
            "[09/02 20:49:09 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0023 s/iter. Inference: 3.8645 s/iter. Eval: 4.1538 s/iter. Total: 8.0218 s/iter. ETA=0:12:50\n",
            "[09/02 20:49:16 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0024 s/iter. Inference: 3.7987 s/iter. Eval: 4.1940 s/iter. Total: 7.9961 s/iter. ETA=0:12:39\n",
            "[09/02 20:49:22 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0024 s/iter. Inference: 3.7267 s/iter. Eval: 4.2347 s/iter. Total: 7.9649 s/iter. ETA=0:12:28\n",
            "[09/02 20:49:29 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0024 s/iter. Inference: 3.6575 s/iter. Eval: 4.2716 s/iter. Total: 7.9326 s/iter. ETA=0:12:17\n",
            "[09/02 20:49:35 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0024 s/iter. Inference: 3.5910 s/iter. Eval: 4.3074 s/iter. Total: 7.9019 s/iter. ETA=0:12:06\n",
            "[09/02 20:49:41 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0024 s/iter. Inference: 3.5269 s/iter. Eval: 4.3419 s/iter. Total: 7.8723 s/iter. ETA=0:11:56\n",
            "[09/02 20:49:48 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0024 s/iter. Inference: 3.4652 s/iter. Eval: 4.3750 s/iter. Total: 7.8436 s/iter. ETA=0:11:45\n",
            "[09/02 20:49:54 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0024 s/iter. Inference: 3.4055 s/iter. Eval: 4.4070 s/iter. Total: 7.8161 s/iter. ETA=0:11:35\n",
            "[09/02 20:50:00 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0024 s/iter. Inference: 3.3480 s/iter. Eval: 4.4377 s/iter. Total: 7.7892 s/iter. ETA=0:11:25\n",
            "[09/02 20:50:07 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0024 s/iter. Inference: 3.2925 s/iter. Eval: 4.4672 s/iter. Total: 7.7631 s/iter. ETA=0:11:15\n",
            "[09/02 20:50:13 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0024 s/iter. Inference: 3.2388 s/iter. Eval: 4.4958 s/iter. Total: 7.7381 s/iter. ETA=0:11:05\n",
            "[09/02 20:50:23 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0023 s/iter. Inference: 3.0870 s/iter. Eval: 4.4435 s/iter. Total: 7.5339 s/iter. ETA=0:10:25\n",
            "[09/02 20:50:30 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0023 s/iter. Inference: 3.0400 s/iter. Eval: 4.4719 s/iter. Total: 7.5154 s/iter. ETA=0:10:16\n",
            "[09/02 20:50:36 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0023 s/iter. Inference: 2.9495 s/iter. Eval: 4.4217 s/iter. Total: 7.3746 s/iter. ETA=0:09:49\n",
            "[09/02 20:50:44 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0023 s/iter. Inference: 2.8649 s/iter. Eval: 4.4097 s/iter. Total: 7.2780 s/iter. ETA=0:09:27\n",
            "[09/02 20:50:52 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0023 s/iter. Inference: 2.7852 s/iter. Eval: 4.3985 s/iter. Total: 7.1870 s/iter. ETA=0:09:06\n",
            "[09/02 20:51:00 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0023 s/iter. Inference: 2.7097 s/iter. Eval: 4.3880 s/iter. Total: 7.1011 s/iter. ETA=0:08:45\n",
            "[09/02 20:51:09 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0023 s/iter. Inference: 2.6384 s/iter. Eval: 4.3775 s/iter. Total: 7.0193 s/iter. ETA=0:08:25\n",
            "[09/02 20:51:17 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0023 s/iter. Inference: 2.5710 s/iter. Eval: 4.3678 s/iter. Total: 6.9422 s/iter. ETA=0:08:05\n",
            "[09/02 20:51:25 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0023 s/iter. Inference: 2.5070 s/iter. Eval: 4.3586 s/iter. Total: 6.8690 s/iter. ETA=0:07:47\n",
            "[09/02 20:51:34 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0023 s/iter. Inference: 2.4472 s/iter. Eval: 4.3500 s/iter. Total: 6.8006 s/iter. ETA=0:07:28\n",
            "[09/02 20:51:42 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0023 s/iter. Inference: 2.3895 s/iter. Eval: 4.3416 s/iter. Total: 6.7344 s/iter. ETA=0:07:11\n",
            "[09/02 20:51:50 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0023 s/iter. Inference: 2.3345 s/iter. Eval: 4.3337 s/iter. Total: 6.6715 s/iter. ETA=0:06:53\n",
            "[09/02 20:51:58 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0023 s/iter. Inference: 2.2821 s/iter. Eval: 4.3265 s/iter. Total: 6.6119 s/iter. ETA=0:06:36\n",
            "[09/02 20:52:07 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0023 s/iter. Inference: 2.2321 s/iter. Eval: 4.3195 s/iter. Total: 6.5548 s/iter. ETA=0:06:20\n",
            "[09/02 20:52:15 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0023 s/iter. Inference: 2.1844 s/iter. Eval: 4.3125 s/iter. Total: 6.5001 s/iter. ETA=0:06:04\n",
            "[09/02 20:52:23 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0023 s/iter. Inference: 2.1387 s/iter. Eval: 4.3060 s/iter. Total: 6.4480 s/iter. ETA=0:05:48\n",
            "[09/02 20:52:31 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0023 s/iter. Inference: 2.0950 s/iter. Eval: 4.3002 s/iter. Total: 6.3985 s/iter. ETA=0:05:32\n",
            "[09/02 20:52:40 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 2.0532 s/iter. Eval: 4.2943 s/iter. Total: 6.3508 s/iter. ETA=0:05:17\n",
            "[09/02 20:52:48 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0022 s/iter. Inference: 2.0131 s/iter. Eval: 4.2886 s/iter. Total: 6.3049 s/iter. ETA=0:05:02\n",
            "[09/02 20:52:56 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0022 s/iter. Inference: 1.9746 s/iter. Eval: 4.2832 s/iter. Total: 6.2609 s/iter. ETA=0:04:48\n",
            "[09/02 20:53:04 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0022 s/iter. Inference: 1.9376 s/iter. Eval: 4.2778 s/iter. Total: 6.2186 s/iter. ETA=0:04:33\n",
            "[09/02 20:53:13 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0022 s/iter. Inference: 1.9021 s/iter. Eval: 4.2726 s/iter. Total: 6.1779 s/iter. ETA=0:04:19\n",
            "[09/02 20:53:21 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0022 s/iter. Inference: 1.8679 s/iter. Eval: 4.2682 s/iter. Total: 6.1392 s/iter. ETA=0:04:05\n",
            "[09/02 20:53:29 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0022 s/iter. Inference: 1.8350 s/iter. Eval: 4.2642 s/iter. Total: 6.1023 s/iter. ETA=0:03:51\n",
            "[09/02 20:53:36 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0022 s/iter. Inference: 1.8032 s/iter. Eval: 4.2466 s/iter. Total: 6.0530 s/iter. ETA=0:03:37\n",
            "[09/02 20:53:41 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:54:02 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0022 s/iter. Inference: 1.9210 s/iter. Eval: 4.2514 s/iter. Total: 6.1756 s/iter. ETA=0:03:29\n",
            "[09/02 20:54:03 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 20:54:23 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0022 s/iter. Inference: 2.0506 s/iter. Eval: 4.2583 s/iter. Total: 6.3121 s/iter. ETA=0:03:28\n",
            "[09/02 20:54:32 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0022 s/iter. Inference: 2.0212 s/iter. Eval: 4.2546 s/iter. Total: 6.2790 s/iter. ETA=0:03:14\n",
            "[09/02 20:54:40 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0022 s/iter. Inference: 1.9883 s/iter. Eval: 4.2505 s/iter. Total: 6.2419 s/iter. ETA=0:03:01\n",
            "[09/02 20:54:49 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0022 s/iter. Inference: 1.9564 s/iter. Eval: 4.2466 s/iter. Total: 6.2062 s/iter. ETA=0:02:47\n",
            "[09/02 20:54:57 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0022 s/iter. Inference: 1.9256 s/iter. Eval: 4.2428 s/iter. Total: 6.1715 s/iter. ETA=0:02:34\n",
            "[09/02 20:55:05 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0022 s/iter. Inference: 1.8958 s/iter. Eval: 4.2392 s/iter. Total: 6.1382 s/iter. ETA=0:02:21\n",
            "[09/02 20:55:11 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0022 s/iter. Inference: 1.8669 s/iter. Eval: 4.2160 s/iter. Total: 6.0861 s/iter. ETA=0:02:07\n",
            "[09/02 20:55:20 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0022 s/iter. Inference: 1.8391 s/iter. Eval: 4.2222 s/iter. Total: 6.0645 s/iter. ETA=0:01:55\n",
            "[09/02 20:55:29 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0022 s/iter. Inference: 1.8122 s/iter. Eval: 4.2238 s/iter. Total: 6.0391 s/iter. ETA=0:01:42\n",
            "[09/02 20:55:38 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0022 s/iter. Inference: 1.7860 s/iter. Eval: 4.2212 s/iter. Total: 6.0103 s/iter. ETA=0:01:30\n",
            "[09/02 20:55:46 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0022 s/iter. Inference: 1.7606 s/iter. Eval: 4.2180 s/iter. Total: 5.9817 s/iter. ETA=0:01:17\n",
            "[09/02 20:55:54 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0022 s/iter. Inference: 1.7360 s/iter. Eval: 4.2150 s/iter. Total: 5.9541 s/iter. ETA=0:01:05\n",
            "[09/02 20:56:02 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0022 s/iter. Inference: 1.7120 s/iter. Eval: 4.2120 s/iter. Total: 5.9271 s/iter. ETA=0:00:53\n",
            "[09/02 20:56:11 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0022 s/iter. Inference: 1.6888 s/iter. Eval: 4.2090 s/iter. Total: 5.9010 s/iter. ETA=0:00:41\n",
            "[09/02 20:56:18 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0022 s/iter. Inference: 1.6661 s/iter. Eval: 4.2011 s/iter. Total: 5.8703 s/iter. ETA=0:00:29\n",
            "[09/02 20:56:26 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0022 s/iter. Inference: 1.6441 s/iter. Eval: 4.1934 s/iter. Total: 5.8407 s/iter. ETA=0:00:17\n",
            "[09/02 20:56:33 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0022 s/iter. Inference: 1.6227 s/iter. Eval: 4.1865 s/iter. Total: 5.8124 s/iter. ETA=0:00:05\n",
            "[09/02 20:56:37 d2.evaluation.evaluator]: Total inference time: 0:14:00.788772 (5.798543 s / iter per device, on 1 devices)\n",
            "[09/02 20:56:37 d2.evaluation.evaluator]: Total inference pure compute time: 0:03:53 (1.612233 s / iter per device, on 1 devices)\n",
            "[09/02 20:56:37 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/02 20:56:37 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/taco10_val/coco_instances_results.json\n",
            "[09/02 20:56:38 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 20:56:38 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/02 20:56:38 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.43 seconds.\n",
            "[09/02 20:56:38 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 20:56:38 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.06 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/02 20:56:38 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/02 20:56:38 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "[09/02 20:56:38 d2.evaluation.coco_evaluation]: Per-category bbox AR: \n",
            "| category              | AR    | category   | AR    | category   | AR    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.46s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 20:56:39 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/02 20:56:39 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.40 seconds.\n",
            "[09/02 20:56:39 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 20:56:39 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.105\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.145\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.111\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.073\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.135\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.248\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.352\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.362\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446\n",
            "[09/02 20:56:39 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 10.470 | 14.530 | 11.093 | 0.524 | 7.288 | 13.535 |\n",
            "[09/02 20:56:39 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 14.241 | Other      | 11.865 | Bottle     | 22.137 |\n",
            "| Bottle cap            | 17.095 | Cup        | 8.485  | Lid        | 2.403  |\n",
            "| Plastic bag + wrapper | 18.525 | Pop tab    | 0.026  | Straw      | 8.974  |\n",
            "| Cigarette             | 0.949  |            |        |            |        |\n",
            "[09/02 20:56:39 d2.evaluation.coco_evaluation]: Per-category segm AR: \n",
            "| category              | AR     | category   | AR     | category   | AR     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 49.048 | Other      | 40.000 | Bottle     | 54.146 |\n",
            "| Bottle cap            | 28.636 | Cup        | 61.200 | Lid        | 36.667 |\n",
            "| Plastic bag + wrapper | 56.087 | Pop tab    | 1.538  | Straw      | 30.000 |\n",
            "| Cigarette             | 5.056  |            |        |            |        |\n",
            "Evaluation results for taco10_val in csv format:\n",
            "[09/02 20:56:39 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/02 20:56:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 20:56:39 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/02 20:56:39 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/02 20:56:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 20:56:39 d2.evaluation.testing]: copypaste: 10.4700,14.5304,11.0933,0.5245,7.2883,13.5347\n",
            "Results (AP metrics):\n",
            "Task: bbox\n",
            "Metric, Value\n",
            "AP, 0.0000\n",
            "AP50, 0.0000\n",
            "AP75, 0.0000\n",
            "APs, 0.0000\n",
            "APm, 0.0000\n",
            "APl, 0.0000\n",
            "AP-Can, 0.0000\n",
            "AP-Other, 0.0000\n",
            "AP-Bottle, 0.0000\n",
            "AP-Bottle cap, 0.0000\n",
            "AP-Cup, 0.0000\n",
            "AP-Lid, 0.0000\n",
            "AP-Plastic bag + wrapper, 0.0000\n",
            "AP-Pop tab, 0.0000\n",
            "AP-Straw, 0.0000\n",
            "AP-Cigarette, 0.0000\n",
            "Task: segm\n",
            "Metric, Value\n",
            "AP, 10.4700\n",
            "AP50, 14.5304\n",
            "AP75, 11.0933\n",
            "APs, 0.5245\n",
            "APm, 7.2883\n",
            "APl, 13.5347\n",
            "AP-Can, 14.2406\n",
            "AP-Other, 11.8647\n",
            "AP-Bottle, 22.1372\n",
            "AP-Bottle cap, 17.0952\n",
            "AP-Cup, 8.4852\n",
            "AP-Lid, 2.4035\n",
            "AP-Plastic bag + wrapper, 18.5251\n",
            "AP-Pop tab, 0.0260\n",
            "AP-Straw, 8.9744\n",
            "AP-Cigarette, 0.9486\n",
            "Recalls (AR metrics):\n",
            "Task: bbox\n",
            "Metric, Value\n",
            "AR-Can, 0.0000\n",
            "AR-Other, 0.0000\n",
            "AR-Bottle, 0.0000\n",
            "AR-Bottle cap, 0.0000\n",
            "AR-Cup, 0.0000\n",
            "AR-Lid, 0.0000\n",
            "AR-Plastic bag + wrapper, 0.0000\n",
            "AR-Pop tab, 0.0000\n",
            "AR-Straw, 0.0000\n",
            "AR-Cigarette, 0.0000\n",
            "Task: segm\n",
            "Metric, Value\n",
            "AR-Can, 49.0476\n",
            "AR-Other, 40.0000\n",
            "AR-Bottle, 54.1463\n",
            "AR-Bottle cap, 28.6364\n",
            "AR-Cup, 61.2000\n",
            "AR-Lid, 36.6667\n",
            "AR-Plastic bag + wrapper, 56.0870\n",
            "AR-Pop tab, 1.5385\n",
            "AR-Straw, 30.0000\n",
            "AR-Cigarette, 5.0562\n",
            "[09/02 20:56:39 d2.utils.events]:  eta: 24 days, 2:24:47  iter: 11999  total_loss: 19.73  loss_ce: 0.7789  loss_mask: 0.07728  loss_dice: 0.8454  loss_ce_0: 1.426  loss_mask_0: 0.08155  loss_dice_0: 0.8027  loss_ce_1: 1.113  loss_mask_1: 0.09264  loss_dice_1: 1.113  loss_ce_2: 0.9186  loss_mask_2: 0.09454  loss_dice_2: 1.012  loss_ce_3: 0.8481  loss_mask_3: 0.07975  loss_dice_3: 0.796  loss_ce_4: 0.8275  loss_mask_4: 0.07572  loss_dice_4: 0.7908  loss_ce_5: 0.7398  loss_mask_5: 0.07436  loss_dice_5: 0.8137  loss_ce_6: 0.7209  loss_mask_6: 0.07208  loss_dice_6: 0.869  loss_ce_7: 0.7334  loss_mask_7: 0.07153  loss_dice_7: 0.91  loss_ce_8: 0.7371  loss_mask_8: 0.07749  loss_dice_8: 1.067     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:56:48 d2.utils.events]:  eta: 5:35:10  iter: 12019  total_loss: 16.5  loss_ce: 0.6397  loss_mask: 0.07149  loss_dice: 0.7793  loss_ce_0: 1.235  loss_mask_0: 0.08063  loss_dice_0: 1.049  loss_ce_1: 0.8028  loss_mask_1: 0.1021  loss_dice_1: 0.9015  loss_ce_2: 0.7871  loss_mask_2: 0.07655  loss_dice_2: 0.6653  loss_ce_3: 0.7286  loss_mask_3: 0.08134  loss_dice_3: 1.085  loss_ce_4: 0.7424  loss_mask_4: 0.0726  loss_dice_4: 0.6236  loss_ce_5: 0.7601  loss_mask_5: 0.07031  loss_dice_5: 0.7224  loss_ce_6: 0.6952  loss_mask_6: 0.07873  loss_dice_6: 0.7367  loss_ce_7: 0.6928  loss_mask_7: 0.08267  loss_dice_7: 0.8108  loss_ce_8: 0.6965  loss_mask_8: 0.0723  loss_dice_8: 0.7817     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:56:56 d2.utils.events]:  eta: 5:31:18  iter: 12039  total_loss: 18.95  loss_ce: 0.5951  loss_mask: 0.1314  loss_dice: 0.8929  loss_ce_0: 1.2  loss_mask_0: 0.1066  loss_dice_0: 0.8812  loss_ce_1: 0.8527  loss_mask_1: 0.1516  loss_dice_1: 0.9208  loss_ce_2: 0.7635  loss_mask_2: 0.1311  loss_dice_2: 0.9655  loss_ce_3: 0.6667  loss_mask_3: 0.147  loss_dice_3: 0.9553  loss_ce_4: 0.6638  loss_mask_4: 0.1517  loss_dice_4: 0.913  loss_ce_5: 0.6811  loss_mask_5: 0.1469  loss_dice_5: 0.8074  loss_ce_6: 0.5735  loss_mask_6: 0.1366  loss_dice_6: 0.8278  loss_ce_7: 0.5883  loss_mask_7: 0.1617  loss_dice_7: 0.869  loss_ce_8: 0.5901  loss_mask_8: 0.1371  loss_dice_8: 0.9113     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:04 d2.utils.events]:  eta: 5:28:31  iter: 12059  total_loss: 23.07  loss_ce: 0.8701  loss_mask: 0.08305  loss_dice: 0.6779  loss_ce_0: 1.354  loss_mask_0: 0.09506  loss_dice_0: 0.8408  loss_ce_1: 0.9491  loss_mask_1: 0.08687  loss_dice_1: 0.7594  loss_ce_2: 0.829  loss_mask_2: 0.07878  loss_dice_2: 0.6652  loss_ce_3: 0.9518  loss_mask_3: 0.06901  loss_dice_3: 0.5766  loss_ce_4: 0.8622  loss_mask_4: 0.07846  loss_dice_4: 0.6805  loss_ce_5: 0.8465  loss_mask_5: 0.09604  loss_dice_5: 0.7401  loss_ce_6: 0.8229  loss_mask_6: 0.08156  loss_dice_6: 0.6518  loss_ce_7: 0.8366  loss_mask_7: 0.08178  loss_dice_7: 0.8018  loss_ce_8: 0.7576  loss_mask_8: 0.09074  loss_dice_8: 0.7353     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:13 d2.utils.events]:  eta: 5:32:53  iter: 12079  total_loss: 24.5  loss_ce: 0.7069  loss_mask: 0.04621  loss_dice: 1.201  loss_ce_0: 1.514  loss_mask_0: 0.05428  loss_dice_0: 1.389  loss_ce_1: 1.027  loss_mask_1: 0.05166  loss_dice_1: 1.081  loss_ce_2: 1.046  loss_mask_2: 0.04182  loss_dice_2: 0.9686  loss_ce_3: 0.9995  loss_mask_3: 0.03908  loss_dice_3: 1.129  loss_ce_4: 0.6619  loss_mask_4: 0.04348  loss_dice_4: 1.079  loss_ce_5: 0.8625  loss_mask_5: 0.04457  loss_dice_5: 1.368  loss_ce_6: 0.7918  loss_mask_6: 0.04235  loss_dice_6: 1.343  loss_ce_7: 0.7809  loss_mask_7: 0.04431  loss_dice_7: 1.257  loss_ce_8: 0.7539  loss_mask_8: 0.04493  loss_dice_8: 1.223     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:21 d2.utils.events]:  eta: 5:32:17  iter: 12099  total_loss: 15.18  loss_ce: 0.5325  loss_mask: 0.09796  loss_dice: 0.4861  loss_ce_0: 1.017  loss_mask_0: 0.09925  loss_dice_0: 0.5766  loss_ce_1: 0.8212  loss_mask_1: 0.1023  loss_dice_1: 0.6299  loss_ce_2: 0.6308  loss_mask_2: 0.1075  loss_dice_2: 0.6407  loss_ce_3: 0.648  loss_mask_3: 0.0881  loss_dice_3: 0.5556  loss_ce_4: 0.5987  loss_mask_4: 0.09465  loss_dice_4: 0.6545  loss_ce_5: 0.6174  loss_mask_5: 0.09998  loss_dice_5: 0.6318  loss_ce_6: 0.5592  loss_mask_6: 0.1001  loss_dice_6: 0.6021  loss_ce_7: 0.648  loss_mask_7: 0.1035  loss_dice_7: 0.6196  loss_ce_8: 0.7206  loss_mask_8: 0.09762  loss_dice_8: 0.6099     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:29 d2.utils.events]:  eta: 5:35:26  iter: 12119  total_loss: 26.97  loss_ce: 0.972  loss_mask: 0.07092  loss_dice: 1.285  loss_ce_0: 1.699  loss_mask_0: 0.07503  loss_dice_0: 1.631  loss_ce_1: 1.289  loss_mask_1: 0.06765  loss_dice_1: 1.305  loss_ce_2: 1.11  loss_mask_2: 0.08155  loss_dice_2: 1.251  loss_ce_3: 1.029  loss_mask_3: 0.07143  loss_dice_3: 1.208  loss_ce_4: 1.052  loss_mask_4: 0.06898  loss_dice_4: 1.29  loss_ce_5: 1.035  loss_mask_5: 0.07165  loss_dice_5: 1.108  loss_ce_6: 0.9523  loss_mask_6: 0.06406  loss_dice_6: 1.274  loss_ce_7: 0.9631  loss_mask_7: 0.05416  loss_dice_7: 1.103  loss_ce_8: 0.9682  loss_mask_8: 0.06666  loss_dice_8: 1.168     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:38 d2.utils.events]:  eta: 5:31:28  iter: 12139  total_loss: 24.22  loss_ce: 1.044  loss_mask: 0.09301  loss_dice: 0.9387  loss_ce_0: 1.486  loss_mask_0: 0.1235  loss_dice_0: 1.194  loss_ce_1: 1.156  loss_mask_1: 0.1362  loss_dice_1: 1.542  loss_ce_2: 1.175  loss_mask_2: 0.1178  loss_dice_2: 0.9465  loss_ce_3: 1.133  loss_mask_3: 0.1093  loss_dice_3: 1.038  loss_ce_4: 1.097  loss_mask_4: 0.1006  loss_dice_4: 0.9316  loss_ce_5: 1.011  loss_mask_5: 0.1067  loss_dice_5: 1.08  loss_ce_6: 1.052  loss_mask_6: 0.09974  loss_dice_6: 0.8352  loss_ce_7: 0.9942  loss_mask_7: 0.1016  loss_dice_7: 0.8905  loss_ce_8: 1.147  loss_mask_8: 0.1062  loss_dice_8: 0.9926     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:46 d2.utils.events]:  eta: 5:33:34  iter: 12159  total_loss: 16.87  loss_ce: 0.6246  loss_mask: 0.05112  loss_dice: 0.6388  loss_ce_0: 1.416  loss_mask_0: 0.1448  loss_dice_0: 0.6468  loss_ce_1: 0.7501  loss_mask_1: 0.1179  loss_dice_1: 0.6047  loss_ce_2: 0.8418  loss_mask_2: 0.1092  loss_dice_2: 0.6477  loss_ce_3: 0.7238  loss_mask_3: 0.09346  loss_dice_3: 0.5517  loss_ce_4: 0.6502  loss_mask_4: 0.0531  loss_dice_4: 0.7634  loss_ce_5: 0.7208  loss_mask_5: 0.06145  loss_dice_5: 0.531  loss_ce_6: 0.6466  loss_mask_6: 0.0487  loss_dice_6: 0.5716  loss_ce_7: 0.6698  loss_mask_7: 0.0485  loss_dice_7: 0.6636  loss_ce_8: 0.6306  loss_mask_8: 0.04817  loss_dice_8: 0.6182     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:57:54 d2.utils.events]:  eta: 5:27:34  iter: 12179  total_loss: 21.02  loss_ce: 0.9039  loss_mask: 0.07729  loss_dice: 0.7213  loss_ce_0: 1.917  loss_mask_0: 0.07383  loss_dice_0: 1.052  loss_ce_1: 1.278  loss_mask_1: 0.08937  loss_dice_1: 0.871  loss_ce_2: 1.161  loss_mask_2: 0.09079  loss_dice_2: 0.9647  loss_ce_3: 1.013  loss_mask_3: 0.08632  loss_dice_3: 0.8004  loss_ce_4: 1.09  loss_mask_4: 0.06972  loss_dice_4: 0.7265  loss_ce_5: 0.9959  loss_mask_5: 0.07297  loss_dice_5: 0.7623  loss_ce_6: 0.9737  loss_mask_6: 0.08435  loss_dice_6: 0.7538  loss_ce_7: 0.9943  loss_mask_7: 0.08007  loss_dice_7: 0.8127  loss_ce_8: 0.9976  loss_mask_8: 0.07573  loss_dice_8: 0.6645     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:03 d2.utils.events]:  eta: 5:35:24  iter: 12199  total_loss: 15.36  loss_ce: 0.6301  loss_mask: 0.04387  loss_dice: 0.5831  loss_ce_0: 1.494  loss_mask_0: 0.07923  loss_dice_0: 0.6555  loss_ce_1: 0.952  loss_mask_1: 0.07749  loss_dice_1: 0.7893  loss_ce_2: 0.8267  loss_mask_2: 0.07469  loss_dice_2: 0.5773  loss_ce_3: 0.7114  loss_mask_3: 0.05392  loss_dice_3: 0.5708  loss_ce_4: 0.6639  loss_mask_4: 0.05055  loss_dice_4: 0.6671  loss_ce_5: 0.6952  loss_mask_5: 0.05063  loss_dice_5: 0.5658  loss_ce_6: 0.6714  loss_mask_6: 0.05357  loss_dice_6: 0.6376  loss_ce_7: 0.714  loss_mask_7: 0.04994  loss_dice_7: 0.4956  loss_ce_8: 0.625  loss_mask_8: 0.04903  loss_dice_8: 0.6557     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:11 d2.utils.events]:  eta: 5:39:42  iter: 12219  total_loss: 22.45  loss_ce: 0.8908  loss_mask: 0.04367  loss_dice: 0.901  loss_ce_0: 1.696  loss_mask_0: 0.06487  loss_dice_0: 1.08  loss_ce_1: 1.197  loss_mask_1: 0.05877  loss_dice_1: 0.9994  loss_ce_2: 0.9085  loss_mask_2: 0.07062  loss_dice_2: 1.146  loss_ce_3: 0.9621  loss_mask_3: 0.05034  loss_dice_3: 0.9801  loss_ce_4: 0.9925  loss_mask_4: 0.05366  loss_dice_4: 1.035  loss_ce_5: 0.9604  loss_mask_5: 0.05305  loss_dice_5: 1.014  loss_ce_6: 0.9468  loss_mask_6: 0.04585  loss_dice_6: 0.9403  loss_ce_7: 0.945  loss_mask_7: 0.04948  loss_dice_7: 0.8256  loss_ce_8: 0.9105  loss_mask_8: 0.04859  loss_dice_8: 0.8849     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:19 d2.utils.events]:  eta: 5:24:10  iter: 12239  total_loss: 21.3  loss_ce: 0.7187  loss_mask: 0.1199  loss_dice: 0.8407  loss_ce_0: 1.55  loss_mask_0: 0.1479  loss_dice_0: 1.313  loss_ce_1: 0.966  loss_mask_1: 0.1129  loss_dice_1: 1.135  loss_ce_2: 0.8719  loss_mask_2: 0.109  loss_dice_2: 1.118  loss_ce_3: 0.8023  loss_mask_3: 0.1255  loss_dice_3: 1.074  loss_ce_4: 0.7124  loss_mask_4: 0.09408  loss_dice_4: 0.9855  loss_ce_5: 0.8544  loss_mask_5: 0.1  loss_dice_5: 1.103  loss_ce_6: 0.7152  loss_mask_6: 0.09874  loss_dice_6: 1.008  loss_ce_7: 0.7219  loss_mask_7: 0.1294  loss_dice_7: 1.227  loss_ce_8: 0.7242  loss_mask_8: 0.1049  loss_dice_8: 0.9632     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:28 d2.utils.events]:  eta: 5:31:29  iter: 12259  total_loss: 17.85  loss_ce: 0.5458  loss_mask: 0.06164  loss_dice: 0.5891  loss_ce_0: 0.9955  loss_mask_0: 0.1412  loss_dice_0: 1.172  loss_ce_1: 0.8352  loss_mask_1: 0.0963  loss_dice_1: 0.8184  loss_ce_2: 0.7651  loss_mask_2: 0.07605  loss_dice_2: 0.7089  loss_ce_3: 0.6578  loss_mask_3: 0.06524  loss_dice_3: 0.7249  loss_ce_4: 0.6245  loss_mask_4: 0.06606  loss_dice_4: 0.7205  loss_ce_5: 0.6051  loss_mask_5: 0.06514  loss_dice_5: 0.7235  loss_ce_6: 0.5326  loss_mask_6: 0.07142  loss_dice_6: 0.7161  loss_ce_7: 0.5717  loss_mask_7: 0.07446  loss_dice_7: 0.7322  loss_ce_8: 0.548  loss_mask_8: 0.06521  loss_dice_8: 0.7101     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:36 d2.utils.events]:  eta: 5:36:17  iter: 12279  total_loss: 10.79  loss_ce: 0.6818  loss_mask: 0.05513  loss_dice: 0.3571  loss_ce_0: 1.156  loss_mask_0: 0.06263  loss_dice_0: 0.6487  loss_ce_1: 0.696  loss_mask_1: 0.05943  loss_dice_1: 0.4805  loss_ce_2: 0.666  loss_mask_2: 0.04989  loss_dice_2: 0.3608  loss_ce_3: 0.6719  loss_mask_3: 0.06104  loss_dice_3: 0.3333  loss_ce_4: 0.634  loss_mask_4: 0.06121  loss_dice_4: 0.3584  loss_ce_5: 0.6525  loss_mask_5: 0.05879  loss_dice_5: 0.3305  loss_ce_6: 0.6595  loss_mask_6: 0.0627  loss_dice_6: 0.3368  loss_ce_7: 0.6751  loss_mask_7: 0.05944  loss_dice_7: 0.3886  loss_ce_8: 0.7032  loss_mask_8: 0.05612  loss_dice_8: 0.3116     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:45 d2.utils.events]:  eta: 5:36:44  iter: 12299  total_loss: 19.77  loss_ce: 0.8239  loss_mask: 0.07179  loss_dice: 0.8051  loss_ce_0: 1.631  loss_mask_0: 0.1027  loss_dice_0: 1.035  loss_ce_1: 1.158  loss_mask_1: 0.07442  loss_dice_1: 1.034  loss_ce_2: 1.182  loss_mask_2: 0.06695  loss_dice_2: 1.041  loss_ce_3: 1.058  loss_mask_3: 0.07288  loss_dice_3: 0.8094  loss_ce_4: 0.8327  loss_mask_4: 0.07284  loss_dice_4: 0.8832  loss_ce_5: 0.8538  loss_mask_5: 0.08566  loss_dice_5: 0.9115  loss_ce_6: 0.8273  loss_mask_6: 0.07535  loss_dice_6: 0.9077  loss_ce_7: 0.8388  loss_mask_7: 0.06625  loss_dice_7: 0.834  loss_ce_8: 0.8194  loss_mask_8: 0.07588  loss_dice_8: 0.8809     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:58:53 d2.utils.events]:  eta: 5:37:55  iter: 12319  total_loss: 11.92  loss_ce: 0.4649  loss_mask: 0.07323  loss_dice: 0.4112  loss_ce_0: 1.068  loss_mask_0: 0.08529  loss_dice_0: 0.619  loss_ce_1: 0.6387  loss_mask_1: 0.1135  loss_dice_1: 0.3743  loss_ce_2: 0.537  loss_mask_2: 0.07245  loss_dice_2: 0.4198  loss_ce_3: 0.589  loss_mask_3: 0.07747  loss_dice_3: 0.4506  loss_ce_4: 0.5271  loss_mask_4: 0.07502  loss_dice_4: 0.4968  loss_ce_5: 0.4902  loss_mask_5: 0.07904  loss_dice_5: 0.4803  loss_ce_6: 0.4593  loss_mask_6: 0.07711  loss_dice_6: 0.4325  loss_ce_7: 0.4506  loss_mask_7: 0.07782  loss_dice_7: 0.3549  loss_ce_8: 0.3851  loss_mask_8: 0.07881  loss_dice_8: 0.4468     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:02 d2.utils.events]:  eta: 5:36:55  iter: 12339  total_loss: 15.21  loss_ce: 0.6181  loss_mask: 0.1056  loss_dice: 0.5571  loss_ce_0: 1.192  loss_mask_0: 0.07388  loss_dice_0: 0.889  loss_ce_1: 0.8178  loss_mask_1: 0.1096  loss_dice_1: 0.6684  loss_ce_2: 0.7988  loss_mask_2: 0.117  loss_dice_2: 0.666  loss_ce_3: 0.6808  loss_mask_3: 0.08886  loss_dice_3: 0.6891  loss_ce_4: 0.6265  loss_mask_4: 0.08216  loss_dice_4: 0.5071  loss_ce_5: 0.6719  loss_mask_5: 0.07881  loss_dice_5: 0.7534  loss_ce_6: 0.648  loss_mask_6: 0.08812  loss_dice_6: 0.8328  loss_ce_7: 0.591  loss_mask_7: 0.08177  loss_dice_7: 0.7022  loss_ce_8: 0.6221  loss_mask_8: 0.08043  loss_dice_8: 0.7462     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:10 d2.utils.events]:  eta: 5:33:34  iter: 12359  total_loss: 15.43  loss_ce: 0.8046  loss_mask: 0.0575  loss_dice: 0.5901  loss_ce_0: 1.497  loss_mask_0: 0.06871  loss_dice_0: 0.9439  loss_ce_1: 0.8401  loss_mask_1: 0.06699  loss_dice_1: 0.6365  loss_ce_2: 0.8525  loss_mask_2: 0.0677  loss_dice_2: 0.6342  loss_ce_3: 0.8689  loss_mask_3: 0.07675  loss_dice_3: 0.757  loss_ce_4: 0.698  loss_mask_4: 0.06735  loss_dice_4: 0.7229  loss_ce_5: 0.8228  loss_mask_5: 0.06614  loss_dice_5: 0.5535  loss_ce_6: 0.7581  loss_mask_6: 0.06406  loss_dice_6: 0.5105  loss_ce_7: 0.8308  loss_mask_7: 0.0643  loss_dice_7: 0.5055  loss_ce_8: 0.8428  loss_mask_8: 0.0701  loss_dice_8: 0.629     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:18 d2.utils.events]:  eta: 5:25:23  iter: 12379  total_loss: 15.8  loss_ce: 0.7712  loss_mask: 0.05796  loss_dice: 0.8788  loss_ce_0: 1.613  loss_mask_0: 0.06597  loss_dice_0: 1.001  loss_ce_1: 1.148  loss_mask_1: 0.07215  loss_dice_1: 0.5279  loss_ce_2: 0.9064  loss_mask_2: 0.06272  loss_dice_2: 0.6847  loss_ce_3: 0.8391  loss_mask_3: 0.05404  loss_dice_3: 0.7235  loss_ce_4: 0.8441  loss_mask_4: 0.0552  loss_dice_4: 0.5203  loss_ce_5: 0.8395  loss_mask_5: 0.05113  loss_dice_5: 0.6177  loss_ce_6: 0.8515  loss_mask_6: 0.05615  loss_dice_6: 0.7668  loss_ce_7: 0.835  loss_mask_7: 0.05984  loss_dice_7: 0.5682  loss_ce_8: 0.8134  loss_mask_8: 0.06043  loss_dice_8: 0.6919     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:27 d2.utils.events]:  eta: 5:32:09  iter: 12399  total_loss: 19.74  loss_ce: 0.9858  loss_mask: 0.08011  loss_dice: 0.7603  loss_ce_0: 1.609  loss_mask_0: 0.08484  loss_dice_0: 0.966  loss_ce_1: 1.333  loss_mask_1: 0.07948  loss_dice_1: 0.9376  loss_ce_2: 1.151  loss_mask_2: 0.07603  loss_dice_2: 0.9033  loss_ce_3: 0.9809  loss_mask_3: 0.07881  loss_dice_3: 0.8002  loss_ce_4: 0.9396  loss_mask_4: 0.08198  loss_dice_4: 0.7693  loss_ce_5: 0.9802  loss_mask_5: 0.09289  loss_dice_5: 0.8362  loss_ce_6: 0.9699  loss_mask_6: 0.07788  loss_dice_6: 0.7883  loss_ce_7: 0.9421  loss_mask_7: 0.07777  loss_dice_7: 0.9528  loss_ce_8: 0.9563  loss_mask_8: 0.08076  loss_dice_8: 0.8017     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:35 d2.utils.events]:  eta: 5:35:20  iter: 12419  total_loss: 14.26  loss_ce: 0.4675  loss_mask: 0.05549  loss_dice: 0.6128  loss_ce_0: 1.15  loss_mask_0: 0.05309  loss_dice_0: 0.8278  loss_ce_1: 0.7729  loss_mask_1: 0.06086  loss_dice_1: 0.7991  loss_ce_2: 0.6731  loss_mask_2: 0.07092  loss_dice_2: 0.7185  loss_ce_3: 0.6172  loss_mask_3: 0.04966  loss_dice_3: 0.8811  loss_ce_4: 0.5217  loss_mask_4: 0.05222  loss_dice_4: 0.9158  loss_ce_5: 0.6379  loss_mask_5: 0.06125  loss_dice_5: 0.8137  loss_ce_6: 0.4722  loss_mask_6: 0.0555  loss_dice_6: 0.6392  loss_ce_7: 0.4695  loss_mask_7: 0.0539  loss_dice_7: 0.6894  loss_ce_8: 0.4596  loss_mask_8: 0.05449  loss_dice_8: 0.7017     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:44 d2.utils.events]:  eta: 5:34:32  iter: 12439  total_loss: 18.88  loss_ce: 0.6802  loss_mask: 0.0912  loss_dice: 0.7415  loss_ce_0: 1.661  loss_mask_0: 0.1126  loss_dice_0: 0.9755  loss_ce_1: 1.003  loss_mask_1: 0.0835  loss_dice_1: 0.7463  loss_ce_2: 0.875  loss_mask_2: 0.09909  loss_dice_2: 0.7424  loss_ce_3: 0.7774  loss_mask_3: 0.09232  loss_dice_3: 0.9568  loss_ce_4: 0.6851  loss_mask_4: 0.1114  loss_dice_4: 0.8099  loss_ce_5: 0.6683  loss_mask_5: 0.1185  loss_dice_5: 0.8053  loss_ce_6: 0.6952  loss_mask_6: 0.09513  loss_dice_6: 0.7335  loss_ce_7: 0.6503  loss_mask_7: 0.09611  loss_dice_7: 0.5948  loss_ce_8: 0.6354  loss_mask_8: 0.09549  loss_dice_8: 0.7199     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 20:59:52 d2.utils.events]:  eta: 5:41:17  iter: 12459  total_loss: 17.87  loss_ce: 0.6937  loss_mask: 0.05882  loss_dice: 0.6919  loss_ce_0: 1.566  loss_mask_0: 0.0901  loss_dice_0: 0.9813  loss_ce_1: 1.046  loss_mask_1: 0.08962  loss_dice_1: 0.9032  loss_ce_2: 0.8199  loss_mask_2: 0.07193  loss_dice_2: 0.8224  loss_ce_3: 0.8273  loss_mask_3: 0.06546  loss_dice_3: 0.7754  loss_ce_4: 0.7651  loss_mask_4: 0.05627  loss_dice_4: 0.8248  loss_ce_5: 0.8131  loss_mask_5: 0.06434  loss_dice_5: 0.7898  loss_ce_6: 0.7615  loss_mask_6: 0.05856  loss_dice_6: 0.7402  loss_ce_7: 0.7391  loss_mask_7: 0.06052  loss_dice_7: 0.8182  loss_ce_8: 0.7032  loss_mask_8: 0.06405  loss_dice_8: 0.6698     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:00 d2.utils.events]:  eta: 5:24:47  iter: 12479  total_loss: 19.69  loss_ce: 0.5492  loss_mask: 0.09123  loss_dice: 0.8322  loss_ce_0: 1.245  loss_mask_0: 0.131  loss_dice_0: 1.171  loss_ce_1: 0.8745  loss_mask_1: 0.1339  loss_dice_1: 0.8774  loss_ce_2: 0.741  loss_mask_2: 0.09747  loss_dice_2: 0.8823  loss_ce_3: 0.7325  loss_mask_3: 0.08943  loss_dice_3: 0.6564  loss_ce_4: 0.7782  loss_mask_4: 0.09267  loss_dice_4: 0.8346  loss_ce_5: 0.5487  loss_mask_5: 0.1175  loss_dice_5: 0.8226  loss_ce_6: 0.7021  loss_mask_6: 0.09488  loss_dice_6: 0.836  loss_ce_7: 0.5658  loss_mask_7: 0.1054  loss_dice_7: 0.8185  loss_ce_8: 0.542  loss_mask_8: 0.08661  loss_dice_8: 0.8578     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:09 d2.utils.events]:  eta: 5:23:33  iter: 12499  total_loss: 20.51  loss_ce: 0.9183  loss_mask: 0.08536  loss_dice: 0.8452  loss_ce_0: 1.549  loss_mask_0: 0.1123  loss_dice_0: 1.002  loss_ce_1: 1.098  loss_mask_1: 0.1088  loss_dice_1: 1.186  loss_ce_2: 1.071  loss_mask_2: 0.1133  loss_dice_2: 0.9444  loss_ce_3: 1.028  loss_mask_3: 0.09092  loss_dice_3: 0.9759  loss_ce_4: 1.109  loss_mask_4: 0.09847  loss_dice_4: 0.8925  loss_ce_5: 0.8725  loss_mask_5: 0.09932  loss_dice_5: 0.9432  loss_ce_6: 0.9159  loss_mask_6: 0.09213  loss_dice_6: 0.7072  loss_ce_7: 0.9681  loss_mask_7: 0.08361  loss_dice_7: 0.7621  loss_ce_8: 0.8854  loss_mask_8: 0.0865  loss_dice_8: 0.8419     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:17 d2.utils.events]:  eta: 5:32:02  iter: 12519  total_loss: 18.05  loss_ce: 0.7009  loss_mask: 0.06049  loss_dice: 0.6339  loss_ce_0: 1.16  loss_mask_0: 0.07462  loss_dice_0: 0.8751  loss_ce_1: 0.941  loss_mask_1: 0.06986  loss_dice_1: 0.8448  loss_ce_2: 0.7259  loss_mask_2: 0.08665  loss_dice_2: 0.8727  loss_ce_3: 0.6767  loss_mask_3: 0.05992  loss_dice_3: 0.7815  loss_ce_4: 0.6693  loss_mask_4: 0.07357  loss_dice_4: 0.7141  loss_ce_5: 0.6741  loss_mask_5: 0.06211  loss_dice_5: 0.8383  loss_ce_6: 0.6921  loss_mask_6: 0.06104  loss_dice_6: 0.6961  loss_ce_7: 0.6908  loss_mask_7: 0.06452  loss_dice_7: 0.8355  loss_ce_8: 0.6747  loss_mask_8: 0.06877  loss_dice_8: 0.6751     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:25 d2.utils.events]:  eta: 5:37:33  iter: 12539  total_loss: 26.44  loss_ce: 0.8999  loss_mask: 0.1167  loss_dice: 1.149  loss_ce_0: 1.485  loss_mask_0: 0.1337  loss_dice_0: 1.527  loss_ce_1: 1.089  loss_mask_1: 0.1705  loss_dice_1: 1.56  loss_ce_2: 0.9913  loss_mask_2: 0.1336  loss_dice_2: 1.312  loss_ce_3: 0.9061  loss_mask_3: 0.1243  loss_dice_3: 1.256  loss_ce_4: 0.8866  loss_mask_4: 0.0974  loss_dice_4: 1.105  loss_ce_5: 0.8601  loss_mask_5: 0.1045  loss_dice_5: 1.228  loss_ce_6: 0.9628  loss_mask_6: 0.09995  loss_dice_6: 1.095  loss_ce_7: 0.874  loss_mask_7: 0.1073  loss_dice_7: 1.196  loss_ce_8: 0.9076  loss_mask_8: 0.1145  loss_dice_8: 1.11     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:34 d2.utils.events]:  eta: 5:29:34  iter: 12559  total_loss: 19.27  loss_ce: 0.6032  loss_mask: 0.05953  loss_dice: 1  loss_ce_0: 1.097  loss_mask_0: 0.09117  loss_dice_0: 1.164  loss_ce_1: 0.8049  loss_mask_1: 0.0738  loss_dice_1: 0.9944  loss_ce_2: 0.8108  loss_mask_2: 0.06574  loss_dice_2: 0.9668  loss_ce_3: 0.6052  loss_mask_3: 0.06297  loss_dice_3: 0.966  loss_ce_4: 0.5853  loss_mask_4: 0.0744  loss_dice_4: 0.7494  loss_ce_5: 0.557  loss_mask_5: 0.06817  loss_dice_5: 0.9237  loss_ce_6: 0.5447  loss_mask_6: 0.06369  loss_dice_6: 0.8651  loss_ce_7: 0.4738  loss_mask_7: 0.05775  loss_dice_7: 0.9739  loss_ce_8: 0.4766  loss_mask_8: 0.06049  loss_dice_8: 0.7658     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:42 d2.utils.events]:  eta: 5:24:15  iter: 12579  total_loss: 30.26  loss_ce: 0.9445  loss_mask: 0.1896  loss_dice: 1.267  loss_ce_0: 1.44  loss_mask_0: 0.1621  loss_dice_0: 1.412  loss_ce_1: 1.299  loss_mask_1: 0.247  loss_dice_1: 1.318  loss_ce_2: 1.15  loss_mask_2: 0.2473  loss_dice_2: 1.425  loss_ce_3: 1.113  loss_mask_3: 0.2209  loss_dice_3: 1.485  loss_ce_4: 1.055  loss_mask_4: 0.2037  loss_dice_4: 1.368  loss_ce_5: 1.011  loss_mask_5: 0.2048  loss_dice_5: 1.335  loss_ce_6: 0.9681  loss_mask_6: 0.183  loss_dice_6: 1.272  loss_ce_7: 0.9259  loss_mask_7: 0.1932  loss_dice_7: 1.367  loss_ce_8: 0.9833  loss_mask_8: 0.1983  loss_dice_8: 1.398     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 21.0 - Losses: {'loss_ce': tensor(0.2860, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.1480, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.1818, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(0.9511, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.1155, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(0.1807, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.3786, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.4335, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.5611, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.4351, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.1379, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.2171, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.3671, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.1582, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.2068, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.3237, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.1606, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.2056, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.3458, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.1634, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.2111, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.3060, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.1366, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.1842, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.2997, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.1368, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.1835, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.2898, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.1466, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.1912, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:00:50 d2.utils.events]:  eta: 5:25:21  iter: 12599  total_loss: 18.32  loss_ce: 0.6251  loss_mask: 0.1159  loss_dice: 0.8671  loss_ce_0: 1.28  loss_mask_0: 0.1187  loss_dice_0: 1.066  loss_ce_1: 0.9736  loss_mask_1: 0.1878  loss_dice_1: 0.7365  loss_ce_2: 0.6854  loss_mask_2: 0.1566  loss_dice_2: 0.8545  loss_ce_3: 0.7163  loss_mask_3: 0.1332  loss_dice_3: 0.6815  loss_ce_4: 0.6029  loss_mask_4: 0.1179  loss_dice_4: 0.7978  loss_ce_5: 0.57  loss_mask_5: 0.1256  loss_dice_5: 0.5908  loss_ce_6: 0.5598  loss_mask_6: 0.1095  loss_dice_6: 0.7633  loss_ce_7: 0.636  loss_mask_7: 0.1166  loss_dice_7: 0.5822  loss_ce_8: 0.6221  loss_mask_8: 0.1076  loss_dice_8: 0.8144     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:00:59 d2.utils.events]:  eta: 5:28:10  iter: 12619  total_loss: 19.21  loss_ce: 0.7671  loss_mask: 0.06279  loss_dice: 0.6381  loss_ce_0: 1.673  loss_mask_0: 0.07579  loss_dice_0: 1.052  loss_ce_1: 1.073  loss_mask_1: 0.07045  loss_dice_1: 0.8532  loss_ce_2: 1.047  loss_mask_2: 0.07015  loss_dice_2: 1.014  loss_ce_3: 0.9356  loss_mask_3: 0.05443  loss_dice_3: 0.937  loss_ce_4: 0.9112  loss_mask_4: 0.06675  loss_dice_4: 0.7878  loss_ce_5: 0.8616  loss_mask_5: 0.0618  loss_dice_5: 0.6963  loss_ce_6: 0.8015  loss_mask_6: 0.06451  loss_dice_6: 0.8234  loss_ce_7: 0.7761  loss_mask_7: 0.07043  loss_dice_7: 0.7918  loss_ce_8: 0.7772  loss_mask_8: 0.07059  loss_dice_8: 0.6809     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:07 d2.utils.events]:  eta: 5:29:27  iter: 12639  total_loss: 17.27  loss_ce: 0.7869  loss_mask: 0.0397  loss_dice: 0.7573  loss_ce_0: 1.422  loss_mask_0: 0.05409  loss_dice_0: 0.851  loss_ce_1: 0.9017  loss_mask_1: 0.05278  loss_dice_1: 0.8592  loss_ce_2: 0.9262  loss_mask_2: 0.04662  loss_dice_2: 0.936  loss_ce_3: 0.7653  loss_mask_3: 0.04361  loss_dice_3: 0.9327  loss_ce_4: 0.7906  loss_mask_4: 0.04348  loss_dice_4: 0.8813  loss_ce_5: 0.7906  loss_mask_5: 0.0465  loss_dice_5: 0.9805  loss_ce_6: 0.7047  loss_mask_6: 0.04474  loss_dice_6: 0.7809  loss_ce_7: 0.7663  loss_mask_7: 0.03873  loss_dice_7: 0.8783  loss_ce_8: 0.8268  loss_mask_8: 0.03994  loss_dice_8: 0.7753     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:15 d2.utils.events]:  eta: 5:27:44  iter: 12659  total_loss: 14.72  loss_ce: 0.5677  loss_mask: 0.09116  loss_dice: 0.5649  loss_ce_0: 1.157  loss_mask_0: 0.07787  loss_dice_0: 0.8329  loss_ce_1: 0.7071  loss_mask_1: 0.09621  loss_dice_1: 0.5857  loss_ce_2: 0.7211  loss_mask_2: 0.0905  loss_dice_2: 0.6538  loss_ce_3: 0.701  loss_mask_3: 0.08208  loss_dice_3: 0.6657  loss_ce_4: 0.6703  loss_mask_4: 0.09395  loss_dice_4: 0.6178  loss_ce_5: 0.4807  loss_mask_5: 0.1071  loss_dice_5: 0.6507  loss_ce_6: 0.5005  loss_mask_6: 0.1078  loss_dice_6: 0.6408  loss_ce_7: 0.6949  loss_mask_7: 0.09599  loss_dice_7: 0.5897  loss_ce_8: 0.5733  loss_mask_8: 0.06623  loss_dice_8: 0.6031     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:24 d2.utils.events]:  eta: 5:28:26  iter: 12679  total_loss: 18.59  loss_ce: 0.7804  loss_mask: 0.049  loss_dice: 0.702  loss_ce_0: 1.107  loss_mask_0: 0.05246  loss_dice_0: 1.266  loss_ce_1: 0.7868  loss_mask_1: 0.05922  loss_dice_1: 0.8921  loss_ce_2: 0.823  loss_mask_2: 0.0473  loss_dice_2: 0.7763  loss_ce_3: 0.7665  loss_mask_3: 0.05314  loss_dice_3: 0.6249  loss_ce_4: 0.7313  loss_mask_4: 0.0445  loss_dice_4: 0.8798  loss_ce_5: 0.7786  loss_mask_5: 0.0459  loss_dice_5: 0.7782  loss_ce_6: 0.759  loss_mask_6: 0.0434  loss_dice_6: 0.7731  loss_ce_7: 0.7501  loss_mask_7: 0.044  loss_dice_7: 0.79  loss_ce_8: 0.6936  loss_mask_8: 0.0502  loss_dice_8: 0.7898     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:32 d2.utils.events]:  eta: 5:22:47  iter: 12699  total_loss: 27.21  loss_ce: 0.8238  loss_mask: 0.06657  loss_dice: 1.118  loss_ce_0: 1.39  loss_mask_0: 0.09644  loss_dice_0: 1.043  loss_ce_1: 1.251  loss_mask_1: 0.09864  loss_dice_1: 1.495  loss_ce_2: 1.061  loss_mask_2: 0.07596  loss_dice_2: 1.49  loss_ce_3: 0.9892  loss_mask_3: 0.09768  loss_dice_3: 1.033  loss_ce_4: 1.024  loss_mask_4: 0.07738  loss_dice_4: 1.336  loss_ce_5: 1.075  loss_mask_5: 0.06526  loss_dice_5: 1.308  loss_ce_6: 0.8659  loss_mask_6: 0.06746  loss_dice_6: 1.207  loss_ce_7: 0.865  loss_mask_7: 0.07261  loss_dice_7: 1.399  loss_ce_8: 0.8344  loss_mask_8: 0.06227  loss_dice_8: 1.129     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:40 d2.utils.events]:  eta: 5:31:24  iter: 12719  total_loss: 16.68  loss_ce: 0.7752  loss_mask: 0.07507  loss_dice: 0.7678  loss_ce_0: 1.255  loss_mask_0: 0.1014  loss_dice_0: 0.868  loss_ce_1: 0.8197  loss_mask_1: 0.08575  loss_dice_1: 0.9064  loss_ce_2: 0.7988  loss_mask_2: 0.06815  loss_dice_2: 0.6263  loss_ce_3: 0.7758  loss_mask_3: 0.07479  loss_dice_3: 0.816  loss_ce_4: 0.7309  loss_mask_4: 0.07958  loss_dice_4: 0.5947  loss_ce_5: 0.7227  loss_mask_5: 0.07433  loss_dice_5: 0.7068  loss_ce_6: 0.6786  loss_mask_6: 0.07333  loss_dice_6: 0.7807  loss_ce_7: 0.6882  loss_mask_7: 0.07796  loss_dice_7: 0.7586  loss_ce_8: 0.7661  loss_mask_8: 0.07008  loss_dice_8: 0.7106     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:48 d2.utils.events]:  eta: 5:19:11  iter: 12739  total_loss: 20.06  loss_ce: 0.7638  loss_mask: 0.07483  loss_dice: 0.6903  loss_ce_0: 1.445  loss_mask_0: 0.05867  loss_dice_0: 1.003  loss_ce_1: 1.068  loss_mask_1: 0.05961  loss_dice_1: 0.9203  loss_ce_2: 0.9089  loss_mask_2: 0.07153  loss_dice_2: 0.8787  loss_ce_3: 0.7529  loss_mask_3: 0.06603  loss_dice_3: 0.7702  loss_ce_4: 0.8433  loss_mask_4: 0.07725  loss_dice_4: 0.6418  loss_ce_5: 0.8322  loss_mask_5: 0.07734  loss_dice_5: 0.6507  loss_ce_6: 0.8473  loss_mask_6: 0.08719  loss_dice_6: 0.8251  loss_ce_7: 0.8227  loss_mask_7: 0.0748  loss_dice_7: 0.6239  loss_ce_8: 0.8018  loss_mask_8: 0.06916  loss_dice_8: 0.6547     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:01:57 d2.utils.events]:  eta: 5:35:53  iter: 12759  total_loss: 19.08  loss_ce: 0.6688  loss_mask: 0.07389  loss_dice: 0.7817  loss_ce_0: 1.205  loss_mask_0: 0.06928  loss_dice_0: 1.163  loss_ce_1: 0.9723  loss_mask_1: 0.06674  loss_dice_1: 0.8617  loss_ce_2: 0.8336  loss_mask_2: 0.06519  loss_dice_2: 0.8719  loss_ce_3: 0.7263  loss_mask_3: 0.06387  loss_dice_3: 0.6449  loss_ce_4: 0.7439  loss_mask_4: 0.07879  loss_dice_4: 0.6582  loss_ce_5: 0.7431  loss_mask_5: 0.06479  loss_dice_5: 0.6587  loss_ce_6: 0.6716  loss_mask_6: 0.079  loss_dice_6: 0.6218  loss_ce_7: 0.7301  loss_mask_7: 0.05773  loss_dice_7: 0.88  loss_ce_8: 0.6955  loss_mask_8: 0.06827  loss_dice_8: 0.9182     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:05 d2.utils.events]:  eta: 5:27:36  iter: 12779  total_loss: 19.15  loss_ce: 0.6264  loss_mask: 0.1104  loss_dice: 0.7304  loss_ce_0: 1.291  loss_mask_0: 0.1152  loss_dice_0: 1.058  loss_ce_1: 0.9768  loss_mask_1: 0.1198  loss_dice_1: 0.9029  loss_ce_2: 0.9611  loss_mask_2: 0.1059  loss_dice_2: 0.8474  loss_ce_3: 0.851  loss_mask_3: 0.1157  loss_dice_3: 0.8018  loss_ce_4: 0.784  loss_mask_4: 0.1043  loss_dice_4: 0.8416  loss_ce_5: 0.7844  loss_mask_5: 0.1168  loss_dice_5: 1.004  loss_ce_6: 0.6805  loss_mask_6: 0.1054  loss_dice_6: 0.856  loss_ce_7: 0.6498  loss_mask_7: 0.1227  loss_dice_7: 0.7612  loss_ce_8: 0.666  loss_mask_8: 0.1008  loss_dice_8: 0.7266     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:13 d2.utils.events]:  eta: 5:23:54  iter: 12799  total_loss: 22.26  loss_ce: 0.9311  loss_mask: 0.142  loss_dice: 0.8645  loss_ce_0: 1.397  loss_mask_0: 0.1997  loss_dice_0: 0.8338  loss_ce_1: 0.9904  loss_mask_1: 0.1683  loss_dice_1: 0.8854  loss_ce_2: 0.9673  loss_mask_2: 0.1877  loss_dice_2: 0.7953  loss_ce_3: 0.9988  loss_mask_3: 0.1439  loss_dice_3: 0.8089  loss_ce_4: 0.9655  loss_mask_4: 0.1558  loss_dice_4: 0.9172  loss_ce_5: 0.9241  loss_mask_5: 0.1715  loss_dice_5: 0.9577  loss_ce_6: 0.8843  loss_mask_6: 0.149  loss_dice_6: 0.7563  loss_ce_7: 0.9205  loss_mask_7: 0.1526  loss_dice_7: 1  loss_ce_8: 0.9446  loss_mask_8: 0.1536  loss_dice_8: 0.71     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:22 d2.utils.events]:  eta: 5:27:21  iter: 12819  total_loss: 18.8  loss_ce: 0.7048  loss_mask: 0.1024  loss_dice: 0.541  loss_ce_0: 1.076  loss_mask_0: 0.09386  loss_dice_0: 1.055  loss_ce_1: 0.764  loss_mask_1: 0.1256  loss_dice_1: 0.6882  loss_ce_2: 0.7431  loss_mask_2: 0.1005  loss_dice_2: 0.8001  loss_ce_3: 0.7578  loss_mask_3: 0.0951  loss_dice_3: 0.9777  loss_ce_4: 0.7275  loss_mask_4: 0.09948  loss_dice_4: 0.8244  loss_ce_5: 0.739  loss_mask_5: 0.09794  loss_dice_5: 0.6475  loss_ce_6: 0.7198  loss_mask_6: 0.08632  loss_dice_6: 0.6333  loss_ce_7: 0.7229  loss_mask_7: 0.0897  loss_dice_7: 0.7003  loss_ce_8: 0.7013  loss_mask_8: 0.09538  loss_dice_8: 0.7441     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:30 d2.utils.events]:  eta: 5:33:21  iter: 12839  total_loss: 17.03  loss_ce: 0.6408  loss_mask: 0.163  loss_dice: 0.5887  loss_ce_0: 1.193  loss_mask_0: 0.2271  loss_dice_0: 1.04  loss_ce_1: 0.8251  loss_mask_1: 0.1775  loss_dice_1: 0.7013  loss_ce_2: 0.7699  loss_mask_2: 0.132  loss_dice_2: 0.669  loss_ce_3: 0.7736  loss_mask_3: 0.1426  loss_dice_3: 0.5245  loss_ce_4: 0.7159  loss_mask_4: 0.1552  loss_dice_4: 0.6921  loss_ce_5: 0.6782  loss_mask_5: 0.1623  loss_dice_5: 0.5632  loss_ce_6: 0.6769  loss_mask_6: 0.1331  loss_dice_6: 0.5308  loss_ce_7: 0.6146  loss_mask_7: 0.1628  loss_dice_7: 0.4864  loss_ce_8: 0.6934  loss_mask_8: 0.1614  loss_dice_8: 0.629     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:39 d2.utils.events]:  eta: 5:39:11  iter: 12859  total_loss: 21.33  loss_ce: 0.7945  loss_mask: 0.1165  loss_dice: 0.5994  loss_ce_0: 1.332  loss_mask_0: 0.1273  loss_dice_0: 0.7257  loss_ce_1: 1.053  loss_mask_1: 0.128  loss_dice_1: 0.9036  loss_ce_2: 0.9397  loss_mask_2: 0.1429  loss_dice_2: 1.089  loss_ce_3: 0.8793  loss_mask_3: 0.1486  loss_dice_3: 0.8279  loss_ce_4: 0.7776  loss_mask_4: 0.1261  loss_dice_4: 1.056  loss_ce_5: 0.8114  loss_mask_5: 0.1073  loss_dice_5: 0.7675  loss_ce_6: 0.8405  loss_mask_6: 0.1021  loss_dice_6: 0.9597  loss_ce_7: 0.7959  loss_mask_7: 0.09595  loss_dice_7: 0.849  loss_ce_8: 0.8306  loss_mask_8: 0.1268  loss_dice_8: 0.9759     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:47 d2.utils.events]:  eta: 5:25:29  iter: 12879  total_loss: 24.39  loss_ce: 0.8012  loss_mask: 0.09017  loss_dice: 0.7839  loss_ce_0: 1.948  loss_mask_0: 0.1307  loss_dice_0: 1.028  loss_ce_1: 1.263  loss_mask_1: 0.1043  loss_dice_1: 0.9321  loss_ce_2: 1.089  loss_mask_2: 0.09943  loss_dice_2: 0.9211  loss_ce_3: 1.045  loss_mask_3: 0.09366  loss_dice_3: 0.8517  loss_ce_4: 0.9357  loss_mask_4: 0.1022  loss_dice_4: 1.084  loss_ce_5: 0.9529  loss_mask_5: 0.1089  loss_dice_5: 0.727  loss_ce_6: 0.8709  loss_mask_6: 0.08811  loss_dice_6: 0.7235  loss_ce_7: 0.9469  loss_mask_7: 0.09438  loss_dice_7: 1.01  loss_ce_8: 0.8868  loss_mask_8: 0.09941  loss_dice_8: 0.8253     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:02:55 d2.utils.events]:  eta: 5:23:32  iter: 12899  total_loss: 18.41  loss_ce: 0.5751  loss_mask: 0.03468  loss_dice: 0.6877  loss_ce_0: 1.414  loss_mask_0: 0.053  loss_dice_0: 1.101  loss_ce_1: 0.9038  loss_mask_1: 0.04184  loss_dice_1: 0.9874  loss_ce_2: 0.804  loss_mask_2: 0.04491  loss_dice_2: 0.8683  loss_ce_3: 0.7974  loss_mask_3: 0.04166  loss_dice_3: 0.8332  loss_ce_4: 0.7464  loss_mask_4: 0.03759  loss_dice_4: 1.093  loss_ce_5: 0.6793  loss_mask_5: 0.03885  loss_dice_5: 0.7794  loss_ce_6: 0.7362  loss_mask_6: 0.03474  loss_dice_6: 0.8113  loss_ce_7: 0.7891  loss_mask_7: 0.03611  loss_dice_7: 0.7353  loss_ce_8: 0.6928  loss_mask_8: 0.03986  loss_dice_8: 0.8255     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:04 d2.utils.events]:  eta: 5:29:03  iter: 12919  total_loss: 17.03  loss_ce: 0.5511  loss_mask: 0.06355  loss_dice: 0.8433  loss_ce_0: 1.135  loss_mask_0: 0.09783  loss_dice_0: 1.15  loss_ce_1: 0.734  loss_mask_1: 0.06482  loss_dice_1: 1.023  loss_ce_2: 0.7015  loss_mask_2: 0.06272  loss_dice_2: 0.9465  loss_ce_3: 0.5404  loss_mask_3: 0.07181  loss_dice_3: 1.1  loss_ce_4: 0.5367  loss_mask_4: 0.07101  loss_dice_4: 0.9929  loss_ce_5: 0.5224  loss_mask_5: 0.07645  loss_dice_5: 0.9233  loss_ce_6: 0.523  loss_mask_6: 0.0718  loss_dice_6: 0.8786  loss_ce_7: 0.4921  loss_mask_7: 0.06817  loss_dice_7: 0.9062  loss_ce_8: 0.4856  loss_mask_8: 0.07034  loss_dice_8: 0.8759     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:12 d2.utils.events]:  eta: 5:27:55  iter: 12939  total_loss: 24.59  loss_ce: 1.063  loss_mask: 0.07177  loss_dice: 1.18  loss_ce_0: 2.428  loss_mask_0: 0.09748  loss_dice_0: 1.256  loss_ce_1: 1.385  loss_mask_1: 0.08882  loss_dice_1: 1.069  loss_ce_2: 1.406  loss_mask_2: 0.07487  loss_dice_2: 1.198  loss_ce_3: 1.233  loss_mask_3: 0.07916  loss_dice_3: 1.325  loss_ce_4: 1.161  loss_mask_4: 0.07134  loss_dice_4: 1.041  loss_ce_5: 1.097  loss_mask_5: 0.07733  loss_dice_5: 1.012  loss_ce_6: 1.059  loss_mask_6: 0.07579  loss_dice_6: 1.227  loss_ce_7: 1.063  loss_mask_7: 0.06451  loss_dice_7: 1.139  loss_ce_8: 1.012  loss_mask_8: 0.07061  loss_dice_8: 1.076     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:20 d2.utils.events]:  eta: 5:25:50  iter: 12959  total_loss: 13.59  loss_ce: 0.574  loss_mask: 0.08732  loss_dice: 0.4833  loss_ce_0: 0.9796  loss_mask_0: 0.1131  loss_dice_0: 0.7367  loss_ce_1: 0.7947  loss_mask_1: 0.1096  loss_dice_1: 0.5168  loss_ce_2: 0.6593  loss_mask_2: 0.09928  loss_dice_2: 0.4791  loss_ce_3: 0.6775  loss_mask_3: 0.09959  loss_dice_3: 0.4121  loss_ce_4: 0.5972  loss_mask_4: 0.1037  loss_dice_4: 0.5438  loss_ce_5: 0.7195  loss_mask_5: 0.09419  loss_dice_5: 0.4827  loss_ce_6: 0.5989  loss_mask_6: 0.08614  loss_dice_6: 0.4387  loss_ce_7: 0.5552  loss_mask_7: 0.09807  loss_dice_7: 0.4468  loss_ce_8: 0.7192  loss_mask_8: 0.09195  loss_dice_8: 0.4272     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:29 d2.utils.events]:  eta: 5:27:21  iter: 12979  total_loss: 16.1  loss_ce: 0.4998  loss_mask: 0.05848  loss_dice: 0.7356  loss_ce_0: 1.268  loss_mask_0: 0.06141  loss_dice_0: 0.8227  loss_ce_1: 0.9535  loss_mask_1: 0.05877  loss_dice_1: 0.8567  loss_ce_2: 0.8526  loss_mask_2: 0.06545  loss_dice_2: 1.023  loss_ce_3: 0.5914  loss_mask_3: 0.0583  loss_dice_3: 0.8527  loss_ce_4: 0.6276  loss_mask_4: 0.05189  loss_dice_4: 0.9339  loss_ce_5: 0.5601  loss_mask_5: 0.06227  loss_dice_5: 0.7474  loss_ce_6: 0.5917  loss_mask_6: 0.05786  loss_dice_6: 0.8037  loss_ce_7: 0.7163  loss_mask_7: 0.05466  loss_dice_7: 0.8294  loss_ce_8: 0.6675  loss_mask_8: 0.05581  loss_dice_8: 0.8234     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:37 d2.utils.events]:  eta: 5:31:57  iter: 12999  total_loss: 20.08  loss_ce: 0.824  loss_mask: 0.03343  loss_dice: 0.8758  loss_ce_0: 1.826  loss_mask_0: 0.05011  loss_dice_0: 0.873  loss_ce_1: 1.192  loss_mask_1: 0.05313  loss_dice_1: 0.7084  loss_ce_2: 0.9767  loss_mask_2: 0.05429  loss_dice_2: 0.8691  loss_ce_3: 0.8543  loss_mask_3: 0.04051  loss_dice_3: 0.7239  loss_ce_4: 0.8102  loss_mask_4: 0.03361  loss_dice_4: 0.8642  loss_ce_5: 0.9739  loss_mask_5: 0.03485  loss_dice_5: 0.7095  loss_ce_6: 0.8201  loss_mask_6: 0.03143  loss_dice_6: 0.7849  loss_ce_7: 0.8189  loss_mask_7: 0.03062  loss_dice_7: 0.6433  loss_ce_8: 0.7405  loss_mask_8: 0.03586  loss_dice_8: 0.812     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:46 d2.utils.events]:  eta: 5:33:08  iter: 13019  total_loss: 24.25  loss_ce: 0.9858  loss_mask: 0.1104  loss_dice: 0.9461  loss_ce_0: 1.577  loss_mask_0: 0.1335  loss_dice_0: 1.117  loss_ce_1: 1.449  loss_mask_1: 0.1474  loss_dice_1: 0.8952  loss_ce_2: 1.313  loss_mask_2: 0.1391  loss_dice_2: 0.7611  loss_ce_3: 1.112  loss_mask_3: 0.1102  loss_dice_3: 0.8232  loss_ce_4: 1.096  loss_mask_4: 0.1289  loss_dice_4: 0.7715  loss_ce_5: 1.045  loss_mask_5: 0.1175  loss_dice_5: 0.9015  loss_ce_6: 0.9378  loss_mask_6: 0.1104  loss_dice_6: 0.8045  loss_ce_7: 0.7776  loss_mask_7: 0.108  loss_dice_7: 0.9293  loss_ce_8: 0.9416  loss_mask_8: 0.1071  loss_dice_8: 0.975     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:03:54 d2.utils.events]:  eta: 5:30:07  iter: 13039  total_loss: 23.5  loss_ce: 0.8502  loss_mask: 0.161  loss_dice: 0.9232  loss_ce_0: 1.548  loss_mask_0: 0.1183  loss_dice_0: 1.219  loss_ce_1: 1.055  loss_mask_1: 0.1579  loss_dice_1: 1.38  loss_ce_2: 1.04  loss_mask_2: 0.1493  loss_dice_2: 1.042  loss_ce_3: 1.027  loss_mask_3: 0.1642  loss_dice_3: 1.094  loss_ce_4: 0.9143  loss_mask_4: 0.1418  loss_dice_4: 1.094  loss_ce_5: 1.004  loss_mask_5: 0.1406  loss_dice_5: 1.132  loss_ce_6: 0.9029  loss_mask_6: 0.148  loss_dice_6: 1.25  loss_ce_7: 0.8618  loss_mask_7: 0.136  loss_dice_7: 1.122  loss_ce_8: 0.8114  loss_mask_8: 0.136  loss_dice_8: 1.031     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:02 d2.utils.events]:  eta: 5:18:26  iter: 13059  total_loss: 20  loss_ce: 0.7053  loss_mask: 0.1086  loss_dice: 0.6944  loss_ce_0: 1.502  loss_mask_0: 0.1193  loss_dice_0: 0.9946  loss_ce_1: 0.9584  loss_mask_1: 0.1058  loss_dice_1: 0.7228  loss_ce_2: 0.8469  loss_mask_2: 0.09543  loss_dice_2: 0.6269  loss_ce_3: 0.8158  loss_mask_3: 0.08558  loss_dice_3: 0.7124  loss_ce_4: 0.7638  loss_mask_4: 0.09363  loss_dice_4: 0.7264  loss_ce_5: 0.6461  loss_mask_5: 0.1011  loss_dice_5: 0.7884  loss_ce_6: 0.6339  loss_mask_6: 0.1079  loss_dice_6: 0.843  loss_ce_7: 0.6514  loss_mask_7: 0.1131  loss_dice_7: 0.7477  loss_ce_8: 0.7138  loss_mask_8: 0.1177  loss_dice_8: 0.6539     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:11 d2.utils.events]:  eta: 5:22:54  iter: 13079  total_loss: 17.89  loss_ce: 0.8221  loss_mask: 0.06696  loss_dice: 0.6615  loss_ce_0: 1.412  loss_mask_0: 0.0961  loss_dice_0: 0.8334  loss_ce_1: 0.9168  loss_mask_1: 0.1107  loss_dice_1: 0.9346  loss_ce_2: 0.9073  loss_mask_2: 0.1117  loss_dice_2: 1.013  loss_ce_3: 0.8835  loss_mask_3: 0.06124  loss_dice_3: 0.7354  loss_ce_4: 0.8334  loss_mask_4: 0.06784  loss_dice_4: 0.7194  loss_ce_5: 0.8605  loss_mask_5: 0.06727  loss_dice_5: 0.7553  loss_ce_6: 0.7756  loss_mask_6: 0.05905  loss_dice_6: 0.6998  loss_ce_7: 0.7919  loss_mask_7: 0.06336  loss_dice_7: 0.6656  loss_ce_8: 0.7928  loss_mask_8: 0.06306  loss_dice_8: 0.7028     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:19 d2.utils.events]:  eta: 5:32:24  iter: 13099  total_loss: 24.1  loss_ce: 0.7356  loss_mask: 0.1028  loss_dice: 0.6477  loss_ce_0: 1.286  loss_mask_0: 0.1232  loss_dice_0: 0.9077  loss_ce_1: 0.8728  loss_mask_1: 0.1374  loss_dice_1: 0.9531  loss_ce_2: 0.8397  loss_mask_2: 0.1245  loss_dice_2: 0.6421  loss_ce_3: 0.6949  loss_mask_3: 0.13  loss_dice_3: 0.7115  loss_ce_4: 0.7735  loss_mask_4: 0.1083  loss_dice_4: 0.693  loss_ce_5: 0.7817  loss_mask_5: 0.1259  loss_dice_5: 0.6895  loss_ce_6: 0.8638  loss_mask_6: 0.1157  loss_dice_6: 0.6136  loss_ce_7: 0.8088  loss_mask_7: 0.1193  loss_dice_7: 0.7878  loss_ce_8: 0.7273  loss_mask_8: 0.1194  loss_dice_8: 0.6336     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:27 d2.utils.events]:  eta: 5:29:26  iter: 13119  total_loss: 24.64  loss_ce: 0.9831  loss_mask: 0.08981  loss_dice: 1.04  loss_ce_0: 1.711  loss_mask_0: 0.09395  loss_dice_0: 1.339  loss_ce_1: 1.096  loss_mask_1: 0.1103  loss_dice_1: 1.174  loss_ce_2: 1.155  loss_mask_2: 0.1072  loss_dice_2: 1.222  loss_ce_3: 1.09  loss_mask_3: 0.0954  loss_dice_3: 1.242  loss_ce_4: 1.076  loss_mask_4: 0.08385  loss_dice_4: 1.221  loss_ce_5: 0.9516  loss_mask_5: 0.08867  loss_dice_5: 1.046  loss_ce_6: 0.9474  loss_mask_6: 0.09769  loss_dice_6: 1.199  loss_ce_7: 0.9718  loss_mask_7: 0.1004  loss_dice_7: 1.148  loss_ce_8: 0.9664  loss_mask_8: 0.1001  loss_dice_8: 1.035     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:36 d2.utils.events]:  eta: 5:24:38  iter: 13139  total_loss: 20.61  loss_ce: 0.8177  loss_mask: 0.07771  loss_dice: 0.9231  loss_ce_0: 1.223  loss_mask_0: 0.1459  loss_dice_0: 0.8336  loss_ce_1: 0.8534  loss_mask_1: 0.1608  loss_dice_1: 1.045  loss_ce_2: 0.6779  loss_mask_2: 0.1214  loss_dice_2: 0.9877  loss_ce_3: 0.891  loss_mask_3: 0.09475  loss_dice_3: 0.9538  loss_ce_4: 0.8403  loss_mask_4: 0.092  loss_dice_4: 0.6188  loss_ce_5: 0.8241  loss_mask_5: 0.08101  loss_dice_5: 0.8494  loss_ce_6: 0.8088  loss_mask_6: 0.0807  loss_dice_6: 0.5692  loss_ce_7: 0.8499  loss_mask_7: 0.0837  loss_dice_7: 0.9233  loss_ce_8: 0.8545  loss_mask_8: 0.07967  loss_dice_8: 0.7146     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:44 d2.utils.events]:  eta: 5:23:15  iter: 13159  total_loss: 20.32  loss_ce: 0.8414  loss_mask: 0.05866  loss_dice: 0.9223  loss_ce_0: 1.33  loss_mask_0: 0.08956  loss_dice_0: 1.083  loss_ce_1: 1.183  loss_mask_1: 0.068  loss_dice_1: 1.069  loss_ce_2: 1.102  loss_mask_2: 0.08242  loss_dice_2: 1.088  loss_ce_3: 1.068  loss_mask_3: 0.07107  loss_dice_3: 0.8827  loss_ce_4: 0.9196  loss_mask_4: 0.07409  loss_dice_4: 1.042  loss_ce_5: 0.8507  loss_mask_5: 0.06434  loss_dice_5: 1.061  loss_ce_6: 0.8968  loss_mask_6: 0.05713  loss_dice_6: 0.8467  loss_ce_7: 0.8797  loss_mask_7: 0.06645  loss_dice_7: 0.8623  loss_ce_8: 0.8445  loss_mask_8: 0.05923  loss_dice_8: 0.9865     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:04:52 d2.utils.events]:  eta: 5:25:48  iter: 13179  total_loss: 20.15  loss_ce: 0.7994  loss_mask: 0.06507  loss_dice: 0.8426  loss_ce_0: 1.824  loss_mask_0: 0.06399  loss_dice_0: 0.8911  loss_ce_1: 1.033  loss_mask_1: 0.07346  loss_dice_1: 0.7333  loss_ce_2: 0.9125  loss_mask_2: 0.07456  loss_dice_2: 0.9555  loss_ce_3: 0.8647  loss_mask_3: 0.05754  loss_dice_3: 0.8924  loss_ce_4: 0.8565  loss_mask_4: 0.06814  loss_dice_4: 0.9945  loss_ce_5: 0.7955  loss_mask_5: 0.06974  loss_dice_5: 1.018  loss_ce_6: 0.8222  loss_mask_6: 0.0582  loss_dice_6: 0.8629  loss_ce_7: 0.7719  loss_mask_7: 0.06781  loss_dice_7: 1.046  loss_ce_8: 0.7947  loss_mask_8: 0.05623  loss_dice_8: 1.148     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 22.0 - Losses: {'loss_ce': tensor(0.6235, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.0358, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(1.1670, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(1.4478, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.1026, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(1.1379, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.8228, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.0202, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.6712, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.8579, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.0311, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.5954, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.7359, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.0378, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.7109, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.8633, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.0380, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(1.2090, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.6450, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.0333, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.9594, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.6427, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.0321, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.9404, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.6400, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.0281, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.4329, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.6266, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.0359, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(1.2153, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:05:01 d2.utils.events]:  eta: 5:34:35  iter: 13199  total_loss: 20.4  loss_ce: 1.026  loss_mask: 0.1427  loss_dice: 1.115  loss_ce_0: 1.311  loss_mask_0: 0.1536  loss_dice_0: 1.185  loss_ce_1: 1.023  loss_mask_1: 0.179  loss_dice_1: 0.9717  loss_ce_2: 0.8586  loss_mask_2: 0.1605  loss_dice_2: 0.8548  loss_ce_3: 0.8061  loss_mask_3: 0.156  loss_dice_3: 0.7751  loss_ce_4: 0.937  loss_mask_4: 0.1725  loss_dice_4: 1.088  loss_ce_5: 0.8827  loss_mask_5: 0.1582  loss_dice_5: 0.9717  loss_ce_6: 1.015  loss_mask_6: 0.1524  loss_dice_6: 1.016  loss_ce_7: 1.011  loss_mask_7: 0.1422  loss_dice_7: 1.054  loss_ce_8: 1.052  loss_mask_8: 0.1368  loss_dice_8: 0.9524     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:05:09 d2.utils.events]:  eta: 5:29:09  iter: 13219  total_loss: 16.71  loss_ce: 0.6579  loss_mask: 0.09566  loss_dice: 0.6385  loss_ce_0: 1.392  loss_mask_0: 0.06686  loss_dice_0: 0.7705  loss_ce_1: 1.017  loss_mask_1: 0.07682  loss_dice_1: 0.7718  loss_ce_2: 0.9157  loss_mask_2: 0.07347  loss_dice_2: 0.636  loss_ce_3: 0.7461  loss_mask_3: 0.0906  loss_dice_3: 0.6529  loss_ce_4: 0.7153  loss_mask_4: 0.08282  loss_dice_4: 0.6439  loss_ce_5: 0.6861  loss_mask_5: 0.08471  loss_dice_5: 0.6211  loss_ce_6: 0.6637  loss_mask_6: 0.1002  loss_dice_6: 0.6662  loss_ce_7: 0.6729  loss_mask_7: 0.0865  loss_dice_7: 0.6143  loss_ce_8: 0.6744  loss_mask_8: 0.09253  loss_dice_8: 0.6483     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:05:18 d2.utils.events]:  eta: 5:21:43  iter: 13239  total_loss: 25.24  loss_ce: 0.8383  loss_mask: 0.07547  loss_dice: 1.17  loss_ce_0: 2.196  loss_mask_0: 0.07329  loss_dice_0: 1.131  loss_ce_1: 1.384  loss_mask_1: 0.1066  loss_dice_1: 1.191  loss_ce_2: 1.249  loss_mask_2: 0.07093  loss_dice_2: 1.172  loss_ce_3: 0.968  loss_mask_3: 0.104  loss_dice_3: 1.274  loss_ce_4: 0.9444  loss_mask_4: 0.1053  loss_dice_4: 0.9078  loss_ce_5: 0.9783  loss_mask_5: 0.09949  loss_dice_5: 1.049  loss_ce_6: 0.8939  loss_mask_6: 0.09056  loss_dice_6: 1.164  loss_ce_7: 0.8962  loss_mask_7: 0.0947  loss_dice_7: 0.9861  loss_ce_8: 0.8601  loss_mask_8: 0.08856  loss_dice_8: 1.226     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:05:26 d2.utils.events]:  eta: 5:30:39  iter: 13259  total_loss: 21.36  loss_ce: 0.6478  loss_mask: 0.1011  loss_dice: 0.8728  loss_ce_0: 1.334  loss_mask_0: 0.1118  loss_dice_0: 0.9678  loss_ce_1: 1.028  loss_mask_1: 0.136  loss_dice_1: 0.7599  loss_ce_2: 0.8469  loss_mask_2: 0.1171  loss_dice_2: 0.7427  loss_ce_3: 0.9208  loss_mask_3: 0.1017  loss_dice_3: 0.8835  loss_ce_4: 0.9032  loss_mask_4: 0.09463  loss_dice_4: 0.8315  loss_ce_5: 0.8881  loss_mask_5: 0.09954  loss_dice_5: 0.8904  loss_ce_6: 0.7895  loss_mask_6: 0.09824  loss_dice_6: 0.8494  loss_ce_7: 0.71  loss_mask_7: 0.1006  loss_dice_7: 0.7772  loss_ce_8: 0.8217  loss_mask_8: 0.1101  loss_dice_8: 0.7657     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:05:34 d2.utils.events]:  eta: 5:23:23  iter: 13279  total_loss: 17.88  loss_ce: 0.6518  loss_mask: 0.07174  loss_dice: 0.8781  loss_ce_0: 1.271  loss_mask_0: 0.07444  loss_dice_0: 0.9801  loss_ce_1: 0.8862  loss_mask_1: 0.09198  loss_dice_1: 0.9423  loss_ce_2: 0.7543  loss_mask_2: 0.06911  loss_dice_2: 1.039  loss_ce_3: 0.7494  loss_mask_3: 0.06818  loss_dice_3: 0.8097  loss_ce_4: 0.7285  loss_mask_4: 0.06749  loss_dice_4: 0.7398  loss_ce_5: 0.6921  loss_mask_5: 0.0657  loss_dice_5: 0.807  loss_ce_6: 0.6727  loss_mask_6: 0.07231  loss_dice_6: 0.5504  loss_ce_7: 0.6707  loss_mask_7: 0.06475  loss_dice_7: 0.7835  loss_ce_8: 0.6602  loss_mask_8: 0.07449  loss_dice_8: 0.6145     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:05:43 d2.utils.events]:  eta: 5:22:41  iter: 13299  total_loss: 21.96  loss_ce: 0.7999  loss_mask: 0.06011  loss_dice: 0.8004  loss_ce_0: 1.493  loss_mask_0: 0.08694  loss_dice_0: 0.8794  loss_ce_1: 1.001  loss_mask_1: 0.05585  loss_dice_1: 0.9849  loss_ce_2: 0.9279  loss_mask_2: 0.0524  loss_dice_2: 0.8729  loss_ce_3: 0.8922  loss_mask_3: 0.06241  loss_dice_3: 0.9617  loss_ce_4: 0.8424  loss_mask_4: 0.06372  loss_dice_4: 0.8578  loss_ce_5: 0.7556  loss_mask_5: 0.06185  loss_dice_5: 0.9598  loss_ce_6: 0.7888  loss_mask_6: 0.08095  loss_dice_6: 0.9515  loss_ce_7: 0.7287  loss_mask_7: 0.06303  loss_dice_7: 0.8529  loss_ce_8: 0.7812  loss_mask_8: 0.05665  loss_dice_8: 0.8394     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:05:51 d2.utils.events]:  eta: 5:38:49  iter: 13319  total_loss: 18.95  loss_ce: 0.5198  loss_mask: 0.05537  loss_dice: 0.9377  loss_ce_0: 1.083  loss_mask_0: 0.07665  loss_dice_0: 1.155  loss_ce_1: 0.6397  loss_mask_1: 0.1176  loss_dice_1: 1.408  loss_ce_2: 0.6467  loss_mask_2: 0.0825  loss_dice_2: 0.8133  loss_ce_3: 0.5005  loss_mask_3: 0.07138  loss_dice_3: 0.9384  loss_ce_4: 0.5223  loss_mask_4: 0.06765  loss_dice_4: 1.042  loss_ce_5: 0.5404  loss_mask_5: 0.06807  loss_dice_5: 0.9765  loss_ce_6: 0.5298  loss_mask_6: 0.07348  loss_dice_6: 0.9389  loss_ce_7: 0.4657  loss_mask_7: 0.0709  loss_dice_7: 0.8032  loss_ce_8: 0.5084  loss_mask_8: 0.0687  loss_dice_8: 0.7177     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:00 d2.utils.events]:  eta: 5:26:20  iter: 13339  total_loss: 16.78  loss_ce: 0.6442  loss_mask: 0.0967  loss_dice: 0.7303  loss_ce_0: 1.304  loss_mask_0: 0.1119  loss_dice_0: 0.8213  loss_ce_1: 0.9306  loss_mask_1: 0.1006  loss_dice_1: 0.6819  loss_ce_2: 0.91  loss_mask_2: 0.08498  loss_dice_2: 0.6596  loss_ce_3: 0.7748  loss_mask_3: 0.09563  loss_dice_3: 0.575  loss_ce_4: 0.8563  loss_mask_4: 0.09712  loss_dice_4: 0.6611  loss_ce_5: 0.7112  loss_mask_5: 0.1057  loss_dice_5: 0.6207  loss_ce_6: 0.6706  loss_mask_6: 0.07738  loss_dice_6: 0.7285  loss_ce_7: 0.6437  loss_mask_7: 0.09207  loss_dice_7: 0.6963  loss_ce_8: 0.6752  loss_mask_8: 0.1071  loss_dice_8: 0.6404     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:08 d2.utils.events]:  eta: 5:22:24  iter: 13359  total_loss: 15.95  loss_ce: 0.7107  loss_mask: 0.04687  loss_dice: 0.5687  loss_ce_0: 1.343  loss_mask_0: 0.07996  loss_dice_0: 0.561  loss_ce_1: 0.9157  loss_mask_1: 0.05075  loss_dice_1: 0.7411  loss_ce_2: 0.8704  loss_mask_2: 0.05506  loss_dice_2: 0.6072  loss_ce_3: 0.8058  loss_mask_3: 0.04736  loss_dice_3: 0.5253  loss_ce_4: 0.7972  loss_mask_4: 0.04896  loss_dice_4: 0.5048  loss_ce_5: 0.8121  loss_mask_5: 0.05194  loss_dice_5: 0.5294  loss_ce_6: 0.8163  loss_mask_6: 0.04492  loss_dice_6: 0.4672  loss_ce_7: 0.745  loss_mask_7: 0.05004  loss_dice_7: 0.5105  loss_ce_8: 0.7916  loss_mask_8: 0.05347  loss_dice_8: 0.606     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:17 d2.utils.events]:  eta: 5:32:03  iter: 13379  total_loss: 14.72  loss_ce: 0.5364  loss_mask: 0.04899  loss_dice: 0.7739  loss_ce_0: 1.346  loss_mask_0: 0.06687  loss_dice_0: 0.9127  loss_ce_1: 0.7867  loss_mask_1: 0.06054  loss_dice_1: 0.7679  loss_ce_2: 0.6862  loss_mask_2: 0.06243  loss_dice_2: 0.8423  loss_ce_3: 0.5932  loss_mask_3: 0.03828  loss_dice_3: 0.7955  loss_ce_4: 0.5793  loss_mask_4: 0.04279  loss_dice_4: 0.8647  loss_ce_5: 0.5346  loss_mask_5: 0.04282  loss_dice_5: 0.9868  loss_ce_6: 0.5405  loss_mask_6: 0.04792  loss_dice_6: 0.9396  loss_ce_7: 0.5697  loss_mask_7: 0.04541  loss_dice_7: 0.8112  loss_ce_8: 0.5165  loss_mask_8: 0.04298  loss_dice_8: 0.6978     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:25 d2.utils.events]:  eta: 5:23:52  iter: 13399  total_loss: 22.15  loss_ce: 0.7357  loss_mask: 0.07347  loss_dice: 0.7475  loss_ce_0: 1.267  loss_mask_0: 0.07575  loss_dice_0: 1.192  loss_ce_1: 1.117  loss_mask_1: 0.09001  loss_dice_1: 1.111  loss_ce_2: 0.9554  loss_mask_2: 0.07864  loss_dice_2: 0.9814  loss_ce_3: 0.826  loss_mask_3: 0.08537  loss_dice_3: 0.8247  loss_ce_4: 0.7411  loss_mask_4: 0.08625  loss_dice_4: 0.8209  loss_ce_5: 0.7436  loss_mask_5: 0.07692  loss_dice_5: 0.7679  loss_ce_6: 0.665  loss_mask_6: 0.07242  loss_dice_6: 0.8066  loss_ce_7: 0.7701  loss_mask_7: 0.08176  loss_dice_7: 0.8464  loss_ce_8: 0.6603  loss_mask_8: 0.07425  loss_dice_8: 0.7297     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:34 d2.utils.events]:  eta: 5:30:03  iter: 13419  total_loss: 14.95  loss_ce: 0.5007  loss_mask: 0.05304  loss_dice: 0.4959  loss_ce_0: 1.25  loss_mask_0: 0.06887  loss_dice_0: 0.842  loss_ce_1: 0.7019  loss_mask_1: 0.06924  loss_dice_1: 0.7781  loss_ce_2: 0.6909  loss_mask_2: 0.0785  loss_dice_2: 0.4957  loss_ce_3: 0.5478  loss_mask_3: 0.06621  loss_dice_3: 0.6586  loss_ce_4: 0.5311  loss_mask_4: 0.07519  loss_dice_4: 0.5835  loss_ce_5: 0.5075  loss_mask_5: 0.05977  loss_dice_5: 0.5695  loss_ce_6: 0.5493  loss_mask_6: 0.05529  loss_dice_6: 0.7127  loss_ce_7: 0.4557  loss_mask_7: 0.05239  loss_dice_7: 0.5768  loss_ce_8: 0.5468  loss_mask_8: 0.05448  loss_dice_8: 0.6398     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:42 d2.utils.events]:  eta: 5:24:12  iter: 13439  total_loss: 25.15  loss_ce: 1.136  loss_mask: 0.1188  loss_dice: 1.123  loss_ce_0: 1.821  loss_mask_0: 0.114  loss_dice_0: 1.296  loss_ce_1: 1.302  loss_mask_1: 0.1437  loss_dice_1: 1.252  loss_ce_2: 1.275  loss_mask_2: 0.1156  loss_dice_2: 1.296  loss_ce_3: 1.076  loss_mask_3: 0.1306  loss_dice_3: 1.379  loss_ce_4: 0.9887  loss_mask_4: 0.1201  loss_dice_4: 1.319  loss_ce_5: 0.9724  loss_mask_5: 0.1186  loss_dice_5: 1.184  loss_ce_6: 1.007  loss_mask_6: 0.1189  loss_dice_6: 1.048  loss_ce_7: 1.075  loss_mask_7: 0.1178  loss_dice_7: 1.16  loss_ce_8: 1.072  loss_mask_8: 0.1228  loss_dice_8: 1.166     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:50 d2.utils.events]:  eta: 5:24:19  iter: 13459  total_loss: 14.55  loss_ce: 0.7073  loss_mask: 0.07671  loss_dice: 0.4258  loss_ce_0: 1.123  loss_mask_0: 0.09037  loss_dice_0: 0.6716  loss_ce_1: 0.7885  loss_mask_1: 0.07527  loss_dice_1: 0.6337  loss_ce_2: 0.7435  loss_mask_2: 0.08613  loss_dice_2: 0.4757  loss_ce_3: 0.6731  loss_mask_3: 0.08631  loss_dice_3: 0.6559  loss_ce_4: 0.7217  loss_mask_4: 0.0786  loss_dice_4: 0.4335  loss_ce_5: 0.6755  loss_mask_5: 0.08697  loss_dice_5: 0.3707  loss_ce_6: 0.7085  loss_mask_6: 0.07873  loss_dice_6: 0.4844  loss_ce_7: 0.7892  loss_mask_7: 0.08137  loss_dice_7: 0.7075  loss_ce_8: 0.7052  loss_mask_8: 0.0785  loss_dice_8: 0.4133     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:06:58 d2.utils.events]:  eta: 5:16:41  iter: 13479  total_loss: 21.39  loss_ce: 0.7219  loss_mask: 0.1036  loss_dice: 0.9783  loss_ce_0: 1.308  loss_mask_0: 0.09259  loss_dice_0: 1.311  loss_ce_1: 0.9419  loss_mask_1: 0.125  loss_dice_1: 1.261  loss_ce_2: 0.8234  loss_mask_2: 0.1079  loss_dice_2: 1.107  loss_ce_3: 0.7696  loss_mask_3: 0.08568  loss_dice_3: 1.054  loss_ce_4: 0.6136  loss_mask_4: 0.105  loss_dice_4: 1.072  loss_ce_5: 0.7741  loss_mask_5: 0.07417  loss_dice_5: 1.034  loss_ce_6: 0.718  loss_mask_6: 0.08496  loss_dice_6: 1.079  loss_ce_7: 0.7091  loss_mask_7: 0.0938  loss_dice_7: 1.076  loss_ce_8: 0.7068  loss_mask_8: 0.07462  loss_dice_8: 1.022     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:07 d2.utils.events]:  eta: 5:21:31  iter: 13499  total_loss: 22.09  loss_ce: 0.846  loss_mask: 0.04652  loss_dice: 0.6638  loss_ce_0: 1.469  loss_mask_0: 0.08719  loss_dice_0: 1.151  loss_ce_1: 1.205  loss_mask_1: 0.0618  loss_dice_1: 0.9146  loss_ce_2: 1.123  loss_mask_2: 0.05738  loss_dice_2: 0.9018  loss_ce_3: 1.011  loss_mask_3: 0.05915  loss_dice_3: 0.7978  loss_ce_4: 0.9477  loss_mask_4: 0.0551  loss_dice_4: 0.6996  loss_ce_5: 0.8998  loss_mask_5: 0.05831  loss_dice_5: 0.7193  loss_ce_6: 0.8531  loss_mask_6: 0.05384  loss_dice_6: 0.7808  loss_ce_7: 0.8177  loss_mask_7: 0.05268  loss_dice_7: 0.8355  loss_ce_8: 0.9087  loss_mask_8: 0.04979  loss_dice_8: 0.7784     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:15 d2.utils.events]:  eta: 5:18:27  iter: 13519  total_loss: 17.66  loss_ce: 0.7918  loss_mask: 0.06784  loss_dice: 0.7301  loss_ce_0: 1.329  loss_mask_0: 0.06506  loss_dice_0: 0.9201  loss_ce_1: 0.9836  loss_mask_1: 0.08992  loss_dice_1: 0.7854  loss_ce_2: 0.9062  loss_mask_2: 0.07341  loss_dice_2: 0.9357  loss_ce_3: 0.808  loss_mask_3: 0.07293  loss_dice_3: 0.9521  loss_ce_4: 0.7977  loss_mask_4: 0.06925  loss_dice_4: 0.7019  loss_ce_5: 0.7716  loss_mask_5: 0.06614  loss_dice_5: 0.8884  loss_ce_6: 0.7592  loss_mask_6: 0.08845  loss_dice_6: 0.6664  loss_ce_7: 0.7645  loss_mask_7: 0.0767  loss_dice_7: 0.6764  loss_ce_8: 0.793  loss_mask_8: 0.07517  loss_dice_8: 0.7349     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:23 d2.utils.events]:  eta: 5:18:01  iter: 13539  total_loss: 16.51  loss_ce: 0.717  loss_mask: 0.06359  loss_dice: 0.7242  loss_ce_0: 1.4  loss_mask_0: 0.09253  loss_dice_0: 0.8684  loss_ce_1: 0.9906  loss_mask_1: 0.07349  loss_dice_1: 0.6633  loss_ce_2: 0.8724  loss_mask_2: 0.06545  loss_dice_2: 0.6646  loss_ce_3: 0.8713  loss_mask_3: 0.06244  loss_dice_3: 0.6011  loss_ce_4: 0.7286  loss_mask_4: 0.06066  loss_dice_4: 0.6207  loss_ce_5: 0.7153  loss_mask_5: 0.06437  loss_dice_5: 0.6983  loss_ce_6: 0.7266  loss_mask_6: 0.06356  loss_dice_6: 0.6594  loss_ce_7: 0.7372  loss_mask_7: 0.06313  loss_dice_7: 0.6252  loss_ce_8: 0.7327  loss_mask_8: 0.0639  loss_dice_8: 0.5826     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:31 d2.utils.events]:  eta: 5:20:58  iter: 13559  total_loss: 15.58  loss_ce: 0.6173  loss_mask: 0.07184  loss_dice: 0.7799  loss_ce_0: 1.3  loss_mask_0: 0.09469  loss_dice_0: 0.8195  loss_ce_1: 0.9389  loss_mask_1: 0.0787  loss_dice_1: 0.6701  loss_ce_2: 0.7821  loss_mask_2: 0.07477  loss_dice_2: 0.8223  loss_ce_3: 0.6721  loss_mask_3: 0.05386  loss_dice_3: 0.8213  loss_ce_4: 0.6184  loss_mask_4: 0.06381  loss_dice_4: 0.6194  loss_ce_5: 0.6369  loss_mask_5: 0.05704  loss_dice_5: 0.7248  loss_ce_6: 0.5664  loss_mask_6: 0.06105  loss_dice_6: 0.643  loss_ce_7: 0.6284  loss_mask_7: 0.05917  loss_dice_7: 0.6418  loss_ce_8: 0.6137  loss_mask_8: 0.06585  loss_dice_8: 0.5954     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:40 d2.utils.events]:  eta: 5:16:09  iter: 13579  total_loss: 19.07  loss_ce: 0.6734  loss_mask: 0.07152  loss_dice: 0.6598  loss_ce_0: 1.05  loss_mask_0: 0.1205  loss_dice_0: 0.933  loss_ce_1: 0.8563  loss_mask_1: 0.0734  loss_dice_1: 0.8615  loss_ce_2: 0.6058  loss_mask_2: 0.0884  loss_dice_2: 0.667  loss_ce_3: 0.7635  loss_mask_3: 0.06865  loss_dice_3: 0.6259  loss_ce_4: 0.7239  loss_mask_4: 0.07174  loss_dice_4: 0.6669  loss_ce_5: 0.751  loss_mask_5: 0.06945  loss_dice_5: 0.6989  loss_ce_6: 0.6641  loss_mask_6: 0.08075  loss_dice_6: 0.693  loss_ce_7: 0.6677  loss_mask_7: 0.07292  loss_dice_7: 0.6773  loss_ce_8: 0.6639  loss_mask_8: 0.06804  loss_dice_8: 0.7183     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:48 d2.utils.events]:  eta: 5:12:18  iter: 13599  total_loss: 13.34  loss_ce: 0.5677  loss_mask: 0.05931  loss_dice: 0.5781  loss_ce_0: 0.9063  loss_mask_0: 0.09499  loss_dice_0: 0.5397  loss_ce_1: 0.6935  loss_mask_1: 0.07279  loss_dice_1: 0.6562  loss_ce_2: 0.6996  loss_mask_2: 0.05423  loss_dice_2: 0.5268  loss_ce_3: 0.6002  loss_mask_3: 0.05671  loss_dice_3: 0.5323  loss_ce_4: 0.6809  loss_mask_4: 0.06109  loss_dice_4: 0.4764  loss_ce_5: 0.6183  loss_mask_5: 0.05511  loss_dice_5: 0.5301  loss_ce_6: 0.552  loss_mask_6: 0.05731  loss_dice_6: 0.5145  loss_ce_7: 0.6077  loss_mask_7: 0.06537  loss_dice_7: 0.4933  loss_ce_8: 0.5614  loss_mask_8: 0.06063  loss_dice_8: 0.6333     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:07:56 d2.utils.events]:  eta: 5:25:22  iter: 13619  total_loss: 19.36  loss_ce: 0.8199  loss_mask: 0.1019  loss_dice: 0.7175  loss_ce_0: 1.439  loss_mask_0: 0.1224  loss_dice_0: 0.9238  loss_ce_1: 1.033  loss_mask_1: 0.08811  loss_dice_1: 0.8179  loss_ce_2: 0.9095  loss_mask_2: 0.07842  loss_dice_2: 0.8561  loss_ce_3: 0.8673  loss_mask_3: 0.09357  loss_dice_3: 0.8366  loss_ce_4: 0.7909  loss_mask_4: 0.0921  loss_dice_4: 0.7912  loss_ce_5: 0.8672  loss_mask_5: 0.09785  loss_dice_5: 0.8665  loss_ce_6: 0.8263  loss_mask_6: 0.1003  loss_dice_6: 0.7607  loss_ce_7: 0.8482  loss_mask_7: 0.09984  loss_dice_7: 0.7529  loss_ce_8: 0.7332  loss_mask_8: 0.1055  loss_dice_8: 0.7759     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:05 d2.utils.events]:  eta: 5:24:09  iter: 13639  total_loss: 23.82  loss_ce: 1.152  loss_mask: 0.08677  loss_dice: 1.228  loss_ce_0: 1.734  loss_mask_0: 0.09315  loss_dice_0: 1.564  loss_ce_1: 1.231  loss_mask_1: 0.0895  loss_dice_1: 1.156  loss_ce_2: 1.085  loss_mask_2: 0.08153  loss_dice_2: 1.048  loss_ce_3: 1.112  loss_mask_3: 0.08076  loss_dice_3: 0.9153  loss_ce_4: 1.026  loss_mask_4: 0.08574  loss_dice_4: 1.326  loss_ce_5: 1.148  loss_mask_5: 0.08608  loss_dice_5: 1.129  loss_ce_6: 1.075  loss_mask_6: 0.08556  loss_dice_6: 1.204  loss_ce_7: 1.068  loss_mask_7: 0.08063  loss_dice_7: 1.046  loss_ce_8: 1.064  loss_mask_8: 0.08247  loss_dice_8: 0.997     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:13 d2.utils.events]:  eta: 5:17:58  iter: 13659  total_loss: 20.94  loss_ce: 0.5532  loss_mask: 0.09949  loss_dice: 0.9498  loss_ce_0: 1.546  loss_mask_0: 0.1089  loss_dice_0: 1.165  loss_ce_1: 0.8577  loss_mask_1: 0.1289  loss_dice_1: 0.9797  loss_ce_2: 0.7373  loss_mask_2: 0.1105  loss_dice_2: 0.9801  loss_ce_3: 0.602  loss_mask_3: 0.09939  loss_dice_3: 0.9092  loss_ce_4: 0.6598  loss_mask_4: 0.09724  loss_dice_4: 0.9177  loss_ce_5: 0.5863  loss_mask_5: 0.08858  loss_dice_5: 0.8843  loss_ce_6: 0.5722  loss_mask_6: 0.09622  loss_dice_6: 0.9157  loss_ce_7: 0.5625  loss_mask_7: 0.1046  loss_dice_7: 0.9498  loss_ce_8: 0.5705  loss_mask_8: 0.1009  loss_dice_8: 0.9798     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:21 d2.utils.events]:  eta: 5:25:44  iter: 13679  total_loss: 23.27  loss_ce: 0.9858  loss_mask: 0.04457  loss_dice: 0.9423  loss_ce_0: 1.816  loss_mask_0: 0.05618  loss_dice_0: 1.293  loss_ce_1: 1.332  loss_mask_1: 0.05015  loss_dice_1: 1.032  loss_ce_2: 1.241  loss_mask_2: 0.0587  loss_dice_2: 1.129  loss_ce_3: 1.212  loss_mask_3: 0.05305  loss_dice_3: 0.8933  loss_ce_4: 1.261  loss_mask_4: 0.05071  loss_dice_4: 0.8825  loss_ce_5: 1.128  loss_mask_5: 0.05674  loss_dice_5: 0.9058  loss_ce_6: 0.9422  loss_mask_6: 0.04364  loss_dice_6: 0.9184  loss_ce_7: 0.9887  loss_mask_7: 0.04587  loss_dice_7: 0.9152  loss_ce_8: 0.9693  loss_mask_8: 0.05206  loss_dice_8: 1.169     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:30 d2.utils.events]:  eta: 5:22:26  iter: 13699  total_loss: 16.52  loss_ce: 0.5274  loss_mask: 0.1002  loss_dice: 0.6703  loss_ce_0: 0.9488  loss_mask_0: 0.1233  loss_dice_0: 0.8147  loss_ce_1: 0.8016  loss_mask_1: 0.1234  loss_dice_1: 0.9357  loss_ce_2: 0.6407  loss_mask_2: 0.1423  loss_dice_2: 0.8887  loss_ce_3: 0.5276  loss_mask_3: 0.1139  loss_dice_3: 0.7057  loss_ce_4: 0.5325  loss_mask_4: 0.1109  loss_dice_4: 0.6181  loss_ce_5: 0.618  loss_mask_5: 0.1091  loss_dice_5: 0.5851  loss_ce_6: 0.5985  loss_mask_6: 0.09941  loss_dice_6: 0.577  loss_ce_7: 0.4638  loss_mask_7: 0.1075  loss_dice_7: 0.5791  loss_ce_8: 0.5206  loss_mask_8: 0.1013  loss_dice_8: 0.6035     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:38 d2.utils.events]:  eta: 5:20:10  iter: 13719  total_loss: 22.27  loss_ce: 0.7844  loss_mask: 0.061  loss_dice: 1.109  loss_ce_0: 1.522  loss_mask_0: 0.05974  loss_dice_0: 0.9983  loss_ce_1: 1.078  loss_mask_1: 0.06858  loss_dice_1: 1.088  loss_ce_2: 0.9745  loss_mask_2: 0.06568  loss_dice_2: 1.231  loss_ce_3: 0.8849  loss_mask_3: 0.04842  loss_dice_3: 0.8503  loss_ce_4: 0.8464  loss_mask_4: 0.05001  loss_dice_4: 0.988  loss_ce_5: 0.8496  loss_mask_5: 0.05626  loss_dice_5: 1.092  loss_ce_6: 0.7232  loss_mask_6: 0.06462  loss_dice_6: 0.9867  loss_ce_7: 0.7354  loss_mask_7: 0.05463  loss_dice_7: 1.051  loss_ce_8: 0.775  loss_mask_8: 0.04961  loss_dice_8: 0.8609     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:46 d2.utils.events]:  eta: 5:24:40  iter: 13739  total_loss: 23.47  loss_ce: 0.9744  loss_mask: 0.05173  loss_dice: 0.9462  loss_ce_0: 1.853  loss_mask_0: 0.06165  loss_dice_0: 1.14  loss_ce_1: 1.351  loss_mask_1: 0.05995  loss_dice_1: 1.065  loss_ce_2: 1.209  loss_mask_2: 0.03544  loss_dice_2: 1.044  loss_ce_3: 1.117  loss_mask_3: 0.03623  loss_dice_3: 1.031  loss_ce_4: 1.072  loss_mask_4: 0.03433  loss_dice_4: 0.9257  loss_ce_5: 0.9928  loss_mask_5: 0.03892  loss_dice_5: 1.065  loss_ce_6: 0.9612  loss_mask_6: 0.03771  loss_dice_6: 0.9576  loss_ce_7: 0.9461  loss_mask_7: 0.0476  loss_dice_7: 0.9577  loss_ce_8: 0.9109  loss_mask_8: 0.03954  loss_dice_8: 1.054     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:08:55 d2.utils.events]:  eta: 5:28:59  iter: 13759  total_loss: 19.41  loss_ce: 0.805  loss_mask: 0.08179  loss_dice: 0.7532  loss_ce_0: 1.631  loss_mask_0: 0.07594  loss_dice_0: 1.124  loss_ce_1: 1.181  loss_mask_1: 0.09411  loss_dice_1: 0.8998  loss_ce_2: 1.006  loss_mask_2: 0.09002  loss_dice_2: 0.8435  loss_ce_3: 0.9609  loss_mask_3: 0.08282  loss_dice_3: 1.151  loss_ce_4: 0.9018  loss_mask_4: 0.0826  loss_dice_4: 0.9573  loss_ce_5: 0.861  loss_mask_5: 0.08031  loss_dice_5: 0.8574  loss_ce_6: 0.8501  loss_mask_6: 0.07206  loss_dice_6: 0.8744  loss_ce_7: 0.8602  loss_mask_7: 0.07917  loss_dice_7: 0.7384  loss_ce_8: 0.9359  loss_mask_8: 0.06681  loss_dice_8: 0.6932     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:09:03 d2.utils.events]:  eta: 5:22:43  iter: 13779  total_loss: 25.53  loss_ce: 1.085  loss_mask: 0.1336  loss_dice: 0.9514  loss_ce_0: 1.64  loss_mask_0: 0.2328  loss_dice_0: 1.139  loss_ce_1: 1.179  loss_mask_1: 0.2239  loss_dice_1: 1.199  loss_ce_2: 0.9654  loss_mask_2: 0.2319  loss_dice_2: 1.112  loss_ce_3: 1.035  loss_mask_3: 0.2017  loss_dice_3: 0.9244  loss_ce_4: 0.9719  loss_mask_4: 0.1785  loss_dice_4: 1.009  loss_ce_5: 1.059  loss_mask_5: 0.1715  loss_dice_5: 0.9588  loss_ce_6: 0.9677  loss_mask_6: 0.1496  loss_dice_6: 0.8706  loss_ce_7: 0.9563  loss_mask_7: 0.181  loss_dice_7: 0.994  loss_ce_8: 1.04  loss_mask_8: 0.1456  loss_dice_8: 0.9766     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 23.0 - Losses: {'loss_ce': tensor(1.0312, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.0136, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.7446, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(1.3271, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.0271, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(0.7091, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(1.1898, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.0169, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.9736, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(1.3316, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.0177, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.6245, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(1.0998, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.0124, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.7773, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(1.0137, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.0136, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.2642, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(1.0261, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.0127, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.7658, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(1.0609, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.0144, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.2194, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(1.0494, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.0120, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.2718, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(1.1317, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.0146, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.8008, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:09:12 d2.utils.events]:  eta: 5:19:33  iter: 13799  total_loss: 19.56  loss_ce: 0.5818  loss_mask: 0.09458  loss_dice: 0.9229  loss_ce_0: 1.285  loss_mask_0: 0.09124  loss_dice_0: 1.213  loss_ce_1: 1.165  loss_mask_1: 0.1053  loss_dice_1: 1.229  loss_ce_2: 0.7926  loss_mask_2: 0.08874  loss_dice_2: 1.171  loss_ce_3: 0.6967  loss_mask_3: 0.1205  loss_dice_3: 1.123  loss_ce_4: 0.6637  loss_mask_4: 0.09383  loss_dice_4: 0.9309  loss_ce_5: 0.59  loss_mask_5: 0.09519  loss_dice_5: 0.8733  loss_ce_6: 0.5717  loss_mask_6: 0.09866  loss_dice_6: 0.9267  loss_ce_7: 0.5935  loss_mask_7: 0.08097  loss_dice_7: 0.9705  loss_ce_8: 0.5899  loss_mask_8: 0.09756  loss_dice_8: 1.033     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:09:20 d2.utils.events]:  eta: 5:25:04  iter: 13819  total_loss: 19.96  loss_ce: 0.8949  loss_mask: 0.07787  loss_dice: 0.6033  loss_ce_0: 1.512  loss_mask_0: 0.08199  loss_dice_0: 1.003  loss_ce_1: 1.163  loss_mask_1: 0.06989  loss_dice_1: 0.8456  loss_ce_2: 1.018  loss_mask_2: 0.06818  loss_dice_2: 0.7835  loss_ce_3: 0.9713  loss_mask_3: 0.06819  loss_dice_3: 0.5792  loss_ce_4: 0.9445  loss_mask_4: 0.06638  loss_dice_4: 0.7053  loss_ce_5: 0.9373  loss_mask_5: 0.09012  loss_dice_5: 1.038  loss_ce_6: 0.8822  loss_mask_6: 0.07923  loss_dice_6: 0.6388  loss_ce_7: 0.84  loss_mask_7: 0.07735  loss_dice_7: 0.6767  loss_ce_8: 0.8367  loss_mask_8: 0.07377  loss_dice_8: 0.5506     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:09:28 d2.utils.events]:  eta: 5:20:56  iter: 13839  total_loss: 23.11  loss_ce: 0.9354  loss_mask: 0.08216  loss_dice: 1.017  loss_ce_0: 1.615  loss_mask_0: 0.1005  loss_dice_0: 1.267  loss_ce_1: 1.325  loss_mask_1: 0.1041  loss_dice_1: 1.069  loss_ce_2: 1.151  loss_mask_2: 0.1096  loss_dice_2: 0.9946  loss_ce_3: 1.091  loss_mask_3: 0.07937  loss_dice_3: 1.024  loss_ce_4: 1.046  loss_mask_4: 0.09698  loss_dice_4: 0.9671  loss_ce_5: 0.988  loss_mask_5: 0.1023  loss_dice_5: 1.091  loss_ce_6: 0.9721  loss_mask_6: 0.09301  loss_dice_6: 0.946  loss_ce_7: 0.9301  loss_mask_7: 0.09547  loss_dice_7: 0.8898  loss_ce_8: 0.9395  loss_mask_8: 0.1013  loss_dice_8: 0.8049     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:09:36 d2.utils.events]:  eta: 5:14:20  iter: 13859  total_loss: 18.23  loss_ce: 0.7921  loss_mask: 0.1005  loss_dice: 0.7557  loss_ce_0: 1.401  loss_mask_0: 0.1652  loss_dice_0: 0.8339  loss_ce_1: 0.8966  loss_mask_1: 0.1361  loss_dice_1: 0.7981  loss_ce_2: 0.9086  loss_mask_2: 0.129  loss_dice_2: 0.8486  loss_ce_3: 0.7899  loss_mask_3: 0.1228  loss_dice_3: 0.6608  loss_ce_4: 0.7483  loss_mask_4: 0.118  loss_dice_4: 0.7179  loss_ce_5: 0.7345  loss_mask_5: 0.1406  loss_dice_5: 0.6923  loss_ce_6: 0.7329  loss_mask_6: 0.1079  loss_dice_6: 0.7261  loss_ce_7: 0.7365  loss_mask_7: 0.1117  loss_dice_7: 0.7845  loss_ce_8: 0.7748  loss_mask_8: 0.1155  loss_dice_8: 0.6759     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:09:45 d2.utils.events]:  eta: 5:27:31  iter: 13879  total_loss: 24.25  loss_ce: 1.094  loss_mask: 0.09143  loss_dice: 0.8853  loss_ce_0: 1.539  loss_mask_0: 0.1168  loss_dice_0: 1.077  loss_ce_1: 1.39  loss_mask_1: 0.113  loss_dice_1: 0.7695  loss_ce_2: 1.136  loss_mask_2: 0.09841  loss_dice_2: 0.9464  loss_ce_3: 1.081  loss_mask_3: 0.09889  loss_dice_3: 1.045  loss_ce_4: 1.167  loss_mask_4: 0.1002  loss_dice_4: 0.8663  loss_ce_5: 1.067  loss_mask_5: 0.1023  loss_dice_5: 0.8931  loss_ce_6: 0.9894  loss_mask_6: 0.09271  loss_dice_6: 0.8661  loss_ce_7: 1.057  loss_mask_7: 0.1032  loss_dice_7: 0.8793  loss_ce_8: 1.105  loss_mask_8: 0.1035  loss_dice_8: 0.7989     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:09:53 d2.utils.events]:  eta: 5:17:27  iter: 13899  total_loss: 16.58  loss_ce: 0.6326  loss_mask: 0.05341  loss_dice: 0.5616  loss_ce_0: 1.16  loss_mask_0: 0.0927  loss_dice_0: 1.019  loss_ce_1: 0.8197  loss_mask_1: 0.05982  loss_dice_1: 0.8247  loss_ce_2: 0.7137  loss_mask_2: 0.05651  loss_dice_2: 0.8793  loss_ce_3: 0.6594  loss_mask_3: 0.05024  loss_dice_3: 0.6335  loss_ce_4: 0.6375  loss_mask_4: 0.0513  loss_dice_4: 0.8853  loss_ce_5: 0.5748  loss_mask_5: 0.07164  loss_dice_5: 0.5964  loss_ce_6: 0.5398  loss_mask_6: 0.05796  loss_dice_6: 0.6408  loss_ce_7: 0.547  loss_mask_7: 0.05701  loss_dice_7: 0.7349  loss_ce_8: 0.563  loss_mask_8: 0.05329  loss_dice_8: 0.7499     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:02 d2.utils.events]:  eta: 5:19:11  iter: 13919  total_loss: 15.38  loss_ce: 0.5974  loss_mask: 0.0571  loss_dice: 0.4976  loss_ce_0: 1.168  loss_mask_0: 0.08722  loss_dice_0: 0.7518  loss_ce_1: 0.7834  loss_mask_1: 0.07338  loss_dice_1: 0.5902  loss_ce_2: 0.7102  loss_mask_2: 0.06946  loss_dice_2: 0.7253  loss_ce_3: 0.7027  loss_mask_3: 0.06065  loss_dice_3: 0.7103  loss_ce_4: 0.7126  loss_mask_4: 0.06794  loss_dice_4: 0.6179  loss_ce_5: 0.6086  loss_mask_5: 0.07456  loss_dice_5: 0.5977  loss_ce_6: 0.6179  loss_mask_6: 0.07137  loss_dice_6: 0.5825  loss_ce_7: 0.6085  loss_mask_7: 0.07817  loss_dice_7: 0.6057  loss_ce_8: 0.6247  loss_mask_8: 0.0716  loss_dice_8: 0.5815     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:10 d2.utils.events]:  eta: 5:21:01  iter: 13939  total_loss: 14.95  loss_ce: 0.663  loss_mask: 0.09216  loss_dice: 0.4886  loss_ce_0: 1.151  loss_mask_0: 0.1117  loss_dice_0: 0.9232  loss_ce_1: 0.9462  loss_mask_1: 0.112  loss_dice_1: 0.6453  loss_ce_2: 0.8263  loss_mask_2: 0.1123  loss_dice_2: 0.8633  loss_ce_3: 0.7821  loss_mask_3: 0.0842  loss_dice_3: 0.545  loss_ce_4: 0.6882  loss_mask_4: 0.09278  loss_dice_4: 0.5844  loss_ce_5: 0.7097  loss_mask_5: 0.09692  loss_dice_5: 0.671  loss_ce_6: 0.6723  loss_mask_6: 0.1025  loss_dice_6: 0.5118  loss_ce_7: 0.6228  loss_mask_7: 0.09639  loss_dice_7: 0.4886  loss_ce_8: 0.6654  loss_mask_8: 0.0945  loss_dice_8: 0.7218     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:18 d2.utils.events]:  eta: 5:22:28  iter: 13959  total_loss: 22.83  loss_ce: 0.8278  loss_mask: 0.06352  loss_dice: 0.9809  loss_ce_0: 1.633  loss_mask_0: 0.06639  loss_dice_0: 1.083  loss_ce_1: 1.168  loss_mask_1: 0.07496  loss_dice_1: 0.8114  loss_ce_2: 1.031  loss_mask_2: 0.0648  loss_dice_2: 0.7128  loss_ce_3: 0.9206  loss_mask_3: 0.05931  loss_dice_3: 0.7712  loss_ce_4: 0.8686  loss_mask_4: 0.0584  loss_dice_4: 0.8019  loss_ce_5: 0.9327  loss_mask_5: 0.05814  loss_dice_5: 0.7262  loss_ce_6: 0.9352  loss_mask_6: 0.06284  loss_dice_6: 0.7591  loss_ce_7: 0.8816  loss_mask_7: 0.06295  loss_dice_7: 0.677  loss_ce_8: 0.8627  loss_mask_8: 0.06879  loss_dice_8: 0.8002     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:27 d2.utils.events]:  eta: 5:23:23  iter: 13979  total_loss: 26.04  loss_ce: 0.9225  loss_mask: 0.1266  loss_dice: 1.107  loss_ce_0: 1.749  loss_mask_0: 0.1532  loss_dice_0: 1.401  loss_ce_1: 1.107  loss_mask_1: 0.1581  loss_dice_1: 1.257  loss_ce_2: 1.11  loss_mask_2: 0.1463  loss_dice_2: 1.254  loss_ce_3: 1.173  loss_mask_3: 0.1489  loss_dice_3: 1.053  loss_ce_4: 1.115  loss_mask_4: 0.1486  loss_dice_4: 1.157  loss_ce_5: 1.052  loss_mask_5: 0.142  loss_dice_5: 1.165  loss_ce_6: 1.004  loss_mask_6: 0.1427  loss_dice_6: 1.211  loss_ce_7: 0.9373  loss_mask_7: 0.1399  loss_dice_7: 1.063  loss_ce_8: 0.9338  loss_mask_8: 0.1375  loss_dice_8: 1.131     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:35 d2.utils.events]:  eta: 5:15:47  iter: 13999  total_loss: 13.35  loss_ce: 0.5148  loss_mask: 0.0607  loss_dice: 0.4239  loss_ce_0: 1.059  loss_mask_0: 0.1217  loss_dice_0: 0.4262  loss_ce_1: 0.7555  loss_mask_1: 0.0685  loss_dice_1: 0.4674  loss_ce_2: 0.766  loss_mask_2: 0.06185  loss_dice_2: 0.3972  loss_ce_3: 0.6506  loss_mask_3: 0.06762  loss_dice_3: 0.4746  loss_ce_4: 0.7402  loss_mask_4: 0.0535  loss_dice_4: 0.5121  loss_ce_5: 0.5498  loss_mask_5: 0.05781  loss_dice_5: 0.4608  loss_ce_6: 0.5301  loss_mask_6: 0.05249  loss_dice_6: 0.5762  loss_ce_7: 0.5182  loss_mask_7: 0.06234  loss_dice_7: 0.4565  loss_ce_8: 0.5125  loss_mask_8: 0.06267  loss_dice_8: 0.7946     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:43 d2.utils.events]:  eta: 5:23:50  iter: 14019  total_loss: 22.67  loss_ce: 0.7667  loss_mask: 0.05676  loss_dice: 1.073  loss_ce_0: 1.927  loss_mask_0: 0.09086  loss_dice_0: 1.517  loss_ce_1: 1.215  loss_mask_1: 0.08612  loss_dice_1: 1.167  loss_ce_2: 1.124  loss_mask_2: 0.07451  loss_dice_2: 1.238  loss_ce_3: 0.9372  loss_mask_3: 0.06573  loss_dice_3: 1.12  loss_ce_4: 0.9425  loss_mask_4: 0.06004  loss_dice_4: 1.217  loss_ce_5: 0.9669  loss_mask_5: 0.0595  loss_dice_5: 0.9858  loss_ce_6: 0.8893  loss_mask_6: 0.06235  loss_dice_6: 1.185  loss_ce_7: 0.7956  loss_mask_7: 0.05974  loss_dice_7: 1.175  loss_ce_8: 0.8684  loss_mask_8: 0.05825  loss_dice_8: 0.9271     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:10:52 d2.utils.events]:  eta: 5:15:04  iter: 14039  total_loss: 17.6  loss_ce: 0.7356  loss_mask: 0.0733  loss_dice: 0.6437  loss_ce_0: 1.222  loss_mask_0: 0.08341  loss_dice_0: 0.9343  loss_ce_1: 0.8879  loss_mask_1: 0.06823  loss_dice_1: 0.823  loss_ce_2: 0.7879  loss_mask_2: 0.07578  loss_dice_2: 0.8256  loss_ce_3: 0.8241  loss_mask_3: 0.07646  loss_dice_3: 0.6512  loss_ce_4: 0.7543  loss_mask_4: 0.07552  loss_dice_4: 0.573  loss_ce_5: 0.7991  loss_mask_5: 0.07864  loss_dice_5: 0.6876  loss_ce_6: 0.798  loss_mask_6: 0.07288  loss_dice_6: 0.5862  loss_ce_7: 0.7226  loss_mask_7: 0.08462  loss_dice_7: 0.6374  loss_ce_8: 0.7336  loss_mask_8: 0.07034  loss_dice_8: 0.5976     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:00 d2.utils.events]:  eta: 5:25:59  iter: 14059  total_loss: 17.27  loss_ce: 0.6479  loss_mask: 0.07783  loss_dice: 0.5756  loss_ce_0: 1.271  loss_mask_0: 0.08089  loss_dice_0: 0.709  loss_ce_1: 1.007  loss_mask_1: 0.08974  loss_dice_1: 0.6121  loss_ce_2: 0.964  loss_mask_2: 0.1016  loss_dice_2: 0.6785  loss_ce_3: 0.7816  loss_mask_3: 0.07501  loss_dice_3: 0.5581  loss_ce_4: 0.7602  loss_mask_4: 0.07622  loss_dice_4: 0.5485  loss_ce_5: 0.6865  loss_mask_5: 0.0771  loss_dice_5: 0.5956  loss_ce_6: 0.6154  loss_mask_6: 0.07258  loss_dice_6: 0.5006  loss_ce_7: 0.5729  loss_mask_7: 0.07823  loss_dice_7: 0.5623  loss_ce_8: 0.5961  loss_mask_8: 0.08877  loss_dice_8: 0.5883     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:08 d2.utils.events]:  eta: 5:17:03  iter: 14079  total_loss: 20.59  loss_ce: 0.8821  loss_mask: 0.06837  loss_dice: 0.9614  loss_ce_0: 1.297  loss_mask_0: 0.07328  loss_dice_0: 0.9898  loss_ce_1: 1.101  loss_mask_1: 0.0875  loss_dice_1: 0.8369  loss_ce_2: 0.9977  loss_mask_2: 0.07422  loss_dice_2: 0.9481  loss_ce_3: 1.016  loss_mask_3: 0.06926  loss_dice_3: 0.7802  loss_ce_4: 0.9676  loss_mask_4: 0.06944  loss_dice_4: 0.8008  loss_ce_5: 0.8901  loss_mask_5: 0.07184  loss_dice_5: 0.751  loss_ce_6: 0.9466  loss_mask_6: 0.05697  loss_dice_6: 0.896  loss_ce_7: 0.947  loss_mask_7: 0.06406  loss_dice_7: 0.745  loss_ce_8: 0.8431  loss_mask_8: 0.07627  loss_dice_8: 0.8786     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:17 d2.utils.events]:  eta: 5:20:02  iter: 14099  total_loss: 21.13  loss_ce: 0.9537  loss_mask: 0.05632  loss_dice: 0.9676  loss_ce_0: 1.815  loss_mask_0: 0.09224  loss_dice_0: 1.141  loss_ce_1: 1.236  loss_mask_1: 0.07625  loss_dice_1: 1.02  loss_ce_2: 1.057  loss_mask_2: 0.06215  loss_dice_2: 1.056  loss_ce_3: 1.048  loss_mask_3: 0.05879  loss_dice_3: 1.005  loss_ce_4: 1.03  loss_mask_4: 0.06291  loss_dice_4: 0.9898  loss_ce_5: 1.005  loss_mask_5: 0.06653  loss_dice_5: 0.869  loss_ce_6: 0.9416  loss_mask_6: 0.05505  loss_dice_6: 0.8389  loss_ce_7: 1.02  loss_mask_7: 0.06279  loss_dice_7: 0.9039  loss_ce_8: 0.9991  loss_mask_8: 0.06012  loss_dice_8: 0.8964     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:25 d2.utils.events]:  eta: 5:16:49  iter: 14119  total_loss: 19.53  loss_ce: 0.7924  loss_mask: 0.1144  loss_dice: 0.7815  loss_ce_0: 1.402  loss_mask_0: 0.1467  loss_dice_0: 0.9365  loss_ce_1: 0.9403  loss_mask_1: 0.1238  loss_dice_1: 1.03  loss_ce_2: 0.8201  loss_mask_2: 0.1099  loss_dice_2: 0.9607  loss_ce_3: 0.8245  loss_mask_3: 0.1085  loss_dice_3: 0.8166  loss_ce_4: 0.806  loss_mask_4: 0.1186  loss_dice_4: 0.8614  loss_ce_5: 0.868  loss_mask_5: 0.1353  loss_dice_5: 0.8624  loss_ce_6: 0.878  loss_mask_6: 0.1174  loss_dice_6: 0.7511  loss_ce_7: 0.8619  loss_mask_7: 0.1221  loss_dice_7: 0.7877  loss_ce_8: 0.8362  loss_mask_8: 0.1204  loss_dice_8: 0.7776     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:34 d2.utils.events]:  eta: 5:23:22  iter: 14139  total_loss: 16.25  loss_ce: 0.7196  loss_mask: 0.04795  loss_dice: 0.703  loss_ce_0: 1.576  loss_mask_0: 0.05554  loss_dice_0: 0.8182  loss_ce_1: 0.8948  loss_mask_1: 0.0674  loss_dice_1: 0.9072  loss_ce_2: 0.8301  loss_mask_2: 0.04592  loss_dice_2: 0.5899  loss_ce_3: 0.6904  loss_mask_3: 0.04728  loss_dice_3: 0.755  loss_ce_4: 0.5568  loss_mask_4: 0.0403  loss_dice_4: 0.8378  loss_ce_5: 0.6415  loss_mask_5: 0.0468  loss_dice_5: 0.6718  loss_ce_6: 0.7048  loss_mask_6: 0.04101  loss_dice_6: 0.7117  loss_ce_7: 0.6373  loss_mask_7: 0.04731  loss_dice_7: 0.75  loss_ce_8: 0.6428  loss_mask_8: 0.04726  loss_dice_8: 0.8487     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:42 d2.utils.events]:  eta: 5:11:29  iter: 14159  total_loss: 16.18  loss_ce: 0.7913  loss_mask: 0.06564  loss_dice: 0.6312  loss_ce_0: 1.575  loss_mask_0: 0.06182  loss_dice_0: 0.8811  loss_ce_1: 1.109  loss_mask_1: 0.08903  loss_dice_1: 0.692  loss_ce_2: 0.8773  loss_mask_2: 0.05833  loss_dice_2: 0.6989  loss_ce_3: 0.8707  loss_mask_3: 0.05713  loss_dice_3: 0.6358  loss_ce_4: 0.8272  loss_mask_4: 0.06934  loss_dice_4: 0.8201  loss_ce_5: 0.8061  loss_mask_5: 0.05876  loss_dice_5: 0.5158  loss_ce_6: 0.7409  loss_mask_6: 0.07304  loss_dice_6: 0.8079  loss_ce_7: 0.8098  loss_mask_7: 0.06756  loss_dice_7: 0.7181  loss_ce_8: 0.7486  loss_mask_8: 0.06109  loss_dice_8: 0.7466     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:50 d2.utils.events]:  eta: 5:16:17  iter: 14179  total_loss: 14.82  loss_ce: 0.6553  loss_mask: 0.0615  loss_dice: 0.4606  loss_ce_0: 1.318  loss_mask_0: 0.08581  loss_dice_0: 0.6491  loss_ce_1: 0.9567  loss_mask_1: 0.06999  loss_dice_1: 0.5857  loss_ce_2: 0.807  loss_mask_2: 0.069  loss_dice_2: 0.4867  loss_ce_3: 0.6348  loss_mask_3: 0.06558  loss_dice_3: 0.479  loss_ce_4: 0.5778  loss_mask_4: 0.05607  loss_dice_4: 0.4986  loss_ce_5: 0.6479  loss_mask_5: 0.06047  loss_dice_5: 0.4994  loss_ce_6: 0.6331  loss_mask_6: 0.05691  loss_dice_6: 0.4817  loss_ce_7: 0.5658  loss_mask_7: 0.06608  loss_dice_7: 0.4252  loss_ce_8: 0.6606  loss_mask_8: 0.05925  loss_dice_8: 0.5118     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:11:59 d2.utils.events]:  eta: 5:25:03  iter: 14199  total_loss: 15.08  loss_ce: 0.5331  loss_mask: 0.1135  loss_dice: 0.3926  loss_ce_0: 1.371  loss_mask_0: 0.1104  loss_dice_0: 0.8226  loss_ce_1: 0.8284  loss_mask_1: 0.1212  loss_dice_1: 0.6953  loss_ce_2: 0.7273  loss_mask_2: 0.1178  loss_dice_2: 0.7639  loss_ce_3: 0.6691  loss_mask_3: 0.1124  loss_dice_3: 0.5191  loss_ce_4: 0.5733  loss_mask_4: 0.1174  loss_dice_4: 0.6378  loss_ce_5: 0.587  loss_mask_5: 0.1194  loss_dice_5: 0.4817  loss_ce_6: 0.5926  loss_mask_6: 0.1051  loss_dice_6: 0.7354  loss_ce_7: 0.5691  loss_mask_7: 0.1026  loss_dice_7: 0.5928  loss_ce_8: 0.5636  loss_mask_8: 0.1144  loss_dice_8: 0.5914     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:07 d2.utils.events]:  eta: 5:25:51  iter: 14219  total_loss: 24.68  loss_ce: 0.6977  loss_mask: 0.1412  loss_dice: 1.133  loss_ce_0: 1.419  loss_mask_0: 0.1509  loss_dice_0: 1.119  loss_ce_1: 0.9813  loss_mask_1: 0.1016  loss_dice_1: 1.101  loss_ce_2: 0.9465  loss_mask_2: 0.2033  loss_dice_2: 1.032  loss_ce_3: 0.8286  loss_mask_3: 0.1527  loss_dice_3: 1.168  loss_ce_4: 0.7923  loss_mask_4: 0.1622  loss_dice_4: 1.118  loss_ce_5: 0.7416  loss_mask_5: 0.1419  loss_dice_5: 1.199  loss_ce_6: 0.7139  loss_mask_6: 0.1476  loss_dice_6: 1.042  loss_ce_7: 0.747  loss_mask_7: 0.1914  loss_dice_7: 1.16  loss_ce_8: 0.6812  loss_mask_8: 0.1462  loss_dice_8: 1.113     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:15 d2.utils.events]:  eta: 5:14:02  iter: 14239  total_loss: 14.72  loss_ce: 0.5675  loss_mask: 0.07764  loss_dice: 0.6081  loss_ce_0: 0.9762  loss_mask_0: 0.08816  loss_dice_0: 0.7834  loss_ce_1: 0.6501  loss_mask_1: 0.08714  loss_dice_1: 0.5813  loss_ce_2: 0.6478  loss_mask_2: 0.06909  loss_dice_2: 0.9064  loss_ce_3: 0.653  loss_mask_3: 0.06055  loss_dice_3: 0.621  loss_ce_4: 0.5728  loss_mask_4: 0.0688  loss_dice_4: 0.6436  loss_ce_5: 0.567  loss_mask_5: 0.06881  loss_dice_5: 0.751  loss_ce_6: 0.5355  loss_mask_6: 0.07871  loss_dice_6: 0.5981  loss_ce_7: 0.6204  loss_mask_7: 0.06478  loss_dice_7: 0.7332  loss_ce_8: 0.603  loss_mask_8: 0.06425  loss_dice_8: 0.5257     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:24 d2.utils.events]:  eta: 5:20:35  iter: 14259  total_loss: 20.3  loss_ce: 0.8177  loss_mask: 0.07577  loss_dice: 1.089  loss_ce_0: 1.584  loss_mask_0: 0.1013  loss_dice_0: 1.136  loss_ce_1: 1.206  loss_mask_1: 0.1018  loss_dice_1: 1.024  loss_ce_2: 1.034  loss_mask_2: 0.07633  loss_dice_2: 0.8885  loss_ce_3: 0.8403  loss_mask_3: 0.07549  loss_dice_3: 0.8716  loss_ce_4: 0.8294  loss_mask_4: 0.08294  loss_dice_4: 0.9199  loss_ce_5: 0.7611  loss_mask_5: 0.08644  loss_dice_5: 0.9973  loss_ce_6: 0.7699  loss_mask_6: 0.06541  loss_dice_6: 0.6613  loss_ce_7: 0.7869  loss_mask_7: 0.08401  loss_dice_7: 0.7212  loss_ce_8: 0.8397  loss_mask_8: 0.07043  loss_dice_8: 0.676     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:32 d2.utils.events]:  eta: 5:10:51  iter: 14279  total_loss: 21.08  loss_ce: 0.8226  loss_mask: 0.08074  loss_dice: 0.8116  loss_ce_0: 1.519  loss_mask_0: 0.09479  loss_dice_0: 0.9704  loss_ce_1: 1.054  loss_mask_1: 0.07962  loss_dice_1: 0.9201  loss_ce_2: 1.13  loss_mask_2: 0.08639  loss_dice_2: 0.9669  loss_ce_3: 0.9504  loss_mask_3: 0.0809  loss_dice_3: 0.9198  loss_ce_4: 0.9285  loss_mask_4: 0.0687  loss_dice_4: 0.7708  loss_ce_5: 0.8669  loss_mask_5: 0.08149  loss_dice_5: 0.8305  loss_ce_6: 0.9193  loss_mask_6: 0.07171  loss_dice_6: 0.9159  loss_ce_7: 1.02  loss_mask_7: 0.0721  loss_dice_7: 0.668  loss_ce_8: 0.8846  loss_mask_8: 0.07624  loss_dice_8: 0.9349     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:40 d2.utils.events]:  eta: 5:23:00  iter: 14299  total_loss: 16.59  loss_ce: 0.8154  loss_mask: 0.08087  loss_dice: 0.7087  loss_ce_0: 1.5  loss_mask_0: 0.1005  loss_dice_0: 0.7129  loss_ce_1: 1.009  loss_mask_1: 0.1019  loss_dice_1: 0.5699  loss_ce_2: 0.8986  loss_mask_2: 0.07869  loss_dice_2: 0.5795  loss_ce_3: 0.8385  loss_mask_3: 0.08655  loss_dice_3: 0.6423  loss_ce_4: 0.7973  loss_mask_4: 0.08172  loss_dice_4: 0.5388  loss_ce_5: 0.8238  loss_mask_5: 0.08689  loss_dice_5: 0.605  loss_ce_6: 0.8318  loss_mask_6: 0.08292  loss_dice_6: 0.5522  loss_ce_7: 0.8163  loss_mask_7: 0.08471  loss_dice_7: 0.7354  loss_ce_8: 0.8064  loss_mask_8: 0.07422  loss_dice_8: 0.4788     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:49 d2.utils.events]:  eta: 5:26:10  iter: 14319  total_loss: 20.72  loss_ce: 0.6483  loss_mask: 0.08093  loss_dice: 1.054  loss_ce_0: 1.544  loss_mask_0: 0.09  loss_dice_0: 1.257  loss_ce_1: 0.8829  loss_mask_1: 0.1027  loss_dice_1: 1.318  loss_ce_2: 0.8564  loss_mask_2: 0.0721  loss_dice_2: 1.009  loss_ce_3: 0.7443  loss_mask_3: 0.0867  loss_dice_3: 1.052  loss_ce_4: 0.7066  loss_mask_4: 0.07636  loss_dice_4: 1.233  loss_ce_5: 0.6409  loss_mask_5: 0.07037  loss_dice_5: 1.155  loss_ce_6: 0.6217  loss_mask_6: 0.08192  loss_dice_6: 0.9901  loss_ce_7: 0.6578  loss_mask_7: 0.08022  loss_dice_7: 1.088  loss_ce_8: 0.6129  loss_mask_8: 0.07484  loss_dice_8: 1.085     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:12:58 d2.utils.events]:  eta: 5:29:44  iter: 14339  total_loss: 20.68  loss_ce: 0.6291  loss_mask: 0.06166  loss_dice: 1.005  loss_ce_0: 1.399  loss_mask_0: 0.04144  loss_dice_0: 1.154  loss_ce_1: 0.8728  loss_mask_1: 0.04377  loss_dice_1: 1.073  loss_ce_2: 0.7705  loss_mask_2: 0.06373  loss_dice_2: 1.065  loss_ce_3: 0.7589  loss_mask_3: 0.0591  loss_dice_3: 1.271  loss_ce_4: 0.6314  loss_mask_4: 0.05196  loss_dice_4: 1.049  loss_ce_5: 0.6434  loss_mask_5: 0.07424  loss_dice_5: 1.154  loss_ce_6: 0.6126  loss_mask_6: 0.05541  loss_dice_6: 0.9778  loss_ce_7: 0.6225  loss_mask_7: 0.04789  loss_dice_7: 1.337  loss_ce_8: 0.637  loss_mask_8: 0.05162  loss_dice_8: 1.117     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:13:06 d2.utils.events]:  eta: 5:18:43  iter: 14359  total_loss: 19.57  loss_ce: 0.6699  loss_mask: 0.1255  loss_dice: 0.6473  loss_ce_0: 1.477  loss_mask_0: 0.143  loss_dice_0: 1.239  loss_ce_1: 0.9147  loss_mask_1: 0.133  loss_dice_1: 1.051  loss_ce_2: 0.8974  loss_mask_2: 0.1478  loss_dice_2: 0.9519  loss_ce_3: 0.8258  loss_mask_3: 0.1311  loss_dice_3: 0.7433  loss_ce_4: 0.7818  loss_mask_4: 0.1467  loss_dice_4: 0.6667  loss_ce_5: 0.7822  loss_mask_5: 0.1388  loss_dice_5: 0.686  loss_ce_6: 0.6542  loss_mask_6: 0.1338  loss_dice_6: 0.8069  loss_ce_7: 0.6793  loss_mask_7: 0.1321  loss_dice_7: 0.5973  loss_ce_8: 0.7476  loss_mask_8: 0.1352  loss_dice_8: 0.6606     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:13:14 d2.utils.events]:  eta: 5:16:14  iter: 14379  total_loss: 15.37  loss_ce: 0.6501  loss_mask: 0.08291  loss_dice: 0.6453  loss_ce_0: 1.383  loss_mask_0: 0.1202  loss_dice_0: 0.6301  loss_ce_1: 0.9813  loss_mask_1: 0.09453  loss_dice_1: 0.8182  loss_ce_2: 0.8942  loss_mask_2: 0.09173  loss_dice_2: 0.7728  loss_ce_3: 0.7612  loss_mask_3: 0.07551  loss_dice_3: 0.6937  loss_ce_4: 0.6765  loss_mask_4: 0.07998  loss_dice_4: 0.7349  loss_ce_5: 0.6506  loss_mask_5: 0.08298  loss_dice_5: 0.8031  loss_ce_6: 0.6019  loss_mask_6: 0.07609  loss_dice_6: 0.6899  loss_ce_7: 0.6262  loss_mask_7: 0.07257  loss_dice_7: 0.5784  loss_ce_8: 0.6459  loss_mask_8: 0.07616  loss_dice_8: 0.595     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 24.0 - Losses: {'loss_ce': tensor(1.2156, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.2305, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.2363, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(0.3811, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.6472, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(2.0726, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.6048, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.3889, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.8798, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.7004, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.3715, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.5326, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.8290, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.3425, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.9132, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.4025, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.3486, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(1.3509, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(1.1748, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.2378, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.6182, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.9469, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.3201, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.3787, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(1.2068, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.2283, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.4451, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(1.2217, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.2400, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.3084, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:13:23 d2.utils.events]:  eta: 5:15:13  iter: 14399  total_loss: 19.23  loss_ce: 0.7053  loss_mask: 0.07434  loss_dice: 0.8334  loss_ce_0: 1.286  loss_mask_0: 0.09878  loss_dice_0: 1.003  loss_ce_1: 0.9509  loss_mask_1: 0.115  loss_dice_1: 0.83  loss_ce_2: 0.7197  loss_mask_2: 0.1009  loss_dice_2: 0.7597  loss_ce_3: 0.6949  loss_mask_3: 0.088  loss_dice_3: 0.9329  loss_ce_4: 0.6541  loss_mask_4: 0.0744  loss_dice_4: 0.9302  loss_ce_5: 0.7886  loss_mask_5: 0.08073  loss_dice_5: 0.8411  loss_ce_6: 0.6999  loss_mask_6: 0.08715  loss_dice_6: 0.7295  loss_ce_7: 0.6547  loss_mask_7: 0.08471  loss_dice_7: 0.8396  loss_ce_8: 0.6913  loss_mask_8: 0.09206  loss_dice_8: 0.7902     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:13:31 d2.utils.events]:  eta: 5:14:58  iter: 14419  total_loss: 17.48  loss_ce: 0.6048  loss_mask: 0.06849  loss_dice: 0.6144  loss_ce_0: 1.226  loss_mask_0: 0.1095  loss_dice_0: 0.7595  loss_ce_1: 0.9287  loss_mask_1: 0.08627  loss_dice_1: 0.6984  loss_ce_2: 0.8103  loss_mask_2: 0.08344  loss_dice_2: 0.6832  loss_ce_3: 0.6873  loss_mask_3: 0.07015  loss_dice_3: 0.6057  loss_ce_4: 0.6215  loss_mask_4: 0.06369  loss_dice_4: 0.5586  loss_ce_5: 0.6156  loss_mask_5: 0.07165  loss_dice_5: 0.744  loss_ce_6: 0.6145  loss_mask_6: 0.06745  loss_dice_6: 0.7053  loss_ce_7: 0.5735  loss_mask_7: 0.06379  loss_dice_7: 0.6094  loss_ce_8: 0.6096  loss_mask_8: 0.06412  loss_dice_8: 0.5732     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:13:39 d2.utils.events]:  eta: 5:16:21  iter: 14439  total_loss: 20.32  loss_ce: 0.7482  loss_mask: 0.1023  loss_dice: 0.7163  loss_ce_0: 1.726  loss_mask_0: 0.06054  loss_dice_0: 0.8266  loss_ce_1: 1.138  loss_mask_1: 0.08086  loss_dice_1: 0.8416  loss_ce_2: 1.128  loss_mask_2: 0.08232  loss_dice_2: 0.628  loss_ce_3: 0.9295  loss_mask_3: 0.09706  loss_dice_3: 0.7129  loss_ce_4: 0.8352  loss_mask_4: 0.0973  loss_dice_4: 0.6745  loss_ce_5: 0.8277  loss_mask_5: 0.09801  loss_dice_5: 0.7614  loss_ce_6: 0.7985  loss_mask_6: 0.09842  loss_dice_6: 0.74  loss_ce_7: 0.738  loss_mask_7: 0.09042  loss_dice_7: 0.7695  loss_ce_8: 0.7771  loss_mask_8: 0.09262  loss_dice_8: 0.6494     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:13:48 d2.utils.events]:  eta: 5:19:43  iter: 14459  total_loss: 15.77  loss_ce: 0.6555  loss_mask: 0.07498  loss_dice: 0.5509  loss_ce_0: 1.312  loss_mask_0: 0.08114  loss_dice_0: 0.7512  loss_ce_1: 0.8838  loss_mask_1: 0.0759  loss_dice_1: 0.5857  loss_ce_2: 0.7956  loss_mask_2: 0.07677  loss_dice_2: 0.7838  loss_ce_3: 0.677  loss_mask_3: 0.07553  loss_dice_3: 0.5238  loss_ce_4: 0.6815  loss_mask_4: 0.07799  loss_dice_4: 0.544  loss_ce_5: 0.668  loss_mask_5: 0.07334  loss_dice_5: 0.6047  loss_ce_6: 0.6703  loss_mask_6: 0.06539  loss_dice_6: 0.7147  loss_ce_7: 0.6969  loss_mask_7: 0.06474  loss_dice_7: 0.6038  loss_ce_8: 0.6507  loss_mask_8: 0.06638  loss_dice_8: 0.6376     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:13:56 d2.utils.events]:  eta: 5:12:52  iter: 14479  total_loss: 16.85  loss_ce: 0.8023  loss_mask: 0.07376  loss_dice: 0.6154  loss_ce_0: 1.598  loss_mask_0: 0.118  loss_dice_0: 1.121  loss_ce_1: 1.003  loss_mask_1: 0.08192  loss_dice_1: 0.8703  loss_ce_2: 1.125  loss_mask_2: 0.07721  loss_dice_2: 0.6948  loss_ce_3: 0.8723  loss_mask_3: 0.06506  loss_dice_3: 0.6059  loss_ce_4: 0.8299  loss_mask_4: 0.06984  loss_dice_4: 0.5443  loss_ce_5: 0.9289  loss_mask_5: 0.07457  loss_dice_5: 0.6778  loss_ce_6: 0.7684  loss_mask_6: 0.07933  loss_dice_6: 0.8512  loss_ce_7: 0.7591  loss_mask_7: 0.07323  loss_dice_7: 0.7905  loss_ce_8: 0.8342  loss_mask_8: 0.06536  loss_dice_8: 0.709     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:04 d2.utils.events]:  eta: 5:12:41  iter: 14499  total_loss: 18.84  loss_ce: 0.8327  loss_mask: 0.08999  loss_dice: 0.9802  loss_ce_0: 1.16  loss_mask_0: 0.09073  loss_dice_0: 0.8646  loss_ce_1: 0.9994  loss_mask_1: 0.1198  loss_dice_1: 0.9403  loss_ce_2: 0.763  loss_mask_2: 0.1192  loss_dice_2: 0.9759  loss_ce_3: 0.7368  loss_mask_3: 0.09991  loss_dice_3: 0.895  loss_ce_4: 0.7243  loss_mask_4: 0.1008  loss_dice_4: 0.8987  loss_ce_5: 0.7659  loss_mask_5: 0.08489  loss_dice_5: 0.8123  loss_ce_6: 0.7231  loss_mask_6: 0.08651  loss_dice_6: 0.9265  loss_ce_7: 0.7757  loss_mask_7: 0.08599  loss_dice_7: 0.8612  loss_ce_8: 0.7381  loss_mask_8: 0.08445  loss_dice_8: 1.094     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:13 d2.utils.events]:  eta: 5:18:49  iter: 14519  total_loss: 19.11  loss_ce: 0.6662  loss_mask: 0.05834  loss_dice: 0.7528  loss_ce_0: 1.195  loss_mask_0: 0.07108  loss_dice_0: 0.9431  loss_ce_1: 1.035  loss_mask_1: 0.08123  loss_dice_1: 0.7687  loss_ce_2: 1.028  loss_mask_2: 0.07169  loss_dice_2: 0.7323  loss_ce_3: 0.7953  loss_mask_3: 0.05471  loss_dice_3: 0.8022  loss_ce_4: 0.8081  loss_mask_4: 0.05488  loss_dice_4: 0.7349  loss_ce_5: 0.7303  loss_mask_5: 0.05674  loss_dice_5: 0.8707  loss_ce_6: 0.6761  loss_mask_6: 0.05894  loss_dice_6: 0.7814  loss_ce_7: 0.6415  loss_mask_7: 0.05791  loss_dice_7: 0.9629  loss_ce_8: 0.6012  loss_mask_8: 0.05275  loss_dice_8: 0.6722     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:21 d2.utils.events]:  eta: 5:16:25  iter: 14539  total_loss: 27.4  loss_ce: 0.9723  loss_mask: 0.08912  loss_dice: 1.002  loss_ce_0: 1.821  loss_mask_0: 0.1046  loss_dice_0: 1.384  loss_ce_1: 1.342  loss_mask_1: 0.1267  loss_dice_1: 1.16  loss_ce_2: 1.284  loss_mask_2: 0.1131  loss_dice_2: 1.202  loss_ce_3: 1.198  loss_mask_3: 0.1011  loss_dice_3: 1.182  loss_ce_4: 1.109  loss_mask_4: 0.1104  loss_dice_4: 1.171  loss_ce_5: 1.084  loss_mask_5: 0.09055  loss_dice_5: 1.268  loss_ce_6: 1.067  loss_mask_6: 0.07974  loss_dice_6: 1.027  loss_ce_7: 0.9749  loss_mask_7: 0.08164  loss_dice_7: 1.103  loss_ce_8: 0.9432  loss_mask_8: 0.09358  loss_dice_8: 1.127     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:29 d2.utils.events]:  eta: 5:23:15  iter: 14559  total_loss: 19.6  loss_ce: 0.914  loss_mask: 0.1156  loss_dice: 0.6969  loss_ce_0: 1.284  loss_mask_0: 0.1232  loss_dice_0: 1.185  loss_ce_1: 0.9487  loss_mask_1: 0.09884  loss_dice_1: 0.8936  loss_ce_2: 1.056  loss_mask_2: 0.09878  loss_dice_2: 0.7914  loss_ce_3: 0.8945  loss_mask_3: 0.1022  loss_dice_3: 0.9126  loss_ce_4: 0.8766  loss_mask_4: 0.09745  loss_dice_4: 0.7589  loss_ce_5: 0.9396  loss_mask_5: 0.09294  loss_dice_5: 0.9173  loss_ce_6: 0.8257  loss_mask_6: 0.1095  loss_dice_6: 0.8025  loss_ce_7: 0.8106  loss_mask_7: 0.1044  loss_dice_7: 0.8074  loss_ce_8: 0.8644  loss_mask_8: 0.108  loss_dice_8: 0.7736     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:38 d2.utils.events]:  eta: 5:09:52  iter: 14579  total_loss: 20.68  loss_ce: 0.9326  loss_mask: 0.1467  loss_dice: 0.6961  loss_ce_0: 1.623  loss_mask_0: 0.1597  loss_dice_0: 1.011  loss_ce_1: 1.146  loss_mask_1: 0.1499  loss_dice_1: 0.892  loss_ce_2: 1.109  loss_mask_2: 0.136  loss_dice_2: 0.7441  loss_ce_3: 1.039  loss_mask_3: 0.1591  loss_dice_3: 0.6932  loss_ce_4: 1.042  loss_mask_4: 0.1307  loss_dice_4: 0.8676  loss_ce_5: 0.9932  loss_mask_5: 0.1419  loss_dice_5: 0.7045  loss_ce_6: 1.037  loss_mask_6: 0.1222  loss_dice_6: 0.6585  loss_ce_7: 0.9598  loss_mask_7: 0.1443  loss_dice_7: 0.6432  loss_ce_8: 0.9458  loss_mask_8: 0.1507  loss_dice_8: 0.7756     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:46 d2.utils.events]:  eta: 5:22:30  iter: 14599  total_loss: 19.51  loss_ce: 0.8753  loss_mask: 0.07193  loss_dice: 0.8697  loss_ce_0: 1.399  loss_mask_0: 0.08458  loss_dice_0: 1.1  loss_ce_1: 1.113  loss_mask_1: 0.08309  loss_dice_1: 0.9205  loss_ce_2: 0.9612  loss_mask_2: 0.09238  loss_dice_2: 0.8719  loss_ce_3: 0.9026  loss_mask_3: 0.08727  loss_dice_3: 0.9644  loss_ce_4: 0.7675  loss_mask_4: 0.06572  loss_dice_4: 0.9239  loss_ce_5: 0.9387  loss_mask_5: 0.05637  loss_dice_5: 0.9691  loss_ce_6: 0.9028  loss_mask_6: 0.0626  loss_dice_6: 0.7936  loss_ce_7: 0.8787  loss_mask_7: 0.06699  loss_dice_7: 0.8777  loss_ce_8: 0.893  loss_mask_8: 0.0592  loss_dice_8: 0.9112     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:14:54 d2.utils.events]:  eta: 5:08:57  iter: 14619  total_loss: 22.83  loss_ce: 0.8229  loss_mask: 0.08457  loss_dice: 0.9142  loss_ce_0: 1.455  loss_mask_0: 0.08344  loss_dice_0: 1.158  loss_ce_1: 1.009  loss_mask_1: 0.07465  loss_dice_1: 1.124  loss_ce_2: 0.9457  loss_mask_2: 0.08523  loss_dice_2: 1.299  loss_ce_3: 0.902  loss_mask_3: 0.08952  loss_dice_3: 0.9277  loss_ce_4: 0.8842  loss_mask_4: 0.09339  loss_dice_4: 0.9348  loss_ce_5: 0.9336  loss_mask_5: 0.08893  loss_dice_5: 0.9418  loss_ce_6: 0.8674  loss_mask_6: 0.1001  loss_dice_6: 0.8431  loss_ce_7: 0.7952  loss_mask_7: 0.09583  loss_dice_7: 0.887  loss_ce_8: 0.8643  loss_mask_8: 0.08737  loss_dice_8: 0.7765     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:03 d2.utils.events]:  eta: 5:18:36  iter: 14639  total_loss: 18.37  loss_ce: 0.6541  loss_mask: 0.05715  loss_dice: 0.7089  loss_ce_0: 1.409  loss_mask_0: 0.08544  loss_dice_0: 0.9098  loss_ce_1: 1.039  loss_mask_1: 0.09515  loss_dice_1: 0.6943  loss_ce_2: 1.016  loss_mask_2: 0.06671  loss_dice_2: 0.6275  loss_ce_3: 0.7614  loss_mask_3: 0.06578  loss_dice_3: 0.6557  loss_ce_4: 0.7329  loss_mask_4: 0.06887  loss_dice_4: 0.6386  loss_ce_5: 0.6811  loss_mask_5: 0.05746  loss_dice_5: 0.7569  loss_ce_6: 0.6424  loss_mask_6: 0.05888  loss_dice_6: 0.6558  loss_ce_7: 0.656  loss_mask_7: 0.05624  loss_dice_7: 0.5559  loss_ce_8: 0.6609  loss_mask_8: 0.05408  loss_dice_8: 0.674     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:11 d2.utils.events]:  eta: 5:26:23  iter: 14659  total_loss: 14.07  loss_ce: 0.6805  loss_mask: 0.05167  loss_dice: 0.473  loss_ce_0: 1.198  loss_mask_0: 0.06282  loss_dice_0: 0.8602  loss_ce_1: 0.9273  loss_mask_1: 0.04792  loss_dice_1: 0.8032  loss_ce_2: 0.7674  loss_mask_2: 0.04396  loss_dice_2: 0.6937  loss_ce_3: 0.7597  loss_mask_3: 0.04008  loss_dice_3: 0.617  loss_ce_4: 0.6901  loss_mask_4: 0.04504  loss_dice_4: 0.4677  loss_ce_5: 0.713  loss_mask_5: 0.05145  loss_dice_5: 0.6563  loss_ce_6: 0.6559  loss_mask_6: 0.05373  loss_dice_6: 0.5506  loss_ce_7: 0.685  loss_mask_7: 0.04344  loss_dice_7: 0.5248  loss_ce_8: 0.6814  loss_mask_8: 0.0462  loss_dice_8: 0.5701     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:20 d2.utils.events]:  eta: 5:20:06  iter: 14679  total_loss: 21.07  loss_ce: 0.8839  loss_mask: 0.06624  loss_dice: 0.8305  loss_ce_0: 1.474  loss_mask_0: 0.09001  loss_dice_0: 0.8914  loss_ce_1: 1.086  loss_mask_1: 0.08544  loss_dice_1: 0.7077  loss_ce_2: 0.9833  loss_mask_2: 0.08539  loss_dice_2: 0.793  loss_ce_3: 0.9968  loss_mask_3: 0.07065  loss_dice_3: 0.7176  loss_ce_4: 0.9672  loss_mask_4: 0.06379  loss_dice_4: 0.8558  loss_ce_5: 0.9778  loss_mask_5: 0.06854  loss_dice_5: 0.8117  loss_ce_6: 0.9322  loss_mask_6: 0.05995  loss_dice_6: 0.6876  loss_ce_7: 1.017  loss_mask_7: 0.06079  loss_dice_7: 0.7084  loss_ce_8: 1.224  loss_mask_8: 0.06658  loss_dice_8: 0.9345     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:28 d2.utils.events]:  eta: 5:15:09  iter: 14699  total_loss: 14.82  loss_ce: 0.5972  loss_mask: 0.1145  loss_dice: 0.4514  loss_ce_0: 1.001  loss_mask_0: 0.1631  loss_dice_0: 0.8522  loss_ce_1: 0.7702  loss_mask_1: 0.1535  loss_dice_1: 0.6279  loss_ce_2: 0.701  loss_mask_2: 0.1381  loss_dice_2: 0.5394  loss_ce_3: 0.6372  loss_mask_3: 0.1133  loss_dice_3: 0.5795  loss_ce_4: 0.6128  loss_mask_4: 0.1194  loss_dice_4: 0.5628  loss_ce_5: 0.5829  loss_mask_5: 0.1302  loss_dice_5: 0.5177  loss_ce_6: 0.5937  loss_mask_6: 0.1232  loss_dice_6: 0.5527  loss_ce_7: 0.5999  loss_mask_7: 0.1281  loss_dice_7: 0.4969  loss_ce_8: 0.6213  loss_mask_8: 0.1163  loss_dice_8: 0.4834     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:37 d2.utils.events]:  eta: 5:22:09  iter: 14719  total_loss: 21.74  loss_ce: 0.8839  loss_mask: 0.09124  loss_dice: 0.8559  loss_ce_0: 1.358  loss_mask_0: 0.09224  loss_dice_0: 1.102  loss_ce_1: 1.16  loss_mask_1: 0.1051  loss_dice_1: 1.184  loss_ce_2: 0.9777  loss_mask_2: 0.09144  loss_dice_2: 1.01  loss_ce_3: 0.919  loss_mask_3: 0.07894  loss_dice_3: 0.8018  loss_ce_4: 0.9443  loss_mask_4: 0.08762  loss_dice_4: 0.8323  loss_ce_5: 0.9577  loss_mask_5: 0.09543  loss_dice_5: 0.9748  loss_ce_6: 0.9088  loss_mask_6: 0.09342  loss_dice_6: 0.7841  loss_ce_7: 0.8574  loss_mask_7: 0.1009  loss_dice_7: 0.9946  loss_ce_8: 0.9283  loss_mask_8: 0.08752  loss_dice_8: 0.9146     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:45 d2.utils.events]:  eta: 5:14:22  iter: 14739  total_loss: 24.93  loss_ce: 1.086  loss_mask: 0.09465  loss_dice: 0.9686  loss_ce_0: 1.333  loss_mask_0: 0.09656  loss_dice_0: 1.332  loss_ce_1: 1.016  loss_mask_1: 0.1144  loss_dice_1: 1.109  loss_ce_2: 1.078  loss_mask_2: 0.1257  loss_dice_2: 1.317  loss_ce_3: 1.037  loss_mask_3: 0.1034  loss_dice_3: 0.814  loss_ce_4: 1.027  loss_mask_4: 0.09565  loss_dice_4: 1.073  loss_ce_5: 1.044  loss_mask_5: 0.08013  loss_dice_5: 1.077  loss_ce_6: 1.043  loss_mask_6: 0.08598  loss_dice_6: 1.037  loss_ce_7: 1.04  loss_mask_7: 0.08279  loss_dice_7: 1.166  loss_ce_8: 1.028  loss_mask_8: 0.08898  loss_dice_8: 1.049     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:15:53 d2.utils.events]:  eta: 5:12:38  iter: 14759  total_loss: 18.61  loss_ce: 0.7452  loss_mask: 0.05763  loss_dice: 0.7365  loss_ce_0: 1.252  loss_mask_0: 0.05838  loss_dice_0: 1.156  loss_ce_1: 0.9613  loss_mask_1: 0.06839  loss_dice_1: 1.07  loss_ce_2: 0.8336  loss_mask_2: 0.07592  loss_dice_2: 0.7641  loss_ce_3: 0.9695  loss_mask_3: 0.04504  loss_dice_3: 0.6963  loss_ce_4: 0.7718  loss_mask_4: 0.04758  loss_dice_4: 0.707  loss_ce_5: 0.8035  loss_mask_5: 0.05297  loss_dice_5: 0.8016  loss_ce_6: 0.8052  loss_mask_6: 0.0529  loss_dice_6: 0.7682  loss_ce_7: 0.7523  loss_mask_7: 0.05737  loss_dice_7: 0.7518  loss_ce_8: 0.7473  loss_mask_8: 0.05185  loss_dice_8: 0.7828     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:02 d2.utils.events]:  eta: 5:15:58  iter: 14779  total_loss: 18.89  loss_ce: 0.8012  loss_mask: 0.04131  loss_dice: 0.7629  loss_ce_0: 1.201  loss_mask_0: 0.07048  loss_dice_0: 1.092  loss_ce_1: 1.077  loss_mask_1: 0.06639  loss_dice_1: 0.9015  loss_ce_2: 1.018  loss_mask_2: 0.05968  loss_dice_2: 0.9071  loss_ce_3: 0.957  loss_mask_3: 0.0481  loss_dice_3: 0.7826  loss_ce_4: 0.7892  loss_mask_4: 0.04775  loss_dice_4: 0.7865  loss_ce_5: 0.8814  loss_mask_5: 0.04947  loss_dice_5: 0.8774  loss_ce_6: 0.7295  loss_mask_6: 0.06157  loss_dice_6: 0.7774  loss_ce_7: 0.7147  loss_mask_7: 0.04382  loss_dice_7: 0.7767  loss_ce_8: 0.7569  loss_mask_8: 0.05145  loss_dice_8: 0.7207     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:10 d2.utils.events]:  eta: 5:05:11  iter: 14799  total_loss: 24.42  loss_ce: 0.9002  loss_mask: 0.1419  loss_dice: 1.178  loss_ce_0: 1.659  loss_mask_0: 0.1911  loss_dice_0: 1.232  loss_ce_1: 1.33  loss_mask_1: 0.1472  loss_dice_1: 1.369  loss_ce_2: 1.144  loss_mask_2: 0.1451  loss_dice_2: 1.084  loss_ce_3: 1.003  loss_mask_3: 0.1079  loss_dice_3: 0.8437  loss_ce_4: 1.017  loss_mask_4: 0.1309  loss_dice_4: 1.093  loss_ce_5: 0.9862  loss_mask_5: 0.1339  loss_dice_5: 0.8755  loss_ce_6: 0.9441  loss_mask_6: 0.1366  loss_dice_6: 1.068  loss_ce_7: 0.9289  loss_mask_7: 0.1311  loss_dice_7: 0.8245  loss_ce_8: 0.915  loss_mask_8: 0.1644  loss_dice_8: 1.003     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:18 d2.utils.events]:  eta: 5:08:59  iter: 14819  total_loss: 21.56  loss_ce: 0.7243  loss_mask: 0.09238  loss_dice: 1.226  loss_ce_0: 1.192  loss_mask_0: 0.1286  loss_dice_0: 1.248  loss_ce_1: 0.8174  loss_mask_1: 0.1358  loss_dice_1: 1.146  loss_ce_2: 0.8271  loss_mask_2: 0.1274  loss_dice_2: 1.106  loss_ce_3: 0.9348  loss_mask_3: 0.1144  loss_dice_3: 1.082  loss_ce_4: 0.9043  loss_mask_4: 0.1167  loss_dice_4: 1.271  loss_ce_5: 0.7656  loss_mask_5: 0.1091  loss_dice_5: 1.123  loss_ce_6: 0.7663  loss_mask_6: 0.09508  loss_dice_6: 1.121  loss_ce_7: 0.7652  loss_mask_7: 0.09313  loss_dice_7: 1.009  loss_ce_8: 0.7355  loss_mask_8: 0.09037  loss_dice_8: 0.9694     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:26 d2.utils.events]:  eta: 5:08:26  iter: 14839  total_loss: 22.45  loss_ce: 0.691  loss_mask: 0.09112  loss_dice: 1.098  loss_ce_0: 1.455  loss_mask_0: 0.1014  loss_dice_0: 0.8829  loss_ce_1: 1.026  loss_mask_1: 0.09471  loss_dice_1: 0.9942  loss_ce_2: 0.878  loss_mask_2: 0.1013  loss_dice_2: 0.9409  loss_ce_3: 0.7355  loss_mask_3: 0.09668  loss_dice_3: 1.054  loss_ce_4: 0.7023  loss_mask_4: 0.1061  loss_dice_4: 0.9663  loss_ce_5: 0.8918  loss_mask_5: 0.08474  loss_dice_5: 0.9785  loss_ce_6: 0.7598  loss_mask_6: 0.09428  loss_dice_6: 0.9301  loss_ce_7: 0.7623  loss_mask_7: 0.09206  loss_dice_7: 1.012  loss_ce_8: 0.8069  loss_mask_8: 0.07032  loss_dice_8: 0.8958     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:35 d2.utils.events]:  eta: 5:14:28  iter: 14859  total_loss: 20.75  loss_ce: 0.7372  loss_mask: 0.09363  loss_dice: 0.7397  loss_ce_0: 1.576  loss_mask_0: 0.1239  loss_dice_0: 1.202  loss_ce_1: 0.9045  loss_mask_1: 0.1615  loss_dice_1: 1.065  loss_ce_2: 0.8339  loss_mask_2: 0.09385  loss_dice_2: 1.08  loss_ce_3: 0.7713  loss_mask_3: 0.09262  loss_dice_3: 0.8519  loss_ce_4: 0.819  loss_mask_4: 0.09397  loss_dice_4: 0.8764  loss_ce_5: 0.8068  loss_mask_5: 0.09238  loss_dice_5: 0.9535  loss_ce_6: 0.7031  loss_mask_6: 0.09392  loss_dice_6: 0.8692  loss_ce_7: 0.7427  loss_mask_7: 0.1033  loss_dice_7: 0.9427  loss_ce_8: 0.7551  loss_mask_8: 0.1033  loss_dice_8: 0.8362     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:43 d2.utils.events]:  eta: 5:23:14  iter: 14879  total_loss: 19.36  loss_ce: 0.7139  loss_mask: 0.1176  loss_dice: 0.8117  loss_ce_0: 1.453  loss_mask_0: 0.1089  loss_dice_0: 1.215  loss_ce_1: 1.019  loss_mask_1: 0.1179  loss_dice_1: 0.7721  loss_ce_2: 0.8715  loss_mask_2: 0.1209  loss_dice_2: 0.8116  loss_ce_3: 0.828  loss_mask_3: 0.1196  loss_dice_3: 0.8092  loss_ce_4: 0.7512  loss_mask_4: 0.1073  loss_dice_4: 0.7205  loss_ce_5: 0.7597  loss_mask_5: 0.117  loss_dice_5: 0.7039  loss_ce_6: 0.7837  loss_mask_6: 0.1192  loss_dice_6: 0.6796  loss_ce_7: 0.7794  loss_mask_7: 0.1199  loss_dice_7: 0.7002  loss_ce_8: 0.7854  loss_mask_8: 0.1056  loss_dice_8: 0.7947     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:16:51 d2.utils.events]:  eta: 5:09:49  iter: 14899  total_loss: 17.11  loss_ce: 0.7218  loss_mask: 0.1244  loss_dice: 0.7206  loss_ce_0: 1.346  loss_mask_0: 0.1385  loss_dice_0: 0.8638  loss_ce_1: 0.9969  loss_mask_1: 0.1177  loss_dice_1: 0.7366  loss_ce_2: 0.9345  loss_mask_2: 0.1263  loss_dice_2: 0.7688  loss_ce_3: 0.6911  loss_mask_3: 0.1326  loss_dice_3: 0.7159  loss_ce_4: 0.7262  loss_mask_4: 0.1308  loss_dice_4: 0.6049  loss_ce_5: 0.8083  loss_mask_5: 0.1237  loss_dice_5: 0.6476  loss_ce_6: 0.6505  loss_mask_6: 0.121  loss_dice_6: 0.641  loss_ce_7: 0.6286  loss_mask_7: 0.113  loss_dice_7: 0.6101  loss_ce_8: 0.736  loss_mask_8: 0.1205  loss_dice_8: 0.7037     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:00 d2.utils.events]:  eta: 5:08:19  iter: 14919  total_loss: 16.89  loss_ce: 0.6418  loss_mask: 0.09974  loss_dice: 0.7823  loss_ce_0: 1.08  loss_mask_0: 0.07546  loss_dice_0: 0.9461  loss_ce_1: 0.7963  loss_mask_1: 0.08025  loss_dice_1: 0.8024  loss_ce_2: 0.7046  loss_mask_2: 0.08439  loss_dice_2: 0.7054  loss_ce_3: 0.6792  loss_mask_3: 0.06714  loss_dice_3: 0.6473  loss_ce_4: 0.7716  loss_mask_4: 0.07109  loss_dice_4: 0.8679  loss_ce_5: 0.7604  loss_mask_5: 0.0811  loss_dice_5: 0.7372  loss_ce_6: 0.6992  loss_mask_6: 0.0574  loss_dice_6: 0.8024  loss_ce_7: 0.7107  loss_mask_7: 0.05622  loss_dice_7: 0.5744  loss_ce_8: 0.6924  loss_mask_8: 0.07032  loss_dice_8: 0.7002     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:08 d2.utils.events]:  eta: 5:09:39  iter: 14939  total_loss: 21.42  loss_ce: 0.8315  loss_mask: 0.06865  loss_dice: 0.739  loss_ce_0: 1.219  loss_mask_0: 0.07667  loss_dice_0: 1.052  loss_ce_1: 0.9588  loss_mask_1: 0.06787  loss_dice_1: 1.045  loss_ce_2: 0.7958  loss_mask_2: 0.08753  loss_dice_2: 1.048  loss_ce_3: 0.8444  loss_mask_3: 0.08364  loss_dice_3: 1.107  loss_ce_4: 0.887  loss_mask_4: 0.07796  loss_dice_4: 0.6823  loss_ce_5: 0.839  loss_mask_5: 0.079  loss_dice_5: 1.106  loss_ce_6: 0.8527  loss_mask_6: 0.07426  loss_dice_6: 0.691  loss_ce_7: 0.8219  loss_mask_7: 0.07624  loss_dice_7: 0.8388  loss_ce_8: 0.8135  loss_mask_8: 0.08251  loss_dice_8: 0.8142     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:16 d2.utils.events]:  eta: 5:09:58  iter: 14959  total_loss: 19.63  loss_ce: 0.7888  loss_mask: 0.08829  loss_dice: 0.9147  loss_ce_0: 1.462  loss_mask_0: 0.1091  loss_dice_0: 1.1  loss_ce_1: 0.9405  loss_mask_1: 0.1145  loss_dice_1: 0.7593  loss_ce_2: 0.932  loss_mask_2: 0.0977  loss_dice_2: 0.853  loss_ce_3: 0.7852  loss_mask_3: 0.09454  loss_dice_3: 0.9091  loss_ce_4: 0.7341  loss_mask_4: 0.08442  loss_dice_4: 0.8768  loss_ce_5: 0.7614  loss_mask_5: 0.095  loss_dice_5: 1.035  loss_ce_6: 0.6845  loss_mask_6: 0.08643  loss_dice_6: 0.857  loss_ce_7: 0.669  loss_mask_7: 0.08993  loss_dice_7: 0.9384  loss_ce_8: 0.6698  loss_mask_8: 0.08586  loss_dice_8: 0.7554     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:25 d2.utils.events]:  eta: 5:12:14  iter: 14979  total_loss: 20.83  loss_ce: 0.9072  loss_mask: 0.06733  loss_dice: 0.8127  loss_ce_0: 1.531  loss_mask_0: 0.07129  loss_dice_0: 0.8822  loss_ce_1: 0.9948  loss_mask_1: 0.05778  loss_dice_1: 0.8611  loss_ce_2: 0.8074  loss_mask_2: 0.06998  loss_dice_2: 1.003  loss_ce_3: 0.8484  loss_mask_3: 0.06775  loss_dice_3: 0.9427  loss_ce_4: 0.6971  loss_mask_4: 0.0649  loss_dice_4: 0.9642  loss_ce_5: 0.7345  loss_mask_5: 0.06784  loss_dice_5: 0.8235  loss_ce_6: 0.8042  loss_mask_6: 0.0536  loss_dice_6: 0.9147  loss_ce_7: 0.8105  loss_mask_7: 0.067  loss_dice_7: 1.035  loss_ce_8: 0.9665  loss_mask_8: 0.07575  loss_dice_8: 0.9751     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 25.0 - Losses: {'loss_ce': tensor(0.2519, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.0440, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.1743, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(0.3706, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.0518, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(0.5784, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.2959, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.0533, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.1941, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.2183, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.0487, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.1928, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.2072, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.0413, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.1403, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.2117, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.0452, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.1375, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.2128, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.0412, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.1425, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.2284, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.0540, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.1820, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.2351, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.0439, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.1389, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.2499, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.0404, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.1623, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:17:33 d2.utils.events]:  eta: 5:12:36  iter: 14999  total_loss: 21.97  loss_ce: 0.7677  loss_mask: 0.04279  loss_dice: 0.842  loss_ce_0: 1.89  loss_mask_0: 0.04973  loss_dice_0: 1.183  loss_ce_1: 1.211  loss_mask_1: 0.05073  loss_dice_1: 1.093  loss_ce_2: 0.8737  loss_mask_2: 0.04831  loss_dice_2: 1.163  loss_ce_3: 0.8979  loss_mask_3: 0.05136  loss_dice_3: 0.9444  loss_ce_4: 0.7694  loss_mask_4: 0.04424  loss_dice_4: 0.829  loss_ce_5: 0.7454  loss_mask_5: 0.04847  loss_dice_5: 1.029  loss_ce_6: 0.784  loss_mask_6: 0.04487  loss_dice_6: 0.9935  loss_ce_7: 0.7443  loss_mask_7: 0.04548  loss_dice_7: 0.9813  loss_ce_8: 0.7097  loss_mask_8: 0.04629  loss_dice_8: 0.9321     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:42 d2.utils.events]:  eta: 5:26:10  iter: 15019  total_loss: 20.64  loss_ce: 0.7998  loss_mask: 0.05596  loss_dice: 1.012  loss_ce_0: 1.557  loss_mask_0: 0.08545  loss_dice_0: 0.8554  loss_ce_1: 1.145  loss_mask_1: 0.06855  loss_dice_1: 0.8103  loss_ce_2: 1.056  loss_mask_2: 0.05191  loss_dice_2: 0.8828  loss_ce_3: 1.019  loss_mask_3: 0.05567  loss_dice_3: 0.9838  loss_ce_4: 0.959  loss_mask_4: 0.06358  loss_dice_4: 0.8774  loss_ce_5: 0.9434  loss_mask_5: 0.04886  loss_dice_5: 1.002  loss_ce_6: 0.8747  loss_mask_6: 0.05903  loss_dice_6: 0.9896  loss_ce_7: 0.8648  loss_mask_7: 0.04682  loss_dice_7: 0.8  loss_ce_8: 0.9504  loss_mask_8: 0.04578  loss_dice_8: 0.8309     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:50 d2.utils.events]:  eta: 5:18:53  iter: 15039  total_loss: 16.84  loss_ce: 0.6349  loss_mask: 0.06314  loss_dice: 0.6364  loss_ce_0: 1.15  loss_mask_0: 0.05817  loss_dice_0: 0.7788  loss_ce_1: 0.8482  loss_mask_1: 0.0539  loss_dice_1: 0.7983  loss_ce_2: 0.6583  loss_mask_2: 0.06053  loss_dice_2: 0.7917  loss_ce_3: 0.6095  loss_mask_3: 0.05262  loss_dice_3: 0.48  loss_ce_4: 0.6181  loss_mask_4: 0.05131  loss_dice_4: 0.7924  loss_ce_5: 0.6845  loss_mask_5: 0.05783  loss_dice_5: 0.7035  loss_ce_6: 0.6951  loss_mask_6: 0.07001  loss_dice_6: 0.8092  loss_ce_7: 0.5819  loss_mask_7: 0.05468  loss_dice_7: 0.7653  loss_ce_8: 0.5816  loss_mask_8: 0.06328  loss_dice_8: 0.6056     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:17:58 d2.utils.events]:  eta: 5:15:03  iter: 15059  total_loss: 20.44  loss_ce: 0.6737  loss_mask: 0.04449  loss_dice: 0.8621  loss_ce_0: 1.479  loss_mask_0: 0.05779  loss_dice_0: 1.016  loss_ce_1: 0.9619  loss_mask_1: 0.05074  loss_dice_1: 0.879  loss_ce_2: 0.7862  loss_mask_2: 0.05426  loss_dice_2: 0.8069  loss_ce_3: 0.771  loss_mask_3: 0.05197  loss_dice_3: 0.7804  loss_ce_4: 0.7534  loss_mask_4: 0.05722  loss_dice_4: 0.8604  loss_ce_5: 0.7003  loss_mask_5: 0.04921  loss_dice_5: 0.9446  loss_ce_6: 0.6956  loss_mask_6: 0.04268  loss_dice_6: 0.7333  loss_ce_7: 0.6592  loss_mask_7: 0.03962  loss_dice_7: 0.6858  loss_ce_8: 0.6487  loss_mask_8: 0.04688  loss_dice_8: 0.7858     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:07 d2.utils.events]:  eta: 5:19:27  iter: 15079  total_loss: 21.38  loss_ce: 0.9584  loss_mask: 0.05516  loss_dice: 0.813  loss_ce_0: 1.6  loss_mask_0: 0.07948  loss_dice_0: 1.317  loss_ce_1: 0.9963  loss_mask_1: 0.06637  loss_dice_1: 0.7372  loss_ce_2: 0.98  loss_mask_2: 0.07184  loss_dice_2: 1.007  loss_ce_3: 0.865  loss_mask_3: 0.05954  loss_dice_3: 0.7543  loss_ce_4: 0.8448  loss_mask_4: 0.05935  loss_dice_4: 0.8227  loss_ce_5: 0.8876  loss_mask_5: 0.04979  loss_dice_5: 0.7855  loss_ce_6: 0.9344  loss_mask_6: 0.0568  loss_dice_6: 0.8884  loss_ce_7: 0.8604  loss_mask_7: 0.0564  loss_dice_7: 0.9183  loss_ce_8: 0.8499  loss_mask_8: 0.052  loss_dice_8: 0.7143     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:15 d2.utils.events]:  eta: 5:12:31  iter: 15099  total_loss: 22.17  loss_ce: 0.923  loss_mask: 0.04894  loss_dice: 0.9822  loss_ce_0: 1.86  loss_mask_0: 0.0643  loss_dice_0: 1.272  loss_ce_1: 1.277  loss_mask_1: 0.05164  loss_dice_1: 1.009  loss_ce_2: 1.159  loss_mask_2: 0.0479  loss_dice_2: 0.9144  loss_ce_3: 1.072  loss_mask_3: 0.04806  loss_dice_3: 1.12  loss_ce_4: 1.025  loss_mask_4: 0.04295  loss_dice_4: 0.9597  loss_ce_5: 1.033  loss_mask_5: 0.04513  loss_dice_5: 0.9693  loss_ce_6: 1.033  loss_mask_6: 0.04934  loss_dice_6: 0.9323  loss_ce_7: 0.9526  loss_mask_7: 0.04652  loss_dice_7: 0.9265  loss_ce_8: 0.9792  loss_mask_8: 0.04755  loss_dice_8: 1.033     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:24 d2.utils.events]:  eta: 5:21:08  iter: 15119  total_loss: 12.95  loss_ce: 0.4375  loss_mask: 0.07086  loss_dice: 0.4703  loss_ce_0: 1.02  loss_mask_0: 0.09784  loss_dice_0: 0.5341  loss_ce_1: 0.5806  loss_mask_1: 0.09035  loss_dice_1: 0.5768  loss_ce_2: 0.5495  loss_mask_2: 0.09901  loss_dice_2: 0.5693  loss_ce_3: 0.4891  loss_mask_3: 0.06899  loss_dice_3: 0.561  loss_ce_4: 0.4746  loss_mask_4: 0.08405  loss_dice_4: 0.4593  loss_ce_5: 0.4748  loss_mask_5: 0.0787  loss_dice_5: 0.6528  loss_ce_6: 0.4405  loss_mask_6: 0.06935  loss_dice_6: 0.4993  loss_ce_7: 0.4444  loss_mask_7: 0.06822  loss_dice_7: 0.6232  loss_ce_8: 0.4441  loss_mask_8: 0.07123  loss_dice_8: 0.4338     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:32 d2.utils.events]:  eta: 5:03:53  iter: 15139  total_loss: 15.53  loss_ce: 0.4417  loss_mask: 0.08381  loss_dice: 0.6046  loss_ce_0: 1.184  loss_mask_0: 0.1021  loss_dice_0: 0.8483  loss_ce_1: 0.6753  loss_mask_1: 0.09962  loss_dice_1: 0.6802  loss_ce_2: 0.6386  loss_mask_2: 0.09042  loss_dice_2: 0.6033  loss_ce_3: 0.6125  loss_mask_3: 0.08769  loss_dice_3: 0.546  loss_ce_4: 0.5914  loss_mask_4: 0.09142  loss_dice_4: 0.6741  loss_ce_5: 0.5036  loss_mask_5: 0.113  loss_dice_5: 0.7404  loss_ce_6: 0.4519  loss_mask_6: 0.1119  loss_dice_6: 0.5225  loss_ce_7: 0.439  loss_mask_7: 0.09768  loss_dice_7: 0.5169  loss_ce_8: 0.4362  loss_mask_8: 0.08964  loss_dice_8: 0.5702     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:40 d2.utils.events]:  eta: 5:13:16  iter: 15159  total_loss: 26.45  loss_ce: 1.023  loss_mask: 0.08865  loss_dice: 1.237  loss_ce_0: 1.626  loss_mask_0: 0.1069  loss_dice_0: 1.453  loss_ce_1: 1.174  loss_mask_1: 0.09781  loss_dice_1: 1.347  loss_ce_2: 1.237  loss_mask_2: 0.08143  loss_dice_2: 1.312  loss_ce_3: 1.168  loss_mask_3: 0.09397  loss_dice_3: 1.279  loss_ce_4: 1.056  loss_mask_4: 0.0801  loss_dice_4: 1.187  loss_ce_5: 0.9423  loss_mask_5: 0.09531  loss_dice_5: 1.281  loss_ce_6: 1.095  loss_mask_6: 0.0884  loss_dice_6: 1.325  loss_ce_7: 0.9843  loss_mask_7: 0.09757  loss_dice_7: 1.249  loss_ce_8: 0.9854  loss_mask_8: 0.09357  loss_dice_8: 1.042     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:49 d2.utils.events]:  eta: 5:11:43  iter: 15179  total_loss: 23.13  loss_ce: 0.9742  loss_mask: 0.1538  loss_dice: 1.299  loss_ce_0: 1.759  loss_mask_0: 0.1501  loss_dice_0: 1.278  loss_ce_1: 1.269  loss_mask_1: 0.1459  loss_dice_1: 1.378  loss_ce_2: 1.179  loss_mask_2: 0.1263  loss_dice_2: 1.161  loss_ce_3: 1.077  loss_mask_3: 0.1588  loss_dice_3: 1.322  loss_ce_4: 1.017  loss_mask_4: 0.1945  loss_dice_4: 1.301  loss_ce_5: 1.052  loss_mask_5: 0.1757  loss_dice_5: 1.19  loss_ce_6: 0.9134  loss_mask_6: 0.1789  loss_dice_6: 1.291  loss_ce_7: 0.9175  loss_mask_7: 0.169  loss_dice_7: 1.089  loss_ce_8: 0.9568  loss_mask_8: 0.1712  loss_dice_8: 1.029     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:18:57 d2.utils.events]:  eta: 5:16:22  iter: 15199  total_loss: 21.32  loss_ce: 0.8015  loss_mask: 0.05912  loss_dice: 0.9169  loss_ce_0: 1.35  loss_mask_0: 0.09937  loss_dice_0: 1.211  loss_ce_1: 1.041  loss_mask_1: 0.1133  loss_dice_1: 0.9681  loss_ce_2: 0.9788  loss_mask_2: 0.07835  loss_dice_2: 0.8752  loss_ce_3: 0.8702  loss_mask_3: 0.05064  loss_dice_3: 0.839  loss_ce_4: 0.8571  loss_mask_4: 0.05243  loss_dice_4: 0.9111  loss_ce_5: 0.89  loss_mask_5: 0.05687  loss_dice_5: 0.7415  loss_ce_6: 0.8558  loss_mask_6: 0.0564  loss_dice_6: 0.7265  loss_ce_7: 0.8538  loss_mask_7: 0.06188  loss_dice_7: 0.8379  loss_ce_8: 0.7997  loss_mask_8: 0.05818  loss_dice_8: 0.7461     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:05 d2.utils.events]:  eta: 5:03:48  iter: 15219  total_loss: 25.14  loss_ce: 0.9351  loss_mask: 0.1713  loss_dice: 1.006  loss_ce_0: 1.81  loss_mask_0: 0.2026  loss_dice_0: 1.468  loss_ce_1: 1.268  loss_mask_1: 0.2029  loss_dice_1: 1.162  loss_ce_2: 0.9948  loss_mask_2: 0.151  loss_dice_2: 1.079  loss_ce_3: 1.05  loss_mask_3: 0.1606  loss_dice_3: 1.26  loss_ce_4: 0.9236  loss_mask_4: 0.1466  loss_dice_4: 1.123  loss_ce_5: 0.9458  loss_mask_5: 0.14  loss_dice_5: 0.9466  loss_ce_6: 0.7424  loss_mask_6: 0.1679  loss_dice_6: 1.059  loss_ce_7: 0.7763  loss_mask_7: 0.1666  loss_dice_7: 1.234  loss_ce_8: 0.7781  loss_mask_8: 0.1443  loss_dice_8: 1.104     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:14 d2.utils.events]:  eta: 5:03:56  iter: 15239  total_loss: 13.94  loss_ce: 0.6526  loss_mask: 0.05496  loss_dice: 0.4033  loss_ce_0: 1.163  loss_mask_0: 0.07933  loss_dice_0: 0.6441  loss_ce_1: 0.9882  loss_mask_1: 0.0747  loss_dice_1: 0.6009  loss_ce_2: 0.8059  loss_mask_2: 0.08537  loss_dice_2: 0.5295  loss_ce_3: 0.8073  loss_mask_3: 0.07551  loss_dice_3: 0.4773  loss_ce_4: 0.7138  loss_mask_4: 0.06368  loss_dice_4: 0.4344  loss_ce_5: 0.6474  loss_mask_5: 0.07177  loss_dice_5: 0.4688  loss_ce_6: 0.6237  loss_mask_6: 0.05504  loss_dice_6: 0.4756  loss_ce_7: 0.6381  loss_mask_7: 0.05859  loss_dice_7: 0.4645  loss_ce_8: 0.7422  loss_mask_8: 0.05352  loss_dice_8: 0.4197     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:22 d2.utils.events]:  eta: 5:13:31  iter: 15259  total_loss: 20.07  loss_ce: 0.6909  loss_mask: 0.09258  loss_dice: 0.627  loss_ce_0: 1.064  loss_mask_0: 0.1351  loss_dice_0: 0.903  loss_ce_1: 0.9096  loss_mask_1: 0.1123  loss_dice_1: 0.7011  loss_ce_2: 0.8185  loss_mask_2: 0.1134  loss_dice_2: 0.7725  loss_ce_3: 0.7774  loss_mask_3: 0.1026  loss_dice_3: 0.6016  loss_ce_4: 0.7037  loss_mask_4: 0.09754  loss_dice_4: 0.6946  loss_ce_5: 0.6306  loss_mask_5: 0.104  loss_dice_5: 0.6427  loss_ce_6: 0.7769  loss_mask_6: 0.09486  loss_dice_6: 0.6968  loss_ce_7: 0.8968  loss_mask_7: 0.08639  loss_dice_7: 0.7696  loss_ce_8: 0.6876  loss_mask_8: 0.09374  loss_dice_8: 0.7709     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:30 d2.utils.events]:  eta: 5:12:39  iter: 15279  total_loss: 18.33  loss_ce: 0.6449  loss_mask: 0.07325  loss_dice: 0.7085  loss_ce_0: 1.091  loss_mask_0: 0.07337  loss_dice_0: 0.9836  loss_ce_1: 0.8321  loss_mask_1: 0.07563  loss_dice_1: 0.9847  loss_ce_2: 0.7493  loss_mask_2: 0.07492  loss_dice_2: 0.9511  loss_ce_3: 0.6773  loss_mask_3: 0.07344  loss_dice_3: 0.8802  loss_ce_4: 0.6938  loss_mask_4: 0.08506  loss_dice_4: 0.814  loss_ce_5: 0.699  loss_mask_5: 0.06526  loss_dice_5: 0.7904  loss_ce_6: 0.6782  loss_mask_6: 0.05802  loss_dice_6: 0.6607  loss_ce_7: 0.675  loss_mask_7: 0.0814  loss_dice_7: 0.7664  loss_ce_8: 0.6623  loss_mask_8: 0.06621  loss_dice_8: 0.7311     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:39 d2.utils.events]:  eta: 5:10:50  iter: 15299  total_loss: 20.41  loss_ce: 0.8034  loss_mask: 0.07097  loss_dice: 1.029  loss_ce_0: 1.503  loss_mask_0: 0.054  loss_dice_0: 0.879  loss_ce_1: 1.044  loss_mask_1: 0.07029  loss_dice_1: 1.216  loss_ce_2: 0.9518  loss_mask_2: 0.06118  loss_dice_2: 0.9298  loss_ce_3: 0.8354  loss_mask_3: 0.06048  loss_dice_3: 0.6098  loss_ce_4: 0.8468  loss_mask_4: 0.05664  loss_dice_4: 0.7464  loss_ce_5: 0.8365  loss_mask_5: 0.0674  loss_dice_5: 0.6746  loss_ce_6: 0.7988  loss_mask_6: 0.05765  loss_dice_6: 0.6236  loss_ce_7: 0.7939  loss_mask_7: 0.05839  loss_dice_7: 0.7769  loss_ce_8: 0.8044  loss_mask_8: 0.05158  loss_dice_8: 0.8379     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:47 d2.utils.events]:  eta: 5:15:29  iter: 15319  total_loss: 19.08  loss_ce: 0.811  loss_mask: 0.07484  loss_dice: 0.5184  loss_ce_0: 1.269  loss_mask_0: 0.1041  loss_dice_0: 0.9685  loss_ce_1: 0.8853  loss_mask_1: 0.08369  loss_dice_1: 0.8963  loss_ce_2: 0.8231  loss_mask_2: 0.08217  loss_dice_2: 0.8561  loss_ce_3: 0.8259  loss_mask_3: 0.07022  loss_dice_3: 0.7391  loss_ce_4: 0.8135  loss_mask_4: 0.07624  loss_dice_4: 0.5758  loss_ce_5: 0.7798  loss_mask_5: 0.08238  loss_dice_5: 0.5881  loss_ce_6: 0.9157  loss_mask_6: 0.08453  loss_dice_6: 0.6696  loss_ce_7: 0.8692  loss_mask_7: 0.07828  loss_dice_7: 0.7225  loss_ce_8: 0.6761  loss_mask_8: 0.08165  loss_dice_8: 0.6724     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:19:56 d2.utils.events]:  eta: 5:12:48  iter: 15339  total_loss: 21.6  loss_ce: 0.7766  loss_mask: 0.1223  loss_dice: 0.6722  loss_ce_0: 1.206  loss_mask_0: 0.1404  loss_dice_0: 1.111  loss_ce_1: 1.017  loss_mask_1: 0.1287  loss_dice_1: 0.7965  loss_ce_2: 0.9672  loss_mask_2: 0.1062  loss_dice_2: 0.6925  loss_ce_3: 0.9203  loss_mask_3: 0.1221  loss_dice_3: 0.7195  loss_ce_4: 0.8619  loss_mask_4: 0.1277  loss_dice_4: 0.7469  loss_ce_5: 0.7966  loss_mask_5: 0.1365  loss_dice_5: 0.8409  loss_ce_6: 0.8038  loss_mask_6: 0.1352  loss_dice_6: 0.7076  loss_ce_7: 0.7708  loss_mask_7: 0.1374  loss_dice_7: 0.6914  loss_ce_8: 0.853  loss_mask_8: 0.1248  loss_dice_8: 0.6888     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:04 d2.utils.events]:  eta: 5:07:44  iter: 15359  total_loss: 21.33  loss_ce: 0.8708  loss_mask: 0.09338  loss_dice: 0.8296  loss_ce_0: 1.559  loss_mask_0: 0.08311  loss_dice_0: 1.071  loss_ce_1: 1.157  loss_mask_1: 0.08733  loss_dice_1: 0.9606  loss_ce_2: 1.155  loss_mask_2: 0.07716  loss_dice_2: 1.119  loss_ce_3: 0.9789  loss_mask_3: 0.07956  loss_dice_3: 1.081  loss_ce_4: 1.036  loss_mask_4: 0.08967  loss_dice_4: 0.8603  loss_ce_5: 0.9527  loss_mask_5: 0.09515  loss_dice_5: 0.9506  loss_ce_6: 0.9344  loss_mask_6: 0.08784  loss_dice_6: 0.9402  loss_ce_7: 0.9092  loss_mask_7: 0.09158  loss_dice_7: 0.9785  loss_ce_8: 0.8591  loss_mask_8: 0.09554  loss_dice_8: 0.8523     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:12 d2.utils.events]:  eta: 5:10:02  iter: 15379  total_loss: 23.04  loss_ce: 0.9296  loss_mask: 0.08905  loss_dice: 0.8806  loss_ce_0: 1.745  loss_mask_0: 0.1123  loss_dice_0: 1.152  loss_ce_1: 1.216  loss_mask_1: 0.1064  loss_dice_1: 1.019  loss_ce_2: 1.096  loss_mask_2: 0.1039  loss_dice_2: 0.982  loss_ce_3: 1.108  loss_mask_3: 0.09411  loss_dice_3: 1.082  loss_ce_4: 1.033  loss_mask_4: 0.08699  loss_dice_4: 0.9701  loss_ce_5: 1.028  loss_mask_5: 0.0902  loss_dice_5: 0.8926  loss_ce_6: 1.021  loss_mask_6: 0.07952  loss_dice_6: 0.7784  loss_ce_7: 0.9682  loss_mask_7: 0.08911  loss_dice_7: 0.9612  loss_ce_8: 0.9395  loss_mask_8: 0.08594  loss_dice_8: 0.9441     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:20 d2.utils.events]:  eta: 5:06:28  iter: 15399  total_loss: 16.81  loss_ce: 0.7311  loss_mask: 0.1034  loss_dice: 0.6827  loss_ce_0: 1.32  loss_mask_0: 0.1179  loss_dice_0: 0.7151  loss_ce_1: 0.9664  loss_mask_1: 0.1029  loss_dice_1: 0.6205  loss_ce_2: 0.7502  loss_mask_2: 0.1088  loss_dice_2: 0.7054  loss_ce_3: 0.735  loss_mask_3: 0.1025  loss_dice_3: 0.7454  loss_ce_4: 0.6245  loss_mask_4: 0.09927  loss_dice_4: 0.7366  loss_ce_5: 0.5938  loss_mask_5: 0.1067  loss_dice_5: 0.7868  loss_ce_6: 0.5486  loss_mask_6: 0.1049  loss_dice_6: 0.7716  loss_ce_7: 0.6546  loss_mask_7: 0.1091  loss_dice_7: 0.7037  loss_ce_8: 0.6196  loss_mask_8: 0.1013  loss_dice_8: 0.6902     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:29 d2.utils.events]:  eta: 5:08:22  iter: 15419  total_loss: 22.19  loss_ce: 0.9276  loss_mask: 0.1101  loss_dice: 0.8586  loss_ce_0: 1.538  loss_mask_0: 0.1239  loss_dice_0: 1.179  loss_ce_1: 1.152  loss_mask_1: 0.1265  loss_dice_1: 1.126  loss_ce_2: 1.099  loss_mask_2: 0.09117  loss_dice_2: 0.8671  loss_ce_3: 0.9314  loss_mask_3: 0.1222  loss_dice_3: 0.8201  loss_ce_4: 0.9446  loss_mask_4: 0.1188  loss_dice_4: 0.797  loss_ce_5: 0.9167  loss_mask_5: 0.11  loss_dice_5: 0.8208  loss_ce_6: 0.9607  loss_mask_6: 0.1055  loss_dice_6: 0.9617  loss_ce_7: 0.9149  loss_mask_7: 0.1105  loss_dice_7: 0.9322  loss_ce_8: 0.9631  loss_mask_8: 0.1068  loss_dice_8: 0.8819     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:37 d2.utils.events]:  eta: 5:16:40  iter: 15439  total_loss: 16.87  loss_ce: 0.7101  loss_mask: 0.1628  loss_dice: 0.5026  loss_ce_0: 1.159  loss_mask_0: 0.1892  loss_dice_0: 0.7875  loss_ce_1: 0.9428  loss_mask_1: 0.1186  loss_dice_1: 0.6915  loss_ce_2: 0.9023  loss_mask_2: 0.1469  loss_dice_2: 0.5999  loss_ce_3: 0.7929  loss_mask_3: 0.1453  loss_dice_3: 0.5984  loss_ce_4: 0.7974  loss_mask_4: 0.149  loss_dice_4: 0.566  loss_ce_5: 0.7851  loss_mask_5: 0.1357  loss_dice_5: 0.5802  loss_ce_6: 0.7304  loss_mask_6: 0.1585  loss_dice_6: 0.495  loss_ce_7: 0.7193  loss_mask_7: 0.159  loss_dice_7: 0.5084  loss_ce_8: 0.7097  loss_mask_8: 0.1572  loss_dice_8: 0.5366     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:46 d2.utils.events]:  eta: 5:12:17  iter: 15459  total_loss: 20.61  loss_ce: 0.8449  loss_mask: 0.08143  loss_dice: 0.7946  loss_ce_0: 1.361  loss_mask_0: 0.1155  loss_dice_0: 1.118  loss_ce_1: 1.062  loss_mask_1: 0.1027  loss_dice_1: 0.8975  loss_ce_2: 1.047  loss_mask_2: 0.06713  loss_dice_2: 0.9708  loss_ce_3: 1.001  loss_mask_3: 0.06593  loss_dice_3: 0.8405  loss_ce_4: 0.9246  loss_mask_4: 0.06573  loss_dice_4: 0.849  loss_ce_5: 0.8354  loss_mask_5: 0.06263  loss_dice_5: 0.991  loss_ce_6: 0.8278  loss_mask_6: 0.08039  loss_dice_6: 0.8301  loss_ce_7: 0.8462  loss_mask_7: 0.0795  loss_dice_7: 0.7395  loss_ce_8: 0.7558  loss_mask_8: 0.08  loss_dice_8: 1.003     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:20:54 d2.utils.events]:  eta: 5:14:02  iter: 15479  total_loss: 19.17  loss_ce: 0.6626  loss_mask: 0.074  loss_dice: 0.7049  loss_ce_0: 1.142  loss_mask_0: 0.1025  loss_dice_0: 1.007  loss_ce_1: 0.8426  loss_mask_1: 0.1752  loss_dice_1: 1.198  loss_ce_2: 0.9138  loss_mask_2: 0.153  loss_dice_2: 0.8692  loss_ce_3: 0.8005  loss_mask_3: 0.1194  loss_dice_3: 0.8288  loss_ce_4: 0.6755  loss_mask_4: 0.08111  loss_dice_4: 1.055  loss_ce_5: 0.7806  loss_mask_5: 0.07915  loss_dice_5: 0.8261  loss_ce_6: 0.6693  loss_mask_6: 0.09694  loss_dice_6: 0.8016  loss_ce_7: 0.6535  loss_mask_7: 0.0926  loss_dice_7: 0.7201  loss_ce_8: 0.7812  loss_mask_8: 0.08595  loss_dice_8: 0.772     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:21:03 d2.utils.events]:  eta: 5:10:55  iter: 15499  total_loss: 18.02  loss_ce: 0.7259  loss_mask: 0.143  loss_dice: 0.6393  loss_ce_0: 1.171  loss_mask_0: 0.1861  loss_dice_0: 0.7925  loss_ce_1: 0.9155  loss_mask_1: 0.2921  loss_dice_1: 0.9373  loss_ce_2: 0.864  loss_mask_2: 0.2569  loss_dice_2: 0.8807  loss_ce_3: 0.8286  loss_mask_3: 0.1552  loss_dice_3: 0.7277  loss_ce_4: 0.7838  loss_mask_4: 0.1804  loss_dice_4: 0.8259  loss_ce_5: 0.7449  loss_mask_5: 0.1864  loss_dice_5: 0.9386  loss_ce_6: 0.7697  loss_mask_6: 0.1544  loss_dice_6: 0.6521  loss_ce_7: 0.7303  loss_mask_7: 0.1603  loss_dice_7: 0.7228  loss_ce_8: 0.7433  loss_mask_8: 0.1515  loss_dice_8: 0.7012     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:21:11 d2.utils.events]:  eta: 5:11:50  iter: 15519  total_loss: 26.14  loss_ce: 1.218  loss_mask: 0.06246  loss_dice: 1.162  loss_ce_0: 1.863  loss_mask_0: 0.06906  loss_dice_0: 1.306  loss_ce_1: 1.384  loss_mask_1: 0.06333  loss_dice_1: 1.188  loss_ce_2: 1.249  loss_mask_2: 0.06279  loss_dice_2: 1.309  loss_ce_3: 1.244  loss_mask_3: 0.06691  loss_dice_3: 1.151  loss_ce_4: 1.179  loss_mask_4: 0.07228  loss_dice_4: 1.052  loss_ce_5: 1.159  loss_mask_5: 0.06275  loss_dice_5: 1.178  loss_ce_6: 1.05  loss_mask_6: 0.05926  loss_dice_6: 1.211  loss_ce_7: 1.136  loss_mask_7: 0.0649  loss_dice_7: 1.18  loss_ce_8: 1.096  loss_mask_8: 0.05362  loss_dice_8: 1.155     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:21:19 d2.utils.events]:  eta: 5:06:14  iter: 15539  total_loss: 23  loss_ce: 0.8308  loss_mask: 0.05439  loss_dice: 0.8776  loss_ce_0: 1.435  loss_mask_0: 0.09168  loss_dice_0: 0.8761  loss_ce_1: 1.014  loss_mask_1: 0.08483  loss_dice_1: 1.069  loss_ce_2: 0.9131  loss_mask_2: 0.06837  loss_dice_2: 1.066  loss_ce_3: 0.91  loss_mask_3: 0.05398  loss_dice_3: 0.899  loss_ce_4: 0.8438  loss_mask_4: 0.05992  loss_dice_4: 0.9395  loss_ce_5: 0.8291  loss_mask_5: 0.07374  loss_dice_5: 0.9498  loss_ce_6: 0.845  loss_mask_6: 0.07302  loss_dice_6: 1.031  loss_ce_7: 0.8469  loss_mask_7: 0.08137  loss_dice_7: 0.9594  loss_ce_8: 0.8473  loss_mask_8: 0.05471  loss_dice_8: 0.9197     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:21:27 d2.utils.events]:  eta: 5:05:06  iter: 15559  total_loss: 19.15  loss_ce: 0.7864  loss_mask: 0.08104  loss_dice: 0.7674  loss_ce_0: 1.366  loss_mask_0: 0.1721  loss_dice_0: 0.8734  loss_ce_1: 1.106  loss_mask_1: 0.1113  loss_dice_1: 1.077  loss_ce_2: 0.9287  loss_mask_2: 0.1087  loss_dice_2: 0.864  loss_ce_3: 0.926  loss_mask_3: 0.09961  loss_dice_3: 0.7355  loss_ce_4: 0.8583  loss_mask_4: 0.08582  loss_dice_4: 0.7671  loss_ce_5: 0.7458  loss_mask_5: 0.118  loss_dice_5: 0.8302  loss_ce_6: 0.7815  loss_mask_6: 0.08073  loss_dice_6: 0.6831  loss_ce_7: 0.8126  loss_mask_7: 0.08781  loss_dice_7: 0.7919  loss_ce_8: 0.8322  loss_mask_8: 0.07948  loss_dice_8: 0.8479     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:21:36 d2.utils.events]:  eta: 5:05:23  iter: 15579  total_loss: 23.75  loss_ce: 0.7675  loss_mask: 0.1531  loss_dice: 1.004  loss_ce_0: 1.486  loss_mask_0: 0.1469  loss_dice_0: 1.113  loss_ce_1: 1.022  loss_mask_1: 0.2302  loss_dice_1: 1.079  loss_ce_2: 1.05  loss_mask_2: 0.2458  loss_dice_2: 1.351  loss_ce_3: 0.908  loss_mask_3: 0.1511  loss_dice_3: 1.07  loss_ce_4: 0.8212  loss_mask_4: 0.1793  loss_dice_4: 1.105  loss_ce_5: 0.8315  loss_mask_5: 0.1439  loss_dice_5: 1.227  loss_ce_6: 0.7695  loss_mask_6: 0.1633  loss_dice_6: 0.9619  loss_ce_7: 0.7678  loss_mask_7: 0.1697  loss_dice_7: 1.043  loss_ce_8: 0.7466  loss_mask_8: 0.1668  loss_dice_8: 0.92     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 26.0 - Losses: {'loss_ce': tensor(0.7001, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.0477, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.5727, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(0.9304, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.2237, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(1.3432, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.8703, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.1168, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.9924, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.7294, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.0815, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(1.4498, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.6528, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.0583, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(1.1232, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.7532, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.0406, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.5623, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.6775, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.0422, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.4239, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.6721, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.0526, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(1.3750, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.6899, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.0402, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.1432, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.6751, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.0420, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.3412, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:21:44 d2.utils.events]:  eta: 5:09:58  iter: 15599  total_loss: 20.39  loss_ce: 0.8083  loss_mask: 0.0933  loss_dice: 0.6606  loss_ce_0: 1.351  loss_mask_0: 0.1076  loss_dice_0: 1.237  loss_ce_1: 1.038  loss_mask_1: 0.118  loss_dice_1: 1.023  loss_ce_2: 1.07  loss_mask_2: 0.1056  loss_dice_2: 1.142  loss_ce_3: 0.8318  loss_mask_3: 0.08429  loss_dice_3: 1.108  loss_ce_4: 0.8725  loss_mask_4: 0.1026  loss_dice_4: 0.9138  loss_ce_5: 0.9827  loss_mask_5: 0.0917  loss_dice_5: 0.5842  loss_ce_6: 0.8746  loss_mask_6: 0.09636  loss_dice_6: 0.8128  loss_ce_7: 0.7507  loss_mask_7: 0.09197  loss_dice_7: 0.6671  loss_ce_8: 0.7934  loss_mask_8: 0.0947  loss_dice_8: 0.7341     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:21:53 d2.utils.events]:  eta: 5:12:35  iter: 15619  total_loss: 19.62  loss_ce: 0.7721  loss_mask: 0.09438  loss_dice: 0.904  loss_ce_0: 1.043  loss_mask_0: 0.1108  loss_dice_0: 1.26  loss_ce_1: 0.8458  loss_mask_1: 0.1071  loss_dice_1: 1.103  loss_ce_2: 0.7654  loss_mask_2: 0.111  loss_dice_2: 1.116  loss_ce_3: 0.8227  loss_mask_3: 0.09692  loss_dice_3: 0.9459  loss_ce_4: 0.7547  loss_mask_4: 0.0935  loss_dice_4: 0.8221  loss_ce_5: 0.806  loss_mask_5: 0.08848  loss_dice_5: 0.9378  loss_ce_6: 0.7319  loss_mask_6: 0.1092  loss_dice_6: 0.9464  loss_ce_7: 0.7689  loss_mask_7: 0.09503  loss_dice_7: 1.001  loss_ce_8: 0.7491  loss_mask_8: 0.09344  loss_dice_8: 0.8549     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:01 d2.utils.events]:  eta: 5:07:48  iter: 15639  total_loss: 24.87  loss_ce: 0.7284  loss_mask: 0.1003  loss_dice: 1.123  loss_ce_0: 1.4  loss_mask_0: 0.1023  loss_dice_0: 1.1  loss_ce_1: 1.103  loss_mask_1: 0.08352  loss_dice_1: 1.275  loss_ce_2: 0.8755  loss_mask_2: 0.094  loss_dice_2: 0.96  loss_ce_3: 0.9252  loss_mask_3: 0.106  loss_dice_3: 0.9093  loss_ce_4: 0.8744  loss_mask_4: 0.105  loss_dice_4: 1.059  loss_ce_5: 0.8919  loss_mask_5: 0.1061  loss_dice_5: 0.8837  loss_ce_6: 0.7748  loss_mask_6: 0.1072  loss_dice_6: 1.058  loss_ce_7: 0.7165  loss_mask_7: 0.0995  loss_dice_7: 1.109  loss_ce_8: 0.7413  loss_mask_8: 0.09757  loss_dice_8: 1.089     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:09 d2.utils.events]:  eta: 5:10:39  iter: 15659  total_loss: 19.39  loss_ce: 0.7193  loss_mask: 0.0652  loss_dice: 0.9867  loss_ce_0: 1.451  loss_mask_0: 0.09437  loss_dice_0: 0.8258  loss_ce_1: 1.024  loss_mask_1: 0.1093  loss_dice_1: 1.077  loss_ce_2: 0.9054  loss_mask_2: 0.1053  loss_dice_2: 0.821  loss_ce_3: 0.7159  loss_mask_3: 0.07818  loss_dice_3: 0.8289  loss_ce_4: 0.6907  loss_mask_4: 0.08982  loss_dice_4: 0.9592  loss_ce_5: 0.7021  loss_mask_5: 0.08277  loss_dice_5: 0.8903  loss_ce_6: 0.6652  loss_mask_6: 0.09227  loss_dice_6: 0.9925  loss_ce_7: 0.6274  loss_mask_7: 0.08157  loss_dice_7: 0.9792  loss_ce_8: 0.6524  loss_mask_8: 0.1018  loss_dice_8: 0.9862     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:18 d2.utils.events]:  eta: 5:12:50  iter: 15679  total_loss: 18.26  loss_ce: 0.6872  loss_mask: 0.06304  loss_dice: 0.7177  loss_ce_0: 1.483  loss_mask_0: 0.08086  loss_dice_0: 0.9874  loss_ce_1: 1.02  loss_mask_1: 0.07731  loss_dice_1: 0.8337  loss_ce_2: 0.9106  loss_mask_2: 0.06594  loss_dice_2: 0.8042  loss_ce_3: 0.7483  loss_mask_3: 0.05919  loss_dice_3: 0.7861  loss_ce_4: 0.7486  loss_mask_4: 0.06888  loss_dice_4: 0.7907  loss_ce_5: 0.7008  loss_mask_5: 0.06262  loss_dice_5: 0.807  loss_ce_6: 0.6412  loss_mask_6: 0.05296  loss_dice_6: 0.747  loss_ce_7: 0.6373  loss_mask_7: 0.06086  loss_dice_7: 0.7449  loss_ce_8: 0.6844  loss_mask_8: 0.05963  loss_dice_8: 0.7581     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:26 d2.utils.events]:  eta: 5:04:46  iter: 15699  total_loss: 15.48  loss_ce: 0.7098  loss_mask: 0.06245  loss_dice: 0.5218  loss_ce_0: 1.353  loss_mask_0: 0.09003  loss_dice_0: 0.9677  loss_ce_1: 0.8473  loss_mask_1: 0.07164  loss_dice_1: 0.5679  loss_ce_2: 0.7692  loss_mask_2: 0.06847  loss_dice_2: 0.674  loss_ce_3: 0.7556  loss_mask_3: 0.06808  loss_dice_3: 0.6236  loss_ce_4: 0.7193  loss_mask_4: 0.06412  loss_dice_4: 0.6516  loss_ce_5: 0.6918  loss_mask_5: 0.08242  loss_dice_5: 0.5875  loss_ce_6: 0.7042  loss_mask_6: 0.06883  loss_dice_6: 0.6842  loss_ce_7: 0.691  loss_mask_7: 0.06575  loss_dice_7: 0.7975  loss_ce_8: 0.694  loss_mask_8: 0.06475  loss_dice_8: 0.5719     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:34 d2.utils.events]:  eta: 5:11:46  iter: 15719  total_loss: 18.87  loss_ce: 0.8318  loss_mask: 0.09773  loss_dice: 0.8129  loss_ce_0: 1.446  loss_mask_0: 0.1026  loss_dice_0: 0.9273  loss_ce_1: 1.248  loss_mask_1: 0.1014  loss_dice_1: 0.7675  loss_ce_2: 1.036  loss_mask_2: 0.09689  loss_dice_2: 0.8324  loss_ce_3: 0.9044  loss_mask_3: 0.1044  loss_dice_3: 0.8214  loss_ce_4: 0.857  loss_mask_4: 0.08416  loss_dice_4: 0.6428  loss_ce_5: 0.8701  loss_mask_5: 0.08516  loss_dice_5: 0.722  loss_ce_6: 0.8623  loss_mask_6: 0.09243  loss_dice_6: 0.6918  loss_ce_7: 0.9042  loss_mask_7: 0.08963  loss_dice_7: 0.7847  loss_ce_8: 0.8127  loss_mask_8: 0.08756  loss_dice_8: 0.7806     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:43 d2.utils.events]:  eta: 5:06:21  iter: 15739  total_loss: 17.05  loss_ce: 0.5742  loss_mask: 0.07888  loss_dice: 0.619  loss_ce_0: 1.257  loss_mask_0: 0.0931  loss_dice_0: 0.9844  loss_ce_1: 0.9245  loss_mask_1: 0.1004  loss_dice_1: 0.8217  loss_ce_2: 0.8908  loss_mask_2: 0.09379  loss_dice_2: 0.6676  loss_ce_3: 0.7478  loss_mask_3: 0.08624  loss_dice_3: 0.6714  loss_ce_4: 0.6913  loss_mask_4: 0.08355  loss_dice_4: 0.6675  loss_ce_5: 0.6738  loss_mask_5: 0.08154  loss_dice_5: 0.6094  loss_ce_6: 0.6023  loss_mask_6: 0.07806  loss_dice_6: 0.7002  loss_ce_7: 0.5765  loss_mask_7: 0.07  loss_dice_7: 0.6966  loss_ce_8: 0.5783  loss_mask_8: 0.07406  loss_dice_8: 0.7036     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:51 d2.utils.events]:  eta: 5:05:26  iter: 15759  total_loss: 21.77  loss_ce: 0.7082  loss_mask: 0.08565  loss_dice: 1.167  loss_ce_0: 1.666  loss_mask_0: 0.0897  loss_dice_0: 1.056  loss_ce_1: 1.024  loss_mask_1: 0.09124  loss_dice_1: 1.013  loss_ce_2: 1.051  loss_mask_2: 0.0782  loss_dice_2: 1.004  loss_ce_3: 0.8097  loss_mask_3: 0.08878  loss_dice_3: 1.036  loss_ce_4: 0.8133  loss_mask_4: 0.09492  loss_dice_4: 1.176  loss_ce_5: 0.7493  loss_mask_5: 0.09231  loss_dice_5: 1.059  loss_ce_6: 0.7221  loss_mask_6: 0.0847  loss_dice_6: 1.081  loss_ce_7: 0.7001  loss_mask_7: 0.09284  loss_dice_7: 0.9278  loss_ce_8: 0.7027  loss_mask_8: 0.09248  loss_dice_8: 1.1     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:22:59 d2.utils.events]:  eta: 5:05:49  iter: 15779  total_loss: 24.54  loss_ce: 0.9073  loss_mask: 0.06041  loss_dice: 1.129  loss_ce_0: 1.862  loss_mask_0: 0.0719  loss_dice_0: 1.195  loss_ce_1: 1.45  loss_mask_1: 0.08231  loss_dice_1: 1.308  loss_ce_2: 1.189  loss_mask_2: 0.05678  loss_dice_2: 1.29  loss_ce_3: 0.9971  loss_mask_3: 0.0569  loss_dice_3: 1.134  loss_ce_4: 1.108  loss_mask_4: 0.04968  loss_dice_4: 1.112  loss_ce_5: 0.9939  loss_mask_5: 0.05197  loss_dice_5: 0.9932  loss_ce_6: 0.9406  loss_mask_6: 0.06687  loss_dice_6: 1.266  loss_ce_7: 0.9005  loss_mask_7: 0.06905  loss_dice_7: 1.227  loss_ce_8: 0.8679  loss_mask_8: 0.06057  loss_dice_8: 1.224     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:08 d2.utils.events]:  eta: 5:13:20  iter: 15799  total_loss: 22.25  loss_ce: 0.8544  loss_mask: 0.04656  loss_dice: 1.088  loss_ce_0: 1.49  loss_mask_0: 0.05484  loss_dice_0: 1.167  loss_ce_1: 1.041  loss_mask_1: 0.06389  loss_dice_1: 1.339  loss_ce_2: 0.9796  loss_mask_2: 0.04862  loss_dice_2: 1.191  loss_ce_3: 0.8952  loss_mask_3: 0.04352  loss_dice_3: 1.186  loss_ce_4: 0.8708  loss_mask_4: 0.03797  loss_dice_4: 1.323  loss_ce_5: 0.8661  loss_mask_5: 0.0425  loss_dice_5: 1.214  loss_ce_6: 0.7839  loss_mask_6: 0.04527  loss_dice_6: 1.23  loss_ce_7: 0.7815  loss_mask_7: 0.0396  loss_dice_7: 1.096  loss_ce_8: 0.8479  loss_mask_8: 0.04566  loss_dice_8: 1.076     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:16 d2.utils.events]:  eta: 5:11:40  iter: 15819  total_loss: 21.26  loss_ce: 0.9493  loss_mask: 0.1326  loss_dice: 0.6049  loss_ce_0: 1.304  loss_mask_0: 0.1187  loss_dice_0: 0.8651  loss_ce_1: 1.159  loss_mask_1: 0.1417  loss_dice_1: 0.9307  loss_ce_2: 1.085  loss_mask_2: 0.1501  loss_dice_2: 0.8948  loss_ce_3: 0.9859  loss_mask_3: 0.1459  loss_dice_3: 0.6024  loss_ce_4: 0.9918  loss_mask_4: 0.1638  loss_dice_4: 0.5829  loss_ce_5: 1.002  loss_mask_5: 0.1457  loss_dice_5: 0.5931  loss_ce_6: 0.9615  loss_mask_6: 0.1144  loss_dice_6: 0.6345  loss_ce_7: 0.8891  loss_mask_7: 0.1089  loss_dice_7: 0.6235  loss_ce_8: 0.9004  loss_mask_8: 0.1118  loss_dice_8: 0.6381     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:25 d2.utils.events]:  eta: 5:04:49  iter: 15839  total_loss: 14.59  loss_ce: 0.7682  loss_mask: 0.06409  loss_dice: 0.5267  loss_ce_0: 1.32  loss_mask_0: 0.07044  loss_dice_0: 0.7969  loss_ce_1: 1.001  loss_mask_1: 0.0835  loss_dice_1: 0.6231  loss_ce_2: 0.8138  loss_mask_2: 0.06138  loss_dice_2: 0.5981  loss_ce_3: 0.8474  loss_mask_3: 0.05572  loss_dice_3: 0.5231  loss_ce_4: 0.7547  loss_mask_4: 0.05717  loss_dice_4: 0.4758  loss_ce_5: 0.7401  loss_mask_5: 0.06033  loss_dice_5: 0.5355  loss_ce_6: 0.6934  loss_mask_6: 0.06405  loss_dice_6: 0.498  loss_ce_7: 0.7267  loss_mask_7: 0.06122  loss_dice_7: 0.5056  loss_ce_8: 0.7911  loss_mask_8: 0.06552  loss_dice_8: 0.5547     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:33 d2.utils.events]:  eta: 5:02:56  iter: 15859  total_loss: 14.8  loss_ce: 0.6072  loss_mask: 0.0428  loss_dice: 0.6935  loss_ce_0: 1.549  loss_mask_0: 0.05824  loss_dice_0: 1.104  loss_ce_1: 0.853  loss_mask_1: 0.06161  loss_dice_1: 0.706  loss_ce_2: 0.7337  loss_mask_2: 0.05614  loss_dice_2: 0.611  loss_ce_3: 0.671  loss_mask_3: 0.05049  loss_dice_3: 0.6879  loss_ce_4: 0.7074  loss_mask_4: 0.04816  loss_dice_4: 0.5339  loss_ce_5: 0.6593  loss_mask_5: 0.04257  loss_dice_5: 0.6803  loss_ce_6: 0.6197  loss_mask_6: 0.04277  loss_dice_6: 0.5749  loss_ce_7: 0.6161  loss_mask_7: 0.0474  loss_dice_7: 0.6043  loss_ce_8: 0.5955  loss_mask_8: 0.04477  loss_dice_8: 0.6191     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:41 d2.utils.events]:  eta: 5:09:57  iter: 15879  total_loss: 20.52  loss_ce: 0.7985  loss_mask: 0.09996  loss_dice: 0.7996  loss_ce_0: 1.551  loss_mask_0: 0.1192  loss_dice_0: 1.179  loss_ce_1: 1.121  loss_mask_1: 0.1009  loss_dice_1: 1.046  loss_ce_2: 1.065  loss_mask_2: 0.1128  loss_dice_2: 0.8692  loss_ce_3: 0.985  loss_mask_3: 0.1048  loss_dice_3: 0.7884  loss_ce_4: 0.947  loss_mask_4: 0.138  loss_dice_4: 0.7757  loss_ce_5: 0.7724  loss_mask_5: 0.1048  loss_dice_5: 0.8951  loss_ce_6: 0.7886  loss_mask_6: 0.09041  loss_dice_6: 0.7888  loss_ce_7: 0.6744  loss_mask_7: 0.09727  loss_dice_7: 0.7506  loss_ce_8: 0.8749  loss_mask_8: 0.1053  loss_dice_8: 0.7456     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:50 d2.utils.events]:  eta: 5:03:23  iter: 15899  total_loss: 16.77  loss_ce: 0.4645  loss_mask: 0.02809  loss_dice: 0.654  loss_ce_0: 1.063  loss_mask_0: 0.04069  loss_dice_0: 0.5926  loss_ce_1: 0.7648  loss_mask_1: 0.04134  loss_dice_1: 0.6344  loss_ce_2: 0.6296  loss_mask_2: 0.02907  loss_dice_2: 0.6401  loss_ce_3: 0.4943  loss_mask_3: 0.02966  loss_dice_3: 0.7132  loss_ce_4: 0.4781  loss_mask_4: 0.03707  loss_dice_4: 0.4757  loss_ce_5: 0.5185  loss_mask_5: 0.03006  loss_dice_5: 0.534  loss_ce_6: 0.4514  loss_mask_6: 0.03274  loss_dice_6: 0.5131  loss_ce_7: 0.4617  loss_mask_7: 0.03275  loss_dice_7: 0.5176  loss_ce_8: 0.45  loss_mask_8: 0.03259  loss_dice_8: 0.662     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:23:58 d2.utils.events]:  eta: 5:01:46  iter: 15919  total_loss: 15.57  loss_ce: 0.6826  loss_mask: 0.06215  loss_dice: 0.6774  loss_ce_0: 1.521  loss_mask_0: 0.0665  loss_dice_0: 0.6594  loss_ce_1: 0.9415  loss_mask_1: 0.07759  loss_dice_1: 0.7446  loss_ce_2: 0.785  loss_mask_2: 0.06839  loss_dice_2: 0.7084  loss_ce_3: 0.8504  loss_mask_3: 0.05615  loss_dice_3: 0.6001  loss_ce_4: 0.6221  loss_mask_4: 0.05983  loss_dice_4: 0.6362  loss_ce_5: 0.6527  loss_mask_5: 0.06133  loss_dice_5: 0.6581  loss_ce_6: 0.6348  loss_mask_6: 0.06725  loss_dice_6: 0.6763  loss_ce_7: 0.7206  loss_mask_7: 0.06709  loss_dice_7: 0.6709  loss_ce_8: 0.6946  loss_mask_8: 0.06251  loss_dice_8: 0.6068     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:06 d2.utils.events]:  eta: 5:10:58  iter: 15939  total_loss: 19.25  loss_ce: 0.9458  loss_mask: 0.1408  loss_dice: 0.8463  loss_ce_0: 1.701  loss_mask_0: 0.1474  loss_dice_0: 1.11  loss_ce_1: 1.231  loss_mask_1: 0.1301  loss_dice_1: 0.7317  loss_ce_2: 1.068  loss_mask_2: 0.1848  loss_dice_2: 0.9066  loss_ce_3: 1.035  loss_mask_3: 0.1542  loss_dice_3: 0.8034  loss_ce_4: 0.9611  loss_mask_4: 0.1701  loss_dice_4: 0.7857  loss_ce_5: 0.9128  loss_mask_5: 0.1627  loss_dice_5: 0.6908  loss_ce_6: 0.9626  loss_mask_6: 0.1526  loss_dice_6: 0.9202  loss_ce_7: 0.9757  loss_mask_7: 0.1424  loss_dice_7: 0.7141  loss_ce_8: 0.9574  loss_mask_8: 0.1534  loss_dice_8: 0.6435     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:15 d2.utils.events]:  eta: 5:05:56  iter: 15959  total_loss: 17.77  loss_ce: 0.7154  loss_mask: 0.078  loss_dice: 0.5876  loss_ce_0: 1.231  loss_mask_0: 0.08625  loss_dice_0: 1.012  loss_ce_1: 1.036  loss_mask_1: 0.09768  loss_dice_1: 0.8735  loss_ce_2: 0.9071  loss_mask_2: 0.1144  loss_dice_2: 0.845  loss_ce_3: 0.7964  loss_mask_3: 0.1003  loss_dice_3: 0.6405  loss_ce_4: 0.8237  loss_mask_4: 0.101  loss_dice_4: 0.8238  loss_ce_5: 0.7609  loss_mask_5: 0.1005  loss_dice_5: 0.5745  loss_ce_6: 0.8328  loss_mask_6: 0.08297  loss_dice_6: 0.6607  loss_ce_7: 0.7197  loss_mask_7: 0.08514  loss_dice_7: 0.6582  loss_ce_8: 0.8019  loss_mask_8: 0.07213  loss_dice_8: 0.6062     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:23 d2.utils.events]:  eta: 5:07:16  iter: 15979  total_loss: 17.03  loss_ce: 0.8277  loss_mask: 0.05295  loss_dice: 0.6629  loss_ce_0: 1.437  loss_mask_0: 0.06109  loss_dice_0: 0.6775  loss_ce_1: 1.014  loss_mask_1: 0.07335  loss_dice_1: 0.7926  loss_ce_2: 0.9846  loss_mask_2: 0.05273  loss_dice_2: 0.7883  loss_ce_3: 0.8801  loss_mask_3: 0.04186  loss_dice_3: 0.6956  loss_ce_4: 0.7824  loss_mask_4: 0.05253  loss_dice_4: 0.6035  loss_ce_5: 0.8019  loss_mask_5: 0.05661  loss_dice_5: 0.6849  loss_ce_6: 0.8421  loss_mask_6: 0.04822  loss_dice_6: 0.6067  loss_ce_7: 0.8213  loss_mask_7: 0.05672  loss_dice_7: 0.7172  loss_ce_8: 0.8187  loss_mask_8: 0.051  loss_dice_8: 0.6426     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:31 d2.utils.events]:  eta: 5:07:18  iter: 15999  total_loss: 22.77  loss_ce: 0.895  loss_mask: 0.1359  loss_dice: 1.24  loss_ce_0: 1.495  loss_mask_0: 0.1386  loss_dice_0: 1.267  loss_ce_1: 1.014  loss_mask_1: 0.139  loss_dice_1: 1.318  loss_ce_2: 0.8891  loss_mask_2: 0.1113  loss_dice_2: 1.288  loss_ce_3: 0.8319  loss_mask_3: 0.111  loss_dice_3: 1.277  loss_ce_4: 0.8109  loss_mask_4: 0.1456  loss_dice_4: 1.178  loss_ce_5: 0.8175  loss_mask_5: 0.1597  loss_dice_5: 1.136  loss_ce_6: 0.8682  loss_mask_6: 0.1377  loss_dice_6: 1.177  loss_ce_7: 0.7978  loss_mask_7: 0.1155  loss_dice_7: 1.089  loss_ce_8: 0.8587  loss_mask_8: 0.1379  loss_dice_8: 1.381     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:39 d2.utils.events]:  eta: 4:57:30  iter: 16019  total_loss: 22.64  loss_ce: 0.7675  loss_mask: 0.05454  loss_dice: 1.047  loss_ce_0: 1.286  loss_mask_0: 0.09311  loss_dice_0: 1.209  loss_ce_1: 1.005  loss_mask_1: 0.07816  loss_dice_1: 1.226  loss_ce_2: 1.006  loss_mask_2: 0.06752  loss_dice_2: 1.21  loss_ce_3: 0.8192  loss_mask_3: 0.05732  loss_dice_3: 0.7494  loss_ce_4: 0.8354  loss_mask_4: 0.05989  loss_dice_4: 1.073  loss_ce_5: 0.8307  loss_mask_5: 0.05757  loss_dice_5: 1.095  loss_ce_6: 0.9142  loss_mask_6: 0.06009  loss_dice_6: 1.006  loss_ce_7: 0.9215  loss_mask_7: 0.06419  loss_dice_7: 1.016  loss_ce_8: 0.8049  loss_mask_8: 0.05453  loss_dice_8: 1.126     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:48 d2.utils.events]:  eta: 5:00:32  iter: 16039  total_loss: 20.79  loss_ce: 0.9029  loss_mask: 0.05886  loss_dice: 0.7062  loss_ce_0: 1.578  loss_mask_0: 0.07898  loss_dice_0: 0.7346  loss_ce_1: 1.301  loss_mask_1: 0.07178  loss_dice_1: 0.672  loss_ce_2: 1.151  loss_mask_2: 0.07263  loss_dice_2: 0.6484  loss_ce_3: 1.052  loss_mask_3: 0.05649  loss_dice_3: 0.7062  loss_ce_4: 0.9784  loss_mask_4: 0.05785  loss_dice_4: 0.5862  loss_ce_5: 0.9236  loss_mask_5: 0.05805  loss_dice_5: 0.8462  loss_ce_6: 0.9117  loss_mask_6: 0.0555  loss_dice_6: 0.6639  loss_ce_7: 0.9524  loss_mask_7: 0.0706  loss_dice_7: 0.7092  loss_ce_8: 0.9606  loss_mask_8: 0.05744  loss_dice_8: 0.7737     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:24:56 d2.utils.events]:  eta: 5:05:42  iter: 16059  total_loss: 18.33  loss_ce: 0.8981  loss_mask: 0.07625  loss_dice: 0.8729  loss_ce_0: 1.271  loss_mask_0: 0.07068  loss_dice_0: 0.9563  loss_ce_1: 0.9537  loss_mask_1: 0.07814  loss_dice_1: 0.8648  loss_ce_2: 0.9135  loss_mask_2: 0.06157  loss_dice_2: 0.8188  loss_ce_3: 0.8901  loss_mask_3: 0.05178  loss_dice_3: 0.799  loss_ce_4: 0.9369  loss_mask_4: 0.08485  loss_dice_4: 0.5782  loss_ce_5: 0.9396  loss_mask_5: 0.07695  loss_dice_5: 0.8617  loss_ce_6: 0.9465  loss_mask_6: 0.08141  loss_dice_6: 0.7489  loss_ce_7: 0.9161  loss_mask_7: 0.07443  loss_dice_7: 0.7913  loss_ce_8: 0.7937  loss_mask_8: 0.07022  loss_dice_8: 0.6182     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:25:04 d2.utils.events]:  eta: 5:06:13  iter: 16079  total_loss: 17.25  loss_ce: 0.6597  loss_mask: 0.07912  loss_dice: 0.9062  loss_ce_0: 1.52  loss_mask_0: 0.1146  loss_dice_0: 1.069  loss_ce_1: 0.962  loss_mask_1: 0.09741  loss_dice_1: 0.7073  loss_ce_2: 0.8818  loss_mask_2: 0.09665  loss_dice_2: 0.7328  loss_ce_3: 0.8348  loss_mask_3: 0.08295  loss_dice_3: 0.7076  loss_ce_4: 0.7589  loss_mask_4: 0.07593  loss_dice_4: 0.701  loss_ce_5: 0.6596  loss_mask_5: 0.07677  loss_dice_5: 0.7626  loss_ce_6: 0.6008  loss_mask_6: 0.07307  loss_dice_6: 0.7935  loss_ce_7: 0.6329  loss_mask_7: 0.07616  loss_dice_7: 0.7692  loss_ce_8: 0.6095  loss_mask_8: 0.08491  loss_dice_8: 0.8718     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:25:13 d2.utils.events]:  eta: 5:22:34  iter: 16099  total_loss: 25.61  loss_ce: 0.9172  loss_mask: 0.08768  loss_dice: 1.308  loss_ce_0: 1.62  loss_mask_0: 0.07481  loss_dice_0: 1.496  loss_ce_1: 1.121  loss_mask_1: 0.08278  loss_dice_1: 1.231  loss_ce_2: 1.124  loss_mask_2: 0.07477  loss_dice_2: 0.9047  loss_ce_3: 1.021  loss_mask_3: 0.07801  loss_dice_3: 1.141  loss_ce_4: 0.9868  loss_mask_4: 0.08333  loss_dice_4: 0.9483  loss_ce_5: 0.8767  loss_mask_5: 0.07847  loss_dice_5: 1.332  loss_ce_6: 0.9208  loss_mask_6: 0.07497  loss_dice_6: 0.917  loss_ce_7: 0.9676  loss_mask_7: 0.08317  loss_dice_7: 1.202  loss_ce_8: 0.9181  loss_mask_8: 0.0787  loss_dice_8: 1.153     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:25:22 d2.utils.events]:  eta: 5:13:24  iter: 16119  total_loss: 23.01  loss_ce: 1.019  loss_mask: 0.1161  loss_dice: 0.9536  loss_ce_0: 1.76  loss_mask_0: 0.1299  loss_dice_0: 0.9459  loss_ce_1: 1.387  loss_mask_1: 0.0984  loss_dice_1: 1.053  loss_ce_2: 1.268  loss_mask_2: 0.08036  loss_dice_2: 0.9231  loss_ce_3: 1.218  loss_mask_3: 0.1127  loss_dice_3: 0.8008  loss_ce_4: 1.046  loss_mask_4: 0.1225  loss_dice_4: 0.7495  loss_ce_5: 1.039  loss_mask_5: 0.1468  loss_dice_5: 0.7913  loss_ce_6: 1.118  loss_mask_6: 0.135  loss_dice_6: 0.8485  loss_ce_7: 1.099  loss_mask_7: 0.1261  loss_dice_7: 0.8821  loss_ce_8: 1.039  loss_mask_8: 0.1479  loss_dice_8: 1.017     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:25:30 d2.utils.events]:  eta: 5:06:54  iter: 16139  total_loss: 23.82  loss_ce: 0.9937  loss_mask: 0.1312  loss_dice: 1.072  loss_ce_0: 1.566  loss_mask_0: 0.1007  loss_dice_0: 1.194  loss_ce_1: 1.199  loss_mask_1: 0.2092  loss_dice_1: 1.17  loss_ce_2: 1.184  loss_mask_2: 0.1088  loss_dice_2: 1.059  loss_ce_3: 1.048  loss_mask_3: 0.09985  loss_dice_3: 1.047  loss_ce_4: 1.081  loss_mask_4: 0.1153  loss_dice_4: 0.9646  loss_ce_5: 1.028  loss_mask_5: 0.1155  loss_dice_5: 0.9635  loss_ce_6: 1.032  loss_mask_6: 0.1169  loss_dice_6: 0.8955  loss_ce_7: 1.002  loss_mask_7: 0.1207  loss_dice_7: 0.8339  loss_ce_8: 1.045  loss_mask_8: 0.1279  loss_dice_8: 0.923     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:25:39 d2.utils.events]:  eta: 5:07:53  iter: 16159  total_loss: 22.85  loss_ce: 0.8997  loss_mask: 0.0973  loss_dice: 1.064  loss_ce_0: 1.566  loss_mask_0: 0.08712  loss_dice_0: 1.111  loss_ce_1: 1.042  loss_mask_1: 0.09961  loss_dice_1: 1.168  loss_ce_2: 0.9815  loss_mask_2: 0.1045  loss_dice_2: 1.092  loss_ce_3: 0.9045  loss_mask_3: 0.08015  loss_dice_3: 1  loss_ce_4: 0.9323  loss_mask_4: 0.06559  loss_dice_4: 0.8428  loss_ce_5: 0.9449  loss_mask_5: 0.06402  loss_dice_5: 0.7904  loss_ce_6: 0.7646  loss_mask_6: 0.06678  loss_dice_6: 0.7879  loss_ce_7: 0.7811  loss_mask_7: 0.0835  loss_dice_7: 1.09  loss_ce_8: 0.8792  loss_mask_8: 0.09133  loss_dice_8: 0.768     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:25:47 d2.utils.events]:  eta: 4:59:43  iter: 16179  total_loss: 21.51  loss_ce: 0.5995  loss_mask: 0.1594  loss_dice: 1.041  loss_ce_0: 1.171  loss_mask_0: 0.1257  loss_dice_0: 1.305  loss_ce_1: 0.8581  loss_mask_1: 0.1483  loss_dice_1: 1.424  loss_ce_2: 0.736  loss_mask_2: 0.1471  loss_dice_2: 1.052  loss_ce_3: 0.6343  loss_mask_3: 0.1394  loss_dice_3: 1.229  loss_ce_4: 0.6896  loss_mask_4: 0.1561  loss_dice_4: 0.8781  loss_ce_5: 0.706  loss_mask_5: 0.153  loss_dice_5: 1.017  loss_ce_6: 0.6474  loss_mask_6: 0.1582  loss_dice_6: 1.006  loss_ce_7: 0.6245  loss_mask_7: 0.1535  loss_dice_7: 0.817  loss_ce_8: 0.6167  loss_mask_8: 0.1495  loss_dice_8: 1.154     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 27.0 - Losses: {'loss_ce': tensor(0.5018, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.1617, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.1446, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(1.4034, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.1550, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(0.7599, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.6718, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.5443, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.8911, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.6856, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.3540, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.4260, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.5747, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.2502, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.7950, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.5641, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.2042, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.1677, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.5726, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.1765, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.5232, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.4886, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.1779, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.2817, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.4997, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.1734, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.1608, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.4995, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.1594, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.7001, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:25:55 d2.utils.events]:  eta: 5:01:26  iter: 16199  total_loss: 15.28  loss_ce: 0.6628  loss_mask: 0.08937  loss_dice: 0.6443  loss_ce_0: 1.407  loss_mask_0: 0.08268  loss_dice_0: 0.9242  loss_ce_1: 0.9585  loss_mask_1: 0.07367  loss_dice_1: 0.8235  loss_ce_2: 0.8124  loss_mask_2: 0.07591  loss_dice_2: 0.7033  loss_ce_3: 0.7224  loss_mask_3: 0.07194  loss_dice_3: 0.7114  loss_ce_4: 0.7523  loss_mask_4: 0.08656  loss_dice_4: 0.6619  loss_ce_5: 0.6935  loss_mask_5: 0.08165  loss_dice_5: 0.6487  loss_ce_6: 0.7858  loss_mask_6: 0.07542  loss_dice_6: 0.6732  loss_ce_7: 0.7645  loss_mask_7: 0.07482  loss_dice_7: 0.5576  loss_ce_8: 0.7309  loss_mask_8: 0.08183  loss_dice_8: 0.7387     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:03 d2.utils.events]:  eta: 4:55:46  iter: 16219  total_loss: 14.49  loss_ce: 0.572  loss_mask: 0.07986  loss_dice: 0.5942  loss_ce_0: 1.046  loss_mask_0: 0.07929  loss_dice_0: 0.6514  loss_ce_1: 0.7158  loss_mask_1: 0.07103  loss_dice_1: 0.6277  loss_ce_2: 0.7107  loss_mask_2: 0.06503  loss_dice_2: 0.6387  loss_ce_3: 0.6469  loss_mask_3: 0.08714  loss_dice_3: 0.6576  loss_ce_4: 0.6745  loss_mask_4: 0.08226  loss_dice_4: 0.5801  loss_ce_5: 0.6311  loss_mask_5: 0.07538  loss_dice_5: 0.6319  loss_ce_6: 0.6109  loss_mask_6: 0.07543  loss_dice_6: 0.5994  loss_ce_7: 0.6192  loss_mask_7: 0.08329  loss_dice_7: 0.6235  loss_ce_8: 0.5711  loss_mask_8: 0.061  loss_dice_8: 0.5151     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:12 d2.utils.events]:  eta: 5:04:52  iter: 16239  total_loss: 19.7  loss_ce: 0.682  loss_mask: 0.08269  loss_dice: 0.6671  loss_ce_0: 1.509  loss_mask_0: 0.1091  loss_dice_0: 1.039  loss_ce_1: 1.263  loss_mask_1: 0.06136  loss_dice_1: 0.9685  loss_ce_2: 1.105  loss_mask_2: 0.06223  loss_dice_2: 0.861  loss_ce_3: 0.9357  loss_mask_3: 0.06618  loss_dice_3: 0.6911  loss_ce_4: 0.831  loss_mask_4: 0.06851  loss_dice_4: 0.8407  loss_ce_5: 0.8284  loss_mask_5: 0.07545  loss_dice_5: 0.8245  loss_ce_6: 0.7178  loss_mask_6: 0.06875  loss_dice_6: 0.8758  loss_ce_7: 0.7509  loss_mask_7: 0.07673  loss_dice_7: 0.8039  loss_ce_8: 0.6823  loss_mask_8: 0.07094  loss_dice_8: 0.7691     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:20 d2.utils.events]:  eta: 5:12:18  iter: 16259  total_loss: 29.27  loss_ce: 1.229  loss_mask: 0.1409  loss_dice: 1.29  loss_ce_0: 1.557  loss_mask_0: 0.1009  loss_dice_0: 1.377  loss_ce_1: 1.462  loss_mask_1: 0.103  loss_dice_1: 1.332  loss_ce_2: 1.368  loss_mask_2: 0.1425  loss_dice_2: 1.316  loss_ce_3: 1.31  loss_mask_3: 0.1724  loss_dice_3: 1.336  loss_ce_4: 1.364  loss_mask_4: 0.1621  loss_dice_4: 1.205  loss_ce_5: 1.271  loss_mask_5: 0.1337  loss_dice_5: 1.052  loss_ce_6: 1.264  loss_mask_6: 0.1547  loss_dice_6: 1.208  loss_ce_7: 1.263  loss_mask_7: 0.164  loss_dice_7: 1.176  loss_ce_8: 1.274  loss_mask_8: 0.1383  loss_dice_8: 1.332     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:28 d2.utils.events]:  eta: 5:01:58  iter: 16279  total_loss: 28.08  loss_ce: 1.309  loss_mask: 0.08132  loss_dice: 1.243  loss_ce_0: 1.536  loss_mask_0: 0.07364  loss_dice_0: 1.371  loss_ce_1: 1.539  loss_mask_1: 0.07051  loss_dice_1: 1.312  loss_ce_2: 1.469  loss_mask_2: 0.07353  loss_dice_2: 1.172  loss_ce_3: 1.265  loss_mask_3: 0.08235  loss_dice_3: 1.062  loss_ce_4: 1.252  loss_mask_4: 0.07221  loss_dice_4: 1.227  loss_ce_5: 1.292  loss_mask_5: 0.08489  loss_dice_5: 1.152  loss_ce_6: 1.213  loss_mask_6: 0.09842  loss_dice_6: 1.278  loss_ce_7: 1.255  loss_mask_7: 0.06637  loss_dice_7: 1.185  loss_ce_8: 1.179  loss_mask_8: 0.09099  loss_dice_8: 1.169     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:37 d2.utils.events]:  eta: 5:02:32  iter: 16299  total_loss: 23.76  loss_ce: 1.094  loss_mask: 0.04902  loss_dice: 1.202  loss_ce_0: 1.445  loss_mask_0: 0.08005  loss_dice_0: 1.157  loss_ce_1: 1.403  loss_mask_1: 0.0629  loss_dice_1: 1.178  loss_ce_2: 1.215  loss_mask_2: 0.05717  loss_dice_2: 1.038  loss_ce_3: 1.122  loss_mask_3: 0.06143  loss_dice_3: 0.9786  loss_ce_4: 1.114  loss_mask_4: 0.07532  loss_dice_4: 1.187  loss_ce_5: 1.058  loss_mask_5: 0.06419  loss_dice_5: 1.015  loss_ce_6: 1.157  loss_mask_6: 0.07529  loss_dice_6: 0.9232  loss_ce_7: 1.113  loss_mask_7: 0.0569  loss_dice_7: 1.015  loss_ce_8: 1.03  loss_mask_8: 0.05098  loss_dice_8: 1.252     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:45 d2.utils.events]:  eta: 4:57:29  iter: 16319  total_loss: 21.65  loss_ce: 0.973  loss_mask: 0.08832  loss_dice: 0.8602  loss_ce_0: 1.504  loss_mask_0: 0.08703  loss_dice_0: 1.039  loss_ce_1: 1.069  loss_mask_1: 0.1122  loss_dice_1: 1.005  loss_ce_2: 0.9214  loss_mask_2: 0.09089  loss_dice_2: 0.8894  loss_ce_3: 1.002  loss_mask_3: 0.08237  loss_dice_3: 0.8811  loss_ce_4: 1.05  loss_mask_4: 0.09487  loss_dice_4: 0.8471  loss_ce_5: 0.9492  loss_mask_5: 0.09112  loss_dice_5: 0.8827  loss_ce_6: 1.017  loss_mask_6: 0.0922  loss_dice_6: 0.7643  loss_ce_7: 0.9082  loss_mask_7: 0.09547  loss_dice_7: 0.7932  loss_ce_8: 0.9647  loss_mask_8: 0.08563  loss_dice_8: 0.8956     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:26:53 d2.utils.events]:  eta: 5:08:10  iter: 16339  total_loss: 22.39  loss_ce: 0.6903  loss_mask: 0.07691  loss_dice: 0.8169  loss_ce_0: 1.405  loss_mask_0: 0.1017  loss_dice_0: 1.206  loss_ce_1: 1.017  loss_mask_1: 0.09736  loss_dice_1: 1.273  loss_ce_2: 0.8977  loss_mask_2: 0.1082  loss_dice_2: 1.059  loss_ce_3: 0.854  loss_mask_3: 0.09748  loss_dice_3: 0.96  loss_ce_4: 0.7871  loss_mask_4: 0.09875  loss_dice_4: 1.189  loss_ce_5: 0.7425  loss_mask_5: 0.08099  loss_dice_5: 1.1  loss_ce_6: 0.7768  loss_mask_6: 0.1055  loss_dice_6: 0.9763  loss_ce_7: 0.7659  loss_mask_7: 0.1047  loss_dice_7: 0.972  loss_ce_8: 0.7489  loss_mask_8: 0.08265  loss_dice_8: 0.8618     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:02 d2.utils.events]:  eta: 5:00:09  iter: 16359  total_loss: 21.38  loss_ce: 0.8546  loss_mask: 0.06981  loss_dice: 0.8715  loss_ce_0: 1.551  loss_mask_0: 0.07426  loss_dice_0: 1.046  loss_ce_1: 1.108  loss_mask_1: 0.0708  loss_dice_1: 0.9353  loss_ce_2: 1.017  loss_mask_2: 0.09003  loss_dice_2: 0.7929  loss_ce_3: 1.069  loss_mask_3: 0.0694  loss_dice_3: 0.757  loss_ce_4: 0.916  loss_mask_4: 0.09332  loss_dice_4: 0.6998  loss_ce_5: 0.8797  loss_mask_5: 0.1062  loss_dice_5: 0.6204  loss_ce_6: 0.8317  loss_mask_6: 0.07394  loss_dice_6: 0.7863  loss_ce_7: 0.8441  loss_mask_7: 0.07029  loss_dice_7: 0.6746  loss_ce_8: 0.8547  loss_mask_8: 0.07643  loss_dice_8: 0.7295     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:10 d2.utils.events]:  eta: 5:04:44  iter: 16379  total_loss: 22.54  loss_ce: 1.089  loss_mask: 0.1069  loss_dice: 0.9776  loss_ce_0: 1.393  loss_mask_0: 0.1019  loss_dice_0: 1.044  loss_ce_1: 1.328  loss_mask_1: 0.082  loss_dice_1: 0.8779  loss_ce_2: 1.149  loss_mask_2: 0.07933  loss_dice_2: 0.9617  loss_ce_3: 1.223  loss_mask_3: 0.0618  loss_dice_3: 1.022  loss_ce_4: 1.166  loss_mask_4: 0.07141  loss_dice_4: 1.052  loss_ce_5: 1.128  loss_mask_5: 0.08009  loss_dice_5: 0.9255  loss_ce_6: 0.9515  loss_mask_6: 0.08367  loss_dice_6: 1.137  loss_ce_7: 0.9714  loss_mask_7: 0.0783  loss_dice_7: 1.028  loss_ce_8: 1.084  loss_mask_8: 0.08205  loss_dice_8: 1.019     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:18 d2.utils.events]:  eta: 5:01:56  iter: 16399  total_loss: 16.06  loss_ce: 0.7131  loss_mask: 0.07196  loss_dice: 0.7723  loss_ce_0: 1.242  loss_mask_0: 0.1063  loss_dice_0: 0.9503  loss_ce_1: 0.9321  loss_mask_1: 0.07373  loss_dice_1: 0.7941  loss_ce_2: 0.9247  loss_mask_2: 0.09692  loss_dice_2: 0.6954  loss_ce_3: 0.7964  loss_mask_3: 0.07388  loss_dice_3: 0.6745  loss_ce_4: 0.8036  loss_mask_4: 0.08453  loss_dice_4: 0.8203  loss_ce_5: 0.8032  loss_mask_5: 0.08771  loss_dice_5: 0.7577  loss_ce_6: 0.6632  loss_mask_6: 0.08187  loss_dice_6: 0.7151  loss_ce_7: 0.6892  loss_mask_7: 0.06823  loss_dice_7: 0.6886  loss_ce_8: 0.7188  loss_mask_8: 0.08061  loss_dice_8: 0.7764     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:27 d2.utils.events]:  eta: 5:00:05  iter: 16419  total_loss: 19.35  loss_ce: 0.7314  loss_mask: 0.06795  loss_dice: 0.7251  loss_ce_0: 1.199  loss_mask_0: 0.06662  loss_dice_0: 0.8764  loss_ce_1: 0.9023  loss_mask_1: 0.07967  loss_dice_1: 0.8648  loss_ce_2: 1.027  loss_mask_2: 0.07127  loss_dice_2: 0.8589  loss_ce_3: 0.7412  loss_mask_3: 0.06419  loss_dice_3: 0.856  loss_ce_4: 0.8216  loss_mask_4: 0.08219  loss_dice_4: 0.8838  loss_ce_5: 0.7681  loss_mask_5: 0.0842  loss_dice_5: 0.7948  loss_ce_6: 0.7459  loss_mask_6: 0.07702  loss_dice_6: 0.8479  loss_ce_7: 0.6862  loss_mask_7: 0.07897  loss_dice_7: 0.7523  loss_ce_8: 0.7556  loss_mask_8: 0.07664  loss_dice_8: 0.6369     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:35 d2.utils.events]:  eta: 4:59:21  iter: 16439  total_loss: 20.13  loss_ce: 0.7431  loss_mask: 0.07793  loss_dice: 0.7356  loss_ce_0: 1.452  loss_mask_0: 0.09741  loss_dice_0: 0.9423  loss_ce_1: 1.011  loss_mask_1: 0.08281  loss_dice_1: 0.9179  loss_ce_2: 0.9241  loss_mask_2: 0.1069  loss_dice_2: 0.6987  loss_ce_3: 0.8048  loss_mask_3: 0.09865  loss_dice_3: 0.8092  loss_ce_4: 0.8832  loss_mask_4: 0.0783  loss_dice_4: 0.69  loss_ce_5: 0.7427  loss_mask_5: 0.08615  loss_dice_5: 0.7537  loss_ce_6: 0.8367  loss_mask_6: 0.0812  loss_dice_6: 0.8016  loss_ce_7: 0.8387  loss_mask_7: 0.07775  loss_dice_7: 0.8062  loss_ce_8: 0.7382  loss_mask_8: 0.0851  loss_dice_8: 0.7399     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:43 d2.utils.events]:  eta: 4:59:30  iter: 16459  total_loss: 24.43  loss_ce: 0.9661  loss_mask: 0.08514  loss_dice: 1.117  loss_ce_0: 1.319  loss_mask_0: 0.09538  loss_dice_0: 1.102  loss_ce_1: 1.05  loss_mask_1: 0.1033  loss_dice_1: 1.335  loss_ce_2: 1.075  loss_mask_2: 0.0926  loss_dice_2: 1.339  loss_ce_3: 0.9449  loss_mask_3: 0.08663  loss_dice_3: 1.36  loss_ce_4: 1.015  loss_mask_4: 0.08897  loss_dice_4: 1.297  loss_ce_5: 0.9482  loss_mask_5: 0.09201  loss_dice_5: 1.12  loss_ce_6: 0.9676  loss_mask_6: 0.098  loss_dice_6: 1.082  loss_ce_7: 0.9588  loss_mask_7: 0.08264  loss_dice_7: 0.8823  loss_ce_8: 0.9482  loss_mask_8: 0.07933  loss_dice_8: 1.296     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:27:51 d2.utils.events]:  eta: 5:03:50  iter: 16479  total_loss: 24.09  loss_ce: 0.9585  loss_mask: 0.06209  loss_dice: 0.8198  loss_ce_0: 1.511  loss_mask_0: 0.06999  loss_dice_0: 0.9199  loss_ce_1: 1.158  loss_mask_1: 0.08344  loss_dice_1: 0.8155  loss_ce_2: 1.086  loss_mask_2: 0.06572  loss_dice_2: 1.116  loss_ce_3: 0.9405  loss_mask_3: 0.0623  loss_dice_3: 0.856  loss_ce_4: 0.8925  loss_mask_4: 0.07183  loss_dice_4: 0.7306  loss_ce_5: 0.8531  loss_mask_5: 0.05989  loss_dice_5: 0.8645  loss_ce_6: 0.9075  loss_mask_6: 0.07009  loss_dice_6: 0.8372  loss_ce_7: 0.9156  loss_mask_7: 0.06391  loss_dice_7: 0.8375  loss_ce_8: 0.9453  loss_mask_8: 0.06235  loss_dice_8: 0.7005     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:00 d2.utils.events]:  eta: 4:57:10  iter: 16499  total_loss: 24.21  loss_ce: 0.8429  loss_mask: 0.1111  loss_dice: 0.9348  loss_ce_0: 1.161  loss_mask_0: 0.1307  loss_dice_0: 1.176  loss_ce_1: 0.999  loss_mask_1: 0.145  loss_dice_1: 0.9772  loss_ce_2: 0.8903  loss_mask_2: 0.1183  loss_dice_2: 1.062  loss_ce_3: 0.7913  loss_mask_3: 0.1102  loss_dice_3: 1.022  loss_ce_4: 0.8048  loss_mask_4: 0.1363  loss_dice_4: 0.8083  loss_ce_5: 0.8453  loss_mask_5: 0.1144  loss_dice_5: 0.8772  loss_ce_6: 0.8291  loss_mask_6: 0.1177  loss_dice_6: 0.9402  loss_ce_7: 0.8347  loss_mask_7: 0.1154  loss_dice_7: 0.8178  loss_ce_8: 0.8563  loss_mask_8: 0.1137  loss_dice_8: 0.8518     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:08 d2.utils.events]:  eta: 5:04:54  iter: 16519  total_loss: 20.67  loss_ce: 0.8441  loss_mask: 0.06549  loss_dice: 0.7968  loss_ce_0: 1.631  loss_mask_0: 0.0857  loss_dice_0: 1.071  loss_ce_1: 1.068  loss_mask_1: 0.08919  loss_dice_1: 0.7916  loss_ce_2: 0.984  loss_mask_2: 0.08522  loss_dice_2: 0.9922  loss_ce_3: 1.028  loss_mask_3: 0.07733  loss_dice_3: 0.7929  loss_ce_4: 0.9257  loss_mask_4: 0.0914  loss_dice_4: 0.9474  loss_ce_5: 0.9235  loss_mask_5: 0.08188  loss_dice_5: 0.9024  loss_ce_6: 0.9038  loss_mask_6: 0.08848  loss_dice_6: 0.7827  loss_ce_7: 0.8763  loss_mask_7: 0.06847  loss_dice_7: 0.9085  loss_ce_8: 0.881  loss_mask_8: 0.06523  loss_dice_8: 0.7531     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:17 d2.utils.events]:  eta: 5:09:00  iter: 16539  total_loss: 21.02  loss_ce: 0.9078  loss_mask: 0.05936  loss_dice: 0.8691  loss_ce_0: 1.424  loss_mask_0: 0.09194  loss_dice_0: 0.9805  loss_ce_1: 1.172  loss_mask_1: 0.06633  loss_dice_1: 0.9152  loss_ce_2: 1.058  loss_mask_2: 0.06236  loss_dice_2: 0.8432  loss_ce_3: 1.054  loss_mask_3: 0.05451  loss_dice_3: 0.8575  loss_ce_4: 0.9531  loss_mask_4: 0.05384  loss_dice_4: 0.8715  loss_ce_5: 0.9525  loss_mask_5: 0.05976  loss_dice_5: 0.6697  loss_ce_6: 0.9513  loss_mask_6: 0.06403  loss_dice_6: 0.7371  loss_ce_7: 0.9372  loss_mask_7: 0.05945  loss_dice_7: 0.793  loss_ce_8: 0.9595  loss_mask_8: 0.06712  loss_dice_8: 0.7486     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:25 d2.utils.events]:  eta: 5:09:33  iter: 16559  total_loss: 24.46  loss_ce: 0.8954  loss_mask: 0.0909  loss_dice: 0.9952  loss_ce_0: 1.434  loss_mask_0: 0.08013  loss_dice_0: 1.467  loss_ce_1: 1.082  loss_mask_1: 0.07857  loss_dice_1: 0.9956  loss_ce_2: 1.151  loss_mask_2: 0.09045  loss_dice_2: 1.117  loss_ce_3: 0.9534  loss_mask_3: 0.08039  loss_dice_3: 1.259  loss_ce_4: 1.054  loss_mask_4: 0.08157  loss_dice_4: 1.408  loss_ce_5: 1.078  loss_mask_5: 0.06194  loss_dice_5: 0.7048  loss_ce_6: 0.9978  loss_mask_6: 0.05946  loss_dice_6: 1.161  loss_ce_7: 1.004  loss_mask_7: 0.05953  loss_dice_7: 0.8336  loss_ce_8: 0.8909  loss_mask_8: 0.0737  loss_dice_8: 1.062     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:34 d2.utils.events]:  eta: 5:05:56  iter: 16579  total_loss: 25.07  loss_ce: 0.8274  loss_mask: 0.1349  loss_dice: 1.15  loss_ce_0: 1.395  loss_mask_0: 0.1112  loss_dice_0: 1.28  loss_ce_1: 1.156  loss_mask_1: 0.1204  loss_dice_1: 1.672  loss_ce_2: 1.088  loss_mask_2: 0.139  loss_dice_2: 1.467  loss_ce_3: 0.8758  loss_mask_3: 0.1379  loss_dice_3: 1.188  loss_ce_4: 0.7663  loss_mask_4: 0.1353  loss_dice_4: 1.288  loss_ce_5: 0.8857  loss_mask_5: 0.136  loss_dice_5: 1.533  loss_ce_6: 0.6762  loss_mask_6: 0.1565  loss_dice_6: 0.9915  loss_ce_7: 0.838  loss_mask_7: 0.1376  loss_dice_7: 1.382  loss_ce_8: 0.8447  loss_mask_8: 0.134  loss_dice_8: 1.174     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:42 d2.utils.events]:  eta: 5:01:35  iter: 16599  total_loss: 19.17  loss_ce: 0.942  loss_mask: 0.1349  loss_dice: 0.6302  loss_ce_0: 1.309  loss_mask_0: 0.2412  loss_dice_0: 0.6478  loss_ce_1: 1.031  loss_mask_1: 0.1614  loss_dice_1: 0.8727  loss_ce_2: 1.05  loss_mask_2: 0.1515  loss_dice_2: 0.9144  loss_ce_3: 1.026  loss_mask_3: 0.1239  loss_dice_3: 0.7822  loss_ce_4: 0.9037  loss_mask_4: 0.1217  loss_dice_4: 0.9963  loss_ce_5: 0.9293  loss_mask_5: 0.1236  loss_dice_5: 0.7542  loss_ce_6: 0.9006  loss_mask_6: 0.1205  loss_dice_6: 0.735  loss_ce_7: 0.9254  loss_mask_7: 0.117  loss_dice_7: 0.8192  loss_ce_8: 0.9333  loss_mask_8: 0.1241  loss_dice_8: 0.6865     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:50 d2.utils.events]:  eta: 5:03:28  iter: 16619  total_loss: 22.48  loss_ce: 0.8806  loss_mask: 0.08025  loss_dice: 1.149  loss_ce_0: 1.685  loss_mask_0: 0.1296  loss_dice_0: 1.214  loss_ce_1: 1.192  loss_mask_1: 0.1054  loss_dice_1: 1.181  loss_ce_2: 1.194  loss_mask_2: 0.1078  loss_dice_2: 0.8872  loss_ce_3: 0.9371  loss_mask_3: 0.09402  loss_dice_3: 1.095  loss_ce_4: 1.063  loss_mask_4: 0.08012  loss_dice_4: 0.8908  loss_ce_5: 1.071  loss_mask_5: 0.07696  loss_dice_5: 1.116  loss_ce_6: 0.9503  loss_mask_6: 0.08572  loss_dice_6: 0.9693  loss_ce_7: 0.8922  loss_mask_7: 0.07874  loss_dice_7: 1.221  loss_ce_8: 0.8752  loss_mask_8: 0.08104  loss_dice_8: 1.098     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:28:58 d2.utils.events]:  eta: 4:53:16  iter: 16639  total_loss: 21.59  loss_ce: 0.8632  loss_mask: 0.1919  loss_dice: 0.7463  loss_ce_0: 1.389  loss_mask_0: 0.1546  loss_dice_0: 1.056  loss_ce_1: 1.003  loss_mask_1: 0.2106  loss_dice_1: 1.267  loss_ce_2: 0.9359  loss_mask_2: 0.2362  loss_dice_2: 1.072  loss_ce_3: 0.7374  loss_mask_3: 0.1926  loss_dice_3: 0.9923  loss_ce_4: 0.7376  loss_mask_4: 0.1794  loss_dice_4: 1.114  loss_ce_5: 0.9923  loss_mask_5: 0.184  loss_dice_5: 0.9113  loss_ce_6: 0.8917  loss_mask_6: 0.1754  loss_dice_6: 0.7776  loss_ce_7: 0.8166  loss_mask_7: 0.1772  loss_dice_7: 0.9202  loss_ce_8: 0.8526  loss_mask_8: 0.1741  loss_dice_8: 0.6009     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:07 d2.utils.events]:  eta: 5:03:56  iter: 16659  total_loss: 23.35  loss_ce: 0.9107  loss_mask: 0.1061  loss_dice: 1.024  loss_ce_0: 1.463  loss_mask_0: 0.1317  loss_dice_0: 1.257  loss_ce_1: 1.266  loss_mask_1: 0.1098  loss_dice_1: 0.939  loss_ce_2: 1.047  loss_mask_2: 0.1036  loss_dice_2: 1.079  loss_ce_3: 0.854  loss_mask_3: 0.09394  loss_dice_3: 1.096  loss_ce_4: 0.8529  loss_mask_4: 0.1044  loss_dice_4: 1.059  loss_ce_5: 0.8485  loss_mask_5: 0.1042  loss_dice_5: 1.063  loss_ce_6: 0.858  loss_mask_6: 0.09831  loss_dice_6: 0.9981  loss_ce_7: 0.7671  loss_mask_7: 0.09983  loss_dice_7: 0.8003  loss_ce_8: 0.7735  loss_mask_8: 0.09289  loss_dice_8: 0.8883     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:15 d2.utils.events]:  eta: 5:00:56  iter: 16679  total_loss: 14.51  loss_ce: 0.4716  loss_mask: 0.08389  loss_dice: 0.6205  loss_ce_0: 0.9459  loss_mask_0: 0.07715  loss_dice_0: 0.6094  loss_ce_1: 0.6114  loss_mask_1: 0.09572  loss_dice_1: 0.6613  loss_ce_2: 0.5227  loss_mask_2: 0.09615  loss_dice_2: 0.6219  loss_ce_3: 0.5558  loss_mask_3: 0.07865  loss_dice_3: 0.5745  loss_ce_4: 0.7063  loss_mask_4: 0.08877  loss_dice_4: 0.6493  loss_ce_5: 0.4873  loss_mask_5: 0.1027  loss_dice_5: 0.645  loss_ce_6: 0.5634  loss_mask_6: 0.06984  loss_dice_6: 0.5526  loss_ce_7: 0.541  loss_mask_7: 0.07248  loss_dice_7: 0.5416  loss_ce_8: 0.5448  loss_mask_8: 0.07632  loss_dice_8: 0.585     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:24 d2.utils.events]:  eta: 5:01:15  iter: 16699  total_loss: 16.82  loss_ce: 0.71  loss_mask: 0.1221  loss_dice: 0.5806  loss_ce_0: 1.491  loss_mask_0: 0.1093  loss_dice_0: 0.8309  loss_ce_1: 0.8766  loss_mask_1: 0.172  loss_dice_1: 0.8078  loss_ce_2: 0.9312  loss_mask_2: 0.1582  loss_dice_2: 0.6586  loss_ce_3: 0.7668  loss_mask_3: 0.1461  loss_dice_3: 0.7247  loss_ce_4: 0.7181  loss_mask_4: 0.1471  loss_dice_4: 0.7804  loss_ce_5: 0.7293  loss_mask_5: 0.1294  loss_dice_5: 0.9468  loss_ce_6: 0.6923  loss_mask_6: 0.1319  loss_dice_6: 0.5933  loss_ce_7: 0.6823  loss_mask_7: 0.1138  loss_dice_7: 0.5715  loss_ce_8: 0.656  loss_mask_8: 0.1171  loss_dice_8: 0.6232     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:32 d2.utils.events]:  eta: 5:02:20  iter: 16719  total_loss: 23.84  loss_ce: 0.8283  loss_mask: 0.07614  loss_dice: 1.078  loss_ce_0: 1.664  loss_mask_0: 0.06319  loss_dice_0: 1.208  loss_ce_1: 1.189  loss_mask_1: 0.07951  loss_dice_1: 1.442  loss_ce_2: 0.9689  loss_mask_2: 0.07747  loss_dice_2: 1.225  loss_ce_3: 0.9735  loss_mask_3: 0.07115  loss_dice_3: 1.06  loss_ce_4: 0.8327  loss_mask_4: 0.07658  loss_dice_4: 1.226  loss_ce_5: 0.8283  loss_mask_5: 0.09039  loss_dice_5: 1.154  loss_ce_6: 0.8612  loss_mask_6: 0.0744  loss_dice_6: 1.43  loss_ce_7: 0.8931  loss_mask_7: 0.07226  loss_dice_7: 1.173  loss_ce_8: 0.9542  loss_mask_8: 0.07409  loss_dice_8: 1.168     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:40 d2.utils.events]:  eta: 5:04:30  iter: 16739  total_loss: 34.68  loss_ce: 1.223  loss_mask: 0.07114  loss_dice: 1.551  loss_ce_0: 1.72  loss_mask_0: 0.09986  loss_dice_0: 1.396  loss_ce_1: 1.453  loss_mask_1: 0.1402  loss_dice_1: 1.632  loss_ce_2: 1.577  loss_mask_2: 0.09991  loss_dice_2: 2.172  loss_ce_3: 1.379  loss_mask_3: 0.1027  loss_dice_3: 1.56  loss_ce_4: 1.333  loss_mask_4: 0.09702  loss_dice_4: 1.69  loss_ce_5: 1.33  loss_mask_5: 0.07105  loss_dice_5: 1.489  loss_ce_6: 1.239  loss_mask_6: 0.07789  loss_dice_6: 1.599  loss_ce_7: 1.265  loss_mask_7: 0.06857  loss_dice_7: 1.862  loss_ce_8: 1.18  loss_mask_8: 0.07704  loss_dice_8: 1.622     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:49 d2.utils.events]:  eta: 5:07:28  iter: 16759  total_loss: 24.08  loss_ce: 0.8601  loss_mask: 0.06498  loss_dice: 0.8607  loss_ce_0: 1.543  loss_mask_0: 0.07831  loss_dice_0: 1.218  loss_ce_1: 1.123  loss_mask_1: 0.08525  loss_dice_1: 1.065  loss_ce_2: 1.105  loss_mask_2: 0.08804  loss_dice_2: 0.9815  loss_ce_3: 0.8824  loss_mask_3: 0.06788  loss_dice_3: 0.7438  loss_ce_4: 0.8824  loss_mask_4: 0.07689  loss_dice_4: 1.196  loss_ce_5: 0.8605  loss_mask_5: 0.07072  loss_dice_5: 0.8529  loss_ce_6: 0.8542  loss_mask_6: 0.06977  loss_dice_6: 0.6742  loss_ce_7: 0.9458  loss_mask_7: 0.06947  loss_dice_7: 0.693  loss_ce_8: 0.7255  loss_mask_8: 0.06556  loss_dice_8: 0.8339     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:29:57 d2.utils.events]:  eta: 4:52:24  iter: 16779  total_loss: 17.62  loss_ce: 0.673  loss_mask: 0.1263  loss_dice: 0.7571  loss_ce_0: 1.124  loss_mask_0: 0.1353  loss_dice_0: 0.9595  loss_ce_1: 0.8368  loss_mask_1: 0.1322  loss_dice_1: 0.7236  loss_ce_2: 0.8187  loss_mask_2: 0.1434  loss_dice_2: 0.677  loss_ce_3: 0.8246  loss_mask_3: 0.1133  loss_dice_3: 0.6698  loss_ce_4: 0.7695  loss_mask_4: 0.1069  loss_dice_4: 0.6893  loss_ce_5: 0.7146  loss_mask_5: 0.1175  loss_dice_5: 0.5825  loss_ce_6: 0.6808  loss_mask_6: 0.1355  loss_dice_6: 0.7169  loss_ce_7: 0.7578  loss_mask_7: 0.1316  loss_dice_7: 0.61  loss_ce_8: 0.6987  loss_mask_8: 0.128  loss_dice_8: 0.5941     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 28.0 - Losses: {'loss_ce': tensor(0.1977, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.0361, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.1378, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(0.5358, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.0466, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(0.2935, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.2704, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.0470, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.1918, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.2476, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.0364, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.1516, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.2359, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.0264, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.1193, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.2108, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.0278, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.1215, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.1985, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.0247, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.1199, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.1877, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.0257, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.1225, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.1942, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.0281, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.1256, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.1937, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.0295, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.1050, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:30:05 d2.utils.events]:  eta: 4:55:16  iter: 16799  total_loss: 17.16  loss_ce: 0.7558  loss_mask: 0.09192  loss_dice: 0.546  loss_ce_0: 1.186  loss_mask_0: 0.1139  loss_dice_0: 0.8338  loss_ce_1: 0.9146  loss_mask_1: 0.09219  loss_dice_1: 0.4785  loss_ce_2: 0.9  loss_mask_2: 0.0831  loss_dice_2: 0.485  loss_ce_3: 0.862  loss_mask_3: 0.1071  loss_dice_3: 0.5779  loss_ce_4: 0.7606  loss_mask_4: 0.1062  loss_dice_4: 0.6037  loss_ce_5: 0.8064  loss_mask_5: 0.1097  loss_dice_5: 0.6012  loss_ce_6: 0.7884  loss_mask_6: 0.1062  loss_dice_6: 0.4986  loss_ce_7: 0.8169  loss_mask_7: 0.09701  loss_dice_7: 0.4983  loss_ce_8: 0.7094  loss_mask_8: 0.09827  loss_dice_8: 0.5512     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:30:13 d2.utils.events]:  eta: 4:55:00  iter: 16819  total_loss: 20.29  loss_ce: 0.6398  loss_mask: 0.09933  loss_dice: 0.7827  loss_ce_0: 1.356  loss_mask_0: 0.1035  loss_dice_0: 0.8534  loss_ce_1: 0.9531  loss_mask_1: 0.1154  loss_dice_1: 1.034  loss_ce_2: 0.8469  loss_mask_2: 0.1109  loss_dice_2: 0.8027  loss_ce_3: 0.7217  loss_mask_3: 0.09672  loss_dice_3: 0.757  loss_ce_4: 0.7284  loss_mask_4: 0.097  loss_dice_4: 0.8487  loss_ce_5: 0.6992  loss_mask_5: 0.112  loss_dice_5: 0.8209  loss_ce_6: 0.7184  loss_mask_6: 0.1075  loss_dice_6: 0.7251  loss_ce_7: 0.6854  loss_mask_7: 0.1104  loss_dice_7: 0.7252  loss_ce_8: 0.631  loss_mask_8: 0.1109  loss_dice_8: 0.7114     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:30:22 d2.utils.events]:  eta: 4:57:10  iter: 16839  total_loss: 17.94  loss_ce: 0.7461  loss_mask: 0.06595  loss_dice: 0.7313  loss_ce_0: 1.282  loss_mask_0: 0.0824  loss_dice_0: 0.9024  loss_ce_1: 1.222  loss_mask_1: 0.09084  loss_dice_1: 0.8383  loss_ce_2: 0.9855  loss_mask_2: 0.08473  loss_dice_2: 0.75  loss_ce_3: 1.022  loss_mask_3: 0.07921  loss_dice_3: 0.6104  loss_ce_4: 0.8281  loss_mask_4: 0.07386  loss_dice_4: 0.7297  loss_ce_5: 0.8218  loss_mask_5: 0.0757  loss_dice_5: 0.6601  loss_ce_6: 0.7844  loss_mask_6: 0.06917  loss_dice_6: 0.731  loss_ce_7: 0.7978  loss_mask_7: 0.06467  loss_dice_7: 0.6728  loss_ce_8: 0.835  loss_mask_8: 0.06963  loss_dice_8: 0.684     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:30:30 d2.utils.events]:  eta: 4:55:02  iter: 16859  total_loss: 17.26  loss_ce: 0.6959  loss_mask: 0.07113  loss_dice: 0.6778  loss_ce_0: 1.277  loss_mask_0: 0.08398  loss_dice_0: 1.052  loss_ce_1: 1.024  loss_mask_1: 0.0775  loss_dice_1: 0.8553  loss_ce_2: 0.9194  loss_mask_2: 0.07443  loss_dice_2: 0.51  loss_ce_3: 0.8508  loss_mask_3: 0.07066  loss_dice_3: 0.8934  loss_ce_4: 0.7205  loss_mask_4: 0.07327  loss_dice_4: 0.558  loss_ce_5: 0.7039  loss_mask_5: 0.07679  loss_dice_5: 0.9052  loss_ce_6: 0.6724  loss_mask_6: 0.06555  loss_dice_6: 0.553  loss_ce_7: 0.7029  loss_mask_7: 0.07239  loss_dice_7: 0.7169  loss_ce_8: 0.7351  loss_mask_8: 0.06544  loss_dice_8: 0.7852     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:30:38 d2.utils.events]:  eta: 5:06:49  iter: 16879  total_loss: 21.21  loss_ce: 0.6745  loss_mask: 0.07538  loss_dice: 0.9649  loss_ce_0: 1.588  loss_mask_0: 0.05958  loss_dice_0: 0.9796  loss_ce_1: 0.959  loss_mask_1: 0.07867  loss_dice_1: 1.272  loss_ce_2: 0.8794  loss_mask_2: 0.06968  loss_dice_2: 1.035  loss_ce_3: 0.7808  loss_mask_3: 0.06216  loss_dice_3: 1.046  loss_ce_4: 0.7428  loss_mask_4: 0.0623  loss_dice_4: 0.9095  loss_ce_5: 0.878  loss_mask_5: 0.0697  loss_dice_5: 1.037  loss_ce_6: 0.7092  loss_mask_6: 0.06488  loss_dice_6: 0.9227  loss_ce_7: 0.686  loss_mask_7: 0.07207  loss_dice_7: 0.8822  loss_ce_8: 0.6499  loss_mask_8: 0.06878  loss_dice_8: 0.8675     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:30:46 d2.utils.events]:  eta: 4:50:55  iter: 16899  total_loss: 17.37  loss_ce: 0.7528  loss_mask: 0.07085  loss_dice: 0.5972  loss_ce_0: 1.249  loss_mask_0: 0.08154  loss_dice_0: 0.7493  loss_ce_1: 0.8841  loss_mask_1: 0.08471  loss_dice_1: 0.7795  loss_ce_2: 0.9416  loss_mask_2: 0.08128  loss_dice_2: 0.7911  loss_ce_3: 0.724  loss_mask_3: 0.09685  loss_dice_3: 0.6202  loss_ce_4: 0.7598  loss_mask_4: 0.07837  loss_dice_4: 0.6695  loss_ce_5: 0.7178  loss_mask_5: 0.0839  loss_dice_5: 0.6256  loss_ce_6: 0.7771  loss_mask_6: 0.07576  loss_dice_6: 0.6286  loss_ce_7: 0.6975  loss_mask_7: 0.0677  loss_dice_7: 0.5765  loss_ce_8: 0.6841  loss_mask_8: 0.07631  loss_dice_8: 0.6892     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:30:55 d2.utils.events]:  eta: 4:54:09  iter: 16919  total_loss: 24.49  loss_ce: 0.9322  loss_mask: 0.1726  loss_dice: 0.7579  loss_ce_0: 1.724  loss_mask_0: 0.23  loss_dice_0: 1.15  loss_ce_1: 1.201  loss_mask_1: 0.2075  loss_dice_1: 1.188  loss_ce_2: 1.052  loss_mask_2: 0.1728  loss_dice_2: 0.8803  loss_ce_3: 0.9901  loss_mask_3: 0.1824  loss_dice_3: 0.828  loss_ce_4: 1.046  loss_mask_4: 0.184  loss_dice_4: 0.8005  loss_ce_5: 0.9647  loss_mask_5: 0.18  loss_dice_5: 0.829  loss_ce_6: 0.9762  loss_mask_6: 0.1662  loss_dice_6: 0.8248  loss_ce_7: 0.9795  loss_mask_7: 0.1851  loss_dice_7: 0.7706  loss_ce_8: 1.011  loss_mask_8: 0.1884  loss_dice_8: 0.746     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:03 d2.utils.events]:  eta: 5:12:30  iter: 16939  total_loss: 22.26  loss_ce: 1.091  loss_mask: 0.09068  loss_dice: 0.8929  loss_ce_0: 1.563  loss_mask_0: 0.1419  loss_dice_0: 1.17  loss_ce_1: 1.144  loss_mask_1: 0.167  loss_dice_1: 0.8963  loss_ce_2: 1.045  loss_mask_2: 0.1079  loss_dice_2: 0.8925  loss_ce_3: 1.095  loss_mask_3: 0.1156  loss_dice_3: 1.038  loss_ce_4: 1.165  loss_mask_4: 0.08635  loss_dice_4: 0.8987  loss_ce_5: 1.052  loss_mask_5: 0.1169  loss_dice_5: 1.015  loss_ce_6: 1.102  loss_mask_6: 0.09253  loss_dice_6: 0.7855  loss_ce_7: 1.049  loss_mask_7: 0.09949  loss_dice_7: 0.9225  loss_ce_8: 1.014  loss_mask_8: 0.09065  loss_dice_8: 0.9041     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:12 d2.utils.events]:  eta: 5:00:11  iter: 16959  total_loss: 22.16  loss_ce: 0.7071  loss_mask: 0.1171  loss_dice: 0.7516  loss_ce_0: 1.079  loss_mask_0: 0.1063  loss_dice_0: 0.9872  loss_ce_1: 0.7919  loss_mask_1: 0.09762  loss_dice_1: 0.9445  loss_ce_2: 0.745  loss_mask_2: 0.117  loss_dice_2: 0.9254  loss_ce_3: 0.7678  loss_mask_3: 0.1087  loss_dice_3: 0.7697  loss_ce_4: 0.7205  loss_mask_4: 0.1111  loss_dice_4: 0.7181  loss_ce_5: 0.7101  loss_mask_5: 0.1157  loss_dice_5: 0.8091  loss_ce_6: 0.853  loss_mask_6: 0.1054  loss_dice_6: 0.8108  loss_ce_7: 0.6952  loss_mask_7: 0.1109  loss_dice_7: 0.834  loss_ce_8: 0.6888  loss_mask_8: 0.1163  loss_dice_8: 0.8172     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:20 d2.utils.events]:  eta: 5:03:15  iter: 16979  total_loss: 28.65  loss_ce: 1.152  loss_mask: 0.0825  loss_dice: 1.018  loss_ce_0: 1.752  loss_mask_0: 0.1204  loss_dice_0: 1.328  loss_ce_1: 1.413  loss_mask_1: 0.1057  loss_dice_1: 1.183  loss_ce_2: 1.26  loss_mask_2: 0.09343  loss_dice_2: 1.084  loss_ce_3: 1.184  loss_mask_3: 0.07604  loss_dice_3: 0.8405  loss_ce_4: 1.152  loss_mask_4: 0.07716  loss_dice_4: 1.041  loss_ce_5: 1.13  loss_mask_5: 0.1056  loss_dice_5: 0.9972  loss_ce_6: 0.9796  loss_mask_6: 0.07764  loss_dice_6: 1.008  loss_ce_7: 1.083  loss_mask_7: 0.1032  loss_dice_7: 0.9005  loss_ce_8: 0.9992  loss_mask_8: 0.07767  loss_dice_8: 1.003     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:29 d2.utils.events]:  eta: 5:00:23  iter: 16999  total_loss: 22.23  loss_ce: 0.7877  loss_mask: 0.1124  loss_dice: 0.845  loss_ce_0: 1.191  loss_mask_0: 0.1437  loss_dice_0: 1.173  loss_ce_1: 1.054  loss_mask_1: 0.1149  loss_dice_1: 0.9275  loss_ce_2: 0.9998  loss_mask_2: 0.1137  loss_dice_2: 0.8583  loss_ce_3: 0.8851  loss_mask_3: 0.1223  loss_dice_3: 0.7627  loss_ce_4: 0.8484  loss_mask_4: 0.1133  loss_dice_4: 0.8716  loss_ce_5: 0.8632  loss_mask_5: 0.1256  loss_dice_5: 0.9907  loss_ce_6: 0.786  loss_mask_6: 0.1166  loss_dice_6: 0.7851  loss_ce_7: 0.829  loss_mask_7: 0.1124  loss_dice_7: 0.8623  loss_ce_8: 0.7945  loss_mask_8: 0.1134  loss_dice_8: 0.8569     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:37 d2.utils.events]:  eta: 4:54:37  iter: 17019  total_loss: 20.2  loss_ce: 0.8065  loss_mask: 0.0617  loss_dice: 0.6795  loss_ce_0: 1.513  loss_mask_0: 0.07561  loss_dice_0: 1.043  loss_ce_1: 1.116  loss_mask_1: 0.06584  loss_dice_1: 0.8955  loss_ce_2: 1.074  loss_mask_2: 0.07733  loss_dice_2: 0.8576  loss_ce_3: 0.9483  loss_mask_3: 0.05863  loss_dice_3: 0.8266  loss_ce_4: 0.9804  loss_mask_4: 0.06682  loss_dice_4: 0.8214  loss_ce_5: 0.9711  loss_mask_5: 0.07977  loss_dice_5: 0.8559  loss_ce_6: 0.9013  loss_mask_6: 0.06061  loss_dice_6: 0.7896  loss_ce_7: 0.9168  loss_mask_7: 0.06125  loss_dice_7: 0.771  loss_ce_8: 0.7992  loss_mask_8: 0.06116  loss_dice_8: 0.7387     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:45 d2.utils.events]:  eta: 5:04:46  iter: 17039  total_loss: 18.17  loss_ce: 0.8641  loss_mask: 0.05859  loss_dice: 0.6549  loss_ce_0: 1.541  loss_mask_0: 0.05785  loss_dice_0: 0.7024  loss_ce_1: 1.166  loss_mask_1: 0.07974  loss_dice_1: 0.7643  loss_ce_2: 0.9934  loss_mask_2: 0.07282  loss_dice_2: 0.6739  loss_ce_3: 0.8927  loss_mask_3: 0.05946  loss_dice_3: 0.6756  loss_ce_4: 0.8943  loss_mask_4: 0.05629  loss_dice_4: 0.6346  loss_ce_5: 0.8465  loss_mask_5: 0.06606  loss_dice_5: 0.6667  loss_ce_6: 0.8909  loss_mask_6: 0.05492  loss_dice_6: 0.6353  loss_ce_7: 0.8515  loss_mask_7: 0.06092  loss_dice_7: 0.6697  loss_ce_8: 0.8135  loss_mask_8: 0.05374  loss_dice_8: 0.6408     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:31:54 d2.utils.events]:  eta: 4:53:20  iter: 17059  total_loss: 17.74  loss_ce: 0.8351  loss_mask: 0.06936  loss_dice: 0.6164  loss_ce_0: 1.455  loss_mask_0: 0.1061  loss_dice_0: 0.795  loss_ce_1: 1.102  loss_mask_1: 0.09676  loss_dice_1: 0.6011  loss_ce_2: 0.9717  loss_mask_2: 0.08404  loss_dice_2: 0.7673  loss_ce_3: 0.9195  loss_mask_3: 0.07054  loss_dice_3: 0.6318  loss_ce_4: 0.9046  loss_mask_4: 0.07013  loss_dice_4: 0.6854  loss_ce_5: 0.8367  loss_mask_5: 0.06552  loss_dice_5: 0.6534  loss_ce_6: 0.9056  loss_mask_6: 0.07983  loss_dice_6: 0.7793  loss_ce_7: 0.9127  loss_mask_7: 0.07235  loss_dice_7: 0.6175  loss_ce_8: 0.8377  loss_mask_8: 0.07491  loss_dice_8: 0.7198     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:02 d2.utils.events]:  eta: 4:57:57  iter: 17079  total_loss: 21.64  loss_ce: 0.7774  loss_mask: 0.1415  loss_dice: 0.8722  loss_ce_0: 1.516  loss_mask_0: 0.167  loss_dice_0: 1.03  loss_ce_1: 0.8527  loss_mask_1: 0.1634  loss_dice_1: 1.113  loss_ce_2: 0.8805  loss_mask_2: 0.139  loss_dice_2: 1.019  loss_ce_3: 0.7489  loss_mask_3: 0.1296  loss_dice_3: 0.8837  loss_ce_4: 0.808  loss_mask_4: 0.1372  loss_dice_4: 0.919  loss_ce_5: 0.8468  loss_mask_5: 0.151  loss_dice_5: 0.9487  loss_ce_6: 0.7965  loss_mask_6: 0.1497  loss_dice_6: 0.8531  loss_ce_7: 0.7092  loss_mask_7: 0.1369  loss_dice_7: 0.7925  loss_ce_8: 0.6725  loss_mask_8: 0.1502  loss_dice_8: 0.8329     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:10 d2.utils.events]:  eta: 4:58:13  iter: 17099  total_loss: 19.87  loss_ce: 0.6796  loss_mask: 0.1036  loss_dice: 0.8348  loss_ce_0: 1.277  loss_mask_0: 0.1072  loss_dice_0: 1.085  loss_ce_1: 0.9978  loss_mask_1: 0.08672  loss_dice_1: 1.134  loss_ce_2: 0.9072  loss_mask_2: 0.1077  loss_dice_2: 1.085  loss_ce_3: 0.7833  loss_mask_3: 0.09355  loss_dice_3: 1.111  loss_ce_4: 0.7427  loss_mask_4: 0.09701  loss_dice_4: 1.186  loss_ce_5: 0.6822  loss_mask_5: 0.08053  loss_dice_5: 1.028  loss_ce_6: 0.5932  loss_mask_6: 0.1054  loss_dice_6: 1.055  loss_ce_7: 0.6089  loss_mask_7: 0.1138  loss_dice_7: 1.049  loss_ce_8: 0.6952  loss_mask_8: 0.1048  loss_dice_8: 1.016     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:18 d2.utils.events]:  eta: 4:52:04  iter: 17119  total_loss: 18.26  loss_ce: 0.7489  loss_mask: 0.08223  loss_dice: 0.8117  loss_ce_0: 1.227  loss_mask_0: 0.09095  loss_dice_0: 1.141  loss_ce_1: 0.9576  loss_mask_1: 0.1057  loss_dice_1: 1.071  loss_ce_2: 0.9383  loss_mask_2: 0.09969  loss_dice_2: 0.9867  loss_ce_3: 0.7839  loss_mask_3: 0.1113  loss_dice_3: 0.9264  loss_ce_4: 0.7463  loss_mask_4: 0.1174  loss_dice_4: 0.9114  loss_ce_5: 0.6562  loss_mask_5: 0.1048  loss_dice_5: 0.9543  loss_ce_6: 0.7076  loss_mask_6: 0.07593  loss_dice_6: 0.9262  loss_ce_7: 0.6469  loss_mask_7: 0.08957  loss_dice_7: 0.95  loss_ce_8: 0.7264  loss_mask_8: 0.08862  loss_dice_8: 0.847     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:27 d2.utils.events]:  eta: 5:02:27  iter: 17139  total_loss: 22.99  loss_ce: 0.8196  loss_mask: 0.08448  loss_dice: 0.9727  loss_ce_0: 1.33  loss_mask_0: 0.1043  loss_dice_0: 1.283  loss_ce_1: 0.9715  loss_mask_1: 0.1609  loss_dice_1: 1.191  loss_ce_2: 0.9225  loss_mask_2: 0.1325  loss_dice_2: 1.19  loss_ce_3: 0.9514  loss_mask_3: 0.09219  loss_dice_3: 1.081  loss_ce_4: 0.8355  loss_mask_4: 0.1059  loss_dice_4: 0.9721  loss_ce_5: 0.7927  loss_mask_5: 0.09453  loss_dice_5: 1.113  loss_ce_6: 0.7816  loss_mask_6: 0.09117  loss_dice_6: 0.967  loss_ce_7: 0.9878  loss_mask_7: 0.09148  loss_dice_7: 1.022  loss_ce_8: 0.8516  loss_mask_8: 0.0944  loss_dice_8: 0.9878     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:35 d2.utils.events]:  eta: 4:56:07  iter: 17159  total_loss: 23.81  loss_ce: 0.9996  loss_mask: 0.07117  loss_dice: 0.9581  loss_ce_0: 1.486  loss_mask_0: 0.0845  loss_dice_0: 1.32  loss_ce_1: 1.213  loss_mask_1: 0.07051  loss_dice_1: 1.165  loss_ce_2: 1.136  loss_mask_2: 0.06962  loss_dice_2: 1.258  loss_ce_3: 1.089  loss_mask_3: 0.07661  loss_dice_3: 1.05  loss_ce_4: 0.9586  loss_mask_4: 0.06635  loss_dice_4: 1.233  loss_ce_5: 0.982  loss_mask_5: 0.07157  loss_dice_5: 1.298  loss_ce_6: 1.028  loss_mask_6: 0.06921  loss_dice_6: 1.102  loss_ce_7: 1.073  loss_mask_7: 0.07011  loss_dice_7: 1.105  loss_ce_8: 1.097  loss_mask_8: 0.07286  loss_dice_8: 1.012     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:44 d2.utils.events]:  eta: 5:00:52  iter: 17179  total_loss: 18.66  loss_ce: 0.6879  loss_mask: 0.1447  loss_dice: 0.7728  loss_ce_0: 1.251  loss_mask_0: 0.1648  loss_dice_0: 0.8404  loss_ce_1: 0.9917  loss_mask_1: 0.1481  loss_dice_1: 0.6797  loss_ce_2: 0.8808  loss_mask_2: 0.1105  loss_dice_2: 0.6935  loss_ce_3: 0.8672  loss_mask_3: 0.1128  loss_dice_3: 0.651  loss_ce_4: 0.743  loss_mask_4: 0.1177  loss_dice_4: 0.802  loss_ce_5: 0.7247  loss_mask_5: 0.1087  loss_dice_5: 0.876  loss_ce_6: 0.7331  loss_mask_6: 0.1081  loss_dice_6: 0.7926  loss_ce_7: 0.7136  loss_mask_7: 0.1115  loss_dice_7: 0.8566  loss_ce_8: 0.7338  loss_mask_8: 0.1448  loss_dice_8: 0.7281     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:32:52 d2.utils.events]:  eta: 5:02:15  iter: 17199  total_loss: 20.83  loss_ce: 0.8024  loss_mask: 0.07001  loss_dice: 1.105  loss_ce_0: 1.502  loss_mask_0: 0.06275  loss_dice_0: 1.252  loss_ce_1: 1.127  loss_mask_1: 0.07757  loss_dice_1: 1.42  loss_ce_2: 1.12  loss_mask_2: 0.07364  loss_dice_2: 1.064  loss_ce_3: 0.981  loss_mask_3: 0.06458  loss_dice_3: 1.051  loss_ce_4: 0.919  loss_mask_4: 0.08399  loss_dice_4: 1.057  loss_ce_5: 0.8898  loss_mask_5: 0.07552  loss_dice_5: 1.212  loss_ce_6: 0.7841  loss_mask_6: 0.06493  loss_dice_6: 1.155  loss_ce_7: 0.8348  loss_mask_7: 0.07794  loss_dice_7: 1.039  loss_ce_8: 0.9036  loss_mask_8: 0.06987  loss_dice_8: 0.9761     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:00 d2.utils.events]:  eta: 4:52:15  iter: 17219  total_loss: 22.13  loss_ce: 0.8743  loss_mask: 0.07058  loss_dice: 0.9545  loss_ce_0: 2.088  loss_mask_0: 0.06388  loss_dice_0: 1.22  loss_ce_1: 1.302  loss_mask_1: 0.06205  loss_dice_1: 1.131  loss_ce_2: 1.138  loss_mask_2: 0.07529  loss_dice_2: 0.9095  loss_ce_3: 0.9382  loss_mask_3: 0.07328  loss_dice_3: 0.9765  loss_ce_4: 0.8924  loss_mask_4: 0.07095  loss_dice_4: 0.7816  loss_ce_5: 0.9247  loss_mask_5: 0.06855  loss_dice_5: 0.9646  loss_ce_6: 0.8858  loss_mask_6: 0.0707  loss_dice_6: 0.8918  loss_ce_7: 0.7759  loss_mask_7: 0.08082  loss_dice_7: 0.896  loss_ce_8: 0.8631  loss_mask_8: 0.07517  loss_dice_8: 1.065     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:09 d2.utils.events]:  eta: 4:58:20  iter: 17239  total_loss: 18.09  loss_ce: 0.7978  loss_mask: 0.05927  loss_dice: 0.8447  loss_ce_0: 1.348  loss_mask_0: 0.0649  loss_dice_0: 0.785  loss_ce_1: 1.094  loss_mask_1: 0.06783  loss_dice_1: 0.709  loss_ce_2: 1.034  loss_mask_2: 0.06101  loss_dice_2: 0.7089  loss_ce_3: 0.8545  loss_mask_3: 0.06488  loss_dice_3: 0.805  loss_ce_4: 0.9239  loss_mask_4: 0.05905  loss_dice_4: 0.6684  loss_ce_5: 0.7605  loss_mask_5: 0.07478  loss_dice_5: 0.7488  loss_ce_6: 0.781  loss_mask_6: 0.06127  loss_dice_6: 0.6649  loss_ce_7: 0.765  loss_mask_7: 0.06459  loss_dice_7: 0.7706  loss_ce_8: 0.8325  loss_mask_8: 0.06654  loss_dice_8: 0.6207     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:17 d2.utils.events]:  eta: 4:57:33  iter: 17259  total_loss: 20.56  loss_ce: 0.7334  loss_mask: 0.07073  loss_dice: 0.7591  loss_ce_0: 1.833  loss_mask_0: 0.08509  loss_dice_0: 1.176  loss_ce_1: 1.258  loss_mask_1: 0.06518  loss_dice_1: 1.01  loss_ce_2: 1.118  loss_mask_2: 0.06563  loss_dice_2: 0.8077  loss_ce_3: 0.8235  loss_mask_3: 0.07162  loss_dice_3: 0.9513  loss_ce_4: 0.8156  loss_mask_4: 0.07216  loss_dice_4: 0.9079  loss_ce_5: 0.8091  loss_mask_5: 0.07033  loss_dice_5: 0.9595  loss_ce_6: 0.7514  loss_mask_6: 0.06878  loss_dice_6: 0.7729  loss_ce_7: 0.7461  loss_mask_7: 0.06355  loss_dice_7: 0.8477  loss_ce_8: 0.8271  loss_mask_8: 0.06453  loss_dice_8: 0.8065     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:25 d2.utils.events]:  eta: 4:55:05  iter: 17279  total_loss: 17.21  loss_ce: 0.6452  loss_mask: 0.09075  loss_dice: 0.6989  loss_ce_0: 1.636  loss_mask_0: 0.103  loss_dice_0: 0.9268  loss_ce_1: 1.087  loss_mask_1: 0.0864  loss_dice_1: 0.8925  loss_ce_2: 0.9648  loss_mask_2: 0.08011  loss_dice_2: 0.8528  loss_ce_3: 0.8111  loss_mask_3: 0.09503  loss_dice_3: 0.9003  loss_ce_4: 0.7854  loss_mask_4: 0.08469  loss_dice_4: 0.8164  loss_ce_5: 0.7996  loss_mask_5: 0.07883  loss_dice_5: 0.7715  loss_ce_6: 0.7376  loss_mask_6: 0.08456  loss_dice_6: 0.6348  loss_ce_7: 0.6545  loss_mask_7: 0.09108  loss_dice_7: 0.9215  loss_ce_8: 0.6488  loss_mask_8: 0.08666  loss_dice_8: 0.7008     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:34 d2.utils.events]:  eta: 4:58:00  iter: 17299  total_loss: 18.6  loss_ce: 0.9053  loss_mask: 0.06293  loss_dice: 0.9273  loss_ce_0: 1.652  loss_mask_0: 0.05954  loss_dice_0: 1.335  loss_ce_1: 1.34  loss_mask_1: 0.05443  loss_dice_1: 0.9559  loss_ce_2: 1.037  loss_mask_2: 0.06558  loss_dice_2: 0.79  loss_ce_3: 0.9194  loss_mask_3: 0.06853  loss_dice_3: 0.849  loss_ce_4: 0.9204  loss_mask_4: 0.0622  loss_dice_4: 0.8368  loss_ce_5: 0.9043  loss_mask_5: 0.06459  loss_dice_5: 0.93  loss_ce_6: 0.8936  loss_mask_6: 0.06276  loss_dice_6: 0.8461  loss_ce_7: 0.9239  loss_mask_7: 0.06123  loss_dice_7: 0.6622  loss_ce_8: 0.9316  loss_mask_8: 0.06788  loss_dice_8: 0.7888     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:42 d2.utils.events]:  eta: 4:57:28  iter: 17319  total_loss: 16.54  loss_ce: 0.7614  loss_mask: 0.06589  loss_dice: 0.5965  loss_ce_0: 1.185  loss_mask_0: 0.07055  loss_dice_0: 0.8857  loss_ce_1: 0.8125  loss_mask_1: 0.09788  loss_dice_1: 0.7671  loss_ce_2: 0.6645  loss_mask_2: 0.08182  loss_dice_2: 0.5035  loss_ce_3: 0.6409  loss_mask_3: 0.07759  loss_dice_3: 0.4881  loss_ce_4: 0.6317  loss_mask_4: 0.06936  loss_dice_4: 0.4721  loss_ce_5: 0.6668  loss_mask_5: 0.07647  loss_dice_5: 0.5038  loss_ce_6: 0.7198  loss_mask_6: 0.05461  loss_dice_6: 0.4925  loss_ce_7: 0.6701  loss_mask_7: 0.07651  loss_dice_7: 0.4823  loss_ce_8: 0.7145  loss_mask_8: 0.05921  loss_dice_8: 0.5396     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:50 d2.utils.events]:  eta: 4:56:16  iter: 17339  total_loss: 16.57  loss_ce: 0.7952  loss_mask: 0.1217  loss_dice: 0.7071  loss_ce_0: 1.168  loss_mask_0: 0.1048  loss_dice_0: 0.5758  loss_ce_1: 1.054  loss_mask_1: 0.1266  loss_dice_1: 0.7349  loss_ce_2: 0.9106  loss_mask_2: 0.1426  loss_dice_2: 0.8891  loss_ce_3: 0.8131  loss_mask_3: 0.135  loss_dice_3: 0.7402  loss_ce_4: 0.8588  loss_mask_4: 0.1358  loss_dice_4: 0.5472  loss_ce_5: 0.7736  loss_mask_5: 0.1329  loss_dice_5: 0.4662  loss_ce_6: 0.8495  loss_mask_6: 0.1085  loss_dice_6: 0.8483  loss_ce_7: 0.8029  loss_mask_7: 0.1176  loss_dice_7: 0.6935  loss_ce_8: 0.903  loss_mask_8: 0.1048  loss_dice_8: 0.6664     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:33:59 d2.utils.events]:  eta: 4:51:07  iter: 17359  total_loss: 20.59  loss_ce: 0.7943  loss_mask: 0.06065  loss_dice: 0.8228  loss_ce_0: 1.471  loss_mask_0: 0.06631  loss_dice_0: 1.024  loss_ce_1: 1.062  loss_mask_1: 0.06981  loss_dice_1: 0.9585  loss_ce_2: 0.915  loss_mask_2: 0.07011  loss_dice_2: 1.13  loss_ce_3: 0.9534  loss_mask_3: 0.06329  loss_dice_3: 0.9325  loss_ce_4: 0.7834  loss_mask_4: 0.06281  loss_dice_4: 0.8544  loss_ce_5: 0.7796  loss_mask_5: 0.0642  loss_dice_5: 0.8828  loss_ce_6: 0.8273  loss_mask_6: 0.06395  loss_dice_6: 0.7439  loss_ce_7: 0.7682  loss_mask_7: 0.05612  loss_dice_7: 0.8235  loss_ce_8: 0.7836  loss_mask_8: 0.06357  loss_dice_8: 0.8363     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:34:07 d2.utils.events]:  eta: 4:53:59  iter: 17379  total_loss: 21.75  loss_ce: 0.8965  loss_mask: 0.06486  loss_dice: 0.9655  loss_ce_0: 1.486  loss_mask_0: 0.1098  loss_dice_0: 1.246  loss_ce_1: 1.089  loss_mask_1: 0.08693  loss_dice_1: 1.189  loss_ce_2: 1.072  loss_mask_2: 0.07849  loss_dice_2: 1.072  loss_ce_3: 1.017  loss_mask_3: 0.05848  loss_dice_3: 1.051  loss_ce_4: 0.9737  loss_mask_4: 0.05442  loss_dice_4: 1.107  loss_ce_5: 0.8516  loss_mask_5: 0.06195  loss_dice_5: 1.12  loss_ce_6: 0.9293  loss_mask_6: 0.05263  loss_dice_6: 1.001  loss_ce_7: 0.8957  loss_mask_7: 0.04803  loss_dice_7: 0.9901  loss_ce_8: 0.9026  loss_mask_8: 0.07523  loss_dice_8: 1.096     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 29.0 - Losses: {'loss_ce': tensor(0.1574, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.0188, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(0.2067, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(0.5371, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.0193, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(0.1783, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(0.3258, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.0194, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(0.1849, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(0.2475, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.0163, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(0.1542, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(0.1897, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.0161, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(0.2155, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(0.1688, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.0190, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(0.1885, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(0.1558, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.0182, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(0.1758, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(0.1471, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.0138, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(0.2506, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(0.1480, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.0217, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(0.1918, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(0.1509, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.0134, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(0.1788, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:34:15 d2.utils.events]:  eta: 5:02:32  iter: 17399  total_loss: 17.39  loss_ce: 0.6349  loss_mask: 0.05544  loss_dice: 0.6784  loss_ce_0: 1.177  loss_mask_0: 0.06586  loss_dice_0: 0.9117  loss_ce_1: 0.9564  loss_mask_1: 0.04967  loss_dice_1: 0.6947  loss_ce_2: 0.7277  loss_mask_2: 0.05188  loss_dice_2: 0.6175  loss_ce_3: 0.6107  loss_mask_3: 0.05325  loss_dice_3: 0.7618  loss_ce_4: 0.6223  loss_mask_4: 0.06406  loss_dice_4: 0.6419  loss_ce_5: 0.56  loss_mask_5: 0.06072  loss_dice_5: 0.6376  loss_ce_6: 0.5718  loss_mask_6: 0.05426  loss_dice_6: 0.7604  loss_ce_7: 0.6077  loss_mask_7: 0.05265  loss_dice_7: 0.6442  loss_ce_8: 0.5475  loss_mask_8: 0.04526  loss_dice_8: 0.6793     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:34:24 d2.utils.events]:  eta: 4:56:18  iter: 17419  total_loss: 12.31  loss_ce: 0.5456  loss_mask: 0.05536  loss_dice: 0.4373  loss_ce_0: 1.257  loss_mask_0: 0.07553  loss_dice_0: 0.799  loss_ce_1: 0.8823  loss_mask_1: 0.06299  loss_dice_1: 0.7163  loss_ce_2: 0.6191  loss_mask_2: 0.05816  loss_dice_2: 0.6499  loss_ce_3: 0.5897  loss_mask_3: 0.04766  loss_dice_3: 0.3274  loss_ce_4: 0.5308  loss_mask_4: 0.04629  loss_dice_4: 0.3416  loss_ce_5: 0.5097  loss_mask_5: 0.04276  loss_dice_5: 0.5627  loss_ce_6: 0.4817  loss_mask_6: 0.04539  loss_dice_6: 0.3347  loss_ce_7: 0.4761  loss_mask_7: 0.05197  loss_dice_7: 0.4373  loss_ce_8: 0.4917  loss_mask_8: 0.04672  loss_dice_8: 0.4032     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:34:32 d2.utils.events]:  eta: 5:01:24  iter: 17439  total_loss: 19.69  loss_ce: 0.8881  loss_mask: 0.06952  loss_dice: 0.9299  loss_ce_0: 1.229  loss_mask_0: 0.06333  loss_dice_0: 0.8154  loss_ce_1: 0.9135  loss_mask_1: 0.05254  loss_dice_1: 0.7119  loss_ce_2: 0.9756  loss_mask_2: 0.04405  loss_dice_2: 0.7966  loss_ce_3: 1.062  loss_mask_3: 0.05658  loss_dice_3: 1.049  loss_ce_4: 0.9537  loss_mask_4: 0.08  loss_dice_4: 1.067  loss_ce_5: 0.9275  loss_mask_5: 0.06126  loss_dice_5: 0.9092  loss_ce_6: 0.9344  loss_mask_6: 0.06731  loss_dice_6: 0.7835  loss_ce_7: 0.8674  loss_mask_7: 0.06641  loss_dice_7: 0.7763  loss_ce_8: 0.9099  loss_mask_8: 0.06568  loss_dice_8: 0.9084     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:34:41 d2.utils.events]:  eta: 4:58:01  iter: 17459  total_loss: 18.19  loss_ce: 0.7147  loss_mask: 0.1582  loss_dice: 0.9526  loss_ce_0: 1.543  loss_mask_0: 0.1622  loss_dice_0: 0.914  loss_ce_1: 1.136  loss_mask_1: 0.1374  loss_dice_1: 1.053  loss_ce_2: 1.066  loss_mask_2: 0.2122  loss_dice_2: 1.106  loss_ce_3: 0.8566  loss_mask_3: 0.1884  loss_dice_3: 0.9135  loss_ce_4: 0.6791  loss_mask_4: 0.1755  loss_dice_4: 1.035  loss_ce_5: 0.744  loss_mask_5: 0.2031  loss_dice_5: 1.005  loss_ce_6: 0.6929  loss_mask_6: 0.163  loss_dice_6: 1.021  loss_ce_7: 0.7671  loss_mask_7: 0.1598  loss_dice_7: 0.8921  loss_ce_8: 0.7751  loss_mask_8: 0.1554  loss_dice_8: 0.7438     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:34:49 d2.utils.events]:  eta: 4:55:33  iter: 17479  total_loss: 22.25  loss_ce: 0.8186  loss_mask: 0.1295  loss_dice: 0.8408  loss_ce_0: 1.699  loss_mask_0: 0.1769  loss_dice_0: 1.12  loss_ce_1: 0.9284  loss_mask_1: 0.1299  loss_dice_1: 1.346  loss_ce_2: 1.128  loss_mask_2: 0.1308  loss_dice_2: 1.187  loss_ce_3: 0.9153  loss_mask_3: 0.1264  loss_dice_3: 0.9946  loss_ce_4: 0.8393  loss_mask_4: 0.1293  loss_dice_4: 0.8732  loss_ce_5: 0.8286  loss_mask_5: 0.1359  loss_dice_5: 0.8169  loss_ce_6: 0.8781  loss_mask_6: 0.1246  loss_dice_6: 0.9486  loss_ce_7: 0.8862  loss_mask_7: 0.134  loss_dice_7: 0.9591  loss_ce_8: 0.914  loss_mask_8: 0.1344  loss_dice_8: 0.8965     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:34:57 d2.utils.events]:  eta: 4:56:08  iter: 17499  total_loss: 14.37  loss_ce: 0.5352  loss_mask: 0.146  loss_dice: 0.718  loss_ce_0: 1.375  loss_mask_0: 0.1556  loss_dice_0: 0.5719  loss_ce_1: 0.8582  loss_mask_1: 0.1738  loss_dice_1: 0.6087  loss_ce_2: 0.7002  loss_mask_2: 0.1442  loss_dice_2: 0.5453  loss_ce_3: 0.6314  loss_mask_3: 0.1432  loss_dice_3: 0.5236  loss_ce_4: 0.6195  loss_mask_4: 0.1533  loss_dice_4: 0.6101  loss_ce_5: 0.5232  loss_mask_5: 0.1553  loss_dice_5: 0.6883  loss_ce_6: 0.5392  loss_mask_6: 0.1523  loss_dice_6: 0.6941  loss_ce_7: 0.5179  loss_mask_7: 0.1807  loss_dice_7: 0.6042  loss_ce_8: 0.5543  loss_mask_8: 0.1752  loss_dice_8: 0.7222     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:06 d2.utils.events]:  eta: 4:51:53  iter: 17519  total_loss: 21.22  loss_ce: 0.9477  loss_mask: 0.0573  loss_dice: 0.865  loss_ce_0: 1.636  loss_mask_0: 0.07314  loss_dice_0: 0.9966  loss_ce_1: 1.402  loss_mask_1: 0.07696  loss_dice_1: 0.8135  loss_ce_2: 1.167  loss_mask_2: 0.06975  loss_dice_2: 0.8824  loss_ce_3: 1.109  loss_mask_3: 0.07459  loss_dice_3: 0.7807  loss_ce_4: 0.9628  loss_mask_4: 0.07744  loss_dice_4: 0.8113  loss_ce_5: 0.9153  loss_mask_5: 0.07107  loss_dice_5: 0.7851  loss_ce_6: 0.9102  loss_mask_6: 0.06982  loss_dice_6: 0.7838  loss_ce_7: 0.8813  loss_mask_7: 0.07306  loss_dice_7: 0.6805  loss_ce_8: 0.9297  loss_mask_8: 0.06128  loss_dice_8: 0.7597     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:14 d2.utils.events]:  eta: 4:52:32  iter: 17539  total_loss: 22.88  loss_ce: 0.8236  loss_mask: 0.07807  loss_dice: 0.8805  loss_ce_0: 1.497  loss_mask_0: 0.06873  loss_dice_0: 1.256  loss_ce_1: 1.041  loss_mask_1: 0.1054  loss_dice_1: 1.162  loss_ce_2: 0.9465  loss_mask_2: 0.0901  loss_dice_2: 0.9485  loss_ce_3: 0.8831  loss_mask_3: 0.08175  loss_dice_3: 0.7761  loss_ce_4: 0.9485  loss_mask_4: 0.08052  loss_dice_4: 0.7001  loss_ce_5: 0.8901  loss_mask_5: 0.07021  loss_dice_5: 0.9871  loss_ce_6: 0.7321  loss_mask_6: 0.06957  loss_dice_6: 0.9799  loss_ce_7: 0.7148  loss_mask_7: 0.07658  loss_dice_7: 0.8374  loss_ce_8: 0.7435  loss_mask_8: 0.06552  loss_dice_8: 0.8481     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:22 d2.utils.events]:  eta: 4:53:28  iter: 17559  total_loss: 18.26  loss_ce: 0.748  loss_mask: 0.072  loss_dice: 0.8753  loss_ce_0: 1.345  loss_mask_0: 0.09743  loss_dice_0: 1.007  loss_ce_1: 1.22  loss_mask_1: 0.073  loss_dice_1: 0.9684  loss_ce_2: 1.029  loss_mask_2: 0.06906  loss_dice_2: 0.8396  loss_ce_3: 0.9153  loss_mask_3: 0.05998  loss_dice_3: 0.6243  loss_ce_4: 0.9223  loss_mask_4: 0.06266  loss_dice_4: 0.6767  loss_ce_5: 0.8097  loss_mask_5: 0.06262  loss_dice_5: 0.624  loss_ce_6: 0.8498  loss_mask_6: 0.05698  loss_dice_6: 0.6474  loss_ce_7: 0.8177  loss_mask_7: 0.04984  loss_dice_7: 0.5855  loss_ce_8: 0.8109  loss_mask_8: 0.04827  loss_dice_8: 0.7905     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:31 d2.utils.events]:  eta: 4:56:57  iter: 17579  total_loss: 21.99  loss_ce: 0.7979  loss_mask: 0.09242  loss_dice: 0.9212  loss_ce_0: 1.591  loss_mask_0: 0.09537  loss_dice_0: 1.053  loss_ce_1: 1.147  loss_mask_1: 0.09978  loss_dice_1: 1.039  loss_ce_2: 1.003  loss_mask_2: 0.0981  loss_dice_2: 1.099  loss_ce_3: 1.021  loss_mask_3: 0.09004  loss_dice_3: 1.054  loss_ce_4: 0.845  loss_mask_4: 0.09107  loss_dice_4: 0.9965  loss_ce_5: 0.8924  loss_mask_5: 0.09449  loss_dice_5: 1.011  loss_ce_6: 0.8677  loss_mask_6: 0.09821  loss_dice_6: 0.9201  loss_ce_7: 0.8385  loss_mask_7: 0.09009  loss_dice_7: 0.8975  loss_ce_8: 0.838  loss_mask_8: 0.09316  loss_dice_8: 0.8517     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:39 d2.utils.events]:  eta: 4:50:27  iter: 17599  total_loss: 16.84  loss_ce: 0.7226  loss_mask: 0.03357  loss_dice: 0.5999  loss_ce_0: 1.201  loss_mask_0: 0.04625  loss_dice_0: 0.864  loss_ce_1: 1.059  loss_mask_1: 0.05418  loss_dice_1: 0.7385  loss_ce_2: 0.7729  loss_mask_2: 0.04981  loss_dice_2: 0.6965  loss_ce_3: 0.6392  loss_mask_3: 0.04499  loss_dice_3: 0.6532  loss_ce_4: 0.7323  loss_mask_4: 0.05048  loss_dice_4: 0.6223  loss_ce_5: 0.6219  loss_mask_5: 0.03483  loss_dice_5: 0.6289  loss_ce_6: 0.6622  loss_mask_6: 0.03625  loss_dice_6: 0.6255  loss_ce_7: 0.5873  loss_mask_7: 0.03372  loss_dice_7: 0.6749  loss_ce_8: 0.7412  loss_mask_8: 0.03451  loss_dice_8: 0.6067     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:47 d2.utils.events]:  eta: 4:52:00  iter: 17619  total_loss: 14.56  loss_ce: 0.5099  loss_mask: 0.04698  loss_dice: 0.4339  loss_ce_0: 1.281  loss_mask_0: 0.05291  loss_dice_0: 0.5615  loss_ce_1: 0.8703  loss_mask_1: 0.0734  loss_dice_1: 0.6105  loss_ce_2: 0.6871  loss_mask_2: 0.05594  loss_dice_2: 0.6167  loss_ce_3: 0.6182  loss_mask_3: 0.04274  loss_dice_3: 0.505  loss_ce_4: 0.5647  loss_mask_4: 0.04683  loss_dice_4: 0.4493  loss_ce_5: 0.6148  loss_mask_5: 0.04869  loss_dice_5: 0.4463  loss_ce_6: 0.6649  loss_mask_6: 0.04397  loss_dice_6: 0.4514  loss_ce_7: 0.4946  loss_mask_7: 0.0435  loss_dice_7: 0.4578  loss_ce_8: 0.5815  loss_mask_8: 0.04304  loss_dice_8: 0.3789     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:35:55 d2.utils.events]:  eta: 4:52:53  iter: 17639  total_loss: 17.81  loss_ce: 0.6887  loss_mask: 0.07063  loss_dice: 0.7377  loss_ce_0: 1.464  loss_mask_0: 0.07234  loss_dice_0: 0.8094  loss_ce_1: 1.059  loss_mask_1: 0.06452  loss_dice_1: 0.7793  loss_ce_2: 0.9172  loss_mask_2: 0.07247  loss_dice_2: 0.994  loss_ce_3: 0.8076  loss_mask_3: 0.0625  loss_dice_3: 0.7583  loss_ce_4: 0.9259  loss_mask_4: 0.05473  loss_dice_4: 0.7487  loss_ce_5: 0.7415  loss_mask_5: 0.05113  loss_dice_5: 0.8948  loss_ce_6: 0.7844  loss_mask_6: 0.07021  loss_dice_6: 0.6878  loss_ce_7: 0.7532  loss_mask_7: 0.05972  loss_dice_7: 0.7652  loss_ce_8: 0.6727  loss_mask_8: 0.06455  loss_dice_8: 0.8591     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:04 d2.utils.events]:  eta: 4:58:26  iter: 17659  total_loss: 22.4  loss_ce: 0.81  loss_mask: 0.1755  loss_dice: 0.8064  loss_ce_0: 1.522  loss_mask_0: 0.2387  loss_dice_0: 0.8285  loss_ce_1: 1.183  loss_mask_1: 0.205  loss_dice_1: 1.041  loss_ce_2: 1.071  loss_mask_2: 0.1915  loss_dice_2: 0.9786  loss_ce_3: 0.9041  loss_mask_3: 0.1959  loss_dice_3: 0.753  loss_ce_4: 0.8384  loss_mask_4: 0.177  loss_dice_4: 0.8896  loss_ce_5: 0.8113  loss_mask_5: 0.1542  loss_dice_5: 0.7856  loss_ce_6: 0.7851  loss_mask_6: 0.1702  loss_dice_6: 0.7789  loss_ce_7: 0.7935  loss_mask_7: 0.2079  loss_dice_7: 0.8312  loss_ce_8: 0.785  loss_mask_8: 0.1671  loss_dice_8: 0.6761     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:12 d2.utils.events]:  eta: 4:56:09  iter: 17679  total_loss: 19.79  loss_ce: 0.7187  loss_mask: 0.06545  loss_dice: 0.749  loss_ce_0: 1.432  loss_mask_0: 0.08719  loss_dice_0: 0.937  loss_ce_1: 1.043  loss_mask_1: 0.06944  loss_dice_1: 1.083  loss_ce_2: 0.9312  loss_mask_2: 0.08836  loss_dice_2: 0.8965  loss_ce_3: 0.7803  loss_mask_3: 0.07076  loss_dice_3: 0.7235  loss_ce_4: 0.8488  loss_mask_4: 0.08323  loss_dice_4: 0.6714  loss_ce_5: 0.748  loss_mask_5: 0.07082  loss_dice_5: 0.9089  loss_ce_6: 0.7261  loss_mask_6: 0.07056  loss_dice_6: 0.6777  loss_ce_7: 0.7173  loss_mask_7: 0.06231  loss_dice_7: 0.6932  loss_ce_8: 0.8208  loss_mask_8: 0.06833  loss_dice_8: 0.685     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:21 d2.utils.events]:  eta: 4:57:04  iter: 17699  total_loss: 16.02  loss_ce: 0.7357  loss_mask: 0.09336  loss_dice: 0.6859  loss_ce_0: 1.254  loss_mask_0: 0.08876  loss_dice_0: 0.735  loss_ce_1: 1.039  loss_mask_1: 0.09909  loss_dice_1: 0.7921  loss_ce_2: 0.8595  loss_mask_2: 0.1063  loss_dice_2: 0.6985  loss_ce_3: 0.7754  loss_mask_3: 0.09756  loss_dice_3: 0.8558  loss_ce_4: 0.7648  loss_mask_4: 0.1164  loss_dice_4: 0.6055  loss_ce_5: 0.7812  loss_mask_5: 0.1077  loss_dice_5: 0.6918  loss_ce_6: 0.7916  loss_mask_6: 0.0949  loss_dice_6: 0.6453  loss_ce_7: 0.6838  loss_mask_7: 0.09166  loss_dice_7: 0.6423  loss_ce_8: 0.7006  loss_mask_8: 0.09518  loss_dice_8: 0.674     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:29 d2.utils.events]:  eta: 4:55:37  iter: 17719  total_loss: 22.71  loss_ce: 0.8246  loss_mask: 0.04787  loss_dice: 0.9351  loss_ce_0: 1.636  loss_mask_0: 0.06993  loss_dice_0: 1.163  loss_ce_1: 1.214  loss_mask_1: 0.05566  loss_dice_1: 0.9119  loss_ce_2: 1.15  loss_mask_2: 0.05343  loss_dice_2: 0.9304  loss_ce_3: 0.9222  loss_mask_3: 0.0456  loss_dice_3: 0.8728  loss_ce_4: 0.9006  loss_mask_4: 0.0461  loss_dice_4: 0.8879  loss_ce_5: 0.9027  loss_mask_5: 0.04813  loss_dice_5: 0.8737  loss_ce_6: 0.8531  loss_mask_6: 0.04959  loss_dice_6: 0.9983  loss_ce_7: 0.8534  loss_mask_7: 0.04985  loss_dice_7: 1.12  loss_ce_8: 0.832  loss_mask_8: 0.0446  loss_dice_8: 1.012     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:37 d2.utils.events]:  eta: 4:55:44  iter: 17739  total_loss: 19.95  loss_ce: 0.7602  loss_mask: 0.1472  loss_dice: 0.7162  loss_ce_0: 1.513  loss_mask_0: 0.1839  loss_dice_0: 0.9292  loss_ce_1: 1.095  loss_mask_1: 0.1781  loss_dice_1: 0.8294  loss_ce_2: 0.9808  loss_mask_2: 0.1768  loss_dice_2: 0.8857  loss_ce_3: 0.8253  loss_mask_3: 0.1833  loss_dice_3: 0.8325  loss_ce_4: 0.8008  loss_mask_4: 0.157  loss_dice_4: 0.7529  loss_ce_5: 0.8425  loss_mask_5: 0.1989  loss_dice_5: 0.7104  loss_ce_6: 0.7697  loss_mask_6: 0.1894  loss_dice_6: 0.7784  loss_ce_7: 0.7195  loss_mask_7: 0.2179  loss_dice_7: 0.7394  loss_ce_8: 0.7465  loss_mask_8: 0.2066  loss_dice_8: 0.8528     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:46 d2.utils.events]:  eta: 4:57:11  iter: 17759  total_loss: 19.09  loss_ce: 0.878  loss_mask: 0.07089  loss_dice: 0.5182  loss_ce_0: 1.438  loss_mask_0: 0.08141  loss_dice_0: 0.6966  loss_ce_1: 1.113  loss_mask_1: 0.09858  loss_dice_1: 0.93  loss_ce_2: 1.049  loss_mask_2: 0.06893  loss_dice_2: 0.8996  loss_ce_3: 0.9993  loss_mask_3: 0.06999  loss_dice_3: 0.5873  loss_ce_4: 0.9735  loss_mask_4: 0.08269  loss_dice_4: 0.5408  loss_ce_5: 1.04  loss_mask_5: 0.08059  loss_dice_5: 0.5639  loss_ce_6: 0.9254  loss_mask_6: 0.07194  loss_dice_6: 0.5383  loss_ce_7: 0.9746  loss_mask_7: 0.06803  loss_dice_7: 0.4911  loss_ce_8: 0.9466  loss_mask_8: 0.07621  loss_dice_8: 0.5489     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:36:54 d2.utils.events]:  eta: 4:48:14  iter: 17779  total_loss: 22.17  loss_ce: 0.7335  loss_mask: 0.05995  loss_dice: 1.088  loss_ce_0: 1.22  loss_mask_0: 0.0529  loss_dice_0: 1.336  loss_ce_1: 1.018  loss_mask_1: 0.06766  loss_dice_1: 1.22  loss_ce_2: 0.846  loss_mask_2: 0.05668  loss_dice_2: 0.9414  loss_ce_3: 0.7984  loss_mask_3: 0.05667  loss_dice_3: 1.072  loss_ce_4: 0.8553  loss_mask_4: 0.06002  loss_dice_4: 0.9569  loss_ce_5: 0.8184  loss_mask_5: 0.05976  loss_dice_5: 0.7816  loss_ce_6: 0.848  loss_mask_6: 0.05271  loss_dice_6: 0.9225  loss_ce_7: 0.7725  loss_mask_7: 0.05829  loss_dice_7: 0.9995  loss_ce_8: 0.7598  loss_mask_8: 0.05501  loss_dice_8: 0.9889     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:02 d2.utils.events]:  eta: 4:56:26  iter: 17799  total_loss: 27.81  loss_ce: 0.6614  loss_mask: 0.1776  loss_dice: 1.185  loss_ce_0: 1.285  loss_mask_0: 0.1164  loss_dice_0: 1.353  loss_ce_1: 1.028  loss_mask_1: 0.2825  loss_dice_1: 1.362  loss_ce_2: 0.9784  loss_mask_2: 0.2696  loss_dice_2: 1.371  loss_ce_3: 0.8689  loss_mask_3: 0.221  loss_dice_3: 1.301  loss_ce_4: 0.8517  loss_mask_4: 0.2238  loss_dice_4: 1.283  loss_ce_5: 0.6595  loss_mask_5: 0.204  loss_dice_5: 1.205  loss_ce_6: 0.7797  loss_mask_6: 0.1849  loss_dice_6: 1.159  loss_ce_7: 0.7251  loss_mask_7: 0.1827  loss_dice_7: 1.262  loss_ce_8: 0.8302  loss_mask_8: 0.1568  loss_dice_8: 1.141     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:11 d2.utils.events]:  eta: 4:50:01  iter: 17819  total_loss: 21.87  loss_ce: 0.8732  loss_mask: 0.06693  loss_dice: 1.009  loss_ce_0: 1.375  loss_mask_0: 0.08773  loss_dice_0: 1.32  loss_ce_1: 1.12  loss_mask_1: 0.0845  loss_dice_1: 1.197  loss_ce_2: 1.041  loss_mask_2: 0.08049  loss_dice_2: 1.134  loss_ce_3: 1.045  loss_mask_3: 0.06914  loss_dice_3: 1.058  loss_ce_4: 0.973  loss_mask_4: 0.07745  loss_dice_4: 1.033  loss_ce_5: 0.8999  loss_mask_5: 0.06573  loss_dice_5: 0.8924  loss_ce_6: 0.8926  loss_mask_6: 0.05654  loss_dice_6: 0.9914  loss_ce_7: 0.8951  loss_mask_7: 0.05825  loss_dice_7: 1.015  loss_ce_8: 0.9523  loss_mask_8: 0.05411  loss_dice_8: 0.8435     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:19 d2.utils.events]:  eta: 4:51:38  iter: 17839  total_loss: 22.46  loss_ce: 0.8495  loss_mask: 0.1211  loss_dice: 1.138  loss_ce_0: 1.305  loss_mask_0: 0.09569  loss_dice_0: 1.277  loss_ce_1: 1.137  loss_mask_1: 0.1125  loss_dice_1: 1.406  loss_ce_2: 0.963  loss_mask_2: 0.1266  loss_dice_2: 1.261  loss_ce_3: 0.8717  loss_mask_3: 0.1028  loss_dice_3: 1.15  loss_ce_4: 0.8379  loss_mask_4: 0.1048  loss_dice_4: 0.9783  loss_ce_5: 0.828  loss_mask_5: 0.1114  loss_dice_5: 1.214  loss_ce_6: 0.8537  loss_mask_6: 0.1129  loss_dice_6: 1.118  loss_ce_7: 0.8436  loss_mask_7: 0.1137  loss_dice_7: 1.266  loss_ce_8: 0.8475  loss_mask_8: 0.1291  loss_dice_8: 1.267     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:27 d2.utils.events]:  eta: 4:54:28  iter: 17859  total_loss: 21.44  loss_ce: 0.925  loss_mask: 0.1422  loss_dice: 0.8484  loss_ce_0: 1.354  loss_mask_0: 0.1189  loss_dice_0: 0.8609  loss_ce_1: 1.017  loss_mask_1: 0.1404  loss_dice_1: 0.9288  loss_ce_2: 1.05  loss_mask_2: 0.1335  loss_dice_2: 0.9102  loss_ce_3: 0.9859  loss_mask_3: 0.1613  loss_dice_3: 0.8556  loss_ce_4: 1.05  loss_mask_4: 0.1634  loss_dice_4: 0.8343  loss_ce_5: 1.007  loss_mask_5: 0.1443  loss_dice_5: 0.8506  loss_ce_6: 0.9568  loss_mask_6: 0.1649  loss_dice_6: 0.8151  loss_ce_7: 0.9343  loss_mask_7: 0.1565  loss_dice_7: 0.8674  loss_ce_8: 1.012  loss_mask_8: 0.1503  loss_dice_8: 0.787     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:36 d2.utils.events]:  eta: 4:57:35  iter: 17879  total_loss: 19.51  loss_ce: 0.6307  loss_mask: 0.1194  loss_dice: 1.01  loss_ce_0: 1.377  loss_mask_0: 0.0814  loss_dice_0: 1.005  loss_ce_1: 1.041  loss_mask_1: 0.112  loss_dice_1: 1.032  loss_ce_2: 0.8313  loss_mask_2: 0.1155  loss_dice_2: 0.9156  loss_ce_3: 0.7311  loss_mask_3: 0.08439  loss_dice_3: 0.7032  loss_ce_4: 0.6917  loss_mask_4: 0.09466  loss_dice_4: 0.9479  loss_ce_5: 0.6574  loss_mask_5: 0.09876  loss_dice_5: 0.8727  loss_ce_6: 0.7374  loss_mask_6: 0.1253  loss_dice_6: 0.9541  loss_ce_7: 0.6766  loss_mask_7: 0.1064  loss_dice_7: 0.8612  loss_ce_8: 0.6399  loss_mask_8: 0.1146  loss_dice_8: 0.7295     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:44 d2.utils.events]:  eta: 4:41:35  iter: 17899  total_loss: 20.58  loss_ce: 0.7686  loss_mask: 0.08462  loss_dice: 1.066  loss_ce_0: 1.343  loss_mask_0: 0.0905  loss_dice_0: 1.313  loss_ce_1: 0.9857  loss_mask_1: 0.085  loss_dice_1: 1.169  loss_ce_2: 0.9819  loss_mask_2: 0.08737  loss_dice_2: 1.026  loss_ce_3: 0.8116  loss_mask_3: 0.09142  loss_dice_3: 1.159  loss_ce_4: 0.8001  loss_mask_4: 0.09587  loss_dice_4: 0.9735  loss_ce_5: 0.7855  loss_mask_5: 0.07984  loss_dice_5: 1.182  loss_ce_6: 0.8236  loss_mask_6: 0.09046  loss_dice_6: 0.6897  loss_ce_7: 0.7843  loss_mask_7: 0.1111  loss_dice_7: 1.25  loss_ce_8: 0.8095  loss_mask_8: 0.1002  loss_dice_8: 1.258     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:37:52 d2.utils.events]:  eta: 5:00:51  iter: 17919  total_loss: 22.94  loss_ce: 0.7874  loss_mask: 0.1075  loss_dice: 1.169  loss_ce_0: 1.352  loss_mask_0: 0.1099  loss_dice_0: 1.302  loss_ce_1: 1.098  loss_mask_1: 0.1507  loss_dice_1: 1.011  loss_ce_2: 0.9486  loss_mask_2: 0.134  loss_dice_2: 0.9641  loss_ce_3: 0.8472  loss_mask_3: 0.1193  loss_dice_3: 1.207  loss_ce_4: 0.7779  loss_mask_4: 0.1157  loss_dice_4: 1.285  loss_ce_5: 0.8474  loss_mask_5: 0.1127  loss_dice_5: 1.166  loss_ce_6: 0.7843  loss_mask_6: 0.1097  loss_dice_6: 1.202  loss_ce_7: 0.7163  loss_mask_7: 0.115  loss_dice_7: 1.091  loss_ce_8: 0.7029  loss_mask_8: 0.1144  loss_dice_8: 0.9498     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:38:01 d2.utils.events]:  eta: 4:57:08  iter: 17939  total_loss: 20.92  loss_ce: 0.7144  loss_mask: 0.1053  loss_dice: 0.5624  loss_ce_0: 1.178  loss_mask_0: 0.1123  loss_dice_0: 1.51  loss_ce_1: 0.8825  loss_mask_1: 0.1342  loss_dice_1: 1.183  loss_ce_2: 0.847  loss_mask_2: 0.1366  loss_dice_2: 1.165  loss_ce_3: 0.7434  loss_mask_3: 0.1188  loss_dice_3: 1.177  loss_ce_4: 0.7369  loss_mask_4: 0.1066  loss_dice_4: 1.189  loss_ce_5: 0.7563  loss_mask_5: 0.1157  loss_dice_5: 1.096  loss_ce_6: 0.7535  loss_mask_6: 0.09929  loss_dice_6: 0.6034  loss_ce_7: 0.7116  loss_mask_7: 0.1096  loss_dice_7: 1.171  loss_ce_8: 0.7169  loss_mask_8: 0.1155  loss_dice_8: 1.232     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:38:09 d2.utils.events]:  eta: 4:52:27  iter: 17959  total_loss: 26.61  loss_ce: 0.9873  loss_mask: 0.1334  loss_dice: 1.078  loss_ce_0: 1.838  loss_mask_0: 0.1307  loss_dice_0: 1.207  loss_ce_1: 1.301  loss_mask_1: 0.1581  loss_dice_1: 1.112  loss_ce_2: 1.125  loss_mask_2: 0.1567  loss_dice_2: 1.186  loss_ce_3: 1.019  loss_mask_3: 0.1039  loss_dice_3: 0.9776  loss_ce_4: 1.02  loss_mask_4: 0.1084  loss_dice_4: 0.9543  loss_ce_5: 0.9883  loss_mask_5: 0.1291  loss_dice_5: 0.9465  loss_ce_6: 1.014  loss_mask_6: 0.1223  loss_dice_6: 1.078  loss_ce_7: 0.9489  loss_mask_7: 0.1286  loss_dice_7: 1.083  loss_ce_8: 0.984  loss_mask_8: 0.1498  loss_dice_8: 1.026     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:38:18 d2.utils.events]:  eta: 4:53:19  iter: 17979  total_loss: 20.93  loss_ce: 0.7183  loss_mask: 0.09501  loss_dice: 1.062  loss_ce_0: 1.18  loss_mask_0: 0.08162  loss_dice_0: 1.582  loss_ce_1: 1.209  loss_mask_1: 0.08408  loss_dice_1: 1.245  loss_ce_2: 0.977  loss_mask_2: 0.08209  loss_dice_2: 1.161  loss_ce_3: 0.8488  loss_mask_3: 0.08981  loss_dice_3: 0.9901  loss_ce_4: 0.8671  loss_mask_4: 0.08953  loss_dice_4: 1.176  loss_ce_5: 0.8346  loss_mask_5: 0.1004  loss_dice_5: 1.057  loss_ce_6: 0.7051  loss_mask_6: 0.09539  loss_dice_6: 1.193  loss_ce_7: 0.8244  loss_mask_7: 0.08186  loss_dice_7: 1.202  loss_ce_8: 0.7129  loss_mask_8: 0.08438  loss_dice_8: 0.9204     lr: 0.0001  max_mem: 37654M\n",
            "Epoch 30.0 - Losses: {'loss_ce': tensor(2.5141, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask': tensor(0.1626, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice': tensor(2.9300, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_0': tensor(3.9733, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_0': tensor(0.2202, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_0': tensor(2.8420, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_1': tensor(2.9656, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_1': tensor(0.1836, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_1': tensor(3.2058, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_2': tensor(2.9202, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_2': tensor(0.1776, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_2': tensor(2.9668, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_3': tensor(2.3696, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_3': tensor(0.1989, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_3': tensor(2.9402, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_4': tensor(2.4129, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_4': tensor(0.2012, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_4': tensor(2.9702, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_5': tensor(2.5573, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_5': tensor(0.1878, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_5': tensor(2.7957, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_6': tensor(2.3800, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_6': tensor(0.2282, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_6': tensor(3.0317, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_7': tensor(2.5567, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_7': tensor(0.1822, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_7': tensor(3.1683, device='cuda:0', grad_fn=<MulBackward0>), 'loss_ce_8': tensor(2.4322, device='cuda:0', grad_fn=<MulBackward0>), 'loss_mask_8': tensor(0.2389, device='cuda:0', grad_fn=<MulBackward0>), 'loss_dice_8': tensor(3.0460, device='cuda:0', grad_fn=<MulBackward0>)}\n",
            "[09/02 21:38:26 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[09/02 21:38:26 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[09/02 21:38:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[09/02 21:38:26 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[09/02 21:38:26 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[09/02 21:38:26 d2.evaluation.evaluator]: Start inference on 150 batches\n",
            "[09/02 21:39:10 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0014 s/iter. Inference: 0.1065 s/iter. Eval: 3.8202 s/iter. Total: 3.9281 s/iter. ETA=0:09:06\n",
            "[09/02 21:39:18 d2.evaluation.evaluator]: Inference done 13/150. Dataloading: 0.0015 s/iter. Inference: 0.1054 s/iter. Eval: 3.7744 s/iter. Total: 3.8817 s/iter. ETA=0:08:51\n",
            "[09/02 21:39:25 d2.evaluation.evaluator]: Inference done 15/150. Dataloading: 0.0016 s/iter. Inference: 0.1047 s/iter. Eval: 3.7481 s/iter. Total: 3.8550 s/iter. ETA=0:08:40\n",
            "[09/02 21:39:33 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0017 s/iter. Inference: 0.1043 s/iter. Eval: 3.7329 s/iter. Total: 3.8393 s/iter. ETA=0:08:30\n",
            "[09/02 21:39:40 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0017 s/iter. Inference: 0.1042 s/iter. Eval: 3.7259 s/iter. Total: 3.8324 s/iter. ETA=0:08:22\n",
            "[09/02 21:39:48 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0018 s/iter. Inference: 0.1039 s/iter. Eval: 3.7184 s/iter. Total: 3.8247 s/iter. ETA=0:08:13\n",
            "[09/02 21:39:52 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:40:13 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0018 s/iter. Inference: 0.9946 s/iter. Eval: 3.8050 s/iter. Total: 4.8020 s/iter. ETA=0:10:09\n",
            "[09/02 21:40:14 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:40:35 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0019 s/iter. Inference: 1.8286 s/iter. Eval: 3.8827 s/iter. Total: 5.7139 s/iter. ETA=0:11:59\n",
            "[09/02 21:40:41 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.0019 s/iter. Inference: 1.7702 s/iter. Eval: 3.9349 s/iter. Total: 5.7076 s/iter. ETA=0:11:53\n",
            "[09/02 21:40:42 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:40:55 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0019 s/iter. Inference: 2.2060 s/iter. Eval: 3.9080 s/iter. Total: 6.1166 s/iter. ETA=0:12:38\n",
            "[09/02 21:41:04 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0019 s/iter. Inference: 2.1268 s/iter. Eval: 4.1032 s/iter. Total: 6.2327 s/iter. ETA=0:12:46\n",
            "[09/02 21:41:11 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0019 s/iter. Inference: 2.0681 s/iter. Eval: 4.2086 s/iter. Total: 6.2794 s/iter. ETA=0:12:46\n",
            "[09/02 21:41:18 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0019 s/iter. Inference: 1.9874 s/iter. Eval: 4.3067 s/iter. Total: 6.2968 s/iter. ETA=0:12:41\n",
            "[09/02 21:41:26 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0020 s/iter. Inference: 1.9341 s/iter. Eval: 4.4541 s/iter. Total: 6.3909 s/iter. ETA=0:12:46\n",
            "[09/02 21:41:33 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0020 s/iter. Inference: 1.8646 s/iter. Eval: 4.5195 s/iter. Total: 6.3869 s/iter. ETA=0:12:40\n",
            "[09/02 21:41:39 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0020 s/iter. Inference: 1.8003 s/iter. Eval: 4.5813 s/iter. Total: 6.3844 s/iter. ETA=0:12:33\n",
            "[09/02 21:41:45 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0020 s/iter. Inference: 1.7406 s/iter. Eval: 4.6378 s/iter. Total: 6.3813 s/iter. ETA=0:12:26\n",
            "[09/02 21:41:52 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0020 s/iter. Inference: 1.6850 s/iter. Eval: 4.6887 s/iter. Total: 6.3766 s/iter. ETA=0:12:19\n",
            "[09/02 21:41:58 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0020 s/iter. Inference: 1.6332 s/iter. Eval: 4.7507 s/iter. Total: 6.3867 s/iter. ETA=0:12:14\n",
            "[09/02 21:42:07 d2.evaluation.evaluator]: Inference done 37/150. Dataloading: 0.0020 s/iter. Inference: 1.5378 s/iter. Eval: 4.7037 s/iter. Total: 6.2444 s/iter. ETA=0:11:45\n",
            "[09/02 21:42:07 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:42:20 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0020 s/iter. Inference: 1.8123 s/iter. Eval: 4.6627 s/iter. Total: 6.4780 s/iter. ETA=0:12:05\n",
            "[09/02 21:42:29 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0020 s/iter. Inference: 1.7727 s/iter. Eval: 4.7693 s/iter. Total: 6.5450 s/iter. ETA=0:12:06\n",
            "[09/02 21:42:34 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0020 s/iter. Inference: 1.7256 s/iter. Eval: 4.7739 s/iter. Total: 6.5025 s/iter. ETA=0:11:55\n",
            "[09/02 21:42:35 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:42:48 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0021 s/iter. Inference: 1.9749 s/iter. Eval: 4.7357 s/iter. Total: 6.7136 s/iter. ETA=0:12:11\n",
            "[09/02 21:42:54 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0021 s/iter. Inference: 1.9341 s/iter. Eval: 4.7431 s/iter. Total: 6.6802 s/iter. ETA=0:12:01\n",
            "[09/02 21:43:01 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0021 s/iter. Inference: 1.8866 s/iter. Eval: 4.7914 s/iter. Total: 6.6811 s/iter. ETA=0:11:54\n",
            "[09/02 21:43:07 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0021 s/iter. Inference: 1.8416 s/iter. Eval: 4.8368 s/iter. Total: 6.6815 s/iter. ETA=0:11:48\n",
            "[09/02 21:43:14 d2.evaluation.evaluator]: Inference done 45/150. Dataloading: 0.0021 s/iter. Inference: 1.7988 s/iter. Eval: 4.8801 s/iter. Total: 6.6820 s/iter. ETA=0:11:41\n",
            "[09/02 21:43:21 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0021 s/iter. Inference: 1.7581 s/iter. Eval: 4.9211 s/iter. Total: 6.6823 s/iter. ETA=0:11:34\n",
            "[09/02 21:43:27 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.0021 s/iter. Inference: 1.7194 s/iter. Eval: 4.9600 s/iter. Total: 6.6826 s/iter. ETA=0:11:28\n",
            "[09/02 21:43:34 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0021 s/iter. Inference: 1.6825 s/iter. Eval: 4.9962 s/iter. Total: 6.6818 s/iter. ETA=0:11:21\n",
            "[09/02 21:43:41 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0021 s/iter. Inference: 1.6472 s/iter. Eval: 5.0344 s/iter. Total: 6.6847 s/iter. ETA=0:11:15\n",
            "[09/02 21:43:48 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0021 s/iter. Inference: 1.6136 s/iter. Eval: 5.0675 s/iter. Total: 6.6843 s/iter. ETA=0:11:08\n",
            "[09/02 21:43:54 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0021 s/iter. Inference: 1.5814 s/iter. Eval: 5.1004 s/iter. Total: 6.6849 s/iter. ETA=0:11:01\n",
            "[09/02 21:44:01 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0021 s/iter. Inference: 1.5505 s/iter. Eval: 5.1311 s/iter. Total: 6.6849 s/iter. ETA=0:10:55\n",
            "[09/02 21:44:08 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0021 s/iter. Inference: 1.5212 s/iter. Eval: 5.1606 s/iter. Total: 6.6849 s/iter. ETA=0:10:48\n",
            "[09/02 21:44:14 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0021 s/iter. Inference: 1.4927 s/iter. Eval: 5.1809 s/iter. Total: 6.6769 s/iter. ETA=0:10:40\n",
            "[09/02 21:44:20 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0021 s/iter. Inference: 1.4654 s/iter. Eval: 5.2005 s/iter. Total: 6.6691 s/iter. ETA=0:10:33\n",
            "[09/02 21:44:26 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0021 s/iter. Inference: 1.4392 s/iter. Eval: 5.2191 s/iter. Total: 6.6615 s/iter. ETA=0:10:26\n",
            "[09/02 21:44:33 d2.evaluation.evaluator]: Inference done 57/150. Dataloading: 0.0021 s/iter. Inference: 1.4140 s/iter. Eval: 5.2368 s/iter. Total: 6.6540 s/iter. ETA=0:10:18\n",
            "[09/02 21:44:39 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0021 s/iter. Inference: 1.3897 s/iter. Eval: 5.2537 s/iter. Total: 6.6466 s/iter. ETA=0:10:11\n",
            "[09/02 21:44:45 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0021 s/iter. Inference: 1.3663 s/iter. Eval: 5.2714 s/iter. Total: 6.6410 s/iter. ETA=0:10:04\n",
            "[09/02 21:44:52 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0022 s/iter. Inference: 1.3438 s/iter. Eval: 5.2866 s/iter. Total: 6.6337 s/iter. ETA=0:09:57\n",
            "[09/02 21:44:58 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0022 s/iter. Inference: 1.3221 s/iter. Eval: 5.3020 s/iter. Total: 6.6273 s/iter. ETA=0:09:49\n",
            "[09/02 21:45:04 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0022 s/iter. Inference: 1.3011 s/iter. Eval: 5.3159 s/iter. Total: 6.6203 s/iter. ETA=0:09:42\n",
            "[09/02 21:45:10 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0022 s/iter. Inference: 1.2809 s/iter. Eval: 5.3304 s/iter. Total: 6.6145 s/iter. ETA=0:09:35\n",
            "[09/02 21:45:17 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0022 s/iter. Inference: 1.2613 s/iter. Eval: 5.3436 s/iter. Total: 6.6082 s/iter. ETA=0:09:28\n",
            "[09/02 21:45:27 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0021 s/iter. Inference: 1.2050 s/iter. Eval: 5.2496 s/iter. Total: 6.4579 s/iter. ETA=0:08:56\n",
            "[09/02 21:45:33 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0021 s/iter. Inference: 1.1879 s/iter. Eval: 5.2639 s/iter. Total: 6.4551 s/iter. ETA=0:08:49\n",
            "[09/02 21:45:39 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0021 s/iter. Inference: 1.1543 s/iter. Eval: 5.1894 s/iter. Total: 6.3469 s/iter. ETA=0:08:27\n",
            "[09/02 21:45:48 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0021 s/iter. Inference: 1.1231 s/iter. Eval: 5.1553 s/iter. Total: 6.2816 s/iter. ETA=0:08:09\n",
            "[09/02 21:45:56 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0021 s/iter. Inference: 1.0937 s/iter. Eval: 5.1224 s/iter. Total: 6.2193 s/iter. ETA=0:07:52\n",
            "[09/02 21:46:04 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0021 s/iter. Inference: 1.0659 s/iter. Eval: 5.0913 s/iter. Total: 6.1604 s/iter. ETA=0:07:35\n",
            "[09/02 21:46:12 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0021 s/iter. Inference: 1.0397 s/iter. Eval: 5.0616 s/iter. Total: 6.1044 s/iter. ETA=0:07:19\n",
            "[09/02 21:46:21 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.0021 s/iter. Inference: 1.0148 s/iter. Eval: 5.0340 s/iter. Total: 6.0519 s/iter. ETA=0:07:03\n",
            "[09/02 21:46:29 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0021 s/iter. Inference: 0.9913 s/iter. Eval: 5.0076 s/iter. Total: 6.0020 s/iter. ETA=0:06:48\n",
            "[09/02 21:46:37 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0021 s/iter. Inference: 0.9689 s/iter. Eval: 4.9825 s/iter. Total: 5.9545 s/iter. ETA=0:06:32\n",
            "[09/02 21:46:45 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0021 s/iter. Inference: 0.9477 s/iter. Eval: 4.9592 s/iter. Total: 5.9101 s/iter. ETA=0:06:18\n",
            "[09/02 21:46:54 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0021 s/iter. Inference: 0.9275 s/iter. Eval: 4.9364 s/iter. Total: 5.8670 s/iter. ETA=0:06:03\n",
            "[09/02 21:47:02 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0021 s/iter. Inference: 0.9083 s/iter. Eval: 4.9143 s/iter. Total: 5.8256 s/iter. ETA=0:05:49\n",
            "[09/02 21:47:11 d2.evaluation.evaluator]: Inference done 92/150. Dataloading: 0.0021 s/iter. Inference: 0.8899 s/iter. Eval: 4.8985 s/iter. Total: 5.7915 s/iter. ETA=0:05:35\n",
            "[09/02 21:47:19 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0021 s/iter. Inference: 0.8723 s/iter. Eval: 4.8782 s/iter. Total: 5.7535 s/iter. ETA=0:05:22\n",
            "[09/02 21:47:27 d2.evaluation.evaluator]: Inference done 96/150. Dataloading: 0.0021 s/iter. Inference: 0.8555 s/iter. Eval: 4.8591 s/iter. Total: 5.7176 s/iter. ETA=0:05:08\n",
            "[09/02 21:47:35 d2.evaluation.evaluator]: Inference done 98/150. Dataloading: 0.0021 s/iter. Inference: 0.8394 s/iter. Eval: 4.8407 s/iter. Total: 5.6831 s/iter. ETA=0:04:55\n",
            "[09/02 21:47:43 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0021 s/iter. Inference: 0.8240 s/iter. Eval: 4.8230 s/iter. Total: 5.6501 s/iter. ETA=0:04:42\n",
            "[09/02 21:47:52 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0021 s/iter. Inference: 0.8092 s/iter. Eval: 4.8067 s/iter. Total: 5.6190 s/iter. ETA=0:04:29\n",
            "[09/02 21:48:00 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0021 s/iter. Inference: 0.7950 s/iter. Eval: 4.7909 s/iter. Total: 5.5890 s/iter. ETA=0:04:17\n",
            "[09/02 21:48:08 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0021 s/iter. Inference: 0.7814 s/iter. Eval: 4.7755 s/iter. Total: 5.5600 s/iter. ETA=0:04:04\n",
            "[09/02 21:48:17 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0021 s/iter. Inference: 0.7684 s/iter. Eval: 4.7608 s/iter. Total: 5.5322 s/iter. ETA=0:03:52\n",
            "[09/02 21:48:25 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0021 s/iter. Inference: 0.7558 s/iter. Eval: 4.7466 s/iter. Total: 5.5054 s/iter. ETA=0:03:40\n",
            "[09/02 21:48:33 d2.evaluation.evaluator]: Inference done 112/150. Dataloading: 0.0021 s/iter. Inference: 0.7437 s/iter. Eval: 4.7326 s/iter. Total: 5.4793 s/iter. ETA=0:03:28\n",
            "[09/02 21:48:40 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0021 s/iter. Inference: 0.7319 s/iter. Eval: 4.7066 s/iter. Total: 5.4416 s/iter. ETA=0:03:15\n",
            "[09/02 21:48:45 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:49:05 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0021 s/iter. Inference: 0.8656 s/iter. Eval: 4.7030 s/iter. Total: 5.5717 s/iter. ETA=0:03:09\n",
            "[09/02 21:49:06 d2.utils.memory]: Attempting to copy inputs of <bound method CustomMaskFormer.instance_inference of CustomMaskFormer(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskFormerHead(\n",
            "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(100, 256)\n",
            "      (query_embed): Embedding(100, 256)\n",
            "      (level_embed): Embedding(3, 256)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=11, bias=True)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion CustomSetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks']\n",
            "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
            "      num_classes: 10\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")> to CPU due to CUDA OOM\n",
            "[09/02 21:49:27 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0021 s/iter. Inference: 1.0100 s/iter. Eval: 4.7055 s/iter. Total: 5.7185 s/iter. ETA=0:03:08\n",
            "[09/02 21:49:36 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0021 s/iter. Inference: 0.9990 s/iter. Eval: 4.6934 s/iter. Total: 5.6954 s/iter. ETA=0:02:56\n",
            "[09/02 21:49:44 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0021 s/iter. Inference: 0.9836 s/iter. Eval: 4.6819 s/iter. Total: 5.6685 s/iter. ETA=0:02:44\n",
            "[09/02 21:49:53 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0021 s/iter. Inference: 0.9688 s/iter. Eval: 4.6711 s/iter. Total: 5.6429 s/iter. ETA=0:02:32\n",
            "[09/02 21:50:01 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0021 s/iter. Inference: 0.9544 s/iter. Eval: 4.6602 s/iter. Total: 5.6176 s/iter. ETA=0:02:20\n",
            "[09/02 21:50:09 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0021 s/iter. Inference: 0.9405 s/iter. Eval: 4.6495 s/iter. Total: 5.5930 s/iter. ETA=0:02:08\n",
            "[09/02 21:50:15 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0021 s/iter. Inference: 0.9270 s/iter. Eval: 4.6197 s/iter. Total: 5.5497 s/iter. ETA=0:01:56\n",
            "[09/02 21:50:24 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0021 s/iter. Inference: 0.9142 s/iter. Eval: 4.6194 s/iter. Total: 5.5366 s/iter. ETA=0:01:45\n",
            "[09/02 21:50:33 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0021 s/iter. Inference: 0.9017 s/iter. Eval: 4.6144 s/iter. Total: 5.5191 s/iter. ETA=0:01:33\n",
            "[09/02 21:50:41 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0021 s/iter. Inference: 0.8895 s/iter. Eval: 4.6052 s/iter. Total: 5.4977 s/iter. ETA=0:01:22\n",
            "[09/02 21:50:50 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.0021 s/iter. Inference: 0.8777 s/iter. Eval: 4.5965 s/iter. Total: 5.4772 s/iter. ETA=0:01:11\n",
            "[09/02 21:50:58 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0021 s/iter. Inference: 0.8663 s/iter. Eval: 4.5879 s/iter. Total: 5.4572 s/iter. ETA=0:01:00\n",
            "[09/02 21:51:06 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0021 s/iter. Inference: 0.8552 s/iter. Eval: 4.5796 s/iter. Total: 5.4378 s/iter. ETA=0:00:48\n",
            "[09/02 21:51:15 d2.evaluation.evaluator]: Inference done 143/150. Dataloading: 0.0021 s/iter. Inference: 0.8444 s/iter. Eval: 4.5716 s/iter. Total: 5.4190 s/iter. ETA=0:00:37\n",
            "[09/02 21:51:22 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0021 s/iter. Inference: 0.8338 s/iter. Eval: 4.5585 s/iter. Total: 5.3953 s/iter. ETA=0:00:26\n",
            "[09/02 21:51:30 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0021 s/iter. Inference: 0.8235 s/iter. Eval: 4.5459 s/iter. Total: 5.3723 s/iter. ETA=0:00:16\n",
            "[09/02 21:51:37 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0021 s/iter. Inference: 0.8134 s/iter. Eval: 4.5335 s/iter. Total: 5.3499 s/iter. ETA=0:00:05\n",
            "[09/02 21:51:41 d2.evaluation.evaluator]: Total inference time: 0:12:54.198722 (5.339302 s / iter per device, on 1 devices)\n",
            "[09/02 21:51:41 d2.evaluation.evaluator]: Total inference pure compute time: 0:01:57 (0.808512 s / iter per device, on 1 devices)\n",
            "[09/02 21:51:41 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[09/02 21:51:41 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/taco10_val/coco_instances_results.json\n",
            "[09/02 21:51:42 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 21:51:42 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[09/02 21:51:42 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.11 seconds.\n",
            "[09/02 21:51:42 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 21:51:42 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[09/02 21:51:42 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
            "[09/02 21:51:42 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "[09/02 21:51:42 d2.evaluation.coco_evaluation]: Per-category bbox AR: \n",
            "| category              | AR    | category   | AR    | category   | AR    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.000 |\n",
            "| Bottle cap            | 0.000 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.55s)\n",
            "creating index...\n",
            "index created!\n",
            "[09/02 21:51:43 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
            "[09/02 21:51:43 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.46 seconds.\n",
            "[09/02 21:51:43 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[09/02 21:51:43 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.05 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.063\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.085\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.069\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.075\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.239\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.247\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.026\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.290\n",
            "[09/02 21:51:43 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 6.283 | 8.531  | 6.876  | 0.264 | 4.418 | 7.546 |\n",
            "[09/02 21:51:43 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP    |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:------|\n",
            "| Can                   | 20.881 | Other      | 7.469  | Bottle     | 7.524 |\n",
            "| Bottle cap            | 7.601  | Cup        | 10.452 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 8.319  | Pop tab    | 0.000  | Straw      | 0.489 |\n",
            "| Cigarette             | 0.094  |            |        |            |       |\n",
            "[09/02 21:51:43 d2.evaluation.coco_evaluation]: Per-category segm AR: \n",
            "| category              | AR     | category   | AR     | category   | AR     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 44.286 | Other      | 36.927 | Bottle     | 40.244 |\n",
            "| Bottle cap            | 23.182 | Cup        | 49.600 | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 42.754 | Pop tab    | 0.000  | Straw      | 7.333  |\n",
            "| Cigarette             | 2.697  |            |        |            |        |\n",
            "Evaluation results for taco10_val in csv format:\n",
            "[09/02 21:51:43 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[09/02 21:51:43 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 21:51:43 d2.evaluation.testing]: copypaste: 0.0000,0.0000,0.0000,0.0000,0.0000,0.0000\n",
            "[09/02 21:51:43 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[09/02 21:51:43 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[09/02 21:51:43 d2.evaluation.testing]: copypaste: 6.2828,8.5307,6.8755,0.2644,4.4182,7.5465\n",
            "Results (AP metrics):\n",
            "Task: bbox\n",
            "Metric, Value\n",
            "AP, 0.0000\n",
            "AP50, 0.0000\n",
            "AP75, 0.0000\n",
            "APs, 0.0000\n",
            "APm, 0.0000\n",
            "APl, 0.0000\n",
            "AP-Can, 0.0000\n",
            "AP-Other, 0.0000\n",
            "AP-Bottle, 0.0000\n",
            "AP-Bottle cap, 0.0000\n",
            "AP-Cup, 0.0000\n",
            "AP-Lid, 0.0000\n",
            "AP-Plastic bag + wrapper, 0.0000\n",
            "AP-Pop tab, 0.0000\n",
            "AP-Straw, 0.0000\n",
            "AP-Cigarette, 0.0000\n",
            "Task: segm\n",
            "Metric, Value\n",
            "AP, 6.2828\n",
            "AP50, 8.5307\n",
            "AP75, 6.8755\n",
            "APs, 0.2644\n",
            "APm, 4.4182\n",
            "APl, 7.5465\n",
            "AP-Can, 20.8813\n",
            "AP-Other, 7.4688\n",
            "AP-Bottle, 7.5238\n",
            "AP-Bottle cap, 7.6012\n",
            "AP-Cup, 10.4521\n",
            "AP-Lid, 0.0000\n",
            "AP-Plastic bag + wrapper, 8.3188\n",
            "AP-Pop tab, 0.0000\n",
            "AP-Straw, 0.4888\n",
            "AP-Cigarette, 0.0936\n",
            "Recalls (AR metrics):\n",
            "Task: bbox\n",
            "Metric, Value\n",
            "AR-Can, 0.0000\n",
            "AR-Other, 0.0000\n",
            "AR-Bottle, 0.0000\n",
            "AR-Bottle cap, 0.0000\n",
            "AR-Cup, 0.0000\n",
            "AR-Lid, 0.0000\n",
            "AR-Plastic bag + wrapper, 0.0000\n",
            "AR-Pop tab, 0.0000\n",
            "AR-Straw, 0.0000\n",
            "AR-Cigarette, 0.0000\n",
            "Task: segm\n",
            "Metric, Value\n",
            "AR-Can, 44.2857\n",
            "AR-Other, 36.9274\n",
            "AR-Bottle, 40.2439\n",
            "AR-Bottle cap, 23.1818\n",
            "AR-Cup, 49.6000\n",
            "AR-Lid, 0.0000\n",
            "AR-Plastic bag + wrapper, 42.7536\n",
            "AR-Pop tab, 0.0000\n",
            "AR-Straw, 7.3333\n",
            "AR-Cigarette, 2.6966\n",
            "[09/02 21:51:43 d2.utils.events]:  eta: 19 days, 13:51:41  iter: 17999  total_loss: 21.82  loss_ce: 0.8893  loss_mask: 0.07829  loss_dice: 1.01  loss_ce_0: 1.575  loss_mask_0: 0.0722  loss_dice_0: 1.143  loss_ce_1: 1.178  loss_mask_1: 0.08078  loss_dice_1: 1.408  loss_ce_2: 1.025  loss_mask_2: 0.06755  loss_dice_2: 1.15  loss_ce_3: 1.048  loss_mask_3: 0.06634  loss_dice_3: 0.9553  loss_ce_4: 0.9624  loss_mask_4: 0.07257  loss_dice_4: 1.069  loss_ce_5: 0.9447  loss_mask_5: 0.07617  loss_dice_5: 1.171  loss_ce_6: 0.9344  loss_mask_6: 0.07755  loss_dice_6: 1.076  loss_ce_7: 0.9238  loss_mask_7: 0.06852  loss_dice_7: 0.9833  loss_ce_8: 0.8534  loss_mask_8: 0.0767  loss_dice_8: 1.329     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:51:52 d2.utils.events]:  eta: 5:06:43  iter: 18019  total_loss: 19.3  loss_ce: 0.635  loss_mask: 0.08889  loss_dice: 0.638  loss_ce_0: 1.17  loss_mask_0: 0.1062  loss_dice_0: 1.177  loss_ce_1: 0.9001  loss_mask_1: 0.1242  loss_dice_1: 1.06  loss_ce_2: 0.9124  loss_mask_2: 0.09469  loss_dice_2: 1.091  loss_ce_3: 0.7614  loss_mask_3: 0.08857  loss_dice_3: 1.087  loss_ce_4: 0.7693  loss_mask_4: 0.1009  loss_dice_4: 0.7478  loss_ce_5: 0.7523  loss_mask_5: 0.08723  loss_dice_5: 0.8331  loss_ce_6: 0.7372  loss_mask_6: 0.08844  loss_dice_6: 0.9306  loss_ce_7: 0.699  loss_mask_7: 0.0879  loss_dice_7: 0.9131  loss_ce_8: 0.7559  loss_mask_8: 0.07116  loss_dice_8: 0.8402     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:52:00 d2.utils.events]:  eta: 4:53:31  iter: 18039  total_loss: 22.91  loss_ce: 0.8181  loss_mask: 0.1234  loss_dice: 0.9174  loss_ce_0: 1.441  loss_mask_0: 0.1137  loss_dice_0: 0.9063  loss_ce_1: 1.051  loss_mask_1: 0.1967  loss_dice_1: 1.019  loss_ce_2: 0.9359  loss_mask_2: 0.1756  loss_dice_2: 1.025  loss_ce_3: 0.8456  loss_mask_3: 0.1755  loss_dice_3: 0.9175  loss_ce_4: 0.8305  loss_mask_4: 0.1361  loss_dice_4: 0.8129  loss_ce_5: 0.8131  loss_mask_5: 0.1864  loss_dice_5: 0.9041  loss_ce_6: 0.824  loss_mask_6: 0.1419  loss_dice_6: 0.8715  loss_ce_7: 0.7329  loss_mask_7: 0.1544  loss_dice_7: 0.7941  loss_ce_8: 0.8702  loss_mask_8: 0.1293  loss_dice_8: 0.7983     lr: 0.0001  max_mem: 37654M\n",
            "[09/02 21:52:09 d2.utils.events]:  eta: 4:50:34  iter: 18059  total_loss: 21.32  loss_ce: 0.7327  loss_mask: 0.1222  loss_dice: 0.9543  loss_ce_0: 1.549  loss_mask_0: 0.1012  loss_dice_0: 1.212  loss_ce_1: 1.058  loss_mask_1: 0.1866  loss_dice_1: 1.078  loss_ce_2: 0.9652  loss_mask_2: 0.1388  loss_dice_2: 1.036  loss_ce_3: 0.8024  loss_mask_3: 0.1644  loss_dice_3: 0.7589  loss_ce_4: 0.8038  loss_mask_4: 0.16  loss_dice_4: 0.7763  loss_ce_5: 0.813  loss_mask_5: 0.1639  loss_dice_5: 0.9258  loss_ce_6: 0.7664  loss_mask_6: 0.1503  loss_dice_6: 0.9116  loss_ce_7: 0.7449  loss_mask_7: 0.1295  loss_dice_7: 1.012  loss_ce_8: 0.7899  loss_mask_8: 0.1345  loss_dice_8: 1.001     lr: 0.0001  max_mem: 37654M\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1457d1e40540>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-ec8770c09d9b>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(cfg, model, resume)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Izvršavanje modela i izračun gubitka\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-bb024aaa2efa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem_seg_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Mask2Former/mask2former/modeling/meta_arch/mask_former_head.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Mask2Former/mask2former/modeling/meta_arch/mask_former_head.py\u001b[0m in \u001b[0;36mlayers\u001b[0;34m(self, features, mask)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mmask_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_scale_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_in_feature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_scale_pixel_decoder\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_scale_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_in_feature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"transformer_encoder\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Mask2Former/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;31m# FFN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             output = self.transformer_ffn_layers[i](\n\u001b[0m\u001b[1;32m    415\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Mask2Former/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Mask2Former/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py\u001b[0m in \u001b[0;36mforward_post\u001b[0;34m(self, tgt)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mtgt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}