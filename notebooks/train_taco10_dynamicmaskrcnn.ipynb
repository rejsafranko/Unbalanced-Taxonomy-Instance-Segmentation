{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_xSFw7ZtgDK",
        "outputId": "2f8ab5ef-600e-482a-b074-b1a0cfeb1534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SzFCz2gt-Za",
        "outputId": "de60283b-078c-4e36-b598-9b6a0aa9369a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
            "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (24.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7) (2.10.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.1) (4.9.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install pyyaml\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcQRkGfyuKeB",
        "outputId": "7b7046c0-0179-4f72-8fa9-d8500c114e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.3 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U8XXSzz1uhmq"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.engine import DefaultTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CluGLxd3wJq7"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CdMWUSrTuvAb"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco10_train\", {}, data_dir_path + \"mapped_annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_val\", {}, data_dir_path + \"mapped_annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco10_test\", {}, data_dir_path + \"mapped_annotations_0_test.json\", data_dir_path + \"images/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PfYkT0FH9g8b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load your JSON file\n",
        "with open('/content/drive/MyDrive/instseg/mapped_annotations_0_train.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Generate unique IDs\n",
        "unique_id = 0\n",
        "for annotation in data['annotations']:\n",
        "    annotation['id'] = unique_id\n",
        "    unique_id += 1\n",
        "\n",
        "# Save the corrected JSON back to file\n",
        "with open('/content/drive/MyDrive/instseg/mapped_annotations_0_train.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V2oCr4kY_32_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import build_detection_train_loader\n",
        "class Trainer(DefaultTrainer):\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "    if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "    return COCOEvaluator(dataset_name, output_dir=output_folder)"
      ],
      "metadata": {
        "id": "b1RV_u2dMujb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDgWWa6wseh",
        "outputId": "80913c76-960a-4f70-bb40-475d9577cad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:34:01 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[08/21 20:34:02 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/drive/MyDrive/instseg/mapped_annotations_0_train.json\n",
            "[08/21 20:34:02 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[08/21 20:34:02 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 194          |   Other    | 1397         |   Bottle   | 344          |\n",
            "|  Bottle cap   | 226          |    Cup     | 150          |    Lid     | 63           |\n",
            "| Plastic bag.. | 697          |  Pop tab   | 75           |   Straw    | 108          |\n",
            "|   Cigarette   | 457          |            |              |            |              |\n",
            "|     total     | 3711         |            |              |            |              |\n",
            "[08/21 20:34:02 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[08/21 20:34:02 d2.data.build]: Using training sampler TrainingSampler\n",
            "[08/21 20:34:02 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[08/21 20:34:02 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[08/21 20:34:02 d2.data.common]: Serialized dataset takes 1.77 MiB\n",
            "[08/21 20:34:02 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[08/21 20:34:02 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (10, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:34:02 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:34:18 d2.utils.events]:  eta: 0:04:53  iter: 19  total_loss: 3.114  loss_cls: 2.192  loss_box_reg: 0.4955  loss_mask: 0.08066  loss_rpn_cls: 0.06332  loss_rpn_loc: 0.01225    time: 0.7153  last_time: 0.9706  data_time: 0.3368  last_data_time: 0.7213   lr: 3.8962e-05  max_mem: 2336M\n",
            "[08/21 20:34:34 d2.utils.events]:  eta: 0:03:51  iter: 39  total_loss: 2.323  loss_cls: 1.431  loss_box_reg: 0.6638  loss_mask: 0.07908  loss_rpn_cls: 0.04767  loss_rpn_loc: 0.01795    time: 0.6803  last_time: 0.8609  data_time: 0.3361  last_data_time: 0.4256   lr: 7.8922e-05  max_mem: 2336M\n",
            "[08/21 20:34:48 d2.utils.events]:  eta: 0:03:45  iter: 59  total_loss: 1.44  loss_cls: 0.6644  loss_box_reg: 0.487  loss_mask: 0.07694  loss_rpn_cls: 0.04439  loss_rpn_loc: 0.01544    time: 0.6747  last_time: 0.3567  data_time: 0.3536  last_data_time: 0.1184   lr: 0.00011888  max_mem: 2337M\n",
            "[08/21 20:35:02 d2.utils.events]:  eta: 0:03:31  iter: 79  total_loss: 1.577  loss_cls: 0.6223  loss_box_reg: 0.4998  loss_mask: 0.1535  loss_rpn_cls: 0.07503  loss_rpn_loc: 0.02678    time: 0.6883  last_time: 0.4003  data_time: 0.4367  last_data_time: 0.0050   lr: 0.00015884  max_mem: 2337M\n",
            "[08/21 20:35:17 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/mapped_annotations_0_val.json\n",
            "[08/21 20:35:17 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 42           |   Other    | 179          |   Bottle   | 41           |\n",
            "|  Bottle cap   | 22           |    Cup     | 25           |    Lid     | 9            |\n",
            "| Plastic bag.. | 69           |  Pop tab   | 13           |   Straw    | 15           |\n",
            "|   Cigarette   | 89           |            |              |            |              |\n",
            "|     total     | 504          |            |              |            |              |\n",
            "[08/21 20:35:17 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[08/21 20:35:17 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[08/21 20:35:17 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[08/21 20:35:17 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[08/21 20:35:17 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[08/21 20:35:17 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:35:35 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0015 s/iter. Inference: 0.2533 s/iter. Eval: 1.6314 s/iter. Total: 1.8862 s/iter. ETA=0:04:22\n",
            "[08/21 20:35:41 d2.evaluation.evaluator]: Inference done 16/150. Dataloading: 0.0016 s/iter. Inference: 0.2061 s/iter. Eval: 1.2919 s/iter. Total: 1.4998 s/iter. ETA=0:03:20\n",
            "[08/21 20:35:47 d2.evaluation.evaluator]: Inference done 20/150. Dataloading: 0.0017 s/iter. Inference: 0.2043 s/iter. Eval: 1.3131 s/iter. Total: 1.5193 s/iter. ETA=0:03:17\n",
            "[08/21 20:35:54 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0017 s/iter. Inference: 0.2211 s/iter. Eval: 1.4661 s/iter. Total: 1.6892 s/iter. ETA=0:03:34\n",
            "[08/21 20:36:01 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0018 s/iter. Inference: 0.2430 s/iter. Eval: 1.7158 s/iter. Total: 1.9610 s/iter. ETA=0:04:07\n",
            "[08/21 20:36:07 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0018 s/iter. Inference: 0.2401 s/iter. Eval: 1.7141 s/iter. Total: 1.9564 s/iter. ETA=0:04:00\n",
            "[08/21 20:36:15 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0018 s/iter. Inference: 0.2450 s/iter. Eval: 1.7934 s/iter. Total: 2.0407 s/iter. ETA=0:04:04\n",
            "[08/21 20:36:21 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0018 s/iter. Inference: 0.2531 s/iter. Eval: 1.8537 s/iter. Total: 2.1090 s/iter. ETA=0:04:08\n",
            "[08/21 20:36:27 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0019 s/iter. Inference: 0.2456 s/iter. Eval: 1.7916 s/iter. Total: 2.0395 s/iter. ETA=0:03:52\n",
            "[08/21 20:36:33 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0019 s/iter. Inference: 0.2481 s/iter. Eval: 1.8363 s/iter. Total: 2.0866 s/iter. ETA=0:03:53\n",
            "[08/21 20:36:40 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0019 s/iter. Inference: 0.2513 s/iter. Eval: 1.8580 s/iter. Total: 2.1117 s/iter. ETA=0:03:50\n",
            "[08/21 20:36:46 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0019 s/iter. Inference: 0.2485 s/iter. Eval: 1.8398 s/iter. Total: 2.0906 s/iter. ETA=0:03:41\n",
            "[08/21 20:36:51 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0019 s/iter. Inference: 0.2510 s/iter. Eval: 1.8777 s/iter. Total: 2.1311 s/iter. ETA=0:03:41\n",
            "[08/21 20:36:58 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0019 s/iter. Inference: 0.2557 s/iter. Eval: 1.9323 s/iter. Total: 2.1904 s/iter. ETA=0:03:43\n",
            "[08/21 20:37:03 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0019 s/iter. Inference: 0.2566 s/iter. Eval: 1.9484 s/iter. Total: 2.2074 s/iter. ETA=0:03:40\n",
            "[08/21 20:37:11 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0019 s/iter. Inference: 0.2492 s/iter. Eval: 1.8801 s/iter. Total: 2.1317 s/iter. ETA=0:03:22\n",
            "[08/21 20:37:17 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0019 s/iter. Inference: 0.2450 s/iter. Eval: 1.8442 s/iter. Total: 2.0915 s/iter. ETA=0:03:10\n",
            "[08/21 20:37:25 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0019 s/iter. Inference: 0.2470 s/iter. Eval: 1.8754 s/iter. Total: 2.1248 s/iter. ETA=0:03:06\n",
            "[08/21 20:37:31 d2.evaluation.evaluator]: Inference done 65/150. Dataloading: 0.0019 s/iter. Inference: 0.2458 s/iter. Eval: 1.8680 s/iter. Total: 2.1162 s/iter. ETA=0:02:59\n",
            "[08/21 20:37:38 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0019 s/iter. Inference: 0.2449 s/iter. Eval: 1.8718 s/iter. Total: 2.1191 s/iter. ETA=0:02:53\n",
            "[08/21 20:37:43 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0019 s/iter. Inference: 0.2412 s/iter. Eval: 1.8346 s/iter. Total: 2.0783 s/iter. ETA=0:02:42\n",
            "[08/21 20:37:49 d2.evaluation.evaluator]: Inference done 77/150. Dataloading: 0.0019 s/iter. Inference: 0.2358 s/iter. Eval: 1.7813 s/iter. Total: 2.0194 s/iter. ETA=0:02:27\n",
            "[08/21 20:37:55 d2.evaluation.evaluator]: Inference done 81/150. Dataloading: 0.0019 s/iter. Inference: 0.2324 s/iter. Eval: 1.7480 s/iter. Total: 1.9829 s/iter. ETA=0:02:16\n",
            "[08/21 20:38:00 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0019 s/iter. Inference: 0.2298 s/iter. Eval: 1.7221 s/iter. Total: 1.9542 s/iter. ETA=0:02:07\n",
            "[08/21 20:38:06 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0019 s/iter. Inference: 0.2299 s/iter. Eval: 1.7224 s/iter. Total: 1.9547 s/iter. ETA=0:02:01\n",
            "[08/21 20:38:12 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.0019 s/iter. Inference: 0.2242 s/iter. Eval: 1.6633 s/iter. Total: 1.8898 s/iter. ETA=0:01:45\n",
            "[08/21 20:38:18 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0019 s/iter. Inference: 0.2202 s/iter. Eval: 1.6230 s/iter. Total: 1.8456 s/iter. ETA=0:01:34\n",
            "[08/21 20:38:23 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0019 s/iter. Inference: 0.2185 s/iter. Eval: 1.6063 s/iter. Total: 1.8271 s/iter. ETA=0:01:25\n",
            "[08/21 20:38:29 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0019 s/iter. Inference: 0.2129 s/iter. Eval: 1.5484 s/iter. Total: 1.7636 s/iter. ETA=0:01:10\n",
            "[08/21 20:38:36 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0019 s/iter. Inference: 0.2140 s/iter. Eval: 1.5610 s/iter. Total: 1.7774 s/iter. ETA=0:01:05\n",
            "[08/21 20:38:47 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0019 s/iter. Inference: 0.2178 s/iter. Eval: 1.6061 s/iter. Total: 1.8263 s/iter. ETA=0:01:02\n",
            "[08/21 20:38:52 d2.evaluation.evaluator]: Inference done 118/150. Dataloading: 0.0019 s/iter. Inference: 0.2190 s/iter. Eval: 1.6220 s/iter. Total: 1.8433 s/iter. ETA=0:00:58\n",
            "[08/21 20:38:58 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0019 s/iter. Inference: 0.2190 s/iter. Eval: 1.6222 s/iter. Total: 1.8435 s/iter. ETA=0:00:53\n",
            "[08/21 20:39:03 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0019 s/iter. Inference: 0.2190 s/iter. Eval: 1.6222 s/iter. Total: 1.8436 s/iter. ETA=0:00:47\n",
            "[08/21 20:39:09 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0019 s/iter. Inference: 0.2187 s/iter. Eval: 1.6195 s/iter. Total: 1.8406 s/iter. ETA=0:00:42\n",
            "[08/21 20:39:15 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0019 s/iter. Inference: 0.2182 s/iter. Eval: 1.6135 s/iter. Total: 1.8340 s/iter. ETA=0:00:34\n",
            "[08/21 20:39:22 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0019 s/iter. Inference: 0.2188 s/iter. Eval: 1.6213 s/iter. Total: 1.8425 s/iter. ETA=0:00:29\n",
            "[08/21 20:39:28 d2.evaluation.evaluator]: Inference done 140/150. Dataloading: 0.0030 s/iter. Inference: 0.2156 s/iter. Eval: 1.5867 s/iter. Total: 1.8057 s/iter. ETA=0:00:18\n",
            "[08/21 20:39:34 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0029 s/iter. Inference: 0.2148 s/iter. Eval: 1.5782 s/iter. Total: 1.7964 s/iter. ETA=0:00:10\n",
            "[08/21 20:39:39 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0029 s/iter. Inference: 0.2146 s/iter. Eval: 1.5764 s/iter. Total: 1.7944 s/iter. ETA=0:00:05\n",
            "[08/21 20:39:40 d2.evaluation.evaluator]: Total inference time: 0:04:15.652129 (1.763118 s / iter per device, on 1 devices)\n",
            "[08/21 20:39:40 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:30 (0.211838 s / iter per device, on 1 devices)\n",
            "[08/21 20:39:40 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[08/21 20:39:40 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[08/21 20:39:40 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.55s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.25s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.026\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.096\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.209\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.319\n",
            "[08/21 20:39:41 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 2.297 | 5.412  | 1.102  | 0.201 | 2.308 | 2.605 |\n",
            "[08/21 20:39:41 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 1.747 | Other      | 2.012 | Bottle     | 8.223 |\n",
            "| Bottle cap            | 0.328 | Cup        | 2.072 | Lid        | 6.166 |\n",
            "| Plastic bag + wrapper | 2.398 | Pop tab    | 0.010 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.017 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.09s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=1.00s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.21s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.015\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.035\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.061\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.061\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.081\n",
            "[08/21 20:39:42 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.827 | 1.543  | 0.234  | 0.000 | 0.077 | 0.993 |\n",
            "[08/21 20:39:42 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.027 |\n",
            "| Bottle cap            | 0.376 | Cup        | 0.035 | Lid        | 7.823 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.008 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "[08/21 20:39:42 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[08/21 20:39:42 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[08/21 20:39:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:39:42 d2.evaluation.testing]: copypaste: 2.2974,5.4116,1.1016,0.2013,2.3084,2.6051\n",
            "[08/21 20:39:42 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[08/21 20:39:42 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:39:42 d2.evaluation.testing]: copypaste: 0.8268,1.5429,0.2337,0.0000,0.0775,0.9930\n",
            "[08/21 20:39:42 d2.utils.events]:  eta: 0:03:19  iter: 99  total_loss: 1.24  loss_cls: 0.4425  loss_box_reg: 0.3261  loss_mask: 0.2671  loss_rpn_cls: 0.08831  loss_rpn_loc: 0.05939    time: 0.6923  last_time: 0.2996  data_time: 0.4326  last_data_time: 0.0457   lr: 0.0001988  max_mem: 7877M\n",
            "[08/21 20:39:57 d2.utils.events]:  eta: 0:03:09  iter: 119  total_loss: 1.359  loss_cls: 0.5288  loss_box_reg: 0.4647  loss_mask: 0.2421  loss_rpn_cls: 0.06192  loss_rpn_loc: 0.01607    time: 0.6969  last_time: 1.3078  data_time: 0.4541  last_data_time: 1.0724   lr: 0.00023876  max_mem: 7877M\n",
            "[08/21 20:40:11 d2.utils.events]:  eta: 0:02:58  iter: 139  total_loss: 1.362  loss_cls: 0.5665  loss_box_reg: 0.5936  loss_mask: 0.1797  loss_rpn_cls: 0.03835  loss_rpn_loc: 0.02055    time: 0.6991  last_time: 0.9576  data_time: 0.4446  last_data_time: 0.7165   lr: 0.00027872  max_mem: 7877M\n",
            "[08/21 20:40:26 d2.utils.events]:  eta: 0:02:48  iter: 159  total_loss: 1.274  loss_cls: 0.5047  loss_box_reg: 0.4897  loss_mask: 0.1755  loss_rpn_cls: 0.05006  loss_rpn_loc: 0.01957    time: 0.7067  last_time: 0.4831  data_time: 0.4690  last_data_time: 0.2286   lr: 0.00031868  max_mem: 7877M\n",
            "[08/21 20:40:40 d2.utils.events]:  eta: 0:02:38  iter: 179  total_loss: 1.143  loss_cls: 0.4306  loss_box_reg: 0.4842  loss_mask: 0.1378  loss_rpn_cls: 0.05706  loss_rpn_loc: 0.01308    time: 0.7059  last_time: 0.2229  data_time: 0.4445  last_data_time: 0.0166   lr: 0.00035864  max_mem: 7877M\n",
            "[08/21 20:40:55 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/mapped_annotations_0_val.json\n",
            "[08/21 20:40:55 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[08/21 20:40:55 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[08/21 20:40:55 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[08/21 20:40:55 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[08/21 20:40:55 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[08/21 20:40:55 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:41:28 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0014 s/iter. Inference: 0.2468 s/iter. Eval: 2.5838 s/iter. Total: 2.8320 s/iter. ETA=0:06:33\n",
            "[08/21 20:41:33 d2.evaluation.evaluator]: Inference done 14/150. Dataloading: 0.0016 s/iter. Inference: 0.2365 s/iter. Eval: 2.2403 s/iter. Total: 2.4787 s/iter. ETA=0:05:37\n",
            "[08/21 20:41:39 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0017 s/iter. Inference: 0.2120 s/iter. Eval: 1.8189 s/iter. Total: 2.0329 s/iter. ETA=0:04:26\n",
            "[08/21 20:41:45 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0018 s/iter. Inference: 0.2156 s/iter. Eval: 1.7972 s/iter. Total: 2.0149 s/iter. ETA=0:04:17\n",
            "[08/21 20:41:52 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0018 s/iter. Inference: 0.2388 s/iter. Eval: 2.0434 s/iter. Total: 2.2844 s/iter. ETA=0:04:50\n",
            "[08/21 20:41:59 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0019 s/iter. Inference: 0.2597 s/iter. Eval: 2.2636 s/iter. Total: 2.5256 s/iter. ETA=0:05:18\n",
            "[08/21 20:42:05 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0019 s/iter. Inference: 0.2633 s/iter. Eval: 2.3116 s/iter. Total: 2.5772 s/iter. ETA=0:05:19\n",
            "[08/21 20:42:11 d2.evaluation.evaluator]: Inference done 28/150. Dataloading: 0.0019 s/iter. Inference: 0.2677 s/iter. Eval: 2.3721 s/iter. Total: 2.6421 s/iter. ETA=0:05:22\n",
            "[08/21 20:42:19 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0019 s/iter. Inference: 0.2789 s/iter. Eval: 2.4692 s/iter. Total: 2.7505 s/iter. ETA=0:05:30\n",
            "[08/21 20:42:26 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0019 s/iter. Inference: 0.2841 s/iter. Eval: 2.5111 s/iter. Total: 2.7977 s/iter. ETA=0:05:30\n",
            "[08/21 20:42:33 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0019 s/iter. Inference: 0.2887 s/iter. Eval: 2.5473 s/iter. Total: 2.8385 s/iter. ETA=0:05:29\n",
            "[08/21 20:42:39 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0020 s/iter. Inference: 0.2903 s/iter. Eval: 2.5522 s/iter. Total: 2.8451 s/iter. ETA=0:05:24\n",
            "[08/21 20:42:45 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0020 s/iter. Inference: 0.2917 s/iter. Eval: 2.5676 s/iter. Total: 2.8618 s/iter. ETA=0:05:20\n",
            "[08/21 20:42:52 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0020 s/iter. Inference: 0.2937 s/iter. Eval: 2.5871 s/iter. Total: 2.8833 s/iter. ETA=0:05:17\n",
            "[08/21 20:42:59 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0020 s/iter. Inference: 0.2982 s/iter. Eval: 2.6345 s/iter. Total: 2.9353 s/iter. ETA=0:05:17\n",
            "[08/21 20:43:06 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0020 s/iter. Inference: 0.2995 s/iter. Eval: 2.6439 s/iter. Total: 2.9460 s/iter. ETA=0:05:12\n",
            "[08/21 20:43:13 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0020 s/iter. Inference: 0.3029 s/iter. Eval: 2.6773 s/iter. Total: 2.9828 s/iter. ETA=0:05:10\n",
            "[08/21 20:43:20 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0020 s/iter. Inference: 0.3058 s/iter. Eval: 2.6996 s/iter. Total: 3.0080 s/iter. ETA=0:05:06\n",
            "[08/21 20:43:27 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0020 s/iter. Inference: 0.3086 s/iter. Eval: 2.7213 s/iter. Total: 3.0325 s/iter. ETA=0:05:03\n",
            "[08/21 20:43:33 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0020 s/iter. Inference: 0.3082 s/iter. Eval: 2.7106 s/iter. Total: 3.0213 s/iter. ETA=0:04:56\n",
            "[08/21 20:43:38 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0020 s/iter. Inference: 0.3082 s/iter. Eval: 2.7063 s/iter. Total: 3.0171 s/iter. ETA=0:04:49\n",
            "[08/21 20:43:44 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0020 s/iter. Inference: 0.3072 s/iter. Eval: 2.6910 s/iter. Total: 3.0007 s/iter. ETA=0:04:42\n",
            "[08/21 20:43:49 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0020 s/iter. Inference: 0.3070 s/iter. Eval: 2.6839 s/iter. Total: 2.9935 s/iter. ETA=0:04:35\n",
            "[08/21 20:43:56 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0020 s/iter. Inference: 0.3086 s/iter. Eval: 2.6963 s/iter. Total: 3.0075 s/iter. ETA=0:04:30\n",
            "[08/21 20:44:02 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0020 s/iter. Inference: 0.3087 s/iter. Eval: 2.6926 s/iter. Total: 3.0039 s/iter. ETA=0:04:24\n",
            "[08/21 20:44:09 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0020 s/iter. Inference: 0.3102 s/iter. Eval: 2.7041 s/iter. Total: 3.0169 s/iter. ETA=0:04:19\n",
            "[08/21 20:44:15 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0020 s/iter. Inference: 0.3054 s/iter. Eval: 2.6606 s/iter. Total: 2.9686 s/iter. ETA=0:04:06\n",
            "[08/21 20:44:21 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0020 s/iter. Inference: 0.3028 s/iter. Eval: 2.6259 s/iter. Total: 2.9313 s/iter. ETA=0:03:54\n",
            "[08/21 20:44:27 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0020 s/iter. Inference: 0.3000 s/iter. Eval: 2.5866 s/iter. Total: 2.8891 s/iter. ETA=0:03:42\n",
            "[08/21 20:44:34 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0020 s/iter. Inference: 0.2982 s/iter. Eval: 2.5604 s/iter. Total: 2.8611 s/iter. ETA=0:03:31\n",
            "[08/21 20:44:40 d2.evaluation.evaluator]: Inference done 79/150. Dataloading: 0.0020 s/iter. Inference: 0.2961 s/iter. Eval: 2.5325 s/iter. Total: 2.8312 s/iter. ETA=0:03:21\n",
            "[08/21 20:44:45 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0020 s/iter. Inference: 0.2927 s/iter. Eval: 2.4923 s/iter. Total: 2.7876 s/iter. ETA=0:03:09\n",
            "[08/21 20:44:50 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0020 s/iter. Inference: 0.2897 s/iter. Eval: 2.4560 s/iter. Total: 2.7483 s/iter. ETA=0:02:58\n",
            "[08/21 20:44:57 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0020 s/iter. Inference: 0.2882 s/iter. Eval: 2.4342 s/iter. Total: 2.7250 s/iter. ETA=0:02:48\n",
            "[08/21 20:45:03 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0020 s/iter. Inference: 0.2818 s/iter. Eval: 2.3608 s/iter. Total: 2.6451 s/iter. ETA=0:02:30\n",
            "[08/21 20:45:09 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0020 s/iter. Inference: 0.2775 s/iter. Eval: 2.3103 s/iter. Total: 2.5904 s/iter. ETA=0:02:17\n",
            "[08/21 20:45:16 d2.evaluation.evaluator]: Inference done 101/150. Dataloading: 0.0020 s/iter. Inference: 0.2749 s/iter. Eval: 2.2777 s/iter. Total: 2.5551 s/iter. ETA=0:02:05\n",
            "[08/21 20:45:23 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0020 s/iter. Inference: 0.2726 s/iter. Eval: 2.2500 s/iter. Total: 2.5252 s/iter. ETA=0:01:53\n",
            "[08/21 20:45:29 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0020 s/iter. Inference: 0.2711 s/iter. Eval: 2.2301 s/iter. Total: 2.5037 s/iter. ETA=0:01:45\n",
            "[08/21 20:45:34 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0020 s/iter. Inference: 0.2697 s/iter. Eval: 2.2122 s/iter. Total: 2.4845 s/iter. ETA=0:01:36\n",
            "[08/21 20:45:40 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0020 s/iter. Inference: 0.2688 s/iter. Eval: 2.1983 s/iter. Total: 2.4696 s/iter. ETA=0:01:28\n",
            "[08/21 20:45:49 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0020 s/iter. Inference: 0.2719 s/iter. Eval: 2.2322 s/iter. Total: 2.5067 s/iter. ETA=0:01:25\n",
            "[08/21 20:45:56 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0020 s/iter. Inference: 0.2752 s/iter. Eval: 2.2675 s/iter. Total: 2.5452 s/iter. ETA=0:01:23\n",
            "[08/21 20:46:02 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0020 s/iter. Inference: 0.2744 s/iter. Eval: 2.2560 s/iter. Total: 2.5329 s/iter. ETA=0:01:15\n",
            "[08/21 20:46:08 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0024 s/iter. Inference: 0.2736 s/iter. Eval: 2.2437 s/iter. Total: 2.5202 s/iter. ETA=0:01:08\n",
            "[08/21 20:46:13 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0024 s/iter. Inference: 0.2722 s/iter. Eval: 2.2258 s/iter. Total: 2.5009 s/iter. ETA=0:01:00\n",
            "[08/21 20:46:19 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0024 s/iter. Inference: 0.2709 s/iter. Eval: 2.2098 s/iter. Total: 2.4836 s/iter. ETA=0:00:52\n",
            "[08/21 20:46:24 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0024 s/iter. Inference: 0.2712 s/iter. Eval: 2.2105 s/iter. Total: 2.4846 s/iter. ETA=0:00:47\n",
            "[08/21 20:46:30 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0024 s/iter. Inference: 0.2706 s/iter. Eval: 2.2018 s/iter. Total: 2.4753 s/iter. ETA=0:00:39\n",
            "[08/21 20:46:35 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0023 s/iter. Inference: 0.2680 s/iter. Eval: 2.1715 s/iter. Total: 2.4424 s/iter. ETA=0:00:29\n",
            "[08/21 20:46:42 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0023 s/iter. Inference: 0.2659 s/iter. Eval: 2.1478 s/iter. Total: 2.4166 s/iter. ETA=0:00:19\n",
            "[08/21 20:46:48 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0023 s/iter. Inference: 0.2654 s/iter. Eval: 2.1409 s/iter. Total: 2.4092 s/iter. ETA=0:00:12\n",
            "[08/21 20:46:54 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0023 s/iter. Inference: 0.2637 s/iter. Eval: 2.1214 s/iter. Total: 2.3880 s/iter. ETA=0:00:02\n",
            "[08/21 20:46:56 d2.evaluation.evaluator]: Total inference time: 0:05:44.894946 (2.378586 s / iter per device, on 1 devices)\n",
            "[08/21 20:46:56 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:38 (0.262821 s / iter per device, on 1 devices)\n",
            "[08/21 20:46:56 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[08/21 20:46:56 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[08/21 20:46:56 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.57s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.23s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.066\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.050\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.080\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.187\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209\n",
            "[08/21 20:46:57 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 2.441 | 6.578  | 1.161  | 0.815 | 5.037 | 2.725 |\n",
            "[08/21 20:46:57 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 2.896 | Other      | 3.542 | Bottle     | 8.097 |\n",
            "| Bottle cap            | 0.724 | Cup        | 2.497 | Lid        | 0.188 |\n",
            "| Plastic bag + wrapper | 6.376 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.092 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.10s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.78s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.21s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.021\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.025\n",
            "[08/21 20:46:58 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.113 | 0.188  | 0.157  | 0.000 | 0.250 | 0.191 |\n",
            "[08/21 20:46:58 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.000 | Other      | 0.000 | Bottle     | 0.085 |\n",
            "| Bottle cap            | 1.048 | Cup        | 0.000 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 0.000 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "[08/21 20:46:58 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[08/21 20:46:58 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[08/21 20:46:58 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:46:58 d2.evaluation.testing]: copypaste: 2.4413,6.5783,1.1609,0.8153,5.0371,2.7246\n",
            "[08/21 20:46:58 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[08/21 20:46:58 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:46:58 d2.evaluation.testing]: copypaste: 0.1133,0.1875,0.1572,0.0000,0.2502,0.1911\n",
            "[08/21 20:46:58 d2.utils.events]:  eta: 0:02:30  iter: 199  total_loss: 1.237  loss_cls: 0.504  loss_box_reg: 0.4897  loss_mask: 0.1586  loss_rpn_cls: 0.02909  loss_rpn_loc: 0.01418    time: 0.7097  last_time: 1.2867  data_time: 0.4640  last_data_time: 1.0261   lr: 0.0003986  max_mem: 8770M\n",
            "[08/21 20:47:11 d2.utils.events]:  eta: 0:02:20  iter: 219  total_loss: 1.221  loss_cls: 0.4949  loss_box_reg: 0.5428  loss_mask: 0.1146  loss_rpn_cls: 0.03592  loss_rpn_loc: 0.01185    time: 0.7055  last_time: 1.7508  data_time: 0.4063  last_data_time: 1.4799   lr: 0.00043856  max_mem: 8770M\n",
            "[08/21 20:47:24 d2.utils.events]:  eta: 0:02:10  iter: 239  total_loss: 1.232  loss_cls: 0.5137  loss_box_reg: 0.5331  loss_mask: 0.1143  loss_rpn_cls: 0.05558  loss_rpn_loc: 0.01181    time: 0.7013  last_time: 0.2741  data_time: 0.3874  last_data_time: 0.0353   lr: 0.00047852  max_mem: 8770M\n",
            "[08/21 20:47:40 d2.utils.events]:  eta: 0:02:04  iter: 259  total_loss: 1.097  loss_cls: 0.4617  loss_box_reg: 0.4312  loss_mask: 0.08018  loss_rpn_cls: 0.05304  loss_rpn_loc: 0.01313    time: 0.7074  last_time: 1.5366  data_time: 0.5131  last_data_time: 1.2973   lr: 0.00051848  max_mem: 8770M\n",
            "[08/21 20:47:53 d2.utils.events]:  eta: 0:01:52  iter: 279  total_loss: 0.8653  loss_cls: 0.3669  loss_box_reg: 0.4111  loss_mask: 0.05406  loss_rpn_cls: 0.04266  loss_rpn_loc: 0.01895    time: 0.7033  last_time: 0.2189  data_time: 0.3921  last_data_time: 0.0047   lr: 0.00055844  max_mem: 8770M\n",
            "[08/21 20:48:04 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/mapped_annotations_0_val.json\n",
            "[08/21 20:48:04 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[08/21 20:48:04 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[08/21 20:48:04 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[08/21 20:48:04 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[08/21 20:48:04 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[08/21 20:48:04 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:48:15 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0015 s/iter. Inference: 0.1726 s/iter. Eval: 1.1041 s/iter. Total: 1.2782 s/iter. ETA=0:02:57\n",
            "[08/21 20:48:20 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0016 s/iter. Inference: 0.1516 s/iter. Eval: 0.9058 s/iter. Total: 1.0593 s/iter. ETA=0:02:20\n",
            "[08/21 20:48:25 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0016 s/iter. Inference: 0.1523 s/iter. Eval: 0.8985 s/iter. Total: 1.0527 s/iter. ETA=0:02:14\n",
            "[08/21 20:48:30 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0017 s/iter. Inference: 0.1708 s/iter. Eval: 1.1035 s/iter. Total: 1.2763 s/iter. ETA=0:02:42\n",
            "[08/21 20:48:37 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0017 s/iter. Inference: 0.1950 s/iter. Eval: 1.3725 s/iter. Total: 1.5696 s/iter. ETA=0:03:17\n",
            "[08/21 20:48:43 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0017 s/iter. Inference: 0.1865 s/iter. Eval: 1.2953 s/iter. Total: 1.4838 s/iter. ETA=0:02:59\n",
            "[08/21 20:48:51 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0017 s/iter. Inference: 0.1970 s/iter. Eval: 1.4068 s/iter. Total: 1.6059 s/iter. ETA=0:03:09\n",
            "[08/21 20:48:57 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0017 s/iter. Inference: 0.2024 s/iter. Eval: 1.4614 s/iter. Total: 1.6659 s/iter. ETA=0:03:11\n",
            "[08/21 20:49:03 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0018 s/iter. Inference: 0.2045 s/iter. Eval: 1.4890 s/iter. Total: 1.6956 s/iter. ETA=0:03:09\n",
            "[08/21 20:49:08 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0018 s/iter. Inference: 0.2048 s/iter. Eval: 1.4909 s/iter. Total: 1.6979 s/iter. ETA=0:03:05\n",
            "[08/21 20:49:16 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0018 s/iter. Inference: 0.2028 s/iter. Eval: 1.4671 s/iter. Total: 1.6720 s/iter. ETA=0:02:53\n",
            "[08/21 20:49:22 d2.evaluation.evaluator]: Inference done 49/150. Dataloading: 0.0018 s/iter. Inference: 0.2041 s/iter. Eval: 1.4811 s/iter. Total: 1.6874 s/iter. ETA=0:02:50\n",
            "[08/21 20:49:27 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0018 s/iter. Inference: 0.2021 s/iter. Eval: 1.4602 s/iter. Total: 1.6645 s/iter. ETA=0:02:41\n",
            "[08/21 20:49:33 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0018 s/iter. Inference: 0.1980 s/iter. Eval: 1.4158 s/iter. Total: 1.6159 s/iter. ETA=0:02:28\n",
            "[08/21 20:49:40 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0018 s/iter. Inference: 0.1984 s/iter. Eval: 1.4225 s/iter. Total: 1.6231 s/iter. ETA=0:02:22\n",
            "[08/21 20:49:45 d2.evaluation.evaluator]: Inference done 65/150. Dataloading: 0.0018 s/iter. Inference: 0.1987 s/iter. Eval: 1.4265 s/iter. Total: 1.6274 s/iter. ETA=0:02:18\n",
            "[08/21 20:49:51 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0018 s/iter. Inference: 0.1989 s/iter. Eval: 1.4387 s/iter. Total: 1.6398 s/iter. ETA=0:02:14\n",
            "[08/21 20:49:57 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0018 s/iter. Inference: 0.1982 s/iter. Eval: 1.4290 s/iter. Total: 1.6294 s/iter. ETA=0:02:07\n",
            "[08/21 20:50:03 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0018 s/iter. Inference: 0.1986 s/iter. Eval: 1.4312 s/iter. Total: 1.6320 s/iter. ETA=0:02:00\n",
            "[08/21 20:50:08 d2.evaluation.evaluator]: Inference done 79/150. Dataloading: 0.0018 s/iter. Inference: 0.1990 s/iter. Eval: 1.4324 s/iter. Total: 1.6336 s/iter. ETA=0:01:55\n",
            "[08/21 20:50:14 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0018 s/iter. Inference: 0.1952 s/iter. Eval: 1.3915 s/iter. Total: 1.5889 s/iter. ETA=0:01:43\n",
            "[08/21 20:50:20 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0018 s/iter. Inference: 0.1927 s/iter. Eval: 1.3637 s/iter. Total: 1.5586 s/iter. ETA=0:01:33\n",
            "[08/21 20:50:25 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0018 s/iter. Inference: 0.1875 s/iter. Eval: 1.3057 s/iter. Total: 1.4954 s/iter. ETA=0:01:19\n",
            "[08/21 20:50:30 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0018 s/iter. Inference: 0.1845 s/iter. Eval: 1.2724 s/iter. Total: 1.4591 s/iter. ETA=0:01:08\n",
            "[08/21 20:50:36 d2.evaluation.evaluator]: Inference done 107/150. Dataloading: 0.0018 s/iter. Inference: 0.1842 s/iter. Eval: 1.2682 s/iter. Total: 1.4546 s/iter. ETA=0:01:02\n",
            "[08/21 20:50:41 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0018 s/iter. Inference: 0.1838 s/iter. Eval: 1.2635 s/iter. Total: 1.4495 s/iter. ETA=0:00:56\n",
            "[08/21 20:50:48 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0018 s/iter. Inference: 0.1847 s/iter. Eval: 1.2718 s/iter. Total: 1.4588 s/iter. ETA=0:00:51\n",
            "[08/21 20:50:53 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0018 s/iter. Inference: 0.1876 s/iter. Eval: 1.3028 s/iter. Total: 1.4926 s/iter. ETA=0:00:50\n",
            "[08/21 20:50:59 d2.evaluation.evaluator]: Inference done 118/150. Dataloading: 0.0028 s/iter. Inference: 0.1900 s/iter. Eval: 1.3294 s/iter. Total: 1.5226 s/iter. ETA=0:00:48\n",
            "[08/21 20:51:05 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0028 s/iter. Inference: 0.1898 s/iter. Eval: 1.3256 s/iter. Total: 1.5186 s/iter. ETA=0:00:42\n",
            "[08/21 20:51:11 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0028 s/iter. Inference: 0.1910 s/iter. Eval: 1.3361 s/iter. Total: 1.5303 s/iter. ETA=0:00:38\n",
            "[08/21 20:51:17 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0027 s/iter. Inference: 0.1888 s/iter. Eval: 1.3113 s/iter. Total: 1.5033 s/iter. ETA=0:00:28\n",
            "[08/21 20:51:22 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0027 s/iter. Inference: 0.1895 s/iter. Eval: 1.3172 s/iter. Total: 1.5098 s/iter. ETA=0:00:24\n",
            "[08/21 20:51:28 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0027 s/iter. Inference: 0.1865 s/iter. Eval: 1.2847 s/iter. Total: 1.4743 s/iter. ETA=0:00:13\n",
            "[08/21 20:51:34 d2.evaluation.evaluator]: Inference done 146/150. Dataloading: 0.0026 s/iter. Inference: 0.1858 s/iter. Eval: 1.2764 s/iter. Total: 1.4653 s/iter. ETA=0:00:05\n",
            "[08/21 20:51:37 d2.evaluation.evaluator]: Total inference time: 0:03:29.248071 (1.443090 s / iter per device, on 1 devices)\n",
            "[08/21 20:51:37 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:26 (0.183878 s / iter per device, on 1 devices)\n",
            "[08/21 20:51:37 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[08/21 20:51:37 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[08/21 20:51:37 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.42s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.19s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.068\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.136\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.061\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.074\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.118\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.181\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.290\n",
            "[08/21 20:51:37 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 6.842 | 13.601 | 5.745  | 3.823 | 6.061 | 7.400 |\n",
            "[08/21 20:51:37 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 11.374 | Other      | 13.654 | Bottle     | 14.041 |\n",
            "| Bottle cap            | 5.697  | Cup        | 8.126  | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 12.594 | Pop tab    | 0.000  | Straw      | 0.000  |\n",
            "| Cigarette             | 2.931  |            |        |            |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.10s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.60s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.18s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.042\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.070\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.074\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.022\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.022\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.095\n",
            "[08/21 20:51:39 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 1.767 | 5.993  | 0.350  | 0.039 | 0.553 | 2.747 |\n",
            "[08/21 20:51:39 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 0.574 | Other      | 3.384 | Bottle     | 5.013 |\n",
            "| Bottle cap            | 4.559 | Cup        | 0.072 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 4.064 | Pop tab    | 0.000 | Straw      | 0.000 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "[08/21 20:51:39 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[08/21 20:51:39 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[08/21 20:51:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:51:39 d2.evaluation.testing]: copypaste: 6.8416,13.6007,5.7447,3.8230,6.0605,7.3995\n",
            "[08/21 20:51:39 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[08/21 20:51:39 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:51:39 d2.evaluation.testing]: copypaste: 1.7666,5.9930,0.3500,0.0392,0.5527,2.7469\n",
            "[08/21 20:51:39 d2.utils.events]:  eta: 0:01:40  iter: 299  total_loss: 0.8837  loss_cls: 0.3655  loss_box_reg: 0.4525  loss_mask: 0.02002  loss_rpn_cls: 0.0531  loss_rpn_loc: 0.02645    time: 0.6919  last_time: 0.3884  data_time: 0.2860  last_data_time: 0.1516   lr: 0.0005984  max_mem: 8770M\n",
            "[08/21 20:51:51 d2.utils.events]:  eta: 0:01:30  iter: 319  total_loss: 0.8501  loss_cls: 0.3683  loss_box_reg: 0.3312  loss_mask: 0.01607  loss_rpn_cls: 0.02827  loss_rpn_loc: 0.018    time: 0.6874  last_time: 0.2046  data_time: 0.3316  last_data_time: 0.0044   lr: 0.00063836  max_mem: 8770M\n",
            "[08/21 20:52:05 d2.utils.events]:  eta: 0:01:20  iter: 339  total_loss: 0.8513  loss_cls: 0.331  loss_box_reg: 0.3176  loss_mask: 0.01942  loss_rpn_cls: 0.07977  loss_rpn_loc: 0.0357    time: 0.6868  last_time: 0.2973  data_time: 0.4146  last_data_time: 0.0108   lr: 0.00067832  max_mem: 8770M\n",
            "[08/21 20:52:17 d2.utils.events]:  eta: 0:01:09  iter: 359  total_loss: 0.7293  loss_cls: 0.3152  loss_box_reg: 0.2908  loss_mask: 0.01474  loss_rpn_cls: 0.05697  loss_rpn_loc: 0.01297    time: 0.6820  last_time: 1.1966  data_time: 0.3359  last_data_time: 0.9572   lr: 0.00071828  max_mem: 8770M\n",
            "[08/21 20:52:33 d2.utils.events]:  eta: 0:00:59  iter: 379  total_loss: 0.67  loss_cls: 0.3415  loss_box_reg: 0.2934  loss_mask: 0.01427  loss_rpn_cls: 0.02853  loss_rpn_loc: 0.01423    time: 0.6900  last_time: 1.8237  data_time: 0.5684  last_data_time: 1.3787   lr: 0.00075824  max_mem: 8770M\n",
            "[08/21 20:52:45 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/mapped_annotations_0_val.json\n",
            "[08/21 20:52:45 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[08/21 20:52:45 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[08/21 20:52:45 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[08/21 20:52:45 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[08/21 20:52:45 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[08/21 20:52:45 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:52:53 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0015 s/iter. Inference: 0.1452 s/iter. Eval: 0.8276 s/iter. Total: 0.9742 s/iter. ETA=0:02:15\n",
            "[08/21 20:52:58 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0017 s/iter. Inference: 0.1286 s/iter. Eval: 0.6479 s/iter. Total: 0.7785 s/iter. ETA=0:01:41\n",
            "[08/21 20:53:06 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0017 s/iter. Inference: 0.1425 s/iter. Eval: 0.8805 s/iter. Total: 1.0250 s/iter. ETA=0:02:10\n",
            "[08/21 20:53:12 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0018 s/iter. Inference: 0.1550 s/iter. Eval: 1.0105 s/iter. Total: 1.1676 s/iter. ETA=0:02:24\n",
            "[08/21 20:53:19 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0018 s/iter. Inference: 0.1577 s/iter. Eval: 1.0355 s/iter. Total: 1.1952 s/iter. ETA=0:02:22\n",
            "[08/21 20:53:24 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0018 s/iter. Inference: 0.1629 s/iter. Eval: 1.0858 s/iter. Total: 1.2509 s/iter. ETA=0:02:25\n",
            "[08/21 20:53:32 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0018 s/iter. Inference: 0.1706 s/iter. Eval: 1.1696 s/iter. Total: 1.3424 s/iter. ETA=0:02:30\n",
            "[08/21 20:53:37 d2.evaluation.evaluator]: Inference done 42/150. Dataloading: 0.0018 s/iter. Inference: 0.1704 s/iter. Eval: 1.1621 s/iter. Total: 1.3347 s/iter. ETA=0:02:24\n",
            "[08/21 20:53:43 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0018 s/iter. Inference: 0.1671 s/iter. Eval: 1.1213 s/iter. Total: 1.2906 s/iter. ETA=0:02:11\n",
            "[08/21 20:53:48 d2.evaluation.evaluator]: Inference done 53/150. Dataloading: 0.0019 s/iter. Inference: 0.1656 s/iter. Eval: 1.1013 s/iter. Total: 1.2691 s/iter. ETA=0:02:03\n",
            "[08/21 20:53:54 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0018 s/iter. Inference: 0.1648 s/iter. Eval: 1.0905 s/iter. Total: 1.2575 s/iter. ETA=0:01:55\n",
            "[08/21 20:54:00 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0018 s/iter. Inference: 0.1671 s/iter. Eval: 1.1122 s/iter. Total: 1.2815 s/iter. ETA=0:01:52\n",
            "[08/21 20:54:06 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0018 s/iter. Inference: 0.1673 s/iter. Eval: 1.1199 s/iter. Total: 1.2894 s/iter. ETA=0:01:48\n",
            "[08/21 20:54:11 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0018 s/iter. Inference: 0.1673 s/iter. Eval: 1.1177 s/iter. Total: 1.2872 s/iter. ETA=0:01:42\n",
            "[08/21 20:54:17 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0018 s/iter. Inference: 0.1680 s/iter. Eval: 1.1216 s/iter. Total: 1.2917 s/iter. ETA=0:01:38\n",
            "[08/21 20:54:22 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0023 s/iter. Inference: 0.1680 s/iter. Eval: 1.1192 s/iter. Total: 1.2898 s/iter. ETA=0:01:32\n",
            "[08/21 20:54:27 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0022 s/iter. Inference: 0.1666 s/iter. Eval: 1.1027 s/iter. Total: 1.2719 s/iter. ETA=0:01:25\n",
            "[08/21 20:54:32 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0022 s/iter. Inference: 0.1661 s/iter. Eval: 1.0956 s/iter. Total: 1.2643 s/iter. ETA=0:01:18\n",
            "[08/21 20:54:38 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0022 s/iter. Inference: 0.1580 s/iter. Eval: 1.0036 s/iter. Total: 1.1640 s/iter. ETA=0:00:58\n",
            "[08/21 20:54:44 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0025 s/iter. Inference: 0.1569 s/iter. Eval: 0.9902 s/iter. Total: 1.1500 s/iter. ETA=0:00:50\n",
            "[08/21 20:54:50 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0025 s/iter. Inference: 0.1583 s/iter. Eval: 1.0035 s/iter. Total: 1.1646 s/iter. ETA=0:00:46\n",
            "[08/21 20:54:55 d2.evaluation.evaluator]: Inference done 114/150. Dataloading: 0.0025 s/iter. Inference: 0.1587 s/iter. Eval: 1.0064 s/iter. Total: 1.1679 s/iter. ETA=0:00:42\n",
            "[08/21 20:55:03 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0025 s/iter. Inference: 0.1620 s/iter. Eval: 1.0425 s/iter. Total: 1.2073 s/iter. ETA=0:00:39\n",
            "[08/21 20:55:09 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0024 s/iter. Inference: 0.1630 s/iter. Eval: 1.0508 s/iter. Total: 1.2165 s/iter. ETA=0:00:35\n",
            "[08/21 20:55:14 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 1.0583 s/iter. Total: 1.2248 s/iter. ETA=0:00:30\n",
            "[08/21 20:55:19 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 1.0502 s/iter. Total: 1.2162 s/iter. ETA=0:00:24\n",
            "[08/21 20:55:25 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0024 s/iter. Inference: 0.1635 s/iter. Eval: 1.0519 s/iter. Total: 1.2181 s/iter. ETA=0:00:19\n",
            "[08/21 20:55:30 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0023 s/iter. Inference: 0.1608 s/iter. Eval: 1.0224 s/iter. Total: 1.1859 s/iter. ETA=0:00:09\n",
            "[08/21 20:55:35 d2.evaluation.evaluator]: Inference done 150/150. Dataloading: 0.0023 s/iter. Inference: 0.1583 s/iter. Eval: 0.9955 s/iter. Total: 1.1565 s/iter. ETA=0:00:00\n",
            "[08/21 20:55:35 d2.evaluation.evaluator]: Total inference time: 0:02:47.753171 (1.156918 s / iter per device, on 1 devices)\n",
            "[08/21 20:55:35 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:22 (0.158313 s / iter per device, on 1 devices)\n",
            "[08/21 20:55:35 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[08/21 20:55:35 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[08/21 20:55:35 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.17s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.206\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.354\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.369\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.082\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.251\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n",
            "[08/21 20:55:36 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 13.726 | 20.605 | 14.409 | 3.552 | 8.378 | 15.653 |\n",
            "[08/21 20:55:36 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 22.703 | Other      | 17.175 | Bottle     | 25.388 |\n",
            "| Bottle cap            | 11.289 | Cup        | 8.023  | Lid        | 24.186 |\n",
            "| Plastic bag + wrapper | 18.532 | Pop tab    | 1.584  | Straw      | 4.250  |\n",
            "| Cigarette             | 4.127  |            |        |            |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.08s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.55s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.16s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.043\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.102\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.022\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.137\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.096\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172\n",
            "[08/21 20:55:37 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 4.302 | 10.184 | 2.217  | 0.074 | 1.168 | 5.715 |\n",
            "[08/21 20:55:37 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP     |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:-------|\n",
            "| Can                   | 4.496 | Other      | 7.135 | Bottle     | 14.858 |\n",
            "| Bottle cap            | 6.611 | Cup        | 2.609 | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 7.312 | Pop tab    | 0.000 | Straw      | 0.000  |\n",
            "| Cigarette             | 0.000 |            |       |            |        |\n",
            "[08/21 20:55:37 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[08/21 20:55:37 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[08/21 20:55:37 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:55:37 d2.evaluation.testing]: copypaste: 13.7258,20.6046,14.4089,3.5525,8.3777,15.6532\n",
            "[08/21 20:55:37 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[08/21 20:55:37 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:55:37 d2.evaluation.testing]: copypaste: 4.3022,10.1839,2.2172,0.0737,1.1680,5.7147\n",
            "[08/21 20:55:37 d2.utils.events]:  eta: 0:00:49  iter: 399  total_loss: 0.8781  loss_cls: 0.4239  loss_box_reg: 0.3221  loss_mask: 0.0164  loss_rpn_cls: 0.0533  loss_rpn_loc: 0.02933    time: 0.6830  last_time: 0.3040  data_time: 0.2674  last_data_time: 0.0607   lr: 0.0007982  max_mem: 8770M\n",
            "[08/21 20:55:49 d2.utils.events]:  eta: 0:00:39  iter: 419  total_loss: 0.8158  loss_cls: 0.4171  loss_box_reg: 0.293  loss_mask: 0.01301  loss_rpn_cls: 0.04942  loss_rpn_loc: 0.03212    time: 0.6803  last_time: 0.2131  data_time: 0.3729  last_data_time: 0.0010   lr: 0.00083816  max_mem: 8770M\n",
            "[08/21 20:56:04 d2.utils.events]:  eta: 0:00:29  iter: 439  total_loss: 0.7559  loss_cls: 0.4072  loss_box_reg: 0.2891  loss_mask: 0.01343  loss_rpn_cls: 0.0196  loss_rpn_loc: 0.008791    time: 0.6826  last_time: 0.4026  data_time: 0.4509  last_data_time: 0.1238   lr: 0.00087812  max_mem: 8770M\n",
            "[08/21 20:56:18 d2.utils.events]:  eta: 0:00:19  iter: 459  total_loss: 1.034  loss_cls: 0.5172  loss_box_reg: 0.3856  loss_mask: 0.01478  loss_rpn_cls: 0.02341  loss_rpn_loc: 0.0163    time: 0.6842  last_time: 0.7778  data_time: 0.4704  last_data_time: 0.4710   lr: 0.00091808  max_mem: 8770M\n",
            "[08/21 20:56:32 d2.utils.events]:  eta: 0:00:09  iter: 479  total_loss: 0.7074  loss_cls: 0.3581  loss_box_reg: 0.2967  loss_mask: 0.01239  loss_rpn_cls: 0.03706  loss_rpn_loc: 0.01437    time: 0.6849  last_time: 0.2587  data_time: 0.4204  last_data_time: 0.0014   lr: 0.00095804  max_mem: 8770M\n",
            "[08/21 20:56:48 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 0.8392  loss_cls: 0.4172  loss_box_reg: 0.3327  loss_mask: 0.01195  loss_rpn_cls: 0.02936  loss_rpn_loc: 0.01065    time: 0.6872  last_time: 2.3637  data_time: 0.4714  last_data_time: 2.1126   lr: 0.000998  max_mem: 8770M\n",
            "[08/21 20:56:48 d2.engine.hooks]: Overall training speed: 498 iterations in 0:05:42 (0.6873 s / it)\n",
            "[08/21 20:56:48 d2.engine.hooks]: Total training time: 0:22:42 (0:17:00 on hooks)\n",
            "[08/21 20:56:48 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/mapped_annotations_0_val.json\n",
            "[08/21 20:56:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[08/21 20:56:48 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[08/21 20:56:48 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[08/21 20:56:48 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[08/21 20:56:48 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[08/21 20:56:48 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[08/21 20:56:55 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0015 s/iter. Inference: 0.1315 s/iter. Eval: 0.6841 s/iter. Total: 0.8170 s/iter. ETA=0:01:53\n",
            "[08/21 20:57:01 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0017 s/iter. Inference: 0.1135 s/iter. Eval: 0.5087 s/iter. Total: 0.6240 s/iter. ETA=0:01:19\n",
            "[08/21 20:57:08 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0017 s/iter. Inference: 0.1392 s/iter. Eval: 0.7925 s/iter. Total: 0.9337 s/iter. ETA=0:01:57\n",
            "[08/21 20:57:14 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0017 s/iter. Inference: 0.1390 s/iter. Eval: 0.7929 s/iter. Total: 0.9338 s/iter. ETA=0:01:51\n",
            "[08/21 20:57:20 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0017 s/iter. Inference: 0.1404 s/iter. Eval: 0.8100 s/iter. Total: 0.9524 s/iter. ETA=0:01:48\n",
            "[08/21 20:57:25 d2.evaluation.evaluator]: Inference done 39/150. Dataloading: 0.0018 s/iter. Inference: 0.1475 s/iter. Eval: 0.8903 s/iter. Total: 1.0398 s/iter. ETA=0:01:55\n",
            "[08/21 20:57:32 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0052 s/iter. Inference: 0.1453 s/iter. Eval: 0.8631 s/iter. Total: 1.0140 s/iter. ETA=0:01:45\n",
            "[08/21 20:57:38 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0049 s/iter. Inference: 0.1478 s/iter. Eval: 0.8890 s/iter. Total: 1.0420 s/iter. ETA=0:01:43\n",
            "[08/21 20:57:44 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0044 s/iter. Inference: 0.1439 s/iter. Eval: 0.8469 s/iter. Total: 0.9955 s/iter. ETA=0:01:30\n",
            "[08/21 20:57:51 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0042 s/iter. Inference: 0.1480 s/iter. Eval: 0.8906 s/iter. Total: 1.0431 s/iter. ETA=0:01:30\n",
            "[08/21 20:57:56 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0040 s/iter. Inference: 0.1461 s/iter. Eval: 0.8786 s/iter. Total: 1.0290 s/iter. ETA=0:01:23\n",
            "[08/21 20:58:02 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0038 s/iter. Inference: 0.1457 s/iter. Eval: 0.8719 s/iter. Total: 1.0217 s/iter. ETA=0:01:16\n",
            "[08/21 20:58:07 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0036 s/iter. Inference: 0.1412 s/iter. Eval: 0.8186 s/iter. Total: 0.9637 s/iter. ETA=0:01:02\n",
            "[08/21 20:58:12 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0034 s/iter. Inference: 0.1371 s/iter. Eval: 0.7725 s/iter. Total: 0.9132 s/iter. ETA=0:00:50\n",
            "[08/21 20:58:17 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0032 s/iter. Inference: 0.1332 s/iter. Eval: 0.7292 s/iter. Total: 0.8659 s/iter. ETA=0:00:38\n",
            "[08/21 20:58:24 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0031 s/iter. Inference: 0.1349 s/iter. Eval: 0.7454 s/iter. Total: 0.8836 s/iter. ETA=0:00:34\n",
            "[08/21 20:58:31 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0031 s/iter. Inference: 0.1359 s/iter. Eval: 0.7690 s/iter. Total: 0.9083 s/iter. ETA=0:00:30\n",
            "[08/21 20:58:38 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0031 s/iter. Inference: 0.1385 s/iter. Eval: 0.8033 s/iter. Total: 0.9450 s/iter. ETA=0:00:29\n",
            "[08/21 20:58:43 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0030 s/iter. Inference: 0.1371 s/iter. Eval: 0.7864 s/iter. Total: 0.9268 s/iter. ETA=0:00:21\n",
            "[08/21 20:58:49 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0029 s/iter. Inference: 0.1369 s/iter. Eval: 0.7806 s/iter. Total: 0.9207 s/iter. ETA=0:00:14\n",
            "[08/21 20:58:54 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0028 s/iter. Inference: 0.1343 s/iter. Eval: 0.7514 s/iter. Total: 0.8888 s/iter. ETA=0:00:04\n",
            "[08/21 20:58:57 d2.evaluation.evaluator]: Total inference time: 0:02:06.918757 (0.875302 s / iter per device, on 1 devices)\n",
            "[08/21 20:58:57 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:19 (0.133144 s / iter per device, on 1 devices)\n",
            "[08/21 20:58:57 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[08/21 20:58:57 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[08/21 20:58:57 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.33s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.15s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.154\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.332\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.338\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.043\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            "[08/21 20:58:58 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 13.738 | 21.889 | 15.514 | 2.150 | 5.989 | 15.435 |\n",
            "[08/21 20:58:58 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 23.581 | Other      | 15.981 | Bottle     | 32.238 |\n",
            "| Bottle cap            | 10.720 | Cup        | 9.809  | Lid        | 12.221 |\n",
            "| Plastic bag + wrapper | 24.276 | Pop tab    | 2.376  | Straw      | 0.000  |\n",
            "| Cigarette             | 6.173  |            |        |            |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.07s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.45s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.14s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.078\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.151\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.072\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.097\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.133\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.193\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.195\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.019\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.131\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.231\n",
            "[08/21 20:58:58 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 7.752 | 15.084 | 7.218  | 0.076 | 1.600 | 9.705 |\n",
            "[08/21 20:58:58 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP    | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:------|:-----------|:-------|\n",
            "| Can                   | 11.361 | Other      | 9.928 | Bottle     | 25.751 |\n",
            "| Bottle cap            | 8.480  | Cup        | 6.358 | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 15.636 | Pop tab    | 0.000 | Straw      | 0.000  |\n",
            "| Cigarette             | 0.000  |            |       |            |        |\n",
            "[08/21 20:58:58 d2.engine.defaults]: Evaluation results for taco10_val in csv format:\n",
            "[08/21 20:58:58 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[08/21 20:58:58 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:58:58 d2.evaluation.testing]: copypaste: 13.7376,21.8895,15.5138,2.1497,5.9885,15.4349\n",
            "[08/21 20:58:58 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[08/21 20:58:58 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[08/21 20:58:58 d2.evaluation.testing]: copypaste: 7.7515,15.0844,7.2177,0.0764,1.6001,9.7052\n"
          ]
        }
      ],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"taco10_train\",)\n",
        "cfg.DATASETS.TEST = (\"taco10_val\",)\n",
        "cfg.TEST.EVAL_PERIOD = 100\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.MAX_ITER = 500\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}