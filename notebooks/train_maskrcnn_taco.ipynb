{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "from imgaug import augmenters as iaa\n",
    "from taco.dataset import Taco\n",
    "import src.maskrcnn.model as modellib\n",
    "from src.maskrcnn.model import MaskRCNN\n",
    "from src.maskrcnn.config import Config\n",
    "import src.maskrcnn.visualize as visualize\n",
    "import src.maskrcnn.utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"./models\") # Root directory of the models.\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\") # Path to trained weights file.\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\") # Directory to save logs and model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMAND = \"train\" # \"evaluate\" | \"test\"\n",
    "MODEL = \"coco\" # or path to weights .h5 file\n",
    "DATASET = None # directory of the dataset\n",
    "ROUND = None # split number (int)\n",
    "LRATE = 0.001\n",
    "USE_AUG = False\n",
    "AUG = None\n",
    "USE_TRANSPLANTS = None\n",
    "CLASS_MAP = None # path to class mapping to target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Command: \", COMMAND)\n",
    "print(\"Model: \", MODEL)\n",
    "print(\"Dataset: \", DATASET)\n",
    "print(\"Logs: \", DEFAULT_LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read map of target classes.\n",
    "class_map = {}\n",
    "map_to_one_class = {}\n",
    "with open(CLASS_MAP) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    class_map = {row[0]: row[1] for row in reader}\n",
    "    map_to_one_class = {c: \"Litter\" for c in class_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCO Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_coco_results(dataset, image_ids, rois, class_ids, scores, masks):\n",
    "    \"\"\"Arrange resutls to match COCO specs in http://cocodataset.org/#format\"\"\"\n",
    "    # If no results, return an empty list.\n",
    "    if rois is None:\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for image_id in image_ids:\n",
    "        # Loop through detections.\n",
    "        for i in range(rois.shape[0]):\n",
    "            class_id = class_ids[i]\n",
    "            score = scores[i]\n",
    "            bbox = np.around(rois[i], 1)\n",
    "            mask = masks[:, :, i]\n",
    "\n",
    "            result = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": (\n",
    "                    dataset.get_source_class_id(class_id, \"taco\")\n",
    "                    if dataset.num_classes > 2\n",
    "                    else 1\n",
    "                ),\n",
    "                \"bbox\": [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]],\n",
    "                \"score\": score,\n",
    "                \"segmentation\": maskUtils.encode(np.asfortranarray(mask)),\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_coco(model, dataset, coco, eval_type=\"bbox\", limit=0, image_ids=None):\n",
    "    \"\"\"Runs official COCO evaluation.\n",
    "    dataset: A Dataset object with valiadtion data\n",
    "    eval_type: \"bbox\" or \"segm\" for bounding box or segmentation evaluation\n",
    "    limit: if not 0, it's the number of images to use for evaluation\n",
    "    \"\"\"\n",
    "    # Pick TACO images from the dataset.\n",
    "    image_ids = image_ids or dataset.image_ids\n",
    "\n",
    "    # Limit to a subset.\n",
    "    if limit:\n",
    "        image_ids = image_ids[:limit]\n",
    "\n",
    "    # Get corresponding TACO image IDs.\n",
    "    taco_image_ids = [dataset.image_info[id][\"id\"] for id in image_ids]\n",
    "\n",
    "    t_prediction = 0\n",
    "    t_start = time.time()\n",
    "    results = []\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        # Load image.\n",
    "        image = dataset.load_image(image_id)\n",
    "\n",
    "        # Run detection.\n",
    "        t = time.time()\n",
    "        r = model.detect([image], verbose=0)[0]\n",
    "        # r = utils.fuse_instances(r)\n",
    "        t_prediction += time.time() - t\n",
    "\n",
    "        if not model.config.DETECTION_SCORE_RATIO:\n",
    "            scores = r[\"scores\"]\n",
    "        else:\n",
    "            scores = r[\"scores\"] / (r[\"full_scores\"][:, 0] + 0.0001)\n",
    "\n",
    "        # Convert results to COCO format.\n",
    "        # Cast masks to uint8 because COCO tools errors out on bool.\n",
    "        image_results = build_coco_results(\n",
    "            dataset,\n",
    "            taco_image_ids[i : i + 1],\n",
    "            r[\"rois\"],\n",
    "            r[\"class_ids\"],\n",
    "            scores,\n",
    "            r[\"masks\"].astype(np.uint8),\n",
    "        )\n",
    "        results.extend(image_results)\n",
    "\n",
    "    # Load results. This modifies results with additional attributes.\n",
    "    coco_results = coco.loadRes(results)\n",
    "\n",
    "    # utils.compute_confusion_matrix(coco_results, coco)\n",
    "\n",
    "    # Evaluate.\n",
    "    cocoEval = COCOeval(coco, coco_results, eval_type)\n",
    "    cocoEval.params.imgIds = taco_image_ids\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n",
    "    print(\n",
    "        \"Prediction time: {}. Average {}/image\".format(\n",
    "            t_prediction, t_prediction / len(image_ids)\n",
    "        )\n",
    "    )\n",
    "    print(\"Total time: \", time.time() - t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(model, dataset, nr_images):\n",
    "    for i in range(nr_images):\n",
    "        image_id = (\n",
    "            dataset.image_ids[i]\n",
    "            if nr_images == len(dataset.image_ids)\n",
    "            else random.choice(dataset.image_ids)\n",
    "        )\n",
    "\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(\n",
    "            dataset, config, image_id, use_mini_mask=False\n",
    "        )\n",
    "        info = dataset.image_info[image_id]\n",
    "\n",
    "        r = model.detect([image], verbose=0)[0]\n",
    "\n",
    "        print(r[\"class_ids\"].shape)\n",
    "        if r[\"class_ids\"].shape[0] > 0:\n",
    "            r_fused = utils.fuse_instances(r)\n",
    "        else:\n",
    "            r_fused = r\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 16))\n",
    "\n",
    "        # Display predictions.\n",
    "        visualize.display_instances(\n",
    "            image,\n",
    "            r[\"rois\"],\n",
    "            r[\"masks\"],\n",
    "            r[\"class_ids\"],\n",
    "            dataset.class_names,\n",
    "            r[\"scores\"],\n",
    "            title=\"Predictions\",\n",
    "            ax=ax1,\n",
    "        )\n",
    "\n",
    "        visualize.display_instances(\n",
    "            image,\n",
    "            r_fused[\"rois\"],\n",
    "            r_fused[\"masks\"],\n",
    "            r_fused[\"class_ids\"],\n",
    "            dataset.class_names,\n",
    "            r_fused[\"scores\"],\n",
    "            title=\"Predictions fused\",\n",
    "            ax=ax2,\n",
    "        )\n",
    "\n",
    "        # # Display ground truth.\n",
    "        visualize.display_instances(\n",
    "            image,\n",
    "            gt_bbox,\n",
    "            gt_mask,\n",
    "            gt_class_id,\n",
    "            dataset.class_names,\n",
    "            title=\"GT\",\n",
    "            ax=ax3,\n",
    "        )\n",
    "\n",
    "        # Voil√†.\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMMAND == \"train\":\n",
    "    # Training dataset.\n",
    "    dataset_train = Taco()\n",
    "    dataset_train.load_taco(\n",
    "        DATASET, ROUND, \"train\", class_map=class_map, auto_download=None\n",
    "    )\n",
    "    if USE_TRANSPLANTS:\n",
    "        dataset_train.add_transplanted_dataset(\n",
    "            USE_TRANSPLANTS, class_map=class_map\n",
    "        )\n",
    "    dataset_train.prepare()\n",
    "    nr_classes = dataset_train.num_classes\n",
    "\n",
    "    # Validation dataset.\n",
    "    dataset_val = Taco()\n",
    "    dataset_val.load_taco(\n",
    "        DATASET, ROUND, \"val\", class_map=class_map, auto_download=None\n",
    "    )\n",
    "    dataset_val.prepare()\n",
    "else:\n",
    "    # Test dataset.\n",
    "    dataset_test = Taco()\n",
    "    taco = dataset_test.load_taco(\n",
    "        DATASET, ROUND, \"test\", class_map=class_map, return_taco=True\n",
    "    )\n",
    "    dataset_test.prepare()\n",
    "    nr_classes = dataset_test.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMMAND == \"train\":\n",
    "    class TacoTrainConfig(Config):\n",
    "        NAME = \"taco\"\n",
    "        IMAGES_PER_GPU = 2\n",
    "        GPU_COUNT = 1\n",
    "        STEPS_PER_EPOCH = min(\n",
    "            1000, int(dataset_train.num_images / (IMAGES_PER_GPU * GPU_COUNT))\n",
    "        )\n",
    "        USE_MINI_MASK = True\n",
    "        MINI_MASK_SHAPE = (512, 512)\n",
    "        NUM_CLASSES = nr_classes\n",
    "        LEARNING_RATE = LRATE\n",
    "\n",
    "    config = TacoTrainConfig()\n",
    "else:\n",
    "    class TacoTestConfig(Config):\n",
    "        NAME = \"taco\"\n",
    "        GPU_COUNT = 1\n",
    "        IMAGES_PER_GPU = 1\n",
    "        DETECTION_MIN_CONFIDENCE = 0 if COMMAND == \"evaluate\" else 10\n",
    "        NUM_CLASSES = nr_classes\n",
    "        USE_OBJECT_ZOOM = False\n",
    "\n",
    "    config = TacoTestConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMMAND == \"train\":\n",
    "    model = MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)\n",
    "else:\n",
    "    model = MaskRCNN(mode=\"inference\", config=config, model_dir=DEFAULT_LOGS_DIR)\n",
    "\n",
    "# Select weights file to load.\n",
    "if MODEL.lower() == \"coco\":\n",
    "    model_path = COCO_MODEL_PATH\n",
    "    # Download weights file.\n",
    "    if not os.path.exists(model_path):\n",
    "        utils.download_trained_weights(model_path)\n",
    "elif MODEL.lower() == \"last\":\n",
    "    # Find last trained weights.\n",
    "    model_path = model.find_last()[1]\n",
    "elif MODEL.lower() == \"imagenet\":\n",
    "    # Start from ImageNet trained weights.\n",
    "    model_path = model.get_imagenet_weights()\n",
    "else:\n",
    "    _, model_path = model.get_last_checkpoint(MODEL)\n",
    "\n",
    "# Load weights.\n",
    "if MODEL.lower() == \"coco\":\n",
    "    # Exclude the last layers because they require a matching number of classes.\n",
    "    model.load_weights(\n",
    "        model_path,\n",
    "        None,\n",
    "        by_name=True,\n",
    "        exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"],\n",
    "    )\n",
    "else:\n",
    "    model.load_weights(model_path, model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMMAND == \"train\":\n",
    "    if AUG:\n",
    "        if not config.USE_OBJECT_ZOOM:\n",
    "            # Image Augmentation Pipeline.\n",
    "            augmentation_pipeline = iaa.Sequential(\n",
    "                [\n",
    "                    iaa.AdditiveGaussianNoise(scale=0.01 * 255, name=\"AWGN\"),\n",
    "                    iaa.GaussianBlur(sigma=(0.0, 3.0), name=\"Blur\"),\n",
    "                    # iaa.Dropout([0.0, 0.05], name='Dropout'), # drop 0-5% of all pixels\n",
    "                    iaa.Fliplr(0.5),\n",
    "                    iaa.Add((-20, 20), name=\"Add\"),\n",
    "                    iaa.Multiply((0.8, 1.2), name=\"Multiply\"),\n",
    "                    iaa.Affine(scale=(0.8, 2.0)),\n",
    "                    iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
    "                    iaa.Affine(rotate=(-45, 45)),  # rotate by -45 to 45 degrees\n",
    "                ],\n",
    "                random_order=True,\n",
    "            )\n",
    "        else:\n",
    "            # Nevermind the image translation and scaling as this is done already during zoom in.\n",
    "            augmentation_pipeline = iaa.Sequential(\n",
    "                [\n",
    "                    iaa.AdditiveGaussianNoise(scale=0.01 * 255, name=\"AWGN\"),\n",
    "                    iaa.GaussianBlur(sigma=(0.0, 3.0), name=\"Blur\"),\n",
    "                    # iaa.Dropout([0.0, 0.05], name='Dropout'), # drop 0-5% of all pixels\n",
    "                    iaa.Fliplr(0.5),\n",
    "                    iaa.Add((-20, 20), name=\"Add\"),\n",
    "                    iaa.Multiply((0.8, 1.2), name=\"Multiply\"),\n",
    "                    iaa.Affine(rotate=(-45, 45)),  # rotate by -45 to 45 degrees\n",
    "                ],\n",
    "                random_order=True,\n",
    "            )\n",
    "    else:\n",
    "        augmentation_pipeline = None\n",
    "\n",
    "        # Save training meta to log dir.\n",
    "        training_meta = {\n",
    "            \"number of classes\": nr_classes,\n",
    "            \"round\": ROUND,\n",
    "            \"use_augmentation\": AUG,\n",
    "            \"use_transplants\": USE_TRANSPLANTS != None,\n",
    "            \"learning_rate\": config.LEARNING_RATE,\n",
    "            \"layers_trained\": \"all\",\n",
    "        }\n",
    "\n",
    "        subdir = os.path.dirname(model.log_dir)\n",
    "        if not os.path.isdir(subdir):\n",
    "            os.mkdir(subdir)\n",
    "\n",
    "        if not os.path.isdir(model.log_dir):\n",
    "            os.mkdir(model.log_dir)\n",
    "\n",
    "        train_meta_file = model.log_dir + \"_meta.json\"\n",
    "        with open(train_meta_file, \"w+\") as f:\n",
    "            f.write(json.dumps(training_meta))\n",
    "\n",
    "        # Training all layers.\n",
    "        model.train(\n",
    "            dataset_train,\n",
    "            dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=100,\n",
    "            layers=\"all\",\n",
    "            augmentation=augmentation_pipeline,\n",
    "        )\n",
    "\n",
    "elif COMMAND == \"evaluate\":\n",
    "    nr_eval_images = len(dataset_test.image_ids)\n",
    "    print(\"Running COCO evaluation on {} images.\".format(nr_eval_images))\n",
    "    evaluate_coco(model, dataset_test, taco, \"segm\", limit=0)\n",
    "\n",
    "elif COMMAND == \"test\":\n",
    "    test_dataset(model, dataset_test, len(dataset_test.image_ids))\n",
    "\n",
    "else:\n",
    "    print(\"'{}' is not recognized. \" \"Use 'train' or 'evaluate'\".format(COMMAND))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
