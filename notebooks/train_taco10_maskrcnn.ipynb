{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_xSFw7ZtgDK",
        "outputId": "9f2c25dd-ce47-4d93-c18f-e4e6bfbb39db"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SzFCz2gt-Za",
        "outputId": "d82a9c83-d22d-4ad5-aad4-ee10b6b3d573"
      },
      "outputs": [],
      "source": [
        "!python -m pip install pyyaml\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcQRkGfyuKeB",
        "outputId": "568d556f-4322-4632-8095-8c0516bb283d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.3 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U8XXSzz1uhmq"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CluGLxd3wJq7"
      },
      "outputs": [],
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CdMWUSrTuvAb"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco_train\", {}, data_dir_path + \"mapped_annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco_val\", {}, data_dir_path + \"mapped_annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco_test\", {}, data_dir_path + \"mapped_annotations_0_test.json\", data_dir_path + \"images/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PfYkT0FH9g8b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load your JSON file\n",
        "with open('/content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Generate unique IDs\n",
        "unique_id = 0\n",
        "for annotation in data['annotations']:\n",
        "    annotation['id'] = unique_id\n",
        "    unique_id += 1\n",
        "\n",
        "# Save the corrected JSON back to file\n",
        "with open('/content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V2oCr4kY_32_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v3bveZC0oz06"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.engine import DefaultTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ONY-33U3pWf5"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import build_detection_train_loader\n",
        "class Trainer(DefaultTrainer):\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "    if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "    return COCOEvaluator(dataset_name, output_dir=output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDgWWa6wseh",
        "outputId": "8c6ff26e-e53f-45a2-e361-a7a18c2f8ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:01:40 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [07/25 07:01:40 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[07/25 07:01:40 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_train.json\n",
            "[07/25 07:01:40 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[07/25 07:01:40 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 194          |   Other    | 1397         |   Bottle   | 344          |\n",
            "|  Bottle cap   | 226          |    Cup     | 150          |    Lid     | 63           |\n",
            "| Plastic bag.. | 697          |  Pop tab   | 75           |   Straw    | 108          |\n",
            "|   Cigarette   | 457          |            |              |            |              |\n",
            "|     total     | 3711         |            |              |            |              |\n",
            "[07/25 07:01:40 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[07/25 07:01:40 d2.data.build]: Using training sampler TrainingSampler\n",
            "[07/25 07:01:40 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/25 07:01:40 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[07/25 07:01:40 d2.data.common]: Serialized dataset takes 1.77 MiB\n",
            "[07/25 07:01:40 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[07/25 07:01:40 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model_final_f10217.pkl: 178MB [00:02, 79.5MB/s]                          \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (10, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:01:43 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:02:18 d2.utils.events]:  eta: 0:09:31  iter: 19  total_loss: 3.847  loss_cls: 2.245  loss_box_reg: 0.6365  loss_mask: 0.6904  loss_rpn_cls: 0.1009  loss_rpn_loc: 0.05004    time: 1.4038  last_time: 1.0003  data_time: 0.8252  last_data_time: 0.4354   lr: 3.8962e-05  max_mem: 2576M\n",
            "[07/25 07:02:46 d2.utils.events]:  eta: 0:08:43  iter: 39  total_loss: 3.097  loss_cls: 1.321  loss_box_reg: 0.564  loss_mask: 0.6762  loss_rpn_cls: 0.04346  loss_rpn_loc: 0.02167    time: 1.2940  last_time: 3.6494  data_time: 0.6154  last_data_time: 2.7406   lr: 7.8922e-05  max_mem: 2669M\n",
            "[07/25 07:03:10 d2.utils.events]:  eta: 0:08:46  iter: 59  total_loss: 2.091  loss_cls: 0.6827  loss_box_reg: 0.5158  loss_mask: 0.6479  loss_rpn_cls: 0.06301  loss_rpn_loc: 0.02084    time: 1.2582  last_time: 1.5014  data_time: 0.6805  last_data_time: 1.0571   lr: 0.00011888  max_mem: 2669M\n",
            "[07/25 07:03:36 d2.utils.events]:  eta: 0:08:24  iter: 79  total_loss: 1.647  loss_cls: 0.533  loss_box_reg: 0.4881  loss_mask: 0.5984  loss_rpn_cls: 0.04938  loss_rpn_loc: 0.0144    time: 1.2740  last_time: 0.7057  data_time: 0.7075  last_data_time: 0.0055   lr: 0.00015884  max_mem: 2669M\n",
            "WARNING [07/25 07:04:04 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[07/25 07:04:04 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[07/25 07:04:04 d2.data.build]: Distribution of instances among all 10 categories:\n",
            "|   category    | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:-------------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|      Can      | 42           |   Other    | 179          |   Bottle   | 41           |\n",
            "|  Bottle cap   | 22           |    Cup     | 25           |    Lid     | 9            |\n",
            "| Plastic bag.. | 69           |  Pop tab   | 13           |   Straw    | 15           |\n",
            "|   Cigarette   | 89           |            |              |            |              |\n",
            "|     total     | 504          |            |              |            |              |\n",
            "[07/25 07:04:04 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/25 07:04:04 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/25 07:04:04 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[07/25 07:04:04 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[07/25 07:04:04 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[07/25 07:04:04 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:04:27 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0016 s/iter. Inference: 0.3794 s/iter. Eval: 1.8535 s/iter. Total: 2.2345 s/iter. ETA=0:05:10\n",
            "[07/25 07:04:34 d2.evaluation.evaluator]: Inference done 14/150. Dataloading: 0.0025 s/iter. Inference: 0.3609 s/iter. Eval: 1.8653 s/iter. Total: 2.2293 s/iter. ETA=0:05:03\n",
            "[07/25 07:04:41 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0024 s/iter. Inference: 0.3147 s/iter. Eval: 1.5938 s/iter. Total: 1.9114 s/iter. ETA=0:04:10\n",
            "[07/25 07:04:46 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0027 s/iter. Inference: 0.3205 s/iter. Eval: 1.7018 s/iter. Total: 2.0256 s/iter. ETA=0:04:21\n",
            "[07/25 07:04:54 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0026 s/iter. Inference: 0.3389 s/iter. Eval: 1.8685 s/iter. Total: 2.2109 s/iter. ETA=0:04:40\n",
            "[07/25 07:05:02 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0026 s/iter. Inference: 0.3771 s/iter. Eval: 2.1226 s/iter. Total: 2.5032 s/iter. ETA=0:05:15\n",
            "[07/25 07:05:07 d2.evaluation.evaluator]: Inference done 26/150. Dataloading: 0.0030 s/iter. Inference: 0.3789 s/iter. Eval: 2.1217 s/iter. Total: 2.5046 s/iter. ETA=0:05:10\n",
            "[07/25 07:05:13 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0029 s/iter. Inference: 0.3724 s/iter. Eval: 2.0867 s/iter. Total: 2.4630 s/iter. ETA=0:04:58\n",
            "[07/25 07:05:19 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0029 s/iter. Inference: 0.3827 s/iter. Eval: 2.2114 s/iter. Total: 2.5980 s/iter. ETA=0:05:11\n",
            "[07/25 07:05:27 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0031 s/iter. Inference: 0.3971 s/iter. Eval: 2.2919 s/iter. Total: 2.6932 s/iter. ETA=0:05:17\n",
            "[07/25 07:05:34 d2.evaluation.evaluator]: Inference done 35/150. Dataloading: 0.0031 s/iter. Inference: 0.3872 s/iter. Eval: 2.2599 s/iter. Total: 2.6513 s/iter. ETA=0:05:04\n",
            "[07/25 07:05:43 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0032 s/iter. Inference: 0.3904 s/iter. Eval: 2.3049 s/iter. Total: 2.6995 s/iter. ETA=0:05:02\n",
            "[07/25 07:05:48 d2.evaluation.evaluator]: Inference done 40/150. Dataloading: 0.0032 s/iter. Inference: 0.3930 s/iter. Eval: 2.2989 s/iter. Total: 2.6962 s/iter. ETA=0:04:56\n",
            "[07/25 07:05:54 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0033 s/iter. Inference: 0.3998 s/iter. Eval: 2.3844 s/iter. Total: 2.7887 s/iter. ETA=0:05:03\n",
            "[07/25 07:06:00 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0032 s/iter. Inference: 0.4012 s/iter. Eval: 2.3763 s/iter. Total: 2.7819 s/iter. ETA=0:04:57\n",
            "[07/25 07:06:10 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0032 s/iter. Inference: 0.4012 s/iter. Eval: 2.4162 s/iter. Total: 2.8218 s/iter. ETA=0:04:53\n",
            "[07/25 07:06:17 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0032 s/iter. Inference: 0.4079 s/iter. Eval: 2.4552 s/iter. Total: 2.8673 s/iter. ETA=0:04:52\n",
            "[07/25 07:06:26 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0032 s/iter. Inference: 0.4114 s/iter. Eval: 2.5129 s/iter. Total: 2.9286 s/iter. ETA=0:04:52\n",
            "[07/25 07:06:31 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0032 s/iter. Inference: 0.3978 s/iter. Eval: 2.3907 s/iter. Total: 2.7928 s/iter. ETA=0:04:28\n",
            "[07/25 07:06:38 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0031 s/iter. Inference: 0.3991 s/iter. Eval: 2.4289 s/iter. Total: 2.8322 s/iter. ETA=0:04:26\n",
            "[07/25 07:06:44 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0031 s/iter. Inference: 0.3976 s/iter. Eval: 2.4260 s/iter. Total: 2.8279 s/iter. ETA=0:04:20\n",
            "[07/25 07:06:50 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0031 s/iter. Inference: 0.3985 s/iter. Eval: 2.4275 s/iter. Total: 2.8301 s/iter. ETA=0:04:14\n",
            "[07/25 07:06:58 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0030 s/iter. Inference: 0.4024 s/iter. Eval: 2.4697 s/iter. Total: 2.8762 s/iter. ETA=0:04:13\n",
            "[07/25 07:07:04 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0030 s/iter. Inference: 0.4035 s/iter. Eval: 2.4669 s/iter. Total: 2.8745 s/iter. ETA=0:04:07\n",
            "[07/25 07:07:10 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0030 s/iter. Inference: 0.3978 s/iter. Eval: 2.4443 s/iter. Total: 2.8461 s/iter. ETA=0:03:56\n",
            "[07/25 07:07:17 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.0030 s/iter. Inference: 0.3948 s/iter. Eval: 2.4129 s/iter. Total: 2.8118 s/iter. ETA=0:03:44\n",
            "[07/25 07:07:22 d2.evaluation.evaluator]: Inference done 73/150. Dataloading: 0.0030 s/iter. Inference: 0.3910 s/iter. Eval: 2.3730 s/iter. Total: 2.7680 s/iter. ETA=0:03:33\n",
            "[07/25 07:07:28 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0029 s/iter. Inference: 0.3899 s/iter. Eval: 2.3758 s/iter. Total: 2.7697 s/iter. ETA=0:03:27\n",
            "[07/25 07:07:34 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0030 s/iter. Inference: 0.3875 s/iter. Eval: 2.3422 s/iter. Total: 2.7338 s/iter. ETA=0:03:16\n",
            "[07/25 07:07:39 d2.evaluation.evaluator]: Inference done 81/150. Dataloading: 0.0030 s/iter. Inference: 0.3843 s/iter. Eval: 2.3136 s/iter. Total: 2.7019 s/iter. ETA=0:03:06\n",
            "[07/25 07:07:45 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0030 s/iter. Inference: 0.3806 s/iter. Eval: 2.2831 s/iter. Total: 2.6677 s/iter. ETA=0:02:56\n",
            "[07/25 07:07:52 d2.evaluation.evaluator]: Inference done 87/150. Dataloading: 0.0029 s/iter. Inference: 0.3803 s/iter. Eval: 2.2694 s/iter. Total: 2.6537 s/iter. ETA=0:02:47\n",
            "[07/25 07:07:57 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0030 s/iter. Inference: 0.3772 s/iter. Eval: 2.2407 s/iter. Total: 2.6220 s/iter. ETA=0:02:37\n",
            "[07/25 07:08:03 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0030 s/iter. Inference: 0.3749 s/iter. Eval: 2.2188 s/iter. Total: 2.5978 s/iter. ETA=0:02:28\n",
            "[07/25 07:08:09 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0031 s/iter. Inference: 0.3699 s/iter. Eval: 2.1820 s/iter. Total: 2.5559 s/iter. ETA=0:02:15\n",
            "[07/25 07:08:17 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0031 s/iter. Inference: 0.3685 s/iter. Eval: 2.1802 s/iter. Total: 2.5528 s/iter. ETA=0:02:07\n",
            "[07/25 07:08:22 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0031 s/iter. Inference: 0.3667 s/iter. Eval: 2.1552 s/iter. Total: 2.5260 s/iter. ETA=0:01:58\n",
            "[07/25 07:08:27 d2.evaluation.evaluator]: Inference done 107/150. Dataloading: 0.0031 s/iter. Inference: 0.3617 s/iter. Eval: 2.1121 s/iter. Total: 2.4779 s/iter. ETA=0:01:46\n",
            "[07/25 07:08:35 d2.evaluation.evaluator]: Inference done 111/150. Dataloading: 0.0040 s/iter. Inference: 0.3605 s/iter. Eval: 2.0924 s/iter. Total: 2.4579 s/iter. ETA=0:01:35\n",
            "[07/25 07:08:40 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0040 s/iter. Inference: 0.3613 s/iter. Eval: 2.0996 s/iter. Total: 2.4659 s/iter. ETA=0:01:31\n",
            "[07/25 07:08:53 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0040 s/iter. Inference: 0.3657 s/iter. Eval: 2.1461 s/iter. Total: 2.5168 s/iter. ETA=0:01:25\n",
            "[07/25 07:09:00 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0039 s/iter. Inference: 0.3660 s/iter. Eval: 2.1406 s/iter. Total: 2.5116 s/iter. ETA=0:01:17\n",
            "[07/25 07:09:06 d2.evaluation.evaluator]: Inference done 121/150. Dataloading: 0.0040 s/iter. Inference: 0.3657 s/iter. Eval: 2.1470 s/iter. Total: 2.5176 s/iter. ETA=0:01:13\n",
            "[07/25 07:09:11 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0040 s/iter. Inference: 0.3643 s/iter. Eval: 2.1300 s/iter. Total: 2.4992 s/iter. ETA=0:01:04\n",
            "[07/25 07:09:18 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0039 s/iter. Inference: 0.3637 s/iter. Eval: 2.1225 s/iter. Total: 2.4912 s/iter. ETA=0:00:57\n",
            "[07/25 07:09:25 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0040 s/iter. Inference: 0.3642 s/iter. Eval: 2.1201 s/iter. Total: 2.4893 s/iter. ETA=0:00:49\n",
            "[07/25 07:09:31 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0040 s/iter. Inference: 0.3656 s/iter. Eval: 2.1244 s/iter. Total: 2.4951 s/iter. ETA=0:00:44\n",
            "[07/25 07:09:37 d2.evaluation.evaluator]: Inference done 134/150. Dataloading: 0.0040 s/iter. Inference: 0.3663 s/iter. Eval: 2.1318 s/iter. Total: 2.5031 s/iter. ETA=0:00:40\n",
            "[07/25 07:09:42 d2.evaluation.evaluator]: Inference done 138/150. Dataloading: 0.0039 s/iter. Inference: 0.3633 s/iter. Eval: 2.0998 s/iter. Total: 2.4681 s/iter. ETA=0:00:29\n",
            "[07/25 07:09:47 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0050 s/iter. Inference: 0.3614 s/iter. Eval: 2.0835 s/iter. Total: 2.4510 s/iter. ETA=0:00:22\n",
            "[07/25 07:09:55 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0050 s/iter. Inference: 0.3606 s/iter. Eval: 2.0837 s/iter. Total: 2.4504 s/iter. ETA=0:00:14\n",
            "[07/25 07:10:01 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0050 s/iter. Inference: 0.3612 s/iter. Eval: 2.0767 s/iter. Total: 2.4438 s/iter. ETA=0:00:07\n",
            "[07/25 07:10:02 d2.evaluation.evaluator]: Total inference time: 0:05:48.401654 (2.402770 s / iter per device, on 1 devices)\n",
            "[07/25 07:10:02 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:51 (0.356772 s / iter per device, on 1 devices)\n",
            "[07/25 07:10:03 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[07/25 07:10:03 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[07/25 07:10:03 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.52s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.25s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.032\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.063\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.024\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.036\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.197\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.147\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227\n",
            "[07/25 07:10:03 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 3.174 | 6.276  | 2.121  | 0.077 | 2.380 | 3.573 |\n",
            "[07/25 07:10:03 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 9.112 | Other      | 3.659 | Bottle     | 6.630 |\n",
            "| Bottle cap            | 9.469 | Cup        | 1.394 | Lid        | 0.089 |\n",
            "| Plastic bag + wrapper | 1.321 | Pop tab    | 0.000 | Straw      | 0.070 |\n",
            "| Cigarette             | 0.000 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.24s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.87s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.46s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.063\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.118\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.241\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.285\n",
            "[07/25 07:10:06 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 4.025 | 6.266  | 4.354  | 0.020 | 1.840 | 4.872 |\n",
            "[07/25 07:10:06 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP    | category   | AP    | category   | AP     |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:-------|\n",
            "| Can                   | 9.177 | Other      | 5.997 | Bottle     | 11.202 |\n",
            "| Bottle cap            | 8.106 | Cup        | 2.890 | Lid        | 0.035  |\n",
            "| Plastic bag + wrapper | 2.848 | Pop tab    | 0.000 | Straw      | 0.000  |\n",
            "| Cigarette             | 0.000 |            |       |            |        |\n",
            "[07/25 07:10:06 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[07/25 07:10:06 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[07/25 07:10:06 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:10:06 d2.evaluation.testing]: copypaste: 3.1744,6.2756,2.1207,0.0766,2.3796,3.5728\n",
            "[07/25 07:10:06 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[07/25 07:10:06 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:10:06 d2.evaluation.testing]: copypaste: 4.0255,6.2663,4.3539,0.0196,1.8405,4.8723\n",
            "[07/25 07:10:06 d2.utils.events]:  eta: 0:08:14  iter: 99  total_loss: 1.481  loss_cls: 0.4537  loss_box_reg: 0.4194  loss_mask: 0.5579  loss_rpn_cls: 0.06336  loss_rpn_loc: 0.01826    time: 1.2917  last_time: 1.3591  data_time: 0.7813  last_data_time: 0.8338   lr: 0.0001988  max_mem: 7601M\n",
            "[07/25 07:10:28 d2.utils.events]:  eta: 0:07:35  iter: 119  total_loss: 1.631  loss_cls: 0.5353  loss_box_reg: 0.4963  loss_mask: 0.4813  loss_rpn_cls: 0.05917  loss_rpn_loc: 0.01976    time: 1.2584  last_time: 0.9783  data_time: 0.4904  last_data_time: 0.4584   lr: 0.00023876  max_mem: 7601M\n",
            "[07/25 07:10:52 d2.utils.events]:  eta: 0:07:11  iter: 139  total_loss: 1.5  loss_cls: 0.49  loss_box_reg: 0.4858  loss_mask: 0.417  loss_rpn_cls: 0.08109  loss_rpn_loc: 0.02382    time: 1.2523  last_time: 1.4853  data_time: 0.6235  last_data_time: 1.0601   lr: 0.00027872  max_mem: 7601M\n",
            "[07/25 07:11:16 d2.utils.events]:  eta: 0:06:47  iter: 159  total_loss: 1.432  loss_cls: 0.5021  loss_box_reg: 0.5252  loss_mask: 0.3204  loss_rpn_cls: 0.06182  loss_rpn_loc: 0.02398    time: 1.2464  last_time: 0.9976  data_time: 0.6721  last_data_time: 0.5560   lr: 0.00031868  max_mem: 7601M\n",
            "[07/25 07:11:43 d2.utils.events]:  eta: 0:06:24  iter: 179  total_loss: 1.522  loss_cls: 0.5003  loss_box_reg: 0.5937  loss_mask: 0.2896  loss_rpn_cls: 0.0585  loss_rpn_loc: 0.0194    time: 1.2550  last_time: 0.7062  data_time: 0.6851  last_data_time: 0.0042   lr: 0.00035864  max_mem: 7601M\n",
            "WARNING [07/25 07:12:08 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[07/25 07:12:08 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[07/25 07:12:08 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/25 07:12:08 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/25 07:12:08 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[07/25 07:12:08 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[07/25 07:12:08 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[07/25 07:12:08 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:12:34 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0018 s/iter. Inference: 0.3920 s/iter. Eval: 1.7096 s/iter. Total: 2.1034 s/iter. ETA=0:04:52\n",
            "[07/25 07:12:41 d2.evaluation.evaluator]: Inference done 14/150. Dataloading: 0.0021 s/iter. Inference: 0.3942 s/iter. Eval: 1.7649 s/iter. Total: 2.1617 s/iter. ETA=0:04:53\n",
            "[07/25 07:12:48 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0022 s/iter. Inference: 0.3445 s/iter. Eval: 1.5327 s/iter. Total: 1.8798 s/iter. ETA=0:04:06\n",
            "[07/25 07:12:54 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.0029 s/iter. Inference: 0.3613 s/iter. Eval: 1.6708 s/iter. Total: 2.0355 s/iter. ETA=0:04:22\n",
            "[07/25 07:13:04 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0034 s/iter. Inference: 0.4019 s/iter. Eval: 1.9478 s/iter. Total: 2.3539 s/iter. ETA=0:04:58\n",
            "[07/25 07:13:13 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0033 s/iter. Inference: 0.4271 s/iter. Eval: 2.2623 s/iter. Total: 2.6935 s/iter. ETA=0:05:39\n",
            "[07/25 07:13:21 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0033 s/iter. Inference: 0.4244 s/iter. Eval: 2.2525 s/iter. Total: 2.6811 s/iter. ETA=0:05:29\n",
            "[07/25 07:13:31 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0039 s/iter. Inference: 0.4417 s/iter. Eval: 2.3152 s/iter. Total: 2.7615 s/iter. ETA=0:05:31\n",
            "[07/25 07:13:38 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0038 s/iter. Inference: 0.4499 s/iter. Eval: 2.3665 s/iter. Total: 2.8209 s/iter. ETA=0:05:32\n",
            "[07/25 07:13:45 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0042 s/iter. Inference: 0.4583 s/iter. Eval: 2.4107 s/iter. Total: 2.8739 s/iter. ETA=0:05:33\n",
            "[07/25 07:13:51 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0041 s/iter. Inference: 0.4608 s/iter. Eval: 2.4156 s/iter. Total: 2.8812 s/iter. ETA=0:05:28\n",
            "[07/25 07:13:59 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0041 s/iter. Inference: 0.4605 s/iter. Eval: 2.4888 s/iter. Total: 2.9541 s/iter. ETA=0:05:30\n",
            "[07/25 07:14:08 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0040 s/iter. Inference: 0.4607 s/iter. Eval: 2.4942 s/iter. Total: 2.9596 s/iter. ETA=0:05:22\n",
            "[07/25 07:14:16 d2.evaluation.evaluator]: Inference done 43/150. Dataloading: 0.0042 s/iter. Inference: 0.4664 s/iter. Eval: 2.5365 s/iter. Total: 3.0078 s/iter. ETA=0:05:21\n",
            "[07/25 07:14:24 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0040 s/iter. Inference: 0.4617 s/iter. Eval: 2.5098 s/iter. Total: 2.9763 s/iter. ETA=0:05:09\n",
            "[07/25 07:14:33 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0043 s/iter. Inference: 0.4710 s/iter. Eval: 2.5811 s/iter. Total: 3.0571 s/iter. ETA=0:05:11\n",
            "[07/25 07:14:40 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0042 s/iter. Inference: 0.4727 s/iter. Eval: 2.5824 s/iter. Total: 3.0601 s/iter. ETA=0:05:06\n",
            "[07/25 07:14:47 d2.evaluation.evaluator]: Inference done 52/150. Dataloading: 0.0042 s/iter. Inference: 0.4748 s/iter. Eval: 2.6098 s/iter. Total: 3.0895 s/iter. ETA=0:05:02\n",
            "[07/25 07:14:55 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0043 s/iter. Inference: 0.4722 s/iter. Eval: 2.5851 s/iter. Total: 3.0624 s/iter. ETA=0:04:50\n",
            "[07/25 07:15:03 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0042 s/iter. Inference: 0.4681 s/iter. Eval: 2.5669 s/iter. Total: 3.0400 s/iter. ETA=0:04:39\n",
            "[07/25 07:15:09 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0043 s/iter. Inference: 0.4715 s/iter. Eval: 2.5617 s/iter. Total: 3.0384 s/iter. ETA=0:04:33\n",
            "[07/25 07:15:14 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0043 s/iter. Inference: 0.4692 s/iter. Eval: 2.5473 s/iter. Total: 3.0216 s/iter. ETA=0:04:25\n",
            "[07/25 07:15:19 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0043 s/iter. Inference: 0.4722 s/iter. Eval: 2.5828 s/iter. Total: 3.0602 s/iter. ETA=0:04:26\n",
            "[07/25 07:15:24 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0042 s/iter. Inference: 0.4644 s/iter. Eval: 2.5253 s/iter. Total: 2.9948 s/iter. ETA=0:04:11\n",
            "[07/25 07:15:32 d2.evaluation.evaluator]: Inference done 68/150. Dataloading: 0.0041 s/iter. Inference: 0.4650 s/iter. Eval: 2.5436 s/iter. Total: 3.0135 s/iter. ETA=0:04:07\n",
            "[07/25 07:15:38 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0040 s/iter. Inference: 0.4623 s/iter. Eval: 2.5107 s/iter. Total: 2.9780 s/iter. ETA=0:03:55\n",
            "[07/25 07:15:44 d2.evaluation.evaluator]: Inference done 74/150. Dataloading: 0.0040 s/iter. Inference: 0.4571 s/iter. Eval: 2.4702 s/iter. Total: 2.9322 s/iter. ETA=0:03:42\n",
            "[07/25 07:15:50 d2.evaluation.evaluator]: Inference done 76/150. Dataloading: 0.0040 s/iter. Inference: 0.4578 s/iter. Eval: 2.4715 s/iter. Total: 2.9342 s/iter. ETA=0:03:37\n",
            "[07/25 07:15:56 d2.evaluation.evaluator]: Inference done 79/150. Dataloading: 0.0041 s/iter. Inference: 0.4549 s/iter. Eval: 2.4360 s/iter. Total: 2.8959 s/iter. ETA=0:03:25\n",
            "[07/25 07:16:02 d2.evaluation.evaluator]: Inference done 82/150. Dataloading: 0.0041 s/iter. Inference: 0.4515 s/iter. Eval: 2.4035 s/iter. Total: 2.8600 s/iter. ETA=0:03:14\n",
            "[07/25 07:16:07 d2.evaluation.evaluator]: Inference done 84/150. Dataloading: 0.0040 s/iter. Inference: 0.4500 s/iter. Eval: 2.3977 s/iter. Total: 2.8527 s/iter. ETA=0:03:08\n",
            "[07/25 07:16:14 d2.evaluation.evaluator]: Inference done 87/150. Dataloading: 0.0039 s/iter. Inference: 0.4482 s/iter. Eval: 2.3725 s/iter. Total: 2.8256 s/iter. ETA=0:02:58\n",
            "[07/25 07:16:19 d2.evaluation.evaluator]: Inference done 91/150. Dataloading: 0.0040 s/iter. Inference: 0.4410 s/iter. Eval: 2.3168 s/iter. Total: 2.7627 s/iter. ETA=0:02:42\n",
            "[07/25 07:16:24 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0040 s/iter. Inference: 0.4407 s/iter. Eval: 2.3121 s/iter. Total: 2.7578 s/iter. ETA=0:02:37\n",
            "[07/25 07:16:31 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0039 s/iter. Inference: 0.4347 s/iter. Eval: 2.2664 s/iter. Total: 2.7060 s/iter. ETA=0:02:23\n",
            "[07/25 07:16:36 d2.evaluation.evaluator]: Inference done 100/150. Dataloading: 0.0039 s/iter. Inference: 0.4297 s/iter. Eval: 2.2388 s/iter. Total: 2.6733 s/iter. ETA=0:02:13\n",
            "[07/25 07:16:42 d2.evaluation.evaluator]: Inference done 103/150. Dataloading: 0.0038 s/iter. Inference: 0.4278 s/iter. Eval: 2.2222 s/iter. Total: 2.6548 s/iter. ETA=0:02:04\n",
            "[07/25 07:16:49 d2.evaluation.evaluator]: Inference done 107/150. Dataloading: 0.0038 s/iter. Inference: 0.4236 s/iter. Eval: 2.1945 s/iter. Total: 2.6228 s/iter. ETA=0:01:52\n",
            "[07/25 07:16:56 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0037 s/iter. Inference: 0.4229 s/iter. Eval: 2.1791 s/iter. Total: 2.6067 s/iter. ETA=0:01:44\n",
            "[07/25 07:17:03 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0037 s/iter. Inference: 0.4236 s/iter. Eval: 2.1767 s/iter. Total: 2.6049 s/iter. ETA=0:01:36\n",
            "[07/25 07:17:15 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0037 s/iter. Inference: 0.4275 s/iter. Eval: 2.2108 s/iter. Total: 2.6429 s/iter. ETA=0:01:29\n",
            "[07/25 07:17:21 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0037 s/iter. Inference: 0.4307 s/iter. Eval: 2.2381 s/iter. Total: 2.6734 s/iter. ETA=0:01:28\n",
            "[07/25 07:17:28 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0038 s/iter. Inference: 0.4335 s/iter. Eval: 2.2463 s/iter. Total: 2.6845 s/iter. ETA=0:01:23\n",
            "[07/25 07:17:33 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0038 s/iter. Inference: 0.4315 s/iter. Eval: 2.2258 s/iter. Total: 2.6619 s/iter. ETA=0:01:14\n",
            "[07/25 07:17:42 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0038 s/iter. Inference: 0.4320 s/iter. Eval: 2.2313 s/iter. Total: 2.6680 s/iter. ETA=0:01:06\n",
            "[07/25 07:17:48 d2.evaluation.evaluator]: Inference done 129/150. Dataloading: 0.0039 s/iter. Inference: 0.4288 s/iter. Eval: 2.1964 s/iter. Total: 2.6300 s/iter. ETA=0:00:55\n",
            "[07/25 07:17:53 d2.evaluation.evaluator]: Inference done 131/150. Dataloading: 0.0038 s/iter. Inference: 0.4290 s/iter. Eval: 2.1948 s/iter. Total: 2.6286 s/iter. ETA=0:00:49\n",
            "[07/25 07:17:59 d2.evaluation.evaluator]: Inference done 133/150. Dataloading: 0.0038 s/iter. Inference: 0.4298 s/iter. Eval: 2.2007 s/iter. Total: 2.6352 s/iter. ETA=0:00:44\n",
            "[07/25 07:18:05 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0038 s/iter. Inference: 0.4280 s/iter. Eval: 2.1872 s/iter. Total: 2.6199 s/iter. ETA=0:00:36\n",
            "[07/25 07:18:13 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0038 s/iter. Inference: 0.4228 s/iter. Eval: 2.1537 s/iter. Total: 2.5813 s/iter. ETA=0:00:23\n",
            "[07/25 07:18:19 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0038 s/iter. Inference: 0.4220 s/iter. Eval: 2.1438 s/iter. Total: 2.5706 s/iter. ETA=0:00:15\n",
            "[07/25 07:18:25 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0038 s/iter. Inference: 0.4200 s/iter. Eval: 2.1314 s/iter. Total: 2.5561 s/iter. ETA=0:00:07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:18:28 d2.evaluation.evaluator]: Total inference time: 0:06:06.157393 (2.525223 s / iter per device, on 1 devices)\n",
            "[07/25 07:18:28 d2.evaluation.evaluator]: Total inference pure compute time: 0:01:00 (0.415340 s / iter per device, on 1 devices)\n",
            "[07/25 07:18:28 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[07/25 07:18:28 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[07/25 07:18:28 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.51s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.21s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.075\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.014\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.009\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.035\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.138\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.198\n",
            "[07/25 07:18:29 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 3.338 | 7.475  | 1.423  | 0.861 | 3.530 | 4.352 |\n",
            "[07/25 07:18:29 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP    | category   | AP    | category   | AP    |\n",
            "|:----------------------|:------|:-----------|:------|:-----------|:------|\n",
            "| Can                   | 3.122 | Other      | 5.973 | Bottle     | 9.504 |\n",
            "| Bottle cap            | 8.046 | Cup        | 1.726 | Lid        | 0.000 |\n",
            "| Plastic bag + wrapper | 4.896 | Pop tab    | 0.000 | Straw      | 0.042 |\n",
            "| Cigarette             | 0.066 |            |       |            |       |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.23s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.85s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.21s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.076\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.034\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.065\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.132\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.187\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.273\n",
            "[07/25 07:18:31 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 5.015 | 7.570  | 5.653  | 0.271 | 3.395 | 6.477 |\n",
            "[07/25 07:18:31 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP    | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:------|:-----------|:-------|\n",
            "| Can                   | 3.774  | Other      | 9.260 | Bottle     | 15.304 |\n",
            "| Bottle cap            | 8.240  | Cup        | 2.896 | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 10.571 | Pop tab    | 0.000 | Straw      | 0.000  |\n",
            "| Cigarette             | 0.105  |            |       |            |        |\n",
            "[07/25 07:18:31 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[07/25 07:18:31 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[07/25 07:18:31 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:18:31 d2.evaluation.testing]: copypaste: 3.3375,7.4746,1.4226,0.8608,3.5302,4.3519\n",
            "[07/25 07:18:31 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[07/25 07:18:31 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:18:31 d2.evaluation.testing]: copypaste: 5.0150,7.5695,5.6527,0.2714,3.3951,6.4770\n",
            "[07/25 07:18:31 d2.utils.events]:  eta: 0:06:00  iter: 199  total_loss: 1.29  loss_cls: 0.4291  loss_box_reg: 0.5114  loss_mask: 0.2471  loss_rpn_cls: 0.03868  loss_rpn_loc: 0.01887    time: 1.2568  last_time: 1.7362  data_time: 0.6884  last_data_time: 1.3223   lr: 0.0003986  max_mem: 8768M\n",
            "[07/25 07:18:52 d2.utils.events]:  eta: 0:05:25  iter: 219  total_loss: 1.372  loss_cls: 0.4665  loss_box_reg: 0.5132  loss_mask: 0.2767  loss_rpn_cls: 0.0358  loss_rpn_loc: 0.017    time: 1.2404  last_time: 0.5804  data_time: 0.4814  last_data_time: 0.1246   lr: 0.00043856  max_mem: 8768M\n",
            "[07/25 07:19:18 d2.utils.events]:  eta: 0:04:57  iter: 239  total_loss: 1.285  loss_cls: 0.425  loss_box_reg: 0.4994  loss_mask: 0.2383  loss_rpn_cls: 0.0265  loss_rpn_loc: 0.01806    time: 1.2430  last_time: 1.2858  data_time: 0.7057  last_data_time: 0.7083   lr: 0.00047852  max_mem: 8768M\n",
            "[07/25 07:19:44 d2.utils.events]:  eta: 0:04:32  iter: 259  total_loss: 0.8638  loss_cls: 0.2867  loss_box_reg: 0.3251  loss_mask: 0.2114  loss_rpn_cls: 0.04004  loss_rpn_loc: 0.00574    time: 1.2484  last_time: 0.7689  data_time: 0.7295  last_data_time: 0.3463   lr: 0.00051848  max_mem: 8768M\n",
            "[07/25 07:20:07 d2.utils.events]:  eta: 0:04:08  iter: 279  total_loss: 1.247  loss_cls: 0.3664  loss_box_reg: 0.4002  loss_mask: 0.2421  loss_rpn_cls: 0.04662  loss_rpn_loc: 0.04027    time: 1.2395  last_time: 0.7523  data_time: 0.5419  last_data_time: 0.2239   lr: 0.00055844  max_mem: 8768M\n",
            "WARNING [07/25 07:20:30 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[07/25 07:20:30 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[07/25 07:20:30 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/25 07:20:30 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/25 07:20:30 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[07/25 07:20:30 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[07/25 07:20:30 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[07/25 07:20:30 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:20:51 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0018 s/iter. Inference: 0.3778 s/iter. Eval: 1.7150 s/iter. Total: 2.0946 s/iter. ETA=0:04:51\n",
            "[07/25 07:20:57 d2.evaluation.evaluator]: Inference done 14/150. Dataloading: 0.0035 s/iter. Inference: 0.3660 s/iter. Eval: 1.7029 s/iter. Total: 2.0728 s/iter. ETA=0:04:41\n",
            "[07/25 07:21:04 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0037 s/iter. Inference: 0.3506 s/iter. Eval: 1.4802 s/iter. Total: 1.8349 s/iter. ETA=0:04:00\n",
            "[07/25 07:21:09 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0034 s/iter. Inference: 0.3454 s/iter. Eval: 1.4771 s/iter. Total: 1.8262 s/iter. ETA=0:03:53\n",
            "[07/25 07:21:25 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0033 s/iter. Inference: 0.3750 s/iter. Eval: 2.2249 s/iter. Total: 2.6037 s/iter. ETA=0:05:30\n",
            "[07/25 07:21:40 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0033 s/iter. Inference: 0.4016 s/iter. Eval: 2.8724 s/iter. Total: 3.2778 s/iter. ETA=0:06:53\n",
            "[07/25 07:21:51 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0037 s/iter. Inference: 0.4154 s/iter. Eval: 2.8817 s/iter. Total: 3.3015 s/iter. ETA=0:06:46\n",
            "[07/25 07:21:58 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0035 s/iter. Inference: 0.4134 s/iter. Eval: 2.7934 s/iter. Total: 3.2112 s/iter. ETA=0:06:25\n",
            "[07/25 07:22:07 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0037 s/iter. Inference: 0.4307 s/iter. Eval: 2.8614 s/iter. Total: 3.2965 s/iter. ETA=0:06:28\n",
            "[07/25 07:22:14 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0037 s/iter. Inference: 0.4376 s/iter. Eval: 2.8562 s/iter. Total: 3.2983 s/iter. ETA=0:06:22\n",
            "[07/25 07:22:22 d2.evaluation.evaluator]: Inference done 36/150. Dataloading: 0.0040 s/iter. Inference: 0.4458 s/iter. Eval: 2.8878 s/iter. Total: 3.3383 s/iter. ETA=0:06:20\n",
            "[07/25 07:22:28 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0039 s/iter. Inference: 0.4452 s/iter. Eval: 2.8656 s/iter. Total: 3.3155 s/iter. ETA=0:06:11\n",
            "[07/25 07:22:38 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0038 s/iter. Inference: 0.4510 s/iter. Eval: 2.8821 s/iter. Total: 3.3377 s/iter. ETA=0:06:03\n",
            "[07/25 07:22:44 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0037 s/iter. Inference: 0.4424 s/iter. Eval: 2.7702 s/iter. Total: 3.2171 s/iter. ETA=0:05:41\n",
            "[07/25 07:22:50 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0037 s/iter. Inference: 0.4398 s/iter. Eval: 2.7681 s/iter. Total: 3.2124 s/iter. ETA=0:05:34\n",
            "[07/25 07:22:58 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0038 s/iter. Inference: 0.4496 s/iter. Eval: 2.8019 s/iter. Total: 3.2562 s/iter. ETA=0:05:32\n",
            "[07/25 07:23:04 d2.evaluation.evaluator]: Inference done 50/150. Dataloading: 0.0038 s/iter. Inference: 0.4499 s/iter. Eval: 2.7851 s/iter. Total: 3.2396 s/iter. ETA=0:05:23\n",
            "[07/25 07:23:09 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0038 s/iter. Inference: 0.4554 s/iter. Eval: 2.8242 s/iter. Total: 3.2843 s/iter. ETA=0:05:25\n",
            "[07/25 07:23:16 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0037 s/iter. Inference: 0.4527 s/iter. Eval: 2.7594 s/iter. Total: 3.2166 s/iter. ETA=0:05:08\n",
            "[07/25 07:23:21 d2.evaluation.evaluator]: Inference done 56/150. Dataloading: 0.0039 s/iter. Inference: 0.4519 s/iter. Eval: 2.7365 s/iter. Total: 3.1932 s/iter. ETA=0:05:00\n",
            "[07/25 07:23:27 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0039 s/iter. Inference: 0.4555 s/iter. Eval: 2.7269 s/iter. Total: 3.1874 s/iter. ETA=0:04:53\n",
            "[07/25 07:23:33 d2.evaluation.evaluator]: Inference done 60/150. Dataloading: 0.0038 s/iter. Inference: 0.4557 s/iter. Eval: 2.7112 s/iter. Total: 3.1718 s/iter. ETA=0:04:45\n",
            "[07/25 07:23:38 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0039 s/iter. Inference: 0.4525 s/iter. Eval: 2.6912 s/iter. Total: 3.1487 s/iter. ETA=0:04:37\n",
            "[07/25 07:23:45 d2.evaluation.evaluator]: Inference done 64/150. Dataloading: 0.0039 s/iter. Inference: 0.4567 s/iter. Eval: 2.6995 s/iter. Total: 3.1612 s/iter. ETA=0:04:31\n",
            "[07/25 07:23:51 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0038 s/iter. Inference: 0.4512 s/iter. Eval: 2.6548 s/iter. Total: 3.1108 s/iter. ETA=0:04:18\n",
            "[07/25 07:23:56 d2.evaluation.evaluator]: Inference done 69/150. Dataloading: 0.0039 s/iter. Inference: 0.4522 s/iter. Eval: 2.6402 s/iter. Total: 3.0973 s/iter. ETA=0:04:10\n",
            "[07/25 07:24:03 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0038 s/iter. Inference: 0.4490 s/iter. Eval: 2.5988 s/iter. Total: 3.0527 s/iter. ETA=0:03:58\n",
            "[07/25 07:24:10 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0038 s/iter. Inference: 0.4464 s/iter. Eval: 2.5703 s/iter. Total: 3.0216 s/iter. ETA=0:03:46\n",
            "[07/25 07:24:17 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0037 s/iter. Inference: 0.4451 s/iter. Eval: 2.5466 s/iter. Total: 2.9965 s/iter. ETA=0:03:35\n",
            "[07/25 07:24:22 d2.evaluation.evaluator]: Inference done 81/150. Dataloading: 0.0039 s/iter. Inference: 0.4416 s/iter. Eval: 2.5041 s/iter. Total: 2.9506 s/iter. ETA=0:03:23\n",
            "[07/25 07:24:28 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.0039 s/iter. Inference: 0.4423 s/iter. Eval: 2.5063 s/iter. Total: 2.9534 s/iter. ETA=0:03:17\n",
            "[07/25 07:24:34 d2.evaluation.evaluator]: Inference done 86/150. Dataloading: 0.0038 s/iter. Inference: 0.4401 s/iter. Eval: 2.4695 s/iter. Total: 2.9144 s/iter. ETA=0:03:06\n",
            "[07/25 07:24:41 d2.evaluation.evaluator]: Inference done 90/150. Dataloading: 0.0038 s/iter. Inference: 0.4354 s/iter. Eval: 2.4204 s/iter. Total: 2.8605 s/iter. ETA=0:02:51\n",
            "[07/25 07:24:47 d2.evaluation.evaluator]: Inference done 93/150. Dataloading: 0.0037 s/iter. Inference: 0.4340 s/iter. Eval: 2.3905 s/iter. Total: 2.8293 s/iter. ETA=0:02:41\n",
            "[07/25 07:24:53 d2.evaluation.evaluator]: Inference done 99/150. Dataloading: 0.0036 s/iter. Inference: 0.4205 s/iter. Eval: 2.2838 s/iter. Total: 2.7089 s/iter. ETA=0:02:18\n",
            "[07/25 07:24:59 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0037 s/iter. Inference: 0.4186 s/iter. Eval: 2.2705 s/iter. Total: 2.6937 s/iter. ETA=0:02:09\n",
            "[07/25 07:25:05 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0036 s/iter. Inference: 0.4162 s/iter. Eval: 2.2434 s/iter. Total: 2.6642 s/iter. ETA=0:01:59\n",
            "[07/25 07:25:11 d2.evaluation.evaluator]: Inference done 108/150. Dataloading: 0.0037 s/iter. Inference: 0.4164 s/iter. Eval: 2.2309 s/iter. Total: 2.6519 s/iter. ETA=0:01:51\n",
            "[07/25 07:25:17 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0037 s/iter. Inference: 0.4176 s/iter. Eval: 2.2344 s/iter. Total: 2.6568 s/iter. ETA=0:01:46\n",
            "[07/25 07:25:25 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0037 s/iter. Inference: 0.4181 s/iter. Eval: 2.2299 s/iter. Total: 2.6527 s/iter. ETA=0:01:38\n",
            "[07/25 07:25:37 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0038 s/iter. Inference: 0.4221 s/iter. Eval: 2.2659 s/iter. Total: 2.6928 s/iter. ETA=0:01:31\n",
            "[07/25 07:25:44 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0038 s/iter. Inference: 0.4254 s/iter. Eval: 2.3011 s/iter. Total: 2.7312 s/iter. ETA=0:01:30\n",
            "[07/25 07:25:50 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0038 s/iter. Inference: 0.4279 s/iter. Eval: 2.3067 s/iter. Total: 2.7394 s/iter. ETA=0:01:24\n",
            "[07/25 07:25:56 d2.evaluation.evaluator]: Inference done 122/150. Dataloading: 0.0037 s/iter. Inference: 0.4259 s/iter. Eval: 2.2842 s/iter. Total: 2.7149 s/iter. ETA=0:01:16\n",
            "[07/25 07:26:01 d2.evaluation.evaluator]: Inference done 124/150. Dataloading: 0.0037 s/iter. Inference: 0.4255 s/iter. Eval: 2.2826 s/iter. Total: 2.7128 s/iter. ETA=0:01:10\n",
            "[07/25 07:26:06 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.0038 s/iter. Inference: 0.4267 s/iter. Eval: 2.2805 s/iter. Total: 2.7120 s/iter. ETA=0:01:05\n",
            "[07/25 07:26:13 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0038 s/iter. Inference: 0.4236 s/iter. Eval: 2.2525 s/iter. Total: 2.6809 s/iter. ETA=0:00:53\n",
            "[07/25 07:26:19 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0038 s/iter. Inference: 0.4225 s/iter. Eval: 2.2539 s/iter. Total: 2.6812 s/iter. ETA=0:00:48\n",
            "[07/25 07:26:24 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0037 s/iter. Inference: 0.4218 s/iter. Eval: 2.2361 s/iter. Total: 2.6626 s/iter. ETA=0:00:39\n",
            "[07/25 07:26:29 d2.evaluation.evaluator]: Inference done 139/150. Dataloading: 0.0037 s/iter. Inference: 0.4172 s/iter. Eval: 2.1990 s/iter. Total: 2.6209 s/iter. ETA=0:00:28\n",
            "[07/25 07:26:36 d2.evaluation.evaluator]: Inference done 142/150. Dataloading: 0.0037 s/iter. Inference: 0.4158 s/iter. Eval: 2.1911 s/iter. Total: 2.6116 s/iter. ETA=0:00:20\n",
            "[07/25 07:26:42 d2.evaluation.evaluator]: Inference done 145/150. Dataloading: 0.0037 s/iter. Inference: 0.4143 s/iter. Eval: 2.1769 s/iter. Total: 2.5959 s/iter. ETA=0:00:12\n",
            "[07/25 07:26:47 d2.evaluation.evaluator]: Inference done 149/150. Dataloading: 0.0037 s/iter. Inference: 0.4104 s/iter. Eval: 2.1496 s/iter. Total: 2.5647 s/iter. ETA=0:00:02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:26:48 d2.evaluation.evaluator]: Total inference time: 0:06:10.366819 (2.554254 s / iter per device, on 1 devices)\n",
            "[07/25 07:26:48 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:59 (0.408918 s / iter per device, on 1 devices)\n",
            "[07/25 07:26:49 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[07/25 07:26:49 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[07/25 07:26:49 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.86s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.42s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.066\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.127\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.052\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.073\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.132\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.273\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.310\n",
            "[07/25 07:26:50 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 6.576 | 12.678 | 5.951  | 2.793 | 5.185 | 7.314 |\n",
            "[07/25 07:26:50 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 13.614 | Other      | 11.181 | Bottle     | 15.820 |\n",
            "| Bottle cap            | 7.685  | Cup        | 4.383  | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 10.885 | Pop tab    | 0.000  | Straw      | 0.000  |\n",
            "| Cigarette             | 2.190  |            |        |            |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.22s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.81s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.22s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.083\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.123\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.094\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.091\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.174\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            "[07/25 07:26:52 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 8.252 | 12.260 | 9.440  | 0.835 | 3.999 | 9.096 |\n",
            "[07/25 07:26:52 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP    | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:------|:-----------|:-------|\n",
            "| Can                   | 17.376 | Other      | 9.717 | Bottle     | 19.143 |\n",
            "| Bottle cap            | 9.659  | Cup        | 6.536 | Lid        | 0.000  |\n",
            "| Plastic bag + wrapper | 17.476 | Pop tab    | 0.000 | Straw      | 0.000  |\n",
            "| Cigarette             | 2.617  |            |       |            |        |\n",
            "[07/25 07:26:52 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[07/25 07:26:52 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[07/25 07:26:52 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:26:52 d2.evaluation.testing]: copypaste: 6.5758,12.6778,5.9513,2.7929,5.1852,7.3140\n",
            "[07/25 07:26:52 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[07/25 07:26:52 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:26:52 d2.evaluation.testing]: copypaste: 8.2524,12.2602,9.4399,0.8353,3.9992,9.0960\n",
            "[07/25 07:26:52 d2.utils.events]:  eta: 0:03:46  iter: 299  total_loss: 1.216  loss_cls: 0.416  loss_box_reg: 0.3702  loss_mask: 0.2151  loss_rpn_cls: 0.04879  loss_rpn_loc: 0.01131    time: 1.2360  last_time: 0.4928  data_time: 0.6049  last_data_time: 0.0270   lr: 0.0005984  max_mem: 8773M\n",
            "[07/25 07:27:15 d2.utils.events]:  eta: 0:03:22  iter: 319  total_loss: 1.23  loss_cls: 0.4813  loss_box_reg: 0.404  loss_mask: 0.1846  loss_rpn_cls: 0.0309  loss_rpn_loc: 0.02129    time: 1.2288  last_time: 1.7587  data_time: 0.5708  last_data_time: 1.2401   lr: 0.00063836  max_mem: 8773M\n",
            "[07/25 07:27:38 d2.utils.events]:  eta: 0:02:59  iter: 339  total_loss: 0.986  loss_cls: 0.401  loss_box_reg: 0.2924  loss_mask: 0.1897  loss_rpn_cls: 0.05811  loss_rpn_loc: 0.02698    time: 1.2252  last_time: 0.8505  data_time: 0.6025  last_data_time: 0.4300   lr: 0.00067832  max_mem: 8773M\n",
            "[07/25 07:28:03 d2.utils.events]:  eta: 0:02:37  iter: 359  total_loss: 1.081  loss_cls: 0.4319  loss_box_reg: 0.3607  loss_mask: 0.202  loss_rpn_cls: 0.05731  loss_rpn_loc: 0.01402    time: 1.2262  last_time: 1.3272  data_time: 0.7133  last_data_time: 0.3504   lr: 0.00071828  max_mem: 8773M\n",
            "[07/25 07:28:25 d2.utils.events]:  eta: 0:02:14  iter: 379  total_loss: 1.085  loss_cls: 0.4179  loss_box_reg: 0.3162  loss_mask: 0.1982  loss_rpn_cls: 0.04057  loss_rpn_loc: 0.01779    time: 1.2179  last_time: 1.0868  data_time: 0.5186  last_data_time: 0.5320   lr: 0.00075824  max_mem: 8773M\n",
            "WARNING [07/25 07:28:48 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[07/25 07:28:48 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[07/25 07:28:48 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/25 07:28:48 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/25 07:28:48 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[07/25 07:28:48 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[07/25 07:28:48 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[07/25 07:28:48 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:29:02 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 1.1928 s/iter. Total: 1.4857 s/iter. ETA=0:03:26\n",
            "[07/25 07:29:07 d2.evaluation.evaluator]: Inference done 17/150. Dataloading: 0.0038 s/iter. Inference: 0.2514 s/iter. Eval: 0.9321 s/iter. Total: 1.1879 s/iter. ETA=0:02:37\n",
            "[07/25 07:29:13 d2.evaluation.evaluator]: Inference done 22/150. Dataloading: 0.0035 s/iter. Inference: 0.2490 s/iter. Eval: 0.9372 s/iter. Total: 1.1901 s/iter. ETA=0:02:32\n",
            "[07/25 07:29:22 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0036 s/iter. Inference: 0.2792 s/iter. Eval: 1.3299 s/iter. Total: 1.6132 s/iter. ETA=0:03:24\n",
            "[07/25 07:29:29 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0036 s/iter. Inference: 0.3096 s/iter. Eval: 1.5707 s/iter. Total: 1.8846 s/iter. ETA=0:03:57\n",
            "[07/25 07:29:36 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.0036 s/iter. Inference: 0.3140 s/iter. Eval: 1.6335 s/iter. Total: 1.9518 s/iter. ETA=0:04:00\n",
            "[07/25 07:29:41 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0051 s/iter. Inference: 0.3129 s/iter. Eval: 1.6003 s/iter. Total: 1.9189 s/iter. ETA=0:03:50\n",
            "[07/25 07:29:48 d2.evaluation.evaluator]: Inference done 32/150. Dataloading: 0.0049 s/iter. Inference: 0.3257 s/iter. Eval: 1.7011 s/iter. Total: 2.0324 s/iter. ETA=0:03:59\n",
            "[07/25 07:29:54 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0047 s/iter. Inference: 0.3375 s/iter. Eval: 1.7778 s/iter. Total: 2.1208 s/iter. ETA=0:04:06\n",
            "[07/25 07:30:04 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0045 s/iter. Inference: 0.3396 s/iter. Eval: 1.8088 s/iter. Total: 2.1536 s/iter. ETA=0:04:01\n",
            "[07/25 07:30:11 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0044 s/iter. Inference: 0.3446 s/iter. Eval: 1.8069 s/iter. Total: 2.1567 s/iter. ETA=0:03:55\n",
            "[07/25 07:30:17 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0043 s/iter. Inference: 0.3345 s/iter. Eval: 1.7148 s/iter. Total: 2.0543 s/iter. ETA=0:03:33\n",
            "[07/25 07:30:23 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0045 s/iter. Inference: 0.3391 s/iter. Eval: 1.7496 s/iter. Total: 2.0940 s/iter. ETA=0:03:33\n",
            "[07/25 07:30:28 d2.evaluation.evaluator]: Inference done 51/150. Dataloading: 0.0043 s/iter. Inference: 0.3394 s/iter. Eval: 1.7282 s/iter. Total: 2.0728 s/iter. ETA=0:03:25\n",
            "[07/25 07:30:34 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0042 s/iter. Inference: 0.3331 s/iter. Eval: 1.6744 s/iter. Total: 2.0125 s/iter. ETA=0:03:11\n",
            "[07/25 07:30:39 d2.evaluation.evaluator]: Inference done 58/150. Dataloading: 0.0042 s/iter. Inference: 0.3315 s/iter. Eval: 1.6648 s/iter. Total: 2.0015 s/iter. ETA=0:03:04\n",
            "[07/25 07:30:44 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0041 s/iter. Inference: 0.3322 s/iter. Eval: 1.6466 s/iter. Total: 1.9839 s/iter. ETA=0:02:56\n",
            "[07/25 07:30:51 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0040 s/iter. Inference: 0.3391 s/iter. Eval: 1.6991 s/iter. Total: 2.0431 s/iter. ETA=0:02:57\n",
            "[07/25 07:30:57 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.0043 s/iter. Inference: 0.3420 s/iter. Eval: 1.6874 s/iter. Total: 2.0346 s/iter. ETA=0:02:50\n",
            "[07/25 07:31:03 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0042 s/iter. Inference: 0.3356 s/iter. Eval: 1.6327 s/iter. Total: 1.9735 s/iter. ETA=0:02:35\n",
            "[07/25 07:31:10 d2.evaluation.evaluator]: Inference done 75/150. Dataloading: 0.0042 s/iter. Inference: 0.3353 s/iter. Eval: 1.6225 s/iter. Total: 1.9628 s/iter. ETA=0:02:27\n",
            "[07/25 07:31:16 d2.evaluation.evaluator]: Inference done 79/150. Dataloading: 0.0042 s/iter. Inference: 0.3333 s/iter. Eval: 1.5908 s/iter. Total: 1.9292 s/iter. ETA=0:02:16\n",
            "[07/25 07:31:21 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0040 s/iter. Inference: 0.3244 s/iter. Eval: 1.5227 s/iter. Total: 1.8520 s/iter. ETA=0:02:00\n",
            "[07/25 07:31:28 d2.evaluation.evaluator]: Inference done 88/150. Dataloading: 0.0041 s/iter. Inference: 0.3255 s/iter. Eval: 1.5343 s/iter. Total: 1.8648 s/iter. ETA=0:01:55\n",
            "[07/25 07:31:33 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0042 s/iter. Inference: 0.3170 s/iter. Eval: 1.4559 s/iter. Total: 1.7780 s/iter. ETA=0:01:37\n",
            "[07/25 07:31:38 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.0041 s/iter. Inference: 0.3090 s/iter. Eval: 1.3902 s/iter. Total: 1.7041 s/iter. ETA=0:01:21\n",
            "[07/25 07:31:45 d2.evaluation.evaluator]: Inference done 106/150. Dataloading: 0.0042 s/iter. Inference: 0.3100 s/iter. Eval: 1.3866 s/iter. Total: 1.7017 s/iter. ETA=0:01:14\n",
            "[07/25 07:31:51 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0042 s/iter. Inference: 0.3100 s/iter. Eval: 1.3808 s/iter. Total: 1.6958 s/iter. ETA=0:01:07\n",
            "[07/25 07:32:00 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0043 s/iter. Inference: 0.3140 s/iter. Eval: 1.4140 s/iter. Total: 1.7331 s/iter. ETA=0:01:04\n",
            "[07/25 07:32:15 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0042 s/iter. Inference: 0.3178 s/iter. Eval: 1.4954 s/iter. Total: 1.8183 s/iter. ETA=0:01:01\n",
            "[07/25 07:32:24 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.0042 s/iter. Inference: 0.3211 s/iter. Eval: 1.5572 s/iter. Total: 1.8834 s/iter. ETA=0:01:02\n",
            "[07/25 07:32:29 d2.evaluation.evaluator]: Inference done 120/150. Dataloading: 0.0048 s/iter. Inference: 0.3215 s/iter. Eval: 1.5532 s/iter. Total: 1.8805 s/iter. ETA=0:00:56\n",
            "[07/25 07:32:35 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0050 s/iter. Inference: 0.3245 s/iter. Eval: 1.5545 s/iter. Total: 1.8848 s/iter. ETA=0:00:50\n",
            "[07/25 07:32:41 d2.evaluation.evaluator]: Inference done 127/150. Dataloading: 0.0049 s/iter. Inference: 0.3229 s/iter. Eval: 1.5396 s/iter. Total: 1.8682 s/iter. ETA=0:00:42\n",
            "[07/25 07:32:49 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0049 s/iter. Inference: 0.3225 s/iter. Eval: 1.5580 s/iter. Total: 1.8862 s/iter. ETA=0:00:37\n",
            "[07/25 07:32:56 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0050 s/iter. Inference: 0.3243 s/iter. Eval: 1.5849 s/iter. Total: 1.9151 s/iter. ETA=0:00:34\n",
            "[07/25 07:33:01 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.0050 s/iter. Inference: 0.3227 s/iter. Eval: 1.5676 s/iter. Total: 1.8961 s/iter. ETA=0:00:26\n",
            "[07/25 07:33:08 d2.evaluation.evaluator]: Inference done 141/150. Dataloading: 0.0051 s/iter. Inference: 0.3240 s/iter. Eval: 1.5432 s/iter. Total: 1.8731 s/iter. ETA=0:00:16\n",
            "[07/25 07:33:13 d2.evaluation.evaluator]: Inference done 147/150. Dataloading: 0.0050 s/iter. Inference: 0.3195 s/iter. Eval: 1.5041 s/iter. Total: 1.8294 s/iter. ETA=0:00:05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:33:15 d2.evaluation.evaluator]: Total inference time: 0:04:22.346486 (1.809286 s / iter per device, on 1 devices)\n",
            "[07/25 07:33:15 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:45 (0.316889 s / iter per device, on 1 devices)\n",
            "[07/25 07:33:15 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[07/25 07:33:15 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[07/25 07:33:15 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.38s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.19s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.116\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.195\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.091\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.201\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.341\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.356\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.248\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            "[07/25 07:33:16 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 11.554 | 19.470 | 13.613 | 2.826 | 9.130 | 12.971 |\n",
            "[07/25 07:33:16 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 20.760 | Other      | 15.279 | Bottle     | 30.137 |\n",
            "| Bottle cap            | 9.791  | Cup        | 8.628  | Lid        | 6.294  |\n",
            "| Plastic bag + wrapper | 18.281 | Pop tab    | 0.000  | Straw      | 3.462  |\n",
            "| Cigarette             | 2.912  |            |        |            |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.15s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.64s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.17s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.126\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.181\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.135\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.049\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.148\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.361\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.375\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.045\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.244\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.421\n",
            "[07/25 07:33:17 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 12.607 | 18.066 | 13.532 | 0.632 | 4.913 | 14.774 |\n",
            "[07/25 07:33:17 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 23.010 | Other      | 16.960 | Bottle     | 32.188 |\n",
            "| Bottle cap            | 11.426 | Cup        | 10.849 | Lid        | 8.943  |\n",
            "| Plastic bag + wrapper | 19.947 | Pop tab    | 0.000  | Straw      | 0.000  |\n",
            "| Cigarette             | 2.744  |            |        |            |        |\n",
            "[07/25 07:33:17 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[07/25 07:33:17 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[07/25 07:33:17 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:33:17 d2.evaluation.testing]: copypaste: 11.5545,19.4698,13.6134,2.8265,9.1299,12.9706\n",
            "[07/25 07:33:17 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[07/25 07:33:17 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:33:17 d2.evaluation.testing]: copypaste: 12.6068,18.0658,13.5320,0.6321,4.9132,14.7738\n",
            "[07/25 07:33:17 d2.utils.events]:  eta: 0:01:52  iter: 399  total_loss: 0.985  loss_cls: 0.3984  loss_box_reg: 0.3341  loss_mask: 0.1999  loss_rpn_cls: 0.02878  loss_rpn_loc: 0.01122    time: 1.2146  last_time: 1.3953  data_time: 0.5373  last_data_time: 0.5752   lr: 0.0007982  max_mem: 8773M\n",
            "[07/25 07:33:41 d2.utils.events]:  eta: 0:01:29  iter: 419  total_loss: 0.996  loss_cls: 0.4107  loss_box_reg: 0.3125  loss_mask: 0.1804  loss_rpn_cls: 0.0403  loss_rpn_loc: 0.01129    time: 1.2129  last_time: 1.1154  data_time: 0.5682  last_data_time: 0.6899   lr: 0.00083816  max_mem: 8773M\n",
            "[07/25 07:34:05 d2.utils.events]:  eta: 0:01:07  iter: 439  total_loss: 0.9427  loss_cls: 0.4051  loss_box_reg: 0.3114  loss_mask: 0.1765  loss_rpn_cls: 0.04524  loss_rpn_loc: 0.014    time: 1.2130  last_time: 1.5464  data_time: 0.6542  last_data_time: 0.9858   lr: 0.00087812  max_mem: 8773M\n",
            "[07/25 07:34:32 d2.utils.events]:  eta: 0:00:45  iter: 459  total_loss: 0.9088  loss_cls: 0.3651  loss_box_reg: 0.2702  loss_mask: 0.1719  loss_rpn_cls: 0.02354  loss_rpn_loc: 0.0169    time: 1.2179  last_time: 1.6398  data_time: 0.7357  last_data_time: 1.0348   lr: 0.00091808  max_mem: 8773M\n",
            "[07/25 07:34:58 d2.utils.events]:  eta: 0:00:22  iter: 479  total_loss: 1.123  loss_cls: 0.4811  loss_box_reg: 0.3066  loss_mask: 0.1985  loss_rpn_cls: 0.02246  loss_rpn_loc: 0.01629    time: 1.2210  last_time: 1.0550  data_time: 0.6314  last_data_time: 0.5251   lr: 0.00095804  max_mem: 8773M\n",
            "[07/25 07:35:26 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 0.987  loss_cls: 0.3927  loss_box_reg: 0.2778  loss_mask: 0.1975  loss_rpn_cls: 0.06272  loss_rpn_loc: 0.02062    time: 1.2204  last_time: 0.6888  data_time: 0.5933  last_data_time: 0.2324   lr: 0.000998  max_mem: 8773M\n",
            "[07/25 07:35:26 d2.engine.hooks]: Overall training speed: 498 iterations in 0:10:07 (1.2204 s / it)\n",
            "[07/25 07:35:26 d2.engine.hooks]: Total training time: 0:33:33 (0:23:25 on hooks)\n",
            "WARNING [07/25 07:35:26 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[07/25 07:35:26 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/mapped_annotations_0_val.json\n",
            "[07/25 07:35:26 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/25 07:35:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/25 07:35:26 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[07/25 07:35:26 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[07/25 07:35:27 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[07/25 07:35:27 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:35:38 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0033 s/iter. Inference: 0.2963 s/iter. Eval: 1.0158 s/iter. Total: 1.3154 s/iter. ETA=0:03:02\n",
            "[07/25 07:35:43 d2.evaluation.evaluator]: Inference done 19/150. Dataloading: 0.0045 s/iter. Inference: 0.2366 s/iter. Eval: 0.6858 s/iter. Total: 0.9272 s/iter. ETA=0:02:01\n",
            "[07/25 07:35:49 d2.evaluation.evaluator]: Inference done 23/150. Dataloading: 0.0047 s/iter. Inference: 0.2430 s/iter. Eval: 0.8129 s/iter. Total: 1.0611 s/iter. ETA=0:02:14\n",
            "[07/25 07:35:56 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0047 s/iter. Inference: 0.2662 s/iter. Eval: 1.0795 s/iter. Total: 1.3511 s/iter. ETA=0:02:50\n",
            "[07/25 07:36:02 d2.evaluation.evaluator]: Inference done 29/150. Dataloading: 0.0041 s/iter. Inference: 0.2580 s/iter. Eval: 1.0709 s/iter. Total: 1.3337 s/iter. ETA=0:02:41\n",
            "[07/25 07:36:09 d2.evaluation.evaluator]: Inference done 31/150. Dataloading: 0.0040 s/iter. Inference: 0.2646 s/iter. Eval: 1.2236 s/iter. Total: 1.4928 s/iter. ETA=0:02:57\n",
            "[07/25 07:36:14 d2.evaluation.evaluator]: Inference done 33/150. Dataloading: 0.0039 s/iter. Inference: 0.2741 s/iter. Eval: 1.2910 s/iter. Total: 1.5696 s/iter. ETA=0:03:03\n",
            "[07/25 07:36:26 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0036 s/iter. Inference: 0.2755 s/iter. Eval: 1.4117 s/iter. Total: 1.6915 s/iter. ETA=0:03:09\n",
            "[07/25 07:36:32 d2.evaluation.evaluator]: Inference done 41/150. Dataloading: 0.0072 s/iter. Inference: 0.2815 s/iter. Eval: 1.4131 s/iter. Total: 1.7025 s/iter. ETA=0:03:05\n",
            "[07/25 07:36:39 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.0068 s/iter. Inference: 0.2742 s/iter. Eval: 1.3919 s/iter. Total: 1.6737 s/iter. ETA=0:02:54\n",
            "[07/25 07:36:45 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0072 s/iter. Inference: 0.2861 s/iter. Eval: 1.4469 s/iter. Total: 1.7410 s/iter. ETA=0:02:57\n",
            "[07/25 07:36:51 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.0067 s/iter. Inference: 0.2781 s/iter. Eval: 1.3658 s/iter. Total: 1.6514 s/iter. ETA=0:02:38\n",
            "[07/25 07:36:58 d2.evaluation.evaluator]: Inference done 59/150. Dataloading: 0.0067 s/iter. Inference: 0.2756 s/iter. Eval: 1.3323 s/iter. Total: 1.6152 s/iter. ETA=0:02:26\n",
            "[07/25 07:37:04 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.0065 s/iter. Inference: 0.2819 s/iter. Eval: 1.3533 s/iter. Total: 1.6424 s/iter. ETA=0:02:24\n",
            "[07/25 07:37:09 d2.evaluation.evaluator]: Inference done 67/150. Dataloading: 0.0061 s/iter. Inference: 0.2780 s/iter. Eval: 1.3152 s/iter. Total: 1.6000 s/iter. ETA=0:02:12\n",
            "[07/25 07:37:16 d2.evaluation.evaluator]: Inference done 72/150. Dataloading: 0.0059 s/iter. Inference: 0.2747 s/iter. Eval: 1.2896 s/iter. Total: 1.5707 s/iter. ETA=0:02:02\n",
            "[07/25 07:37:21 d2.evaluation.evaluator]: Inference done 78/150. Dataloading: 0.0059 s/iter. Inference: 0.2729 s/iter. Eval: 1.2408 s/iter. Total: 1.5201 s/iter. ETA=0:01:49\n",
            "[07/25 07:37:27 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0056 s/iter. Inference: 0.2664 s/iter. Eval: 1.1838 s/iter. Total: 1.4564 s/iter. ETA=0:01:34\n",
            "[07/25 07:37:32 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.0058 s/iter. Inference: 0.2681 s/iter. Eval: 1.1753 s/iter. Total: 1.4498 s/iter. ETA=0:01:28\n",
            "[07/25 07:37:37 d2.evaluation.evaluator]: Inference done 97/150. Dataloading: 0.0057 s/iter. Inference: 0.2690 s/iter. Eval: 1.1070 s/iter. Total: 1.3823 s/iter. ETA=0:01:13\n",
            "[07/25 07:37:43 d2.evaluation.evaluator]: Inference done 105/150. Dataloading: 0.0055 s/iter. Inference: 0.2628 s/iter. Eval: 1.0535 s/iter. Total: 1.3224 s/iter. ETA=0:00:59\n",
            "[07/25 07:37:48 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0054 s/iter. Inference: 0.2630 s/iter. Eval: 1.0424 s/iter. Total: 1.3114 s/iter. ETA=0:00:52\n",
            "[07/25 07:37:54 d2.evaluation.evaluator]: Inference done 113/150. Dataloading: 0.0054 s/iter. Inference: 0.2658 s/iter. Eval: 1.0599 s/iter. Total: 1.3316 s/iter. ETA=0:00:49\n",
            "[07/25 07:38:01 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.0053 s/iter. Inference: 0.2655 s/iter. Eval: 1.0822 s/iter. Total: 1.3536 s/iter. ETA=0:00:46\n",
            "[07/25 07:38:08 d2.evaluation.evaluator]: Inference done 119/150. Dataloading: 0.0054 s/iter. Inference: 0.2694 s/iter. Eval: 1.1055 s/iter. Total: 1.3809 s/iter. ETA=0:00:42\n",
            "[07/25 07:38:14 d2.evaluation.evaluator]: Inference done 125/150. Dataloading: 0.0053 s/iter. Inference: 0.2681 s/iter. Eval: 1.0869 s/iter. Total: 1.3608 s/iter. ETA=0:00:34\n",
            "[07/25 07:38:21 d2.evaluation.evaluator]: Inference done 132/150. Dataloading: 0.0053 s/iter. Inference: 0.2676 s/iter. Eval: 1.0717 s/iter. Total: 1.3452 s/iter. ETA=0:00:24\n",
            "[07/25 07:38:27 d2.evaluation.evaluator]: Inference done 140/150. Dataloading: 0.0052 s/iter. Inference: 0.2642 s/iter. Eval: 1.0358 s/iter. Total: 1.3058 s/iter. ETA=0:00:13\n",
            "[07/25 07:38:32 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.0050 s/iter. Inference: 0.2596 s/iter. Eval: 1.0047 s/iter. Total: 1.2699 s/iter. ETA=0:00:02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[07/25 07:38:33 d2.evaluation.evaluator]: Total inference time: 0:03:02.632409 (1.259534 s / iter per device, on 1 devices)\n",
            "[07/25 07:38:33 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:37 (0.258074 s / iter per device, on 1 devices)\n",
            "[07/25 07:38:33 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[07/25 07:38:33 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[07/25 07:38:33 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.55s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.27s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.220\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.162\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.095\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.160\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.227\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.343\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.026\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.249\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
            "[07/25 07:38:34 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 14.023 | 21.984 | 16.171 | 0.487 | 9.517 | 16.036 |\n",
            "[07/25 07:38:34 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 25.408 | Other      | 14.217 | Bottle     | 32.412 |\n",
            "| Bottle cap            | 10.987 | Cup        | 22.930 | Lid        | 2.872  |\n",
            "| Plastic bag + wrapper | 22.942 | Pop tab    | 0.000  | Straw      | 4.746  |\n",
            "| Cigarette             | 3.713  |            |        |            |        |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.14s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=1.23s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.28s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.150\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.216\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.162\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.069\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.177\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.240\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.367\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.375\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.026\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.249\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.427\n",
            "[07/25 07:38:36 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 15.014 | 21.585 | 16.162 | 0.176 | 6.860 | 17.715 |\n",
            "[07/25 07:38:36 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category              | AP     | category   | AP     | category   | AP     |\n",
            "|:----------------------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| Can                   | 27.005 | Other      | 15.412 | Bottle     | 35.334 |\n",
            "| Bottle cap            | 12.686 | Cup        | 26.076 | Lid        | 4.981  |\n",
            "| Plastic bag + wrapper | 22.969 | Pop tab    | 0.000  | Straw      | 1.699  |\n",
            "| Cigarette             | 3.975  |            |        |            |        |\n",
            "[07/25 07:38:36 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[07/25 07:38:36 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[07/25 07:38:36 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:38:36 d2.evaluation.testing]: copypaste: 14.0228,21.9843,16.1712,0.4866,9.5172,16.0364\n",
            "[07/25 07:38:36 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[07/25 07:38:36 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[07/25 07:38:36 d2.evaluation.testing]: copypaste: 15.0137,21.5852,16.1619,0.1761,6.8600,17.7151\n"
          ]
        }
      ],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"taco_train\",)\n",
        "cfg.DATASETS.TEST = (\"taco_val\",)\n",
        "cfg.TEST.EVAL_PERIOD = 100\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.MAX_ITER = 500\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
