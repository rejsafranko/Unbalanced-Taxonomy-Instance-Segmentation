{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_xSFw7ZtgDK",
        "outputId": "59665fb1-bd87-49ad-ce58-e80c286fd3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "id": "-SzFCz2gt-Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcQRkGfyuKeB",
        "outputId": "30504ee4-1788-4829-f70e-056d6d71cf02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.2 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "U8XXSzz1uhmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir_path = \"/content/drive/MyDrive/instseg/data/\""
      ],
      "metadata": {
        "id": "CluGLxd3wJq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"taco_train\", {}, data_dir_path + \"annotations_0_train.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco_val\", {}, data_dir_path + \"annotations_0_val.json\", data_dir_path + \"images/\")\n",
        "register_coco_instances(\"taco_test\", {}, data_dir_path + \"annotations_0_test.json\", data_dir_path + \"images/\")"
      ],
      "metadata": {
        "id": "CdMWUSrTuvAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load your JSON file\n",
        "with open('/content/drive/MyDrive/instseg/data/annotations_0_train.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Generate unique IDs\n",
        "unique_id = 0\n",
        "for annotation in data['annotations']:\n",
        "    annotation['id'] = unique_id\n",
        "    unique_id += 1\n",
        "\n",
        "# Save the corrected JSON back to file\n",
        "with open('/content/drive/MyDrive/instseg/data/annotations_0_train.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n"
      ],
      "metadata": {
        "id": "PfYkT0FH9g8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "V2oCr4kY_32_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.engine import DefaultTrainer"
      ],
      "metadata": {
        "id": "v3bveZC0oz06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(DefaultTrainer):\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "    if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "    return COCOEvaluator(dataset_name, output_dir=output_folder)"
      ],
      "metadata": {
        "id": "ONY-33U3pWf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"taco_train\",)\n",
        "cfg.DATASETS.TEST = (\"taco_val\",)\n",
        "cfg.TEST.EVAL_PERIOD = 100\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 300\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 60\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDgWWa6wseh",
        "outputId": "cb7f9882-caac-4e0d-ab71-a3f772d0069c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:25:25 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=61, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=240, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING [05/13 11:25:25 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[05/13 11:25:25 d2.data.datasets.coco]: Loaded 1200 images in COCO format from /content/drive/MyDrive/instseg/data/annotations_0_train.json\n",
            "[05/13 11:25:25 d2.data.build]: Removed 0 images with no usable annotations. 1200 images left.\n",
            "[05/13 11:25:25 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[05/13 11:25:25 d2.data.build]: Using training sampler TrainingSampler\n",
            "[05/13 11:25:25 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/13 11:25:25 d2.data.common]: Serializing 1200 elements to byte tensors and concatenating them all ...\n",
            "[05/13 11:25:25 d2.data.common]: Serialized dataset takes 1.77 MiB\n",
            "[05/13 11:25:25 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[05/13 11:25:25 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (61, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (61,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (240, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (240,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (60, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (60,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:25:26 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:25:48 d2.utils.events]:  eta: 0:03:58  iter: 19  total_loss: 5.794  loss_cls: 4.188  loss_box_reg: 0.71  loss_mask: 0.6921  loss_rpn_cls: 0.2146  loss_rpn_loc: 0.03303    time: 1.0612  last_time: 1.2036  data_time: 0.3982  last_data_time: 0.6644   lr: 1.6068e-05  max_mem: 3261M\n",
            "[05/13 11:26:15 d2.utils.events]:  eta: 0:04:05  iter: 39  total_loss: 5.245  loss_cls: 3.733  loss_box_reg: 0.5451  loss_mask: 0.6892  loss_rpn_cls: 0.1399  loss_rpn_loc: 0.02677    time: 1.2154  last_time: 0.7119  data_time: 0.6528  last_data_time: 0.1878   lr: 3.2718e-05  max_mem: 3263M\n",
            "[05/13 11:26:37 d2.utils.events]:  eta: 0:03:58  iter: 59  total_loss: 3.95  loss_cls: 2.57  loss_box_reg: 0.5504  loss_mask: 0.6884  loss_rpn_cls: 0.02641  loss_rpn_loc: 0.01063    time: 1.1728  last_time: 1.0533  data_time: 0.3952  last_data_time: 0.0018   lr: 4.9367e-05  max_mem: 3353M\n",
            "[05/13 11:26:59 d2.utils.events]:  eta: 0:03:42  iter: 79  total_loss: 2.425  loss_cls: 1.116  loss_box_reg: 0.5834  loss_mask: 0.6799  loss_rpn_cls: 0.05134  loss_rpn_loc: 0.01794    time: 1.1641  last_time: 1.5924  data_time: 0.3844  last_data_time: 0.7174   lr: 6.6017e-05  max_mem: 3353M\n",
            "WARNING [05/13 11:27:20 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[05/13 11:27:20 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/annotations_0_val.json\n",
            "[05/13 11:27:20 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/13 11:27:20 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/13 11:27:20 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/13 11:27:20 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[05/13 11:27:20 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[05/13 11:27:20 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:27:26 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.3174 s/iter. Inference: 0.1458 s/iter. Eval: 0.0001 s/iter. Total: 0.4633 s/iter. ETA=0:01:04\n",
            "[05/13 11:27:31 d2.evaluation.evaluator]: Inference done 20/150. Dataloading: 0.3622 s/iter. Inference: 0.1691 s/iter. Eval: 0.0001 s/iter. Total: 0.5315 s/iter. ETA=0:01:09\n",
            "[05/13 11:27:36 d2.evaluation.evaluator]: Inference done 25/150. Dataloading: 0.4695 s/iter. Inference: 0.2080 s/iter. Eval: 0.0001 s/iter. Total: 0.6782 s/iter. ETA=0:01:24\n",
            "[05/13 11:27:41 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.5236 s/iter. Inference: 0.2220 s/iter. Eval: 0.0001 s/iter. Total: 0.7466 s/iter. ETA=0:01:29\n",
            "[05/13 11:27:48 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.5420 s/iter. Inference: 0.2117 s/iter. Eval: 0.0001 s/iter. Total: 0.7547 s/iter. ETA=0:01:24\n",
            "[05/13 11:27:53 d2.evaluation.evaluator]: Inference done 46/150. Dataloading: 0.5275 s/iter. Inference: 0.2181 s/iter. Eval: 0.0001 s/iter. Total: 0.7464 s/iter. ETA=0:01:17\n",
            "[05/13 11:27:59 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.5137 s/iter. Inference: 0.2050 s/iter. Eval: 0.0001 s/iter. Total: 0.7196 s/iter. ETA=0:01:08\n",
            "[05/13 11:28:04 d2.evaluation.evaluator]: Inference done 62/150. Dataloading: 0.5193 s/iter. Inference: 0.2050 s/iter. Eval: 0.0001 s/iter. Total: 0.7251 s/iter. ETA=0:01:03\n",
            "[05/13 11:28:09 d2.evaluation.evaluator]: Inference done 70/150. Dataloading: 0.5103 s/iter. Inference: 0.2038 s/iter. Eval: 0.0001 s/iter. Total: 0.7147 s/iter. ETA=0:00:57\n",
            "[05/13 11:28:14 d2.evaluation.evaluator]: Inference done 83/150. Dataloading: 0.4687 s/iter. Inference: 0.1914 s/iter. Eval: 0.0001 s/iter. Total: 0.6608 s/iter. ETA=0:00:44\n",
            "[05/13 11:28:21 d2.evaluation.evaluator]: Inference done 94/150. Dataloading: 0.4685 s/iter. Inference: 0.1854 s/iter. Eval: 0.0001 s/iter. Total: 0.6545 s/iter. ETA=0:00:36\n",
            "[05/13 11:28:26 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.4518 s/iter. Inference: 0.1876 s/iter. Eval: 0.0001 s/iter. Total: 0.6401 s/iter. ETA=0:00:29\n",
            "[05/13 11:28:32 d2.evaluation.evaluator]: Inference done 116/150. Dataloading: 0.4418 s/iter. Inference: 0.1815 s/iter. Eval: 0.0001 s/iter. Total: 0.6240 s/iter. ETA=0:00:21\n",
            "[05/13 11:28:37 d2.evaluation.evaluator]: Inference done 126/150. Dataloading: 0.4379 s/iter. Inference: 0.1792 s/iter. Eval: 0.0001 s/iter. Total: 0.6177 s/iter. ETA=0:00:14\n",
            "[05/13 11:28:43 d2.evaluation.evaluator]: Inference done 136/150. Dataloading: 0.4271 s/iter. Inference: 0.1823 s/iter. Eval: 0.0001 s/iter. Total: 0.6100 s/iter. ETA=0:00:08\n",
            "[05/13 11:28:48 d2.evaluation.evaluator]: Inference done 148/150. Dataloading: 0.4164 s/iter. Inference: 0.1783 s/iter. Eval: 0.0001 s/iter. Total: 0.5953 s/iter. ETA=0:00:01\n",
            "[05/13 11:28:49 d2.evaluation.evaluator]: Total inference time: 0:01:25.984629 (0.592997 s / iter per device, on 1 devices)\n",
            "[05/13 11:28:49 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:25 (0.177659 s / iter per device, on 1 devices)\n",
            "[05/13 11:28:49 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/13 11:28:49 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[05/13 11:28:49 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "WARNING [05/13 11:28:49 d2.evaluation.coco_evaluation]: No predictions from the model!\n",
            "[05/13 11:28:49 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[05/13 11:28:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/13 11:28:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/13 11:28:49 d2.evaluation.testing]: copypaste: nan,nan,nan,nan,nan,nan\n",
            "[05/13 11:28:49 d2.utils.events]:  eta: 0:03:15  iter: 99  total_loss: 2.408  loss_cls: 0.8531  loss_box_reg: 0.4021  loss_mask: 0.6735  loss_rpn_cls: 0.08766  loss_rpn_loc: 0.01936    time: 1.1338  last_time: 0.6045  data_time: 0.3435  last_data_time: 0.1624   lr: 8.2668e-05  max_mem: 3353M\n",
            "[05/13 11:29:12 d2.utils.events]:  eta: 0:03:01  iter: 119  total_loss: 2.026  loss_cls: 0.7579  loss_box_reg: 0.4352  loss_mask: 0.6662  loss_rpn_cls: 0.06986  loss_rpn_loc: 0.01181    time: 1.1389  last_time: 1.1158  data_time: 0.4309  last_data_time: 0.0060   lr: 9.9318e-05  max_mem: 3353M\n",
            "[05/13 11:29:34 d2.utils.events]:  eta: 0:02:37  iter: 139  total_loss: 1.758  loss_cls: 0.6707  loss_box_reg: 0.399  loss_mask: 0.6541  loss_rpn_cls: 0.04291  loss_rpn_loc: 0.01158    time: 1.1295  last_time: 0.5021  data_time: 0.4094  last_data_time: 0.0050   lr: 0.00011597  max_mem: 3353M\n",
            "[05/13 11:29:55 d2.utils.events]:  eta: 0:02:18  iter: 159  total_loss: 2.053  loss_cls: 0.8174  loss_box_reg: 0.4745  loss_mask: 0.6422  loss_rpn_cls: 0.03089  loss_rpn_loc: 0.01519    time: 1.1223  last_time: 0.6079  data_time: 0.4029  last_data_time: 0.0512   lr: 0.00013262  max_mem: 3353M\n",
            "[05/13 11:30:19 d2.utils.events]:  eta: 0:02:00  iter: 179  total_loss: 1.909  loss_cls: 0.7208  loss_box_reg: 0.4881  loss_mask: 0.6301  loss_rpn_cls: 0.06941  loss_rpn_loc: 0.01601    time: 1.1295  last_time: 0.9930  data_time: 0.4329  last_data_time: 0.3384   lr: 0.00014927  max_mem: 3353M\n",
            "WARNING [05/13 11:30:38 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[05/13 11:30:38 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/annotations_0_val.json\n",
            "[05/13 11:30:39 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/13 11:30:39 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/13 11:30:39 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/13 11:30:39 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[05/13 11:30:39 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[05/13 11:30:39 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:30:43 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0901 s/iter. Inference: 0.2446 s/iter. Eval: 0.0001 s/iter. Total: 0.3348 s/iter. ETA=0:00:46\n",
            "[05/13 11:30:48 d2.evaluation.evaluator]: Inference done 21/150. Dataloading: 0.1261 s/iter. Inference: 0.3379 s/iter. Eval: 0.0005 s/iter. Total: 0.4651 s/iter. ETA=0:01:00\n",
            "[05/13 11:30:54 d2.evaluation.evaluator]: Inference done 27/150. Dataloading: 0.2573 s/iter. Inference: 0.3145 s/iter. Eval: 0.0004 s/iter. Total: 0.5734 s/iter. ETA=0:01:10\n",
            "[05/13 11:30:59 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.2493 s/iter. Inference: 0.2854 s/iter. Eval: 0.0003 s/iter. Total: 0.5358 s/iter. ETA=0:01:00\n",
            "[05/13 11:31:05 d2.evaluation.evaluator]: Inference done 47/150. Dataloading: 0.2765 s/iter. Inference: 0.2891 s/iter. Eval: 0.0003 s/iter. Total: 0.5666 s/iter. ETA=0:00:58\n",
            "[05/13 11:31:10 d2.evaluation.evaluator]: Inference done 54/150. Dataloading: 0.2800 s/iter. Inference: 0.3104 s/iter. Eval: 0.0003 s/iter. Total: 0.5914 s/iter. ETA=0:00:56\n",
            "[05/13 11:31:15 d2.evaluation.evaluator]: Inference done 66/150. Dataloading: 0.2627 s/iter. Inference: 0.2970 s/iter. Eval: 0.0003 s/iter. Total: 0.5609 s/iter. ETA=0:00:47\n",
            "[05/13 11:31:20 d2.evaluation.evaluator]: Inference done 80/150. Dataloading: 0.2402 s/iter. Inference: 0.2826 s/iter. Eval: 0.0003 s/iter. Total: 0.5238 s/iter. ETA=0:00:36\n",
            "[05/13 11:31:25 d2.evaluation.evaluator]: Inference done 89/150. Dataloading: 0.2322 s/iter. Inference: 0.2952 s/iter. Eval: 0.0003 s/iter. Total: 0.5284 s/iter. ETA=0:00:32\n",
            "[05/13 11:31:31 d2.evaluation.evaluator]: Inference done 102/150. Dataloading: 0.2227 s/iter. Inference: 0.2880 s/iter. Eval: 0.0003 s/iter. Total: 0.5117 s/iter. ETA=0:00:24\n",
            "[05/13 11:31:36 d2.evaluation.evaluator]: Inference done 117/150. Dataloading: 0.2101 s/iter. Inference: 0.2780 s/iter. Eval: 0.0002 s/iter. Total: 0.4890 s/iter. ETA=0:00:16\n",
            "[05/13 11:31:41 d2.evaluation.evaluator]: Inference done 128/150. Dataloading: 0.2043 s/iter. Inference: 0.2837 s/iter. Eval: 0.0002 s/iter. Total: 0.4889 s/iter. ETA=0:00:10\n",
            "[05/13 11:31:46 d2.evaluation.evaluator]: Inference done 137/150. Dataloading: 0.1996 s/iter. Inference: 0.2942 s/iter. Eval: 0.0002 s/iter. Total: 0.4947 s/iter. ETA=0:00:06\n",
            "[05/13 11:31:50 d2.evaluation.evaluator]: Total inference time: 0:01:08.651040 (0.473455 s / iter per device, on 1 devices)\n",
            "[05/13 11:31:50 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:41 (0.283659 s / iter per device, on 1 devices)\n",
            "[05/13 11:31:50 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/13 11:31:50 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[05/13 11:31:50 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "WARNING [05/13 11:31:50 d2.evaluation.coco_evaluation]: No predictions from the model!\n",
            "[05/13 11:31:50 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[05/13 11:31:50 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/13 11:31:50 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/13 11:31:50 d2.evaluation.testing]: copypaste: nan,nan,nan,nan,nan,nan\n",
            "[05/13 11:31:50 d2.utils.events]:  eta: 0:01:38  iter: 199  total_loss: 1.979  loss_cls: 0.8322  loss_box_reg: 0.5015  loss_mask: 0.5825  loss_rpn_cls: 0.03679  loss_rpn_loc: 0.01234    time: 1.1149  last_time: 0.6692  data_time: 0.3187  last_data_time: 0.0066   lr: 0.00016592  max_mem: 3353M\n",
            "[05/13 11:32:11 d2.utils.events]:  eta: 0:01:18  iter: 219  total_loss: 1.917  loss_cls: 0.7061  loss_box_reg: 0.4696  loss_mask: 0.5915  loss_rpn_cls: 0.04982  loss_rpn_loc: 0.02701    time: 1.1090  last_time: 1.1179  data_time: 0.3688  last_data_time: 0.2175   lr: 0.00018257  max_mem: 3353M\n",
            "[05/13 11:32:36 d2.utils.events]:  eta: 0:00:59  iter: 239  total_loss: 1.938  loss_cls: 0.8127  loss_box_reg: 0.5424  loss_mask: 0.523  loss_rpn_cls: 0.0275  loss_rpn_loc: 0.01922    time: 1.1226  last_time: 1.1412  data_time: 0.4959  last_data_time: 0.3743   lr: 0.00019922  max_mem: 3353M\n",
            "[05/13 11:32:58 d2.utils.events]:  eta: 0:00:39  iter: 259  total_loss: 1.919  loss_cls: 0.7708  loss_box_reg: 0.509  loss_mask: 0.537  loss_rpn_cls: 0.05225  loss_rpn_loc: 0.01961    time: 1.1186  last_time: 0.7127  data_time: 0.3398  last_data_time: 0.0301   lr: 0.00021587  max_mem: 3353M\n",
            "[05/13 11:33:19 d2.utils.events]:  eta: 0:00:19  iter: 279  total_loss: 1.914  loss_cls: 0.7778  loss_box_reg: 0.5042  loss_mask: 0.4724  loss_rpn_cls: 0.06625  loss_rpn_loc: 0.02025    time: 1.1147  last_time: 1.8522  data_time: 0.3145  last_data_time: 0.6967   lr: 0.00023252  max_mem: 3353M\n",
            "[05/13 11:33:43 d2.utils.events]:  eta: 0:00:00  iter: 299  total_loss: 1.745  loss_cls: 0.71  loss_box_reg: 0.5169  loss_mask: 0.4261  loss_rpn_cls: 0.04049  loss_rpn_loc: 0.02034    time: 1.1155  last_time: 0.7056  data_time: 0.3200  last_data_time: 0.0056   lr: 0.00024917  max_mem: 3353M\n",
            "[05/13 11:33:43 d2.engine.hooks]: Overall training speed: 298 iterations in 0:05:32 (1.1155 s / it)\n",
            "[05/13 11:33:43 d2.engine.hooks]: Total training time: 0:08:14 (0:02:42 on hooks)\n",
            "WARNING [05/13 11:33:43 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[05/13 11:33:43 d2.data.datasets.coco]: Loaded 150 images in COCO format from /content/drive/MyDrive/instseg/data/annotations_0_val.json\n",
            "[05/13 11:33:44 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[05/13 11:33:44 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[05/13 11:33:44 d2.data.common]: Serializing 150 elements to byte tensors and concatenating them all ...\n",
            "[05/13 11:33:44 d2.data.common]: Serialized dataset takes 0.22 MiB\n",
            "[05/13 11:33:44 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[05/13 11:33:44 d2.evaluation.evaluator]: Start inference on 150 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:33:51 d2.evaluation.evaluator]: Inference done 11/150. Dataloading: 0.0018 s/iter. Inference: 0.2251 s/iter. Eval: 0.3592 s/iter. Total: 0.5862 s/iter. ETA=0:01:21\n",
            "[05/13 11:33:57 d2.evaluation.evaluator]: Inference done 18/150. Dataloading: 0.0174 s/iter. Inference: 0.3148 s/iter. Eval: 0.4173 s/iter. Total: 0.7503 s/iter. ETA=0:01:39\n",
            "[05/13 11:34:05 d2.evaluation.evaluator]: Inference done 24/150. Dataloading: 0.0484 s/iter. Inference: 0.3342 s/iter. Eval: 0.5732 s/iter. Total: 0.9566 s/iter. ETA=0:02:00\n",
            "[05/13 11:34:12 d2.evaluation.evaluator]: Inference done 30/150. Dataloading: 0.0380 s/iter. Inference: 0.3319 s/iter. Eval: 0.6313 s/iter. Total: 1.0018 s/iter. ETA=0:02:00\n",
            "[05/13 11:34:18 d2.evaluation.evaluator]: Inference done 34/150. Dataloading: 0.0330 s/iter. Inference: 0.3353 s/iter. Eval: 0.6805 s/iter. Total: 1.0497 s/iter. ETA=0:02:01\n",
            "[05/13 11:34:23 d2.evaluation.evaluator]: Inference done 38/150. Dataloading: 0.0293 s/iter. Inference: 0.3266 s/iter. Eval: 0.7197 s/iter. Total: 1.0765 s/iter. ETA=0:02:00\n",
            "[05/13 11:34:29 d2.evaluation.evaluator]: Inference done 44/150. Dataloading: 0.0274 s/iter. Inference: 0.3292 s/iter. Eval: 0.7111 s/iter. Total: 1.0685 s/iter. ETA=0:01:53\n",
            "[05/13 11:34:34 d2.evaluation.evaluator]: Inference done 48/150. Dataloading: 0.0258 s/iter. Inference: 0.3386 s/iter. Eval: 0.7299 s/iter. Total: 1.0953 s/iter. ETA=0:01:51\n",
            "[05/13 11:34:40 d2.evaluation.evaluator]: Inference done 55/150. Dataloading: 0.0230 s/iter. Inference: 0.3245 s/iter. Eval: 0.7043 s/iter. Total: 1.0526 s/iter. ETA=0:01:40\n",
            "[05/13 11:34:45 d2.evaluation.evaluator]: Inference done 61/150. Dataloading: 0.0211 s/iter. Inference: 0.3194 s/iter. Eval: 0.6963 s/iter. Total: 1.0376 s/iter. ETA=0:01:32\n",
            "[05/13 11:34:51 d2.evaluation.evaluator]: Inference done 63/150. Dataloading: 0.0204 s/iter. Inference: 0.3260 s/iter. Eval: 0.7479 s/iter. Total: 1.0953 s/iter. ETA=0:01:35\n",
            "[05/13 11:34:56 d2.evaluation.evaluator]: Inference done 71/150. Dataloading: 0.0184 s/iter. Inference: 0.3145 s/iter. Eval: 0.7060 s/iter. Total: 1.0397 s/iter. ETA=0:01:22\n",
            "[05/13 11:35:02 d2.evaluation.evaluator]: Inference done 79/150. Dataloading: 0.0167 s/iter. Inference: 0.3024 s/iter. Eval: 0.6866 s/iter. Total: 1.0064 s/iter. ETA=0:01:11\n",
            "[05/13 11:35:07 d2.evaluation.evaluator]: Inference done 85/150. Dataloading: 0.0158 s/iter. Inference: 0.3086 s/iter. Eval: 0.6734 s/iter. Total: 0.9986 s/iter. ETA=0:01:04\n",
            "[05/13 11:35:12 d2.evaluation.evaluator]: Inference done 95/150. Dataloading: 0.0145 s/iter. Inference: 0.2985 s/iter. Eval: 0.6305 s/iter. Total: 0.9442 s/iter. ETA=0:00:51\n",
            "[05/13 11:35:18 d2.evaluation.evaluator]: Inference done 104/150. Dataloading: 0.0135 s/iter. Inference: 0.2899 s/iter. Eval: 0.6106 s/iter. Total: 0.9148 s/iter. ETA=0:00:42\n",
            "[05/13 11:35:23 d2.evaluation.evaluator]: Inference done 110/150. Dataloading: 0.0136 s/iter. Inference: 0.2955 s/iter. Eval: 0.6061 s/iter. Total: 0.9159 s/iter. ETA=0:00:36\n",
            "[05/13 11:35:29 d2.evaluation.evaluator]: Inference done 115/150. Dataloading: 0.0132 s/iter. Inference: 0.2954 s/iter. Eval: 0.6150 s/iter. Total: 0.9243 s/iter. ETA=0:00:32\n",
            "[05/13 11:35:34 d2.evaluation.evaluator]: Inference done 123/150. Dataloading: 0.0124 s/iter. Inference: 0.2895 s/iter. Eval: 0.6016 s/iter. Total: 0.9043 s/iter. ETA=0:00:24\n",
            "[05/13 11:35:39 d2.evaluation.evaluator]: Inference done 130/150. Dataloading: 0.0120 s/iter. Inference: 0.2909 s/iter. Eval: 0.5935 s/iter. Total: 0.8972 s/iter. ETA=0:00:17\n",
            "[05/13 11:35:45 d2.evaluation.evaluator]: Inference done 135/150. Dataloading: 0.0117 s/iter. Inference: 0.2921 s/iter. Eval: 0.5981 s/iter. Total: 0.9026 s/iter. ETA=0:00:13\n",
            "[05/13 11:35:50 d2.evaluation.evaluator]: Inference done 144/150. Dataloading: 0.0126 s/iter. Inference: 0.2867 s/iter. Eval: 0.5824 s/iter. Total: 0.8825 s/iter. ETA=0:00:05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05/13 11:35:52 d2.evaluation.evaluator]: Total inference time: 0:02:05.018587 (0.862197 s / iter per device, on 1 devices)\n",
            "[05/13 11:35:52 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:40 (0.281371 s / iter per device, on 1 devices)\n",
            "[05/13 11:35:52 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[05/13 11:35:52 d2.evaluation.coco_evaluation]: Saving results to ./output/inference/coco_instances_results.json\n",
            "[05/13 11:35:52 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.79s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.007\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.012\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
            "[05/13 11:35:54 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.182 | 0.531  | 0.004  | 0.000 | 0.335 | 0.139 |\n",
            "[05/13 11:35:54 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category                  | AP    | category               | AP    | category                | AP    |\n",
            "|:--------------------------|:------|:-----------------------|:------|:------------------------|:------|\n",
            "| Aluminium foil            | 0.000 | Battery                | nan   | Aluminium blister pack  | 0.000 |\n",
            "| Carded blister pack       | nan   | Other plastic bottle   | 0.000 | Clear plastic bottle    | 2.561 |\n",
            "| Glass bottle              | 0.000 | Plastic bottle cap     | 3.479 | Metal bottle cap        | 0.000 |\n",
            "| Broken glass              | 0.000 | Food Can               | 0.000 | Aerosol                 | nan   |\n",
            "| Drink can                 | 1.874 | Toilet tube            | nan   | Other carton            | 0.000 |\n",
            "| Egg carton                | nan   | Drink carton           | 0.000 | Corrugated carton       | 0.000 |\n",
            "| Meal carton               | 0.000 | Pizza box              | 0.000 | Paper cup               | 0.000 |\n",
            "| Disposable plastic cup    | 0.000 | Foam cup               | 0.000 | Glass cup               | nan   |\n",
            "| Other plastic cup         | nan   | Food waste             | 0.000 | Glass jar               | 0.000 |\n",
            "| Plastic lid               | 0.000 | Metal lid              | 0.000 | Other plastic           | 0.000 |\n",
            "| Magazine paper            | 0.000 | Tissues                | 0.000 | Wrapping paper          | nan   |\n",
            "| Normal paper              | 0.000 | Paper bag              | nan   | Plastified paper bag    | nan   |\n",
            "| Plastic film              | 0.381 | Six pack rings         | 0.000 | Garbage bag             | 0.000 |\n",
            "| Other plastic wrapper     | 0.000 | Single-use carrier bag | 0.000 | Polypropylene bag       | nan   |\n",
            "| Crisp packet              | 0.000 | Spread tub             | 0.000 | Tupperware              | nan   |\n",
            "| Disposable food container | 0.000 | Foam food container    | 0.000 | Other plastic container | 0.000 |\n",
            "| Plastic glooves           | 0.000 | Plastic utensils       | 0.000 | Pop tab                 | 0.000 |\n",
            "| Rope & strings            | 0.000 | Scrap metal            | 0.000 | Shoe                    | 0.000 |\n",
            "| Squeezable tube           | 0.000 | Plastic straw          | 0.000 | Paper straw             | nan   |\n",
            "| Styrofoam piece           | 0.000 | Unlabeled litter       | 0.256 | Cigarette               | 0.000 |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.08s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.44s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.32s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.026\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.038\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.033\n",
            "[05/13 11:35:55 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.355 | 0.592  | 0.441  | 0.000 | 0.659 | 0.316 |\n",
            "[05/13 11:35:55 d2.evaluation.coco_evaluation]: Per-category segm AP: \n",
            "| category                  | AP    | category               | AP    | category                | AP    |\n",
            "|:--------------------------|:------|:-----------------------|:------|:------------------------|:------|\n",
            "| Aluminium foil            | 0.000 | Battery                | nan   | Aluminium blister pack  | 0.000 |\n",
            "| Carded blister pack       | nan   | Other plastic bottle   | 0.000 | Clear plastic bottle    | 5.415 |\n",
            "| Glass bottle              | 0.000 | Plastic bottle cap     | 5.347 | Metal bottle cap        | 0.000 |\n",
            "| Broken glass              | 0.000 | Food Can               | 0.000 | Aerosol                 | nan   |\n",
            "| Drink can                 | 4.342 | Toilet tube            | nan   | Other carton            | 0.000 |\n",
            "| Egg carton                | nan   | Drink carton           | 0.000 | Corrugated carton       | 0.000 |\n",
            "| Meal carton               | 0.000 | Pizza box              | 0.000 | Paper cup               | 0.000 |\n",
            "| Disposable plastic cup    | 0.000 | Foam cup               | 0.000 | Glass cup               | nan   |\n",
            "| Other plastic cup         | nan   | Food waste             | 0.000 | Glass jar               | 0.000 |\n",
            "| Plastic lid               | 0.000 | Metal lid              | 0.000 | Other plastic           | 0.000 |\n",
            "| Magazine paper            | 0.000 | Tissues                | 0.000 | Wrapping paper          | nan   |\n",
            "| Normal paper              | 0.000 | Paper bag              | nan   | Plastified paper bag    | nan   |\n",
            "| Plastic film              | 0.969 | Six pack rings         | 0.000 | Garbage bag             | 0.000 |\n",
            "| Other plastic wrapper     | 0.000 | Single-use carrier bag | 0.000 | Polypropylene bag       | nan   |\n",
            "| Crisp packet              | 0.000 | Spread tub             | 0.000 | Tupperware              | nan   |\n",
            "| Disposable food container | 0.000 | Foam food container    | 0.000 | Other plastic container | 0.000 |\n",
            "| Plastic glooves           | 0.000 | Plastic utensils       | 0.000 | Pop tab                 | 0.000 |\n",
            "| Rope & strings            | 0.000 | Scrap metal            | 0.000 | Shoe                    | 0.000 |\n",
            "| Squeezable tube           | 0.000 | Plastic straw          | 0.000 | Paper straw             | nan   |\n",
            "| Styrofoam piece           | 0.000 | Unlabeled litter       | 0.631 | Cigarette               | 0.000 |\n",
            "[05/13 11:35:55 d2.engine.defaults]: Evaluation results for taco_val in csv format:\n",
            "[05/13 11:35:55 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[05/13 11:35:55 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/13 11:35:55 d2.evaluation.testing]: copypaste: 0.1819,0.5306,0.0043,0.0000,0.3351,0.1391\n",
            "[05/13 11:35:55 d2.evaluation.testing]: copypaste: Task: segm\n",
            "[05/13 11:35:55 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[05/13 11:35:55 d2.evaluation.testing]: copypaste: 0.3554,0.5916,0.4414,0.0000,0.6589,0.3162\n"
          ]
        }
      ]
    }
  ]
}