# Unbalanced Taxonomy Instance Segmentation
experiments -> notebooks/

base configs -> models/configs

This thesis explores different approaches to instance segmentation in computer vision, with a special focus on conventional deep learning models and contemporary models which use language embeddings. In the first part of the thesis, a detailed evaluation of classical models such as Mask R-CNN and Mask2Former, trained on the TACO dataset for waste segmentation, was conducted. The results show that Mask R-CNN provides solid performance, while Mask2Former encounters challenges with contextual understanding of small and visually similar waste objects. In the second part of the thesis, models utilizing CLIP and FC-CLIP language embeddings were investigated, which combine visual and textual information for zero-shot classification and instance segmentation. FC-CLIP uses the CLIP model as its backbone and demonstrated interesting results in instance segmentation, suggesting the need for further adjustments and exploration. The thesis offers insights into the advantages and disadvantages of different segmentation models and highlights the importance of tailoring models to specific tasks and datasets.

Mask R-CNN, with its Region Proposal Network (RPN), has proven to be the most reliable model, achieving the highest overall average precision and demonstrating strong performance across all object sizes and classes. However, the introduction of dynamic loss weighting for segmentation in Mask R-CNN led to a performance drop, especially for medium and large objects, as the model overly prioritized classes with lower response rates at the expense of others. Mask2Former, which uses an architecture based on an attention-based model, in its basic form achieved weaker results than Mask R-CNN but showed improvement with dynamic loss weighting. The modified Mask2Former achieved higher average precision, particularly for large objects, but still performs unsatisfactorily in small object segmentation. Mask2Former utilizes attention-based models that can leverage the global context of the entire image. Dynamic loss weighting allowed the model to direct the attention mechanism towards classes with poorer response, which improved the segmentation of large objects.

FC-CLIP, a model with language embeddings for panoptic segmentation that uses the pre-trained CLIP model as its backbone, demonstrated surprising strength for certain classes, such as Bottle and Cup, and outperformed Mask2Former on medium and large objects. However, its inability to segment small objects and reliance on general semantic features limited its performance in tasks requiring precise instance segmentation.

The results of this work open several interesting directions for future research in the field of instance segmentation. The first potential direction is further investigation and optimization of dynamic loss weighting. Future work could focus on adjusting the weighting mechanisms during training to prevent performance drops in well-represented classes, as observed with Mask R-CNN. Another research direction involves combining visual and textual information, as implemented in the FC-CLIP model. Although FC-CLIP showed potential in zero-shot instance segmentation, its approach could be improved with additional training and adaptation of text embeddings for specific segmentation tasks.
